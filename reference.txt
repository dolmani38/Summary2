Text Generation - Natural Language Processing
https://paperswithcode.com/task/text-generation


Adversarial Text Generation via Feature-Mover’s Distance => GAN Train의 다른 기법으로 적용 가능 할 것!
https://papers.nips.cc/paper/2018/file/074177d3eb6371e32c16c55a3b8f706b-Paper.pdf
여러 distance를 적용해 볼 수 있을 것
1. wasserstein distance
2. Earth Mover distance
3. Feature Mover distance

Text generation에 대한 모든것이 여기 다 있음!!!!!!!! -> 관련 논문도 다 링크되어 있음 !!!1 논문 쓰기에도 아주 좋음!!!!
https://github.com/RUCAIBox/TextBox

Text generation의 evaluation BLEU
https://machinelearningmastery.com/calculate-bleu-score-for-text-python/

http://nlpprogress.com/english/summarization.html  <________________________

Sequence to Sequence (seq2seq) and Attention
https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html

Tutorial 
https://github.com/bentrevett/pytorch-seq2seq


세미나 대상

2/27
1. LSTM 기반 Text Generator (안)
https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/
-> 40분
2. 일단 GAN : https://arxiv.org/pdf/1406.2661.pdf (이)
-> 40분
3. 우리논문 본문1 (이)
-> 40분

3/6
4. GAN for text-generator (이)
   SeqGAN:https://arxiv.org/pdf/1609.05473.pdf
   LeakGAN : https://arxiv.org/pdf/1709.08624v2.pdf
5. Attention Is All You Need : https://arxiv.org/pdf/1706.03762.pdf (안)
   참고 : https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html
   참고 : https://github.com/bentrevett/pytorch-seq2seq
6. 우리논문 본문2 (이)



