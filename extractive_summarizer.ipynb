{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled10.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNIq6oKQaRjjz3ofVmNSnJL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dolmani38/Summary2/blob/main/extractive_summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VifiJtL1PL1d",
        "outputId": "a62cbb3e-e3a4-4c39-c97a-38d7cf61c713"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.10.3-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 6.9 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.0.12\n",
            "  Downloading huggingface_hub-0.0.17-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.6 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 65.4 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 63.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 58.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.17 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.10.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mnpjp09cPYua"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import re\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import BertTokenizer, BertModel"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMOd2xsmPSS4"
      },
      "source": [
        "# Print iterations progress\n",
        "class ExtactiveSummarizer:\n",
        "    # 한국어의 경우, 'kykim/bert-kor-base'\n",
        "    def __init__(self,model_name='bert-base-uncased'):\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "        self.model = BertModel.from_pretrained(model_name, return_dict=True)\n",
        "        self.cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "        # If there's a GPU available...\n",
        "        if torch.cuda.is_available():    \n",
        "            # Tell PyTorch to use the GPU.    \n",
        "            self.device = torch.device(\"cuda\")\n",
        "        # If not...\n",
        "        else:\n",
        "            self.device = torch.device(\"cpu\")\n",
        "\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def read_article(self,text):        \n",
        "        sentences =[]        \n",
        "        sentences = sent_tokenize(text)    \n",
        "        for sentence in sentences:        \n",
        "            sentence.replace(\"[^a-zA-Z0-9]\",\" \")     \n",
        "        return sentences\n",
        "\n",
        "    def sentence_similarity(self,sent1,sent2):\n",
        "        tok_sent1 = self.tokenizer(sent1, return_tensors=\"pt\")\n",
        "        tok_sent2 = self.tokenizer(sent2, return_tensors=\"pt\")\n",
        "        tok_sent1.to(self.device)\n",
        "        tok_sent2.to(self.device)\n",
        "        outputs = self.model(**tok_sent1)\n",
        "        sent_1_pooler_output = outputs.pooler_output\n",
        "\n",
        "        outputs = self.model(**tok_sent2)\n",
        "        sent_2_pooler_output = outputs.pooler_output\n",
        "        return self.cos(sent_1_pooler_output, sent_2_pooler_output).cpu().detach().numpy()\n",
        "\n",
        "\n",
        "    # Create similarity matrix among all sentences\n",
        "    def build_similarity_matrix(self,sentences,stop_words):\n",
        "        #create an empty similarity matrix\n",
        "        similarity_matrix = np.zeros((len(sentences),len(sentences)))\n",
        "        \n",
        "        for idx1 in range(len(sentences)):\n",
        "            for idx2 in range(len(sentences)):\n",
        "                if idx1!=idx2:\n",
        "                    similarity_matrix[idx1][idx2] = self.sentence_similarity(sentences[idx1],sentences[idx2])\n",
        "                    \n",
        "        return similarity_matrix\n",
        "\n",
        "    # Generate and return text summary\n",
        "    def generate_summary(self,text,top_n):\n",
        "        \n",
        "        nltk.download('stopwords')\n",
        "        nltk.download('punkt')\n",
        "        \n",
        "        stop_words = stopwords.words('english')\n",
        "        summarize_text = []\n",
        "        \n",
        "        # Step1: read text and tokenize\n",
        "        sentences = self.read_article(text)\n",
        "        \n",
        "        # Steo2: generate similarity matrix across sentences\n",
        "        sentence_similarity_matrix = self.build_similarity_matrix(sentences,stop_words)\n",
        "        \n",
        "        # Step3: Rank sentences in similarirty matrix\n",
        "        sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_matrix)\n",
        "        scores = nx.pagerank(sentence_similarity_graph)\n",
        "        \n",
        "        #Step4: sort the rank and place top sentences\n",
        "        ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)),reverse=True)\n",
        "        \n",
        "        #print(ranked_sentences)\n",
        "        # Step 5: get the top n number of sentences based on rank    \n",
        "        for i in range(top_n):\n",
        "            summarize_text.append(ranked_sentences[i][1])\n",
        "        \n",
        "        # Step 6 : outpur the summarized version\n",
        "        return \" \".join(summarize_text),len(sentences)       "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tIddPH8RpiA",
        "outputId": "834157a7-c015-4109-f65a-34edd2f9ac70"
      },
      "source": [
        "es = ExtactiveSummarizer()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-7kpK4jRsqC"
      },
      "source": [
        "txt = \"\"\"\n",
        "WASHINGTON - The Trump administration has ordered the military to start withdrawing roughly 7,000 troops from Afghanistan in the coming months, two defense officials said Thursday, an abrupt shift in the 17-year-old war there and a decision that stunned Afghan officials, who said they had not been briefed on the plans.\n",
        "President Trump made the decision to pull the troops - about half the number the United States has in Afghanistan now - at the same time he decided to pull American forces out of Syria, one official said.\n",
        "The announcement came hours after Jim Mattis, the secretary of defense, said that he would resign from his position at the end of February after disagreeing with the president over his approach to policy in the Middle East.\n",
        "The whirlwind of troop withdrawals and the resignation of Mr. Mattis leave a murky picture for what is next in the United States’ longest war, and they come as Afghanistan has been troubled by spasms of violence afflicting the capital, Kabul, and other important areas. \n",
        "The United States has also been conducting talks with representatives of the Taliban, in what officials have described as discussions that could lead to formal talks to end the conflict.\n",
        "Senior Afghan officials and Western diplomats in Kabul woke up to the shock of the news on Friday morning, and many of them braced for chaos ahead. \n",
        "Several Afghan officials, often in the loop on security planning and decision-making, said they had received no indication in recent days that the Americans would pull troops out. \n",
        "The fear that Mr. Trump might take impulsive actions, however, often loomed in the background of discussions with the United States, they said.\n",
        "They saw the abrupt decision as a further sign that voices from the ground were lacking in the debate over the war and that with Mr. Mattis’s resignation, Afghanistan had lost one of the last influential voices in Washington who channeled the reality of the conflict into the White House’s deliberations.\n",
        "The president long campaigned on bringing troops home, but in 2017, at the request of Mr. Mattis, he begrudgingly pledged an additional 4,000 troops to the Afghan campaign to try to hasten an end to the conflict.\n",
        "Though Pentagon officials have said the influx of forces - coupled with a more aggressive air campaign - was helping the war effort, Afghan forces continued to take nearly unsustainable levels of casualties and lose ground to the Taliban.\n",
        "The renewed American effort in 2017 was the first step in ensuring Afghan forces could become more independent without a set timeline for a withdrawal. \n",
        "But with plans to quickly reduce the number of American troops in the country, it is unclear if the Afghans can hold their own against an increasingly aggressive Taliban.\n",
        "Currently, American airstrikes are at levels not seen since the height of the war, when tens of thousands of American troops were spread throughout the country. \n",
        "That air support, officials say, consists mostly of propping up Afghan troops while they try to hold territory from a resurgent Taliban.\n",
        "\"\"\""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "1349rNxsRwkw",
        "outputId": "8cfac753-ea57-474b-c8c2-ebdc8b12213a"
      },
      "source": [
        "summary = es.generate_summary(txt,3)\n",
        "summary[0]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The renewed American effort in 2017 was the first step in ensuring Afghan forces could become more independent without a set timeline for a withdrawal. But with plans to quickly reduce the number of American troops in the country, it is unclear if the Afghans can hold their own against an increasingly aggressive Taliban. Several Afghan officials, often in the loop on security planning and decision-making, said they had received no indication in recent days that the Americans would pull troops out.'"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}