{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "6 - Attention is All You Need.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6f8eddf7466744a5a16dfcfa3afc66fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1a47f0bfd2154353b2b9ca49594f766a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_58d05b397c284cc09b4b6654f5a99ffb",
              "IPY_MODEL_f984bdb8ac014a5e85b8fc699f677ce7"
            ]
          }
        },
        "1a47f0bfd2154353b2b9ca49594f766a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "58d05b397c284cc09b4b6654f5a99ffb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_37a75b428eee4ef996924ea2ca42875b",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 371391,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 371391,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_58c413a61a9542768ee1d9ed2dc5c1b1"
          }
        },
        "f984bdb8ac014a5e85b8fc699f677ce7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7956c222f15949a4a3a1aa7d43bdd189",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 371k/371k [00:00&lt;00:00, 393kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3624517e797e44d09f795757561cbcd8"
          }
        },
        "37a75b428eee4ef996924ea2ca42875b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "58c413a61a9542768ee1d9ed2dc5c1b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7956c222f15949a4a3a1aa7d43bdd189": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3624517e797e44d09f795757561cbcd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "02bd54328b484332ac5cf3b22bbbc366": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_da39b2fe0aab41f28e9d0ff6d31e6496",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_20ee16c3a5f44b989a51e97f103b5fe6",
              "IPY_MODEL_e25c4c51417349b290929a10d3f4b75a"
            ]
          }
        },
        "da39b2fe0aab41f28e9d0ff6d31e6496": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "20ee16c3a5f44b989a51e97f103b5fe6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3be600671be14e7bbc4d564c3925ea59",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 77779,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 77779,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9b7f63b0a9894e1e92412c6c7e53a723"
          }
        },
        "e25c4c51417349b290929a10d3f4b75a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7e7e61f0e1a441d6b3652319c4b451bb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 77.8k/77.8k [03:03&lt;00:00, 423B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_36cce342dd1541c2bbc63e60b41d9d4b"
          }
        },
        "3be600671be14e7bbc4d564c3925ea59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9b7f63b0a9894e1e92412c6c7e53a723": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7e7e61f0e1a441d6b3652319c4b451bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "36cce342dd1541c2bbc63e60b41d9d4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dolmani38/Summary2/blob/main/korean_sentence_construction_v0.1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTKaX6slIIAy"
      },
      "source": [
        "# encoder/decoder를 이용하여 keyword를 입력하면, 유추할 수 있는 완성된 문장을 자동 생성한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hp2D7nhwnY38"
      },
      "source": [
        "# 여기서부터 테스트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1S2PM1n2ePq"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchtext\n",
        "#from torchtext.datasets import Multi30k\n",
        "#from torchtext.data import Field, BucketIterator\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBOQ4O8S2oOv"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8G99UP3gzA-",
        "outputId": "c818e7d4-0579-47ca-ebfd-3b6131311dde"
      },
      "source": [
        "import urllib.request\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# 간단한 전처리\n",
        "def clean_text(txt):\n",
        "    txt = txt.replace('\\n',' ')\n",
        "    txt = txt.replace('\\r',' ')    \n",
        "    txt = txt.replace('=','')\n",
        "    txt = txt.replace('\\\"','')   \n",
        "    txt = txt.replace('\\'','')\n",
        "    #txt = txt.replace(',','')\n",
        "    txt = txt.replace('..','')\n",
        "    txt = txt.replace('...','')\n",
        "    #txt = txt.replace('.','. ')\n",
        "    txt = txt.replace('.','. ')\n",
        "    txt = txt.replace('  ',' ')\n",
        "    txt = txt.replace('  ',' ')    \n",
        "    txt = txt.replace('  ',' ')   \n",
        "    txt = txt.replace('  ',' ')           \n",
        "    txt = txt.replace('  ',' ')\n",
        "    txt = txt.replace('  ',' ')    \n",
        "    txt = txt.replace('  ',' ')   \n",
        "    txt = txt.replace('  ',' ')             \n",
        "    return txt.strip()\n",
        "\n",
        "def collect_training_dataset_for_s2s_generator(source_urls=[]):\n",
        "    ko_sentences_dataset = []\n",
        "    for url in source_urls:\n",
        "        raw_text = urllib.request.urlopen(url).read().decode('utf-8')\n",
        "        ko_sentences_dataset += nltk.sent_tokenize(clean_text(raw_text))\n",
        "\n",
        "    sentences = []\n",
        "    for txt in ko_sentences_dataset:\n",
        "        txt = txt.strip()\n",
        "        if len(txt) > 40 and txt.endswith('다.'):\n",
        "            #ko_grammar_dataset.append([txt,1])\n",
        "            txt = txt.replace('.','')\n",
        "            sentences.append(txt)\n",
        "\n",
        "    return sentences"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "696xz3yTqbs9",
        "outputId": "ddc58176-88f1-4034-fb76-9d23fb23a563"
      },
      "source": [
        "if True:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7jRZO8ANIkp"
      },
      "source": [
        "# dataset 구성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "pFoSKILDiVDE",
        "outputId": "3f84d6cc-a129-47f0-e26c-9a65e793d462"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/summary/korean_news_corpus.csv')\n",
        "df"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>contents</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>문 대통령 변창흠 국토장관 사의표명 사실상 수용</td>\n",
              "      <td>정만호 국민소통수석이 12일 오후 청와대 춘추관 대브리핑룸에서 변창흠 국토부 장관 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>계급장 수여하는 문 대통령</td>\n",
              "      <td>(아산뉴스1) 이광호 기자 문재인 대통령이 12일 오후 충남 아산시 경찰대학에서 열...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>계급장 수여하는 문 대통령</td>\n",
              "      <td>(아산뉴스1) 이광호 기자 문재인 대통령이 12일 오후 충남 아산시 경찰대학에서 열...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>수상자 메달 걸어주는 문 대통령</td>\n",
              "      <td>(아산뉴스1) 이광호 기자 문재인 대통령이 12일 오후 충남 아산시 경찰대학에서 열...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>정몽구 서울아산병원에 50억 쾌척</td>\n",
              "      <td>인재 양성·소외 계층 지원 등 계획 “부친 질병·가난 악순환 끊기 원해 국내 최고 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128815</th>\n",
              "      <td>스테인리스보다 1. 5배 단단…고엔트로피 합금기술 개발</td>\n",
              "      <td>- 포항공대 김형섭 교수팀 기존보다 3~10배 가격경쟁력 확보 헤테로구조 고엔트로피...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128816</th>\n",
              "      <td>철-구리 합금으로 더 강한 금속 소재 탄생</td>\n",
              "      <td>포스텍 다상 헤테로구조 고엔트로피 합금 개발 [아이뉴스24 최상국 기자] 서로 섞이...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128817</th>\n",
              "      <td>이에이트 한국기계연구원에 NFLOW SPH 공급</td>\n",
              "      <td>[파이낸셜뉴스]이에이트는 지난 6일 한국기계연구원(KIMM)에 유체 시뮬레이션 소프...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128818</th>\n",
              "      <td>이에이트 한국기계연구원에 전산유체역학(CFD) 솔루션 공급</td>\n",
              "      <td>이에이트 직원이 한국기계연구원(KIMM) 엔플로우 사용자를 대상으로 교육을 진행하고...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128819</th>\n",
              "      <td>이에이트 한국기계연구원에 NFLOW SPH 공급</td>\n",
              "      <td>한국기계연구원 사용 관련자들 교육도 성공적 실시 [아이뉴스24 박명진 기자] 국내 ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>128820 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   title                                           contents\n",
              "0             문 대통령 변창흠 국토장관 사의표명 사실상 수용  정만호 국민소통수석이 12일 오후 청와대 춘추관 대브리핑룸에서 변창흠 국토부 장관 ...\n",
              "1                         계급장 수여하는 문 대통령  (아산뉴스1) 이광호 기자 문재인 대통령이 12일 오후 충남 아산시 경찰대학에서 열...\n",
              "2                         계급장 수여하는 문 대통령  (아산뉴스1) 이광호 기자 문재인 대통령이 12일 오후 충남 아산시 경찰대학에서 열...\n",
              "3                      수상자 메달 걸어주는 문 대통령  (아산뉴스1) 이광호 기자 문재인 대통령이 12일 오후 충남 아산시 경찰대학에서 열...\n",
              "4                     정몽구 서울아산병원에 50억 쾌척  인재 양성·소외 계층 지원 등 계획 “부친 질병·가난 악순환 끊기 원해 국내 최고 ...\n",
              "...                                  ...                                                ...\n",
              "128815    스테인리스보다 1. 5배 단단…고엔트로피 합금기술 개발  - 포항공대 김형섭 교수팀 기존보다 3~10배 가격경쟁력 확보 헤테로구조 고엔트로피...\n",
              "128816           철-구리 합금으로 더 강한 금속 소재 탄생  포스텍 다상 헤테로구조 고엔트로피 합금 개발 [아이뉴스24 최상국 기자] 서로 섞이...\n",
              "128817        이에이트 한국기계연구원에 NFLOW SPH 공급  [파이낸셜뉴스]이에이트는 지난 6일 한국기계연구원(KIMM)에 유체 시뮬레이션 소프...\n",
              "128818  이에이트 한국기계연구원에 전산유체역학(CFD) 솔루션 공급  이에이트 직원이 한국기계연구원(KIMM) 엔플로우 사용자를 대상으로 교육을 진행하고...\n",
              "128819        이에이트 한국기계연구원에 NFLOW SPH 공급  한국기계연구원 사용 관련자들 교육도 성공적 실시 [아이뉴스24 박명진 기자] 국내 ...\n",
              "\n",
              "[128820 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xInRrlum7PiB",
        "outputId": "789d6089-761e-4d43-dcbf-c430f1232928",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df = df.dropna(axis=0)\n",
        "df['contents'].count()"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128792"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cbf_yNd3quK8",
        "outputId": "1be5aea8-189e-46d8-ec39-c86cb0f686d8"
      },
      "source": [
        "import re\n",
        "import sys\n",
        "# 검사...\n",
        "pattens = [\"[34569][0-9]{3}[\\;.\\;-\\; ][0-9]{4}[\\;.\\;-\\; ][0-9]{4}[\\;.\\;-\\; ][0-9]{4}\",\n",
        "           \"[0-9]{2,3}[\\:\\s\\;.\\;,\\;-;)][0-9]{3,4}[\\:\\s\\;.\\;,\\;-][0-9]{4}\",\n",
        "           \"[0-9]{1}[0-9]{1}[\\W]?[0-1]{1}[0-9]{1}[\\W]?[0-3]{1}[\\W]?[0-9]{1}[\\W]?[1-4]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}\",\n",
        "           \"[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{6}|[0-9]{3}[\\:\\s\\;.\\;,\\;-]([0-9]{5,6}[\\:\\s\\;.\\;,\\;-][0-9]{3}|[0-9]{6}[\\:\\s\\;.\\;,\\;-][0-9]{5}|[0-9]{2,3}[\\:\\s\\;.\\;,\\;-][0-9]{6}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{7}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{4,6}[\\:\\s\\;.\\;,\\;-][0-9]|[0-9]{5}[\\:\\s\\;.\\;,\\;-][0-9]{3}[\\:\\s\\;.\\;,\\;-][0-9]{2}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{5}[\\:\\s\\;.\\;,\\;-][0-9]{3}|[0-9]{4}[\\:\\s\\;.\\;,\\;-][0-9]{4}[\\:\\s\\;.\\;,\\;-][0-9]{3}|[0-9]{6}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{3}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{7})|[0-9]{4}[\\:\\s\\;.\\;,\\;-]([0-9]{3}[\\:\\s\\;.\\;,\\;-][0-9]{6}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{6}[\\:\\s\\;.\\;,\\;-][0-9])|[0-9]{5}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{6}|[0-9]{6}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{5,6}\"\n",
        "           ]\n",
        "\n",
        "filters = []\n",
        "for p in pattens:\n",
        "    filters.append(re.compile(p))\n",
        "\n",
        "sentences = []\n",
        "df = df.dropna(axis=0)\n",
        "cnt = df['contents'].count()\n",
        "#print('Total row count:',cnt)\n",
        "i=0\n",
        "for raw_text in df['contents']:\n",
        "    i=i+1\n",
        "    try:\n",
        "        if i%100 == 0:\n",
        "            percent = (\"{0:.2f}\").format(100 * (i / float(cnt)))\n",
        "            print(f'\\r {percent}% {i}/{str(cnt)}', end=\"\", flush=True)\n",
        "\n",
        "        docs = nltk.sent_tokenize(clean_text(raw_text))\n",
        "        for txt in docs:\n",
        "            if txt.find('▶') > -1 or txt.find('@') > -1 or txt.find('ⓒ') > -1: \n",
        "                pass\n",
        "            else:\n",
        "                txt = txt.strip()\n",
        "                if any(chr.isdigit() for chr in txt) :\n",
        "                    pass\n",
        "                else:\n",
        "                    sentences.append(txt)\n",
        "    except KeyboardInterrupt as ki:\n",
        "        raise ki        \n",
        "    except:\n",
        "        pass #print(\"Unexpected error:\", sys.exc_info()[0])      \n",
        "\n"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 99.93% 128700/128792"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYBs7FYFrEze",
        "outputId": "182b336a-3a30-4de8-9509-f86f9632b5d1"
      },
      "source": [
        "len(sentences)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2710820"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT_bAT4ix3yn"
      },
      "source": [
        "import re\n",
        "import sys\n",
        "import io\n",
        "\n",
        "#텍스트 정제(전처리)\n",
        "def cleanText(readData):\n",
        "    #텍스트에 포함되어 있는 특수 문자 제거\n",
        "    text = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》◆◇●🎧○▲\\t―△━▷]', '', readData)\n",
        "    return text"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lBwyjW8x55f"
      },
      "source": [
        "c_sentences = []\n",
        "for sentence in sentences:\n",
        "    s = cleanText(sentence)\n",
        "    c = len(s.split())\n",
        "    if c >= 3 and c < 10 and s.find('재배포') < 0 and s.find('기자') < 0  and s.find('유투브') < 0 and s.find('www') < 0 and s.find('com') < 0 and s.find('접속하기') < 0 and s.find('http') < 0 and s.find('뉴스') < 0 and s.find('일보') < 0 :\n",
        "        if s.endswith(('다','요','까','죠','냐')):\n",
        "            c_sentences.append(s.strip())"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wW7kLnpyiYF",
        "outputId": "2a329473-7e14-482e-bb6b-e74cc05762c5"
      },
      "source": [
        "len(c_sentences)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "849997"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAGY99EK5Zqn",
        "outputId": "8fee8643-c2f9-4778-cea9-c120b3c840f1"
      },
      "source": [
        "c_sentences[6000:]"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['학력·경력과 무관하게 지원할 수 있다',\n",
              " '넥슨컴퍼니 채용 홈페이지에서 확인 가능하다',\n",
              " '눈길을 끄는 부분은 기존 정형화된 게임을 탈피한 엠오디MOD’와 페이스플레이’다',\n",
              " '엠오디MOD는 게임 제작 플랫폼’이다',\n",
              " '페이스플레이FACEPLAY도 눈길을 끈다',\n",
              " '신개념 놀이 플랫폼’으로 정의했다',\n",
              " '창의적인 도면을 제작해 다른 이용자와 공유하는 것도 가능하다',\n",
              " '티저 페이지 개설과 함께 게임 플레이 영상도 공개됐다',\n",
              " '국제 학술사회에서 평가는 이미 끝났다고 봐야 한다며 이같이 밝혔다',\n",
              " '호킹은 소변이 마려웠지만 그곳에는 장애인 화장실이 없었다',\n",
              " '이에 화가 머리끝까지 난 호킹은 충격적인 행동을 저지른다',\n",
              " '주방장이 놀라서 나와 소리를 지르자 호킹은 이렇게 답한다',\n",
              " '책은 대외적으로 알려지지 않았던 호킹의 인간적이고 내밀한 모습들을 보여준다',\n",
              " '물론 호킹의 과학적 성취들을 전하는 것도 저자는 잊지 않는다',\n",
              " '일견 비정해 보일 수 있으나 진실은 다르다',\n",
              " '학생들에 대한 애정이 남다른 호킹은 지원 요청을 항상 승인해줬다',\n",
              " '그의 비서 주디스는 이에 대해 이렇게 평한다',\n",
              " '정말 이상한 게임이죠',\n",
              " '이상한 사람들이 하는 이상한 게임이요',\n",
              " '서로를 얕봐서 그러는 건 아니에요',\n",
              " '가족 관계에 대한 호킹의 관점도 꽤나 진보적이었다',\n",
              " '요즘 진보파 사이에서 쓰이는 말로 그는 폴리아모리스트비독점적 다자연애주의자였다',\n",
              " '호킹의 첫 아내는 간병인이었던 제인 와일드 호킹이었다',\n",
              " '그때 일레인은 아직 데이비드 메이슨이라는 남자와 혼인 중인 상태였다',\n",
              " '이 기묘한 가족 관계는 한동안 지속됐다',\n",
              " '아직 여야가 합의를 보지 못하고 있습니다',\n",
              " '최진 대통령리더십연구원장과 짚어보겠습니다',\n",
              " '많이 부족하죠 미흡하죠',\n",
              " '박영선  더불어민주당 서울시장 후보  저 박영선 특검을 정식으로 건의합니다',\n",
              " '단호하게 책임을 물어야 합니다',\n",
              " '반드시 끊어내야 합니다',\n",
              " '앵커 박영선 후보가 특검카드를 꺼냈습니다',\n",
              " '특검 카드를 꺼낸 이유는 어디에 있을까요',\n",
              " '최진 원래 이런 특검카드는 원내대표나 당대표가 해야 되는 건데요',\n",
              " '아시다시피 지금 선거가 눈앞이지 않습니까',\n",
              " '그런데 김태년 원내대표가 그것과 관계 없다 이렇게 이야기를 했습니다',\n",
              " '최진 그렇게 얘기를 했지만 모양이 이상하죠',\n",
              " '야당은 지금 여당부터 조사하자 이런 주장이거든요',\n",
              " '누가 더 설득력이 있는 것 같습니까',\n",
              " '그러니까 특검에 대한 순수성이 사실은 좀 흐려지기 마련이죠',\n",
              " '오세훈 안철수 후보 발언 이어서 들어보겠습니다',\n",
              " '오세훈  국민의힘 서울시장 후보  정말 전대미문의 정부입니다',\n",
              " '특검하려면 시간이 많이 걸립니다',\n",
              " '원내대표의 얘기 들어보고 오겠습니다',\n",
              " '김태년  더불어민주당 원내대표  지체하지 말고 바로 행동에 옮겨야 합니다',\n",
              " '더불어민주당은 준비가 돼 있습니다',\n",
              " '전수조사에 한계가 있다고 보십니까',\n",
              " '최진 한계가 있다고 봅니다',\n",
              " '국민들 입장에서는 속시원하죠',\n",
              " '아주 시원하지만 실효성 부분에서 대단히 의문이라고 봅니다',\n",
              " '최진 만약에 국회의원들이 전수조사 결과 숫자가 아주 적다',\n",
              " '그러면 국민들이 납득하겠습니까',\n",
              " '그러면 국민들도 화나죠',\n",
              " '이래도 저래도 화나는 것 아닙니까',\n",
              " '그러면 민주당이 더 나쁜 겁니까',\n",
              " '더 나은 겁니까',\n",
              " '이것도 애매한 거잖아요',\n",
              " '선거에도 악영향을 미칠 것 같은데 어떻습니까',\n",
              " '최진 저는 악재 중에 악재라고 봅니다',\n",
              " '그래서 이런 위기상황 속의 해법은 속전속결입니다',\n",
              " '어제 총리께서 사과하셨지만 사실은 대통령이 빨리 나서줘야 됩니다',\n",
              " '지금 청와대가 잠시 뒤 브리핑을 할 예정이라고 하거든요',\n",
              " '청와대를 잠깐 연결해서 보겠습니다',\n",
              " '청와대 대변인을 통해서 이야기한 바가 있는데요',\n",
              " '앵커 지금 정만호 수석의 발표 내용 들어보겠습니다',\n",
              " '이에 대해 대통령은 책임지는 모습을 보일 수밖에 없다고 생각한다',\n",
              " '앵커 일단 책임 있는 모습 지금 보여줄 필요가 있다',\n",
              " '최진 일단 예고된 사퇴고요',\n",
              " '앵커 한 단계 더 나갔으면 좋겠다는 게 뭐죠',\n",
              " '이에 대해 대통령은 책임지는 모습을 보일 수밖에 없다고 생각한다',\n",
              " '이런 입장인 것 같아요',\n",
              " '앵커 오늘 변창흠 장관이 국회 국토위에 출석을 했습니다',\n",
              " '분위기가 어땠는지 한번 영상을 보고서 이야기를 나누도록 하겠습니다',\n",
              " '김은혜  국민의힘 의원  장관님 최근에 사의 표명하신 바 있습니까',\n",
              " '김은혜  국민의힘 의원  그러면 사의 표명을 하실 생각이 있으십니까',\n",
              " '홍기원  더불어민주당 의원  수사는 국가수사본부가 하고 있는 것이고요',\n",
              " '감사든 수사든 그런 거는 시간이 많이 걸리는 거 아닙니까',\n",
              " '답변 안 하셔도 될 것 같아요',\n",
              " '에이 진짜 너무하잖아요',\n",
              " '물론 아직 할 일이 많이 남아 있는 상황 아니겠습니까',\n",
              " '상당히 짧은 기간 장관을 하고 사의를 표명하게 됐어요',\n",
              " '최진 이건 정말 정공법밖에 없습니다',\n",
              " '일제히 검경이 합동으로 조사를 해서 제대로 밝혀내는 길밖에 없습니다',\n",
              " '그래서 읍참마속 내지는 읍참골단을 하는 수밖에 없습니다',\n",
              " '그렇지 않으면 지금 서울시장 부산시장 선거 있지 않습니까',\n",
              " '그런 것들도 아쉬운 점이 많았습니다',\n",
              " '최진 청문회에서 이미 여러 건 들어가죠',\n",
              " '이미 개인적으로 사석에서 했던 발언들이라든지 말이죠',\n",
              " '그런 부분들이 이미 국민들 마음을 상당히 상하게 했었죠',\n",
              " '사실상 변창흠 장관의 사의를 대통령이 수용한 것으로 보면 돼요',\n",
              " '최진 보면 되는데 바로 수용을 해야죠',\n",
              " '앵커 바로 수리는 아니고요',\n",
              " '바로 수리는 아닙니다',\n",
              " '최진 그게 저는 문제라고 보는 겁니다',\n",
              " '그게 며칠 시간을 끌었지 않습니까',\n",
              " '또 검찰총장 윤석열의 전격적인 사퇴 포석이 있었지 않습니까',\n",
              " '쭉 이어져 내려오거든요',\n",
              " '정책이 제대로 돌아갈 리가 없습니다',\n",
              " '특히 LH공사의 위에 상층부 임원들의 책임론이 나오지 않습니까',\n",
              " '앵커 그렇다면 국토부를 누가 이끌고 갑니까',\n",
              " '최진 빨리 대행체제로 하고 후임자를 물색해야 되는 거죠',\n",
              " '앵커 대행체제로 가는 게 낫다',\n",
              " '속전속결로 해야 돼요',\n",
              " '앵커 대통령 입장에서 참 힘든 상황입니다',\n",
              " '공급대책을 미룰 수도 없는 상황이고요',\n",
              " '부동산 민심이 매우 안 좋은 상황이니까요',\n",
              " 'O는 그대로 변창흠 장관을 안고 가느냐',\n",
              " 'O 아니면 X의 문제거든요',\n",
              " '이 부분을 안고 가기는 정말 힘들 겁니다',\n",
              " '야당이 가만히 있겠습니까',\n",
              " '최진 그게 선거를 떠나서 정책적으로도 바람직하고 필요하다고 보는 거죠',\n",
              " '앵커 강조를 하고 있습니다',\n",
              " '이 시점이 언제쯤 될지는 저희가 지켜봐야 할 것 같습니다',\n",
              " '그리고 재보선 단일화 상황도 한번 살펴보도록 하죠',\n",
              " '지금 오세훈 후보 그리고 안철수 후보가 단일화에 상당히 적극적입니다',\n",
              " '그런데 실무협상 분위기가 별로 좋지 않은 것 같거든요',\n",
              " '최진 저는 막판 진통이라고 봅니다',\n",
              " '참 희한한 현상이죠',\n",
              " '잠깐 듣고 오겠습니다',\n",
              " '다음 일정은 어떻게 되시나요',\n",
              " '그거는 제가 따로 정양석 총장님께 연락을 드릴 겁니다',\n",
              " '비전 발표회도 일요일날 못하시는 거에요',\n",
              " '최근의 추세를 보면 오세훈 후보의 지지율이 조금씩 오르고 있어요',\n",
              " '그래서 국민의당 입장에서는 가능하면 빨리 단일화 협상을 마무리짓고 싶어하거든요',\n",
              " '토론도 해야 되고요',\n",
              " '박빙으로 나올 것 같습니까',\n",
              " '이미 상당히 민심이 어느 정도 결정됐다고 보는 거고요',\n",
              " '오늘 정치권 이슈 최진 대통령리더십연구원장과 함께 짚어봤습니다',\n",
              " '오늘 말씀 잘 들었습니다',\n",
              " '미국이 먼저 회담을 제안한 사실도 뒤늦게 공개했다',\n",
              " '회담 개최 발표도 미국이 중국보다 한나절 빨랐다',\n",
              " '미국이 중국 대표단을 본토로 초청할 여력이 없다는 것이다',\n",
              " '미국은 용어 사용을 놓고도 민감한 반응을 보였다',\n",
              " '자오리젠趙立堅 중국 외교부 대변인이 “고위급 전략대화”라고 언급하자 반박한 것이다',\n",
              " '홈시어터는 한번쯤을 꾸려보고 싶은 로망이다',\n",
              " '집에서 영화관처럼 콘텐츠를 즐기고 싶다는 욕망이 누구에게나 있다',\n",
              " '이를 반영한 듯 시장에는 다양한 제품들이 출시돼 있다',\n",
              " '그럼에도 어마무시한 가격 탓에 선뜻 손을 내밀기가 어렵다',\n",
              " '대안으로 떠오른 제품이 대형 화면을 갖춘 TV다',\n",
              " '디스플레이 시장에서는 거거익선이란 말이 정설로 통한다',\n",
              " '화면이 크면 클수록 좋다는 말이다',\n",
              " '하지만 화면 크기만 같을 뿐 내용물은 천차만별이다',\n",
              " '화면의 크기와 함께 해상도도 꼼꼼히 살펴야 한다',\n",
              " '이와 함께 색 표현 기술도 많은 연구가 진행됐다',\n",
              " '대표적인 기술이 사진을 좋아하는 소비자라면 익숙할 HDR이다',\n",
              " '최근 제작되는 대부분의 영화와 콘텐츠는 HDR이 적용돼 있다',\n",
              " '넷플릭스 등 자연 다큐멘터리 프로그램이 대표적인 예다',\n",
              " 'OTT로 검색하면 수많은 서비스가 나온다',\n",
              " '이때마다 TV를 바꿀 수도 없다',\n",
              " '안드로이드 TV는 이런 단점을 간단하게 해결해준다',\n",
              " '마치 스마트폰처럼 손쉽게 원하는 서비스를 설치하고 시청하면 된다',\n",
              " '따라서 관련 기능 탑재 여부는 물론 버전도 중요하다',\n",
              " '최신 기술일수록 빠르고 안정적으로 동작하기 때문이다',\n",
              " '스트리밍은 영상의 전유물이 아니다',\n",
              " '디스플레이와 함께 고가의 스피커가 방 한구석을 차지하고 있을 것이다',\n",
              " '영상을 실감나게 즐기려면 생생한 화질은 물론 소리의 품질도 중요하다',\n",
              " '음질을 평가하는 기준은 소비자마다 다르다',\n",
              " '제품 스펙상으로는 이런 부분을 가늠하기도 어렵다',\n",
              " '이때 스피커 출력은 하나의 기준으로 활용할 수 있다',\n",
              " '자세한 내용은 공식 홈페이지 에서 확인할 수 있다',\n",
              " '엔씨소프트가 게임업계에서 처음으로 ESG환경·사회·지배구조 경영실을 신설했습니다',\n",
              " '하겐다즈가 프리미엄 아이스크림 마카롱 세트사진를 내놓았다',\n",
              " '또 프리미엄 선물 패키지와 함께 제공한다',\n",
              " '카카오톡 선물하기 페이지에서 단독으로 만나볼 수 있다',\n",
              " '삼성증권은 한일시멘트의 지배구조 등과 관련해 자문을 했던 바 있다',\n",
              " '경찰은 스스로 목숨을 끊었을 가능성을 높게 보고 있다',\n",
              " '그는 전북에서 본부장으로 근무할 때 바람직하지 않은 일을 했다',\n",
              " '국민에게 죄송하다는 내용의 유서를 남긴 것으로 전해졌다',\n",
              " '조합원이 되면 출자배당금과 이용배당금을 받을 수 있다',\n",
              " '비금전적 혜택도 쏠쏠하다',\n",
              " 'LH 직원들은 먼저 대출을 받은 뒤 농협 조합원으로 등록했다',\n",
              " '조합원이 되면 출자배당금과 이용배당금을 받을 수 있다',\n",
              " '비금전적 혜택도 쏠쏠하다',\n",
              " 'LH 직원들은 먼저 대출을 받은 뒤 농협 조합원으로 등록했다',\n",
              " '입학을 축하하는 꽃다발과 축하 메시지를 전달 받은 것이다',\n",
              " '자금은 모두 김 의장 개인 주식을 처분해 마련한다는 계획이구요',\n",
              " '김 의장은 ”지난달 기부 계획과는 별도로 이뤄지는 것“이라고 밝혔는데요',\n",
              " '직원들이 이번에 받게 되는 딜리버리 히어로 주식은 어떨까요',\n",
              " '갖고 있으면 유망하다 라고 볼 수 있습니까',\n",
              " '딜리버리 히어로 즉 DH는 배달서비스 기업인데요',\n",
              " '덕분에 주가도 많이 올랐습니다',\n",
              " '반응들이 아주 뜨겁겠습니다',\n",
              " '다만 일부 배달의 민족 가맹주들 사이에서는 불만도 나왔는데요',\n",
              " '앵커 일회성 보상도 노동환경 개선에 속하긴 하죠',\n",
              " '다른 CEO들에게서도 이런 모습 찾아볼 수 있으면 좋겠네요',\n",
              " '바라 봄 전시회는 보통의 꽃 전시회와는 다른 꽃을 사용한다',\n",
              " '관람객이 직접 참여할 수 있는 프로그램도 마련했다',\n",
              " '전용 출석 이벤트와 설문조사 이벤트 등으로 구성됐다',\n",
              " '이외에도 동맹전 시스템 추가 체스트 시스템 개선 등이 이뤄졌다',\n",
              " '시즌 업데이트 사전예약자를 대상으로 추첨을 통해 경품도 선물한다',\n",
              " '이달 말 모바일 및 스팀을 통해 서비스 예정이다',\n",
              " 'S스테이트 오브 서바이벌’에 영웅본색’ 콜라보레이션 콘텐츠를 추가했다',\n",
              " '킹스그룹은 이번 콜라보레이션을 기념한 신규 홍보영상도 공개했다',\n",
              " '해당 영상은 영화 속 장면을 연출해 눈길을 끈다',\n",
              " '참여자에게는 장수 소교와 금화를 지급한다',\n",
              " '공식카페 이벤트도 마련했다',\n",
              " '이 게임은 인기 애니메이션 원펀맨을 활용한 것이 특징이다',\n",
              " '공식카페를 통해 출석체크 이벤트를 진행하며 덕담 이벤트도 개최할 계획이다',\n",
              " '국고채 선물 시장도 약세채권 가격 하락를 보였다',\n",
              " '채권 금리는 채권 가격과 반대이다',\n",
              " '바로 카카오톡 이모티콘입니다',\n",
              " '농식품부와 농협이 웬 이모티콘을 배포하나 의아하신 분들 계실 겁니다',\n",
              " '다 나름의 뜻이 있습니다',\n",
              " '농협 관계자가 밝힌 이모티콘 배포의 이유를 들어보겠습니다',\n",
              " '앞으로도 쌀 소비가 늘어날 수 있도록 최선을 다하겠다',\n",
              " '빼빼로 데이만은 아닙니다',\n",
              " '결국 제목에 대한 답을 정리해보겠습니다',\n",
              " '바로 정부 부처 산하 공공기관들이 만들거나 홍보해온 기념일이라는 점입니다',\n",
              " '그 효과는 어떨까요',\n",
              " '우선 쌀을 보겠습니다',\n",
              " '쌀은 요새 폭등이란 말이 따라붙습니다',\n",
              " '삼겹살과 오이는 상황이 좀 다릅니다',\n",
              " '삼겹살 데이의 효과는 크지 않았다는 평가가 나오더군요',\n",
              " '이들 세력의 가치관과 주장에 대해 분석한다',\n",
              " '대안 세력으로서 보수의 재건도 촉구한다',\n",
              " '멤버들의 응원도 큰 힘이 됐다',\n",
              " '너무 고마웠다”며 감사 인사를 전했다',\n",
              " '안네가 그토록 필사적으로 원했던 삶을 대신한 것은 일기였습니다',\n",
              " '그러나 사람들은 감정을 잘 표현하지 못합니다',\n",
              " '특히 분노 우울 불안과 같은 부정적인 감정은 표현하기가 힘듭니다',\n",
              " '애써 감정을 외면하고 무시합니다',\n",
              " '그런다고 해서 이런 감정이 사라지지 않습니다',\n",
              " '무의식에 쌓여 삶을 괴롭게 합니다',\n",
              " '글을 쓰면서 얻는 위로 격려 공감은 사람을 변화시킵니다',\n",
              " '먼저 오늘은 긍정의 기억을 일깨우도록 하겠습니다',\n",
              " '어린 시절 또는 과거의 어느 행복했던 순간을 떠올려 보세요',\n",
              " '신기하게도 단어를 하나씩 쓰면서 따뜻한 에너지가 생기는 것을 경험했습니다',\n",
              " '내가 좋아하는 것들의 목록을 먼저 써보세요',\n",
              " '그 자체가 한 편의 시가 될 수 있습니다',\n",
              " '나에게 쉼을 주는 시간입니다',\n",
              " '먼저 현재의 나를 바라보고 내가 어떤 사람인지 알아가야 합니다',\n",
              " '요즘 나는 어떤 표정으로 살아가고 있나요',\n",
              " '자신의 모습을 조금 떨어져 바라보세요',\n",
              " '거울을 통해 자신을 바라보듯이 느낌을 적어보세요',\n",
              " '그’ 또는 그녀’를 바라보듯 객관적으로 적습니다',\n",
              " '하루 중 가장 좋아하는 순간을 떠올려 보세요',\n",
              " '오늘 하루도 무사히 살아낸 나에게 어떤 말을 해주고 싶나요',\n",
              " '그 일들을 마지막으로 한 것은 언제인가요',\n",
              " '항목마다 언제 그 일을 마지막으로 했는지 적습니다',\n",
              " '지금 이 순간 느끼는 감정에 주목해 써주세요',\n",
              " '해방 직전 함태영 목사부통령가 신사참배를 거부하며 교회를 지켰다',\n",
              " '송파·강남 및 청계산·남한산성 자락 복음화의 중심이었다',\n",
              " '지금의 성남시 최초의 교회는 모란시장’으로 많이 알려진 동네의 둔전교회이다',\n",
              " '신약성서’ 번역을 완료했던 해이기도 하다',\n",
              " '지금의 성남공군기지서울공항 활주로 땅이었다',\n",
              " '이 지역이 철거 풍문으로 들떠 있습니다',\n",
              " '더구나 유신 말기였다',\n",
              " '교회 뒤로 서울공항과 청계산이 보였다',\n",
              " '헬리콥터 한 대가 예배당 십자가 위로 날아가고 있었다',\n",
              " '그 예배당 옆으로 순교기념비가 눈에 들어왔다',\n",
              " '순교자 현석진 목사·김태수 장로’였다',\n",
              " '신앙의 순수함이 묻어난다',\n",
              " '복원을 추진하고 있다',\n",
              " '“제 아버지김효성 장로·작고는 세상적으로 불쌍한 분이셨습니다',\n",
              " '생전 할아버지 얘기가 나오면 먼 산을 보시곤 했어요',\n",
              " '말 수가 줄어드셨죠',\n",
              " '그런 아버지에게 신앙이 없었다면 견뎌내기 힘들었을 거예요',\n",
              " '독립운동가 후손과 같은 어려움이 그들에게도 배어 있었다',\n",
              " '그리고 해방과 함께 공직에 나가 심계원장감사원장 등을 지냈다',\n",
              " '그때 김태수는 현석진과 함께 교회를 이끌었다',\n",
              " '현석진은 평북 개천 출신으로 메이지대학 법학과를 다니던 엘리트였다',\n",
              " '지금은 비행장이 된 둔전리 풍경이다',\n",
              " '순식간에 마을이 점령됐다',\n",
              " '요시찰 인물이 된 두 사람은 피난 권유에도 교회를 지켰다',\n",
              " '김태수도 잠자리에 들지 못하고 있었다',\n",
              " '대왕면 인민위원회가 반동 검거에 나서던 무렵이었다',\n",
              " '교인들은 교회 장로이자 반공청년단장인 그에게 피난을 떠나라고 종용했다',\n",
              " '“현석진 월남자로 미국놈 앞잡이 노릇을 하였고 국방군에 음식을 제공했다',\n",
              " '그렇게 전쟁이 끝났다',\n",
              " '겐슬러에 대한 시장의 반응은 일단 회의적이다',\n",
              " '국내 증시에 유일하게 상장된 명품 테마 ETF다',\n",
              " '호텔·카지노·크루즈 회사가 편입돼 있는 점도 눈에 띈다',\n",
              " '목숨을 내놓고 싸우는 조직폭력배의 암투나 마약 거래보다 더 긴장된다',\n",
              " '영화 도둑들과 기술자들을 합쳐놓은 것보다 짜릿하고 강렬하다',\n",
              " '영화에서는 하지 않기로 했던 수능 시험지 탈취작전까지 펼쳐진다',\n",
              " '학비가 비싼 만큼 최고의 교육을 받을 수 있다',\n",
              " '친구인 그레이스는 연극반에 들어가고 싶지만 성적이 기준에 못 미친다',\n",
              " '학교 선생이 비밀과외를 하고 문제를 그대로 출제했기 때문이다',\n",
              " '충격을 받은 린은 지우개에 정답을 적어서 그레이스에게 건네준다',\n",
              " '그렇게 린의 첫 커닝은 성공한다',\n",
              " '타이는 사실 광고 영화 드라마에서 세계적으로 주목받는 문화 강국이다',\n",
              " '처음에는 누군지 몰랐던 타이 아역들에 푹 빠지게 될 것이다',\n",
              " '특히 린을 연기하는 플른피차야 꼬말라라춘은 단번의 시선을 잡는다',\n",
              " '의 설계자이기 때문이다',\n",
              " '드라마와 다른 점이 있다면 린과 달리 금수저’ 집안 출신이다',\n",
              " '아버지는 타이 통신사 대표이며 어머니는 베이커리 프랜차이즈의 대표라고 한다',\n",
              " '그러나 누구도 완벽하게 악하거나 완벽하게 선하지 않다',\n",
              " '덕분에 드라마는 반전에 반전을 거듭한다',\n",
              " '하지만 어떤 이유에서든 해서는 안 될 나쁜 짓이다',\n",
              " '한편 해당 프라이팬은 전국 할인점을 통해 구매 가능하다',\n",
              " '마세라티가 뉴 스타일’ 컬렉션을 선보인다',\n",
              " '여기에 블루 컬러의 악어가죽 무늬 스트랩으로 고급스러움을 더했다',\n",
              " '그가 지켜본 정 회장은 어떤 모습이었을까',\n",
              " '말투가 또박또박하고 힘이 있었다',\n",
              " '처음에 들어갈 때는 전경련 조사역이었다',\n",
              " '이름만 걸쳐 놓은 비상근 위원장이 아니었다',\n",
              " '상임위원장처럼 매우 적극적으로 일했다',\n",
              " '영국 산업위원회와 함께 매년 한국과 영국을 오가며 합동회의를 했다',\n",
              " '이를 기화로 정 회장은 본격적으로 국제적인 명사로 부상하기 시작했다',\n",
              " '정 회장은 국제협력위원회를 최대한 활용하는 글로벌 감각을 가진 사람이었다',\n",
              " '“나는 기억력이 나쁘지 않지만 메모를 열심히 하는 습관이 있다',\n",
              " '국가에 대한 사명감 때문이었다',\n",
              " '낮에는 수행하고 저녁에는 다음날 일정을 잡아야 했다',\n",
              " '연설문도 써야 했다',\n",
              " '지금 생각하면 그 힘든 일을 어떻게 견뎠나 싶다',\n",
              " '“꼭 그런 것은 아니다',\n",
              " '따사로운 면도 있다',\n",
              " '자네 하와이에서 쉬었다 와’ 이렇게 배려해 주곤 했다',\n",
              " '그래서 연설문 작성 때 수시로 이 표현을 넣었다',\n",
              " '“타고 난 측면이 있다',\n",
              " '다만 그는 신문 사설을 열심히 읽었다',\n",
              " '“이봐 해봤어 정신이다',\n",
              " '중간 관리자들은 원래 힘든 일을 두려워하기 마련이다',\n",
              " '“현대중공업 초창기에 배를 만들어 그리스와 중동의 선주에게 건네줬다',\n",
              " '실무자들이 반대하자 이봐 해봤어라고 면박을 주며 공기 단축을 지시했다',\n",
              " '도저히 그 기한에 배를 만들 수 없다고 본 것이다',\n",
              " '그 때마다 정 회장은 이봐 해 봤어’라며 면박을 줬다',\n",
              " '하지만 정 회장은 비용 절감에 대한 복안이 있었다',\n",
              " '제일 핵심은 공사기간 단축이었다',\n",
              " '공기를 단축하면 인건비와 장비 임대료를 줄일 수 있다',\n",
              " '또 수금을 앞당길 수 있다',\n",
              " '서울 외환 시장이 난리가 났었다',\n",
              " '” 정 회장이 역발상의 대가인 것처럼 들린다',\n",
              " '정 회장은 이런 파격적인 발상을 과감하게 밀어부치는 실천력이 있었다',\n",
              " '길이 없으면 만들어갔다',\n",
              " '내가 통역을 했다',\n",
              " '드러커는 정 회장을 만나자마자 제가 잘못했습니다’라고 사과를 했다',\n",
              " '어리둥절하고 있는데 드러커가 말을 이어갔다',\n",
              " '모든 경영자들이 나를 경영의 구루스승라고 한다',\n",
              " '그런데도 나는 한국에 오기 전에 당신에 대해 잘 몰랐다',\n",
              " '그래서 사과한다’고 했다',\n",
              " '하지만 정 회장은 아무런 지지 기반이 없었다',\n",
              " '그런 점에서 드러커가 높이 평가했다',\n",
              " '“자신의 사업보다도 국가를 먼저 생각하는 애국지사적인 기업인이라는 점이다',\n",
              " '정 회장은 주식 공개를 늦춘 이유에 대해 이렇게 말했다',\n",
              " '주식 투자는 돈 있는 사람들이 하는 것이다',\n",
              " '현대중공업도 예외가 아니었다',\n",
              " '그러나 미리 그런 상황에 대비한 마이크 시스템도 없었다',\n",
              " '농성 현장은 순식간에 아수라장이 되고 말았다',\n",
              " '” 현대중공업과 현대자동차는 노사분규가 잦기로 유명하다',\n",
              " '내가 나오라고 할 사람이 없어서 관뒀다’고 했다',\n",
              " '노동자에 대한 비난은 없었다',\n",
              " '남의 부축을 받는 것을 매우 싫어했다',\n",
              " '“건강 관리를 위해 별도로 운동을 한 것 같지는 않다',\n",
              " '다만 평생 동안 새벽에 집에서 사무실까지 걸어서 출근했다',\n",
              " '그는 독일 바덴바덴의 올림픽 유치 홍보관 개관식에도 나타나지 않았다',\n",
              " '오직 정 회장만이 유치가 가능하다고 믿었다',\n",
              " '“대한민국 국민들이 모두 놀랐는데 특히 전두환 대통령이 놀랐다',\n",
              " '정 회장은 정부에 대해 할 말은 하는 사람이었다',\n",
              " '우리 정부가 사회주의 정부가 됐다’고 비판했다',\n",
              " '그러자 정 회장은 나는 흥분하지 않았다’며 목소리를 높였다',\n",
              " '노태우 대통령이 당선자 시절이었다',\n",
              " '그래서 전 대통령이 노태우 당선자에게 찾아 갔었다',\n",
              " '그러나 노 당선자는 참모진과 상의해보겠다’고 하면서 결국 거절했다고 한다',\n",
              " '무역협회대한상의중소기업중앙회는 법정 단체이므로 기업들이 의무적으로 가입하고 회비를 내야 한다',\n",
              " '이에 반해 전경련과 경총은 자율 단체이다',\n",
              " '“우선 민간 경제계의 목소리를 정부에 반영할 언로가 막혔다',\n",
              " '우리 경제인들이 한국 안보에 앞장 서야 한다',\n",
              " '정부는 나서기가 어렵다',\n",
              " '주한 미군 사령관인 베시 대장 싱글러브 참모장소장도 참석했다',\n",
              " '지금은 고인이 된 안기부 김영광 판단기획국장이 전경련을 찾아왔다',\n",
              " '안기부가 이 정보를 늦게 파악해 박정희 대통령이 노발대발했다고 한다',\n",
              " '그래서 정주영 회장을 찾아와 그 일을 맡아달라고 부탁했다',\n",
              " '정 회장이 이를 수락하고 우리에게 그 일을 하라고 지시했다',\n",
              " '그리고 북한의 계획을 무산시키는데 성공했다',\n",
              " '창업자 세대와 지금을 비교해 보면 어떨까',\n",
              " '국가경제에 대한 장기적인 발전 비전을 가져야 한다',\n",
              " '“첫째 국가 발전을 위한 장기적 안목을 가져야 한다',\n",
              " '둘째 사회 기여 정신을 가져야 한다',\n",
              " '“예전에는 기업과 정부간에 대화 창구가 열려 있었다',\n",
              " '기업인들이 언제나 정부 각료와 실무자들과 대화가 가능했다',\n",
              " '하지만 지금은 이런 대화 통로가 대부분 막혀 있는 느낌이다',\n",
              " '민간기업의 의견을 제대로 반영하는 통로가 없다',\n",
              " '그래서 요즘 기업인들은 정말 기업하기 어려워졌다고 한다',\n",
              " '삼성 거인’의 단점 스마트폰을 꺼내 시계를 들여다 봤다',\n",
              " '마무리 지어야 할 시간이다',\n",
              " '“김영삼 대통령 시절이다',\n",
              " '김영삼 정부 동안 현대그룹은 시련의 연속이었다',\n",
              " '그 때가 정 회장이 가장 어려웠던 시기가 아닌가 싶다',\n",
              " '“아무튼 나에게 여러차례 그렇게 말했다',\n",
              " '“성격이 급하고 남들에게 지는 것을 참지 못한다',\n",
              " '특히 분을 삭이지 못하는 성격이었다',\n",
              " '박영선 선거대책위원회 출범식에서 기념촬영을 하고 있다',\n",
              " '그거 쉬운 거 아닙니다',\n",
              " '중소벤처기업부 장관도 참 잘 하셨어요',\n",
              " '서울시장에 나가지 말라는 의견도 있었다죠',\n",
              " '이 정부에서 이런 인기 짱 장관은 처음 봤습니다',\n",
              " '그런데 최근 두 가지 점에서 후보님께 실망했습니다',\n",
              " '아니 이게 뭐지 싶었습니다',\n",
              " '박원순 전 시장을 둘러싼 성희롱 사건이 원인입니다',\n",
              " '후보님의 무신경이 제 마음을 아프게 합니다',\n",
              " '눈물이 핑 돌았다였습니다',\n",
              " '이해가 안 됩니다',\n",
              " '이건 달을 가르키는데 손가락을 두고 시비를 거는 격입니다',\n",
              " '이런 엇박자가 또 있을까요',\n",
              " '이에 대한 후보님의 의견을 묻지 않을 수 없습니다',\n",
              " '일시적인 판단 착오라면 차라리 괜찮습니다',\n",
              " '아차 내가 실수했다 바로잡겠다고 하면 됩니다',\n",
              " '지금도 늦지 않았습니다',\n",
              " '남·진 두 의원을 불러 공동선대본부장에서 내려오도록 양해를 구하기 바랍니다',\n",
              " '캠프의 얼굴인 고 대변인은 당장 바꾸는 게 좋습니다',\n",
              " '보선 캠프 참여가 피해자에게 또다른 상처라는 생각은 안 드시는지요',\n",
              " '애매모호 그 자체입니다',\n",
              " '후보의 소신을 듣고 유권자가 판단하는 자리 아닌가요',\n",
              " '후보가 소신을 감추면 유권자는 뭘 보고 투표해야 하나요',\n",
              " '박원순 전 시장은 퀴어축제를 서울광장에서 여는 걸 허락했습니다',\n",
              " '퀴어축제를 여는 게 옳다고 말하는 게 아닙니다',\n",
              " 'LGBTQ를 보는 우리 사회의 시선은 천차만별입니다',\n",
              " '반대하는 사람도 무지 많습니다',\n",
              " '멋진 승부가 펼쳐지길 바랍니다',\n",
              " '수십 분 뒤 거대한 지진해일이 몰려옵니다',\n",
              " '재앙은 최악의 원전 폭발사고로도 이어졌습니다',\n",
              " '방사능 오염 지역이라 사람이 거주할 수 없는 곳입니다',\n",
              " '입주자들은 고향에서 여생을 보내려는 고령자들입니다',\n",
              " '부흥주택 거주 주민  젊은이나 아이는 없습니까',\n",
              " '젊은 사람은 없어요',\n",
              " '원전 폭발 당시 사고대책본부가 있었던 자리엔 체육시설인 J빌리지가 조성됐습니다',\n",
              " '그러나 현실과 괴리가 큽니다',\n",
              " '숨돌릴 틈도 없이 미·중 무역전쟁이 터졌다',\n",
              " '사태 초기 늑장 대응과 정보 은폐 의혹이 불거졌다',\n",
              " '최대의 정치적 위기였다',\n",
              " '시진핑 리더십을 비판한 지식인과 네티즌들의 입은 틀어 막혔다',\n",
              " '시 주석 시대를 정확히 예측한 셈이다',\n",
              " '이제 미국 패권을 따라잡는 중국몽中國夢으로 중화중심주의 완성을 노리고 있다',\n",
              " '“모든 연구 주제와 과정 내용을 공산당 중앙이 감찰하고 있다',\n",
              " '”대학교수 “중국에 언론은 없다',\n",
              " '오직 하나의 목소리one voice만 있을 뿐이다',\n",
              " '”전직 언론인 “빈부 격차가 너무 크다',\n",
              " '농민공은 외곽에서도 쫓겨나고 있다',\n",
              " '추신수는 로저드뷔의 홍보대사이기도 하다',\n",
              " '덕분에 청담동 명품 거리가 다시 살아나고 있네요',\n",
              " '한동안 한산했던 거리가 다시 시끌벅적해졌다',\n",
              " '새롭게 오픈하는 브랜드도 대다수가 명품이다',\n",
              " '곳곳에는 새롭게 둥지를 틀기 위한 공사가 바쁘게 진행되고 있었다',\n",
              " '인근 갤러리아백화점 명품관도 평일 오후였음에도 불구하고 사람들로 북적였다',\n",
              " '당신의 이야기를 들려주세요',\n",
              " '여기 오시면 깜짝 놀랍니다란 제목의 기사를 우연히 읽게 됐어요',\n",
              " '간단한 자기소개를 부탁드립니다',\n",
              " '안녕하세요 역사 여행작가 운민입니다',\n",
              " '김포의 매력은 무엇인가요',\n",
              " '장릉 김포성당 등 스토리가 있는 문화재도 있습니다',\n",
              " '또 한강신도시엔 운하의 도시 콘셉트로 지어진 라베니체가 있는데요',\n",
              " '경인운하가 지나가는 수변 상가인데 밤에 정말 아름답습니다',\n",
              " '앞으로의 김포가 더욱 기대됩니다',\n",
              " '울산과 경기도의 다른 점은 무엇일까요',\n",
              " '김포와 비교했을땐 일단 동해안과 서해안의 차이가 눈에 띄었습니다',\n",
              " '해산물도 다른 점이 많아요',\n",
              " '경기도도 마찬가지 상황 아닌가 싶어요',\n",
              " '많은 분들이 경기도는 너무 넓다고 하는데요',\n",
              " '김포에서 경기 동부를 취재하러 다니기 힘들진 않나요',\n",
              " '경기도는 사통팔달로 길이 뻗어있고 자차를 이용하기에 큰 불편함은 없습니다',\n",
              " '다만 서울을 돌아가야 하는 어려움이 있어요',\n",
              " '또 경기 동부는 주말과 공휴일이면 항상 사람들로 북적여요',\n",
              " '남양주 가평 양평이 특히 그래요',\n",
              " '대답하기 제일 어려운 질문이네요',\n",
              " '저에겐 전부 애정이 가는 도시라서요',\n",
              " '그래도 인상적이었던 도시를 꼽자면 우선 여주를 꼽고 싶습니다',\n",
              " '그리고 연천을 이야기할 수 있겠네요',\n",
              " '남한에서 유일하게 고구려 성곽을 제대로 느낄 수 있는 곳이에요',\n",
              " '인스타그램 등 sns에서 사진 맛집으로 유명한 호로고루도 연천에 있습니다',\n",
              " '많은 출판사가 관심을 가져주면 좋겠습니다',\n",
              " '서울처럼 경기도에도 조선시대부터 중요한 역할을 해온 도시가 많아요',\n",
              " '대표적으로 정조가 쌓아 올렸던 수원화성이 있습니다',\n",
              " '농산물이 풍부해 먹을거리도 정말 훌륭합니다',\n",
              " '조선왕가의 왕릉은 물론 역사적으로 유명한 사람들의 발자취도 많이 남아있지요',\n",
              " '공업도시였던 시흥과 안산이 생태도시와 다문화도시로 변화하고 있는 것처럼 말이죠',\n",
              " '여행작가 일은 어떻게 시작하게 됐나요',\n",
              " '작년에 정말 우연히 시작하게 됐어요',\n",
              " '이 모멘텀을 이어가고 싶어서 여행작가 일을 시작했습니다',\n",
              " '현재는 팟캐스트 탁피디의 여행수다에서 삼국지 이야기를 하고 있고요',\n",
              " '한국관광공사 홈페이지축제 기고란에서도 제 글을 만나보실 수 있습니다',\n",
              " '네 어렸을 때부터 여행은 인생의 전부라 해도 과언이 아니었습니다',\n",
              " '여행작가 일을 전업으로 하고 있나요',\n",
              " '일단 전업은 아니고요 무역일을 병행하고 있습니다',\n",
              " '여행을 정말 좋아하나요',\n",
              " '여행에도 수많은 형태의 여행이 존재합니다',\n",
              " '단지 일상에서 도피하기 위한 여행이라면 여행작가의 꿈은 허황된 것일지도요',\n",
              " '그리고 본인만의 확실한 아이덴티티가 있어야 할 것 같아요',\n",
              " '저는 여행작가로서 저를 소개할 때 항상 역사란 말을 붙입니다',\n",
              " '저는 역사 이야기가 담긴 곳을 특히 좋아하거든요',\n",
              " '여행작가일을 시작하게 되기까지 감사한 분이 많습니다',\n",
              " '일단 목표는 확실합니다',\n",
              " '이를 위해 우선 좋은 출판사를 만나고 싶고요',\n",
              " '훗날 기회가 된다면 EBS 세계테마기행에도 출연해보고 싶네요',\n",
              " '더 많은 분들과 소통하고 싶습니다',\n",
              " '금융 당국도 도입에 관해 큰 틀에서 긍정적인 입장이다',\n",
              " '오랫동안 보기 힘들었던 출판 형태인데 최근 들어서는 그렇지만도 않다',\n",
              " '그리고 무슨 내용인지 들여다보면 유독 기후위기 관련한 책이 많다',\n",
              " '모두 다 커다란 정치적 변혁을 앞둔 긴장되고 급박한 시기였다',\n",
              " '기후위기를 놓고 말이다',\n",
              " '이것이 현재 우리에게 닥친 크나큰 도전이다',\n",
              " '그러나 문제는 정치적 시간 감각이다',\n",
              " '지배 세력은 산업화의 시간을 살았고 이것을 대중에게 강요했다',\n",
              " '이번이 아니면 그런 기회는 영영 오지 않을 것이었다',\n",
              " '이에 따른 시간 감각이 군사독재정부의 권력자들을 지배했다',\n",
              " '그들의 시간 지평에는 뚜렷한 한계선이 존재했다',\n",
              " '저들의 독재는 이런 시간 감각에 바탕을 두고 있었다',\n",
              " '그러나 이 모두는 지나간 옛 이야기다',\n",
              " '실은 이게 현대 자본주의의 정상적 시간 감각이다',\n",
              " '그러나 한국 사회의 적응 속도는 유난히 빨랐다',\n",
              " '특히 민주화 흐름을 이어받았다는 정치 세력의 적응은 놀라웠다',\n",
              " '그저 적응만이 아니었다',\n",
              " '시간은 제한돼 있고 순간순간마다 시간의 끝이 다가오기 때문이다',\n",
              " '기후위기를 고민한다면 그럴 수밖에 없다',\n",
              " '탄소예산을 안다면 기후위기를 마주하며 이토록 태평할 수가 없다는 것이었다',\n",
              " '말 그대로의 예산은 아니다',\n",
              " '그는 사물을 오랫동안 관찰해 그리기로 유명했다',\n",
              " '” 이중섭의 말이다',\n",
              " '이중섭의 그림에 특히 자주 등장하는 흰 소는 그의 자화상이다',\n",
              " '골든듀 올해는 신축년辛丑年이다',\n",
              " '신축에서 신辛은 백색 축丑은 소를 의미한다',\n",
              " '바로 흰 소의 해’인 것이다',\n",
              " '이중섭의 흰 소’를 소환한 이유다',\n",
              " '주얼리 세계에서도 동물과의 교감이 다양하게 일어나고 있다',\n",
              " '아프리카에서 주얼리는 권력과 아름다움 선망의 상징이다',\n",
              " '부적으로 사용되는 한편 수백만 가지의 착용 방법이 개발되기도 했다',\n",
              " '얼룩말 기린 플라밍고 코끼리 개미 그리고 사자가 주인공이다',\n",
              " '개미들은 감미로운 향과 맛을 지닌 과일 사이를 곡예하듯 넘나든다',\n",
              " '아프리카의 이국적인 정취에 흠뻑 취할 수 있는 작품들이다',\n",
              " '주얼리의 동물 컬렉션을 즐기면서 말이다',\n",
              " '캔디젤리 전문점 위니비니도 화이트데이 기획전을 실시한다',\n",
              " '엑스칼리버 시리즈는 고가의 명품 시계로 수억원에 이르는 제품도 있다',\n",
              " '올해 맹활약을 펼칠 수 있도록 노력하겠다고 화답했다',\n",
              " '다만 김씨도 현재의 영업제한은 과하다고 지적했다',\n",
              " '하지만 LH를 둘러싼 지금의 행태는 국민경제 발전과 거리가 멀다',\n",
              " 'LH는 대체 어떤 곳이기에 목적과 다른 길을 걸어왔을까',\n",
              " '대통령이 권한을 행사할 수 있다',\n",
              " '이는 서민의 주거 안정만을 위해 사용하라고 위임해 준 권력이다',\n",
              " '김헌동 경제정의실천시민연합경실련 부동산건설개혁본부장의 말이다',\n",
              " '개성공단을 비롯해 추후 북한과 토지 개발을 이끌어갈 주체도 LH다',\n",
              " '문제는 이 많은 사업권을 지나친 수익 추구에 이용하면서 불거졌다',\n",
              " '그 약속은 불과 한 달도 안 돼 무색해졌다',\n",
              " '서민 주거안정이란 명분을 내세워 돈을 챙겼다는 논란에도 휩싸였다',\n",
              " '이명박 정부에서 보금자리주택이란 이름으로 추진한 공공임대주택이다',\n",
              " '보금자리주택을 비판하며 출범한 박근혜 정부에서도 논란은 계속됐다',\n",
              " '주거안정의 틀인 장기공공임대 약속을 지키기 위해서다',\n",
              " '공공성이 강한 임대주택 용지를 수익 실현 도구로 이용하려는 셈이다',\n",
              " '이는 결국 세금 부담으로 돌아올 가능성이 크다',\n",
              " '단 공기업의 수익 추구와 직원의 사익 추구는 별개다',\n",
              " '그럼에도 직원들은 수차례 선을 넘었다',\n",
              " '공공임대아파트 임차권을 양도 승인해 주는 권한을 악용한 것이다',\n",
              " '문재인 정부 들어서도 비리는 이어졌다',\n",
              " '브로커들에게서 뇌물을 받은 혐의다',\n",
              " 'LH 투기 의혹이 공론화되자 이들도 주목을 받게 됐다',\n",
              " '김헌동 경실련 본부장은 아예 LH 해체론을 주장했다',\n",
              " '부동산 정책을 복지 차원으로 접근해 공공주택을 확보해야 한다는 취지다',\n",
              " '그가 지켜본 정 회장은 어떤 모습이었을까',\n",
              " '말투가 또박또박하고 힘이 있었다',\n",
              " '처음에 들어갈 때는 전경련 조사역이었다',\n",
              " '이름만 걸쳐 놓은 비상근 위원장이 아니었다',\n",
              " '상임위원장처럼 매우 적극적으로 일했다',\n",
              " '영국 산업위원회와 함께 매년 한국과 영국을 오가며 합동회의를 했다',\n",
              " '이를 기화로 정 회장은 본격적으로 국제적인 명사로 부상하기 시작했다',\n",
              " '정 회장은 국제협력위원회를 최대한 활용하는 글로벌 감각을 가진 사람이었다',\n",
              " '“나는 기억력이 나쁘지 않지만 메모를 열심히 하는 습관이 있다',\n",
              " '국가에 대한 사명감 때문이었다',\n",
              " '낮에는 수행하고 저녁에는 다음날 일정을 잡아야 했다',\n",
              " '연설문도 써야 했다',\n",
              " '지금 생각하면 그 힘든 일을 어떻게 견뎠나 싶다',\n",
              " '“꼭 그런 것은 아니다',\n",
              " '따사로운 면도 있다',\n",
              " '자네 하와이에서 쉬었다 와’ 이렇게 배려해 주곤 했다',\n",
              " '그래서 연설문 작성 때 수시로 이 표현을 넣었다',\n",
              " '“타고 난 측면이 있다',\n",
              " '다만 그는 신문 사설을 열심히 읽었다',\n",
              " '“이봐 해봤어 정신이다',\n",
              " '중간 관리자들은 원래 힘든 일을 두려워하기 마련이다',\n",
              " '“현대중공업 초창기에 배를 만들어 그리스와 중동의 선주에게 건네줬다',\n",
              " '실무자들이 반대하자 이봐 해봤어라고 면박을 주며 공기 단축을 지시했다',\n",
              " '도저히 그 기한에 배를 만들 수 없다고 본 것이다',\n",
              " '그 때마다 정 회장은 이봐 해 봤어’라며 면박을 줬다',\n",
              " '하지만 정 회장은 비용 절감에 대한 복안이 있었다',\n",
              " '제일 핵심은 공사기간 단축이었다',\n",
              " '공기를 단축하면 인건비와 장비 임대료를 줄일 수 있다',\n",
              " '또 수금을 앞당길 수 있다',\n",
              " '서울 외환 시장이 난리가 났었다',\n",
              " '” 정 회장이 역발상의 대가인 것처럼 들린다',\n",
              " '정 회장은 이런 파격적인 발상을 과감하게 밀어부치는 실천력이 있었다',\n",
              " '길이 없으면 만들어갔다',\n",
              " '내가 통역을 했다',\n",
              " '드러커는 정 회장을 만나자마자 제가 잘못했습니다’라고 사과를 했다',\n",
              " '어리둥절하고 있는데 드러커가 말을 이어갔다',\n",
              " '모든 경영자들이 나를 경영의 구루스승라고 한다',\n",
              " '그런데도 나는 한국에 오기 전에 당신에 대해 잘 몰랐다',\n",
              " '그래서 사과한다’고 했다',\n",
              " '하지만 정 회장은 아무런 지지 기반이 없었다',\n",
              " '그런 점에서 드러커가 높이 평가했다',\n",
              " '“자신의 사업보다도 국가를 먼저 생각하는 애국지사적인 기업인이라는 점이다',\n",
              " '정 회장은 주식 공개를 늦춘 이유에 대해 이렇게 말했다',\n",
              " '주식 투자는 돈 있는 사람들이 하는 것이다',\n",
              " '현대중공업도 예외가 아니었다',\n",
              " '그러나 미리 그런 상황에 대비한 마이크 시스템도 없었다',\n",
              " '농성 현장은 순식간에 아수라장이 되고 말았다',\n",
              " '” 현대중공업과 현대자동차는 노사분규가 잦기로 유명하다',\n",
              " '내가 나오라고 할 사람이 없어서 관뒀다’고 했다',\n",
              " '노동자에 대한 비난은 없었다',\n",
              " '남의 부축을 받는 것을 매우 싫어했다',\n",
              " '“건강 관리를 위해 별도로 운동을 한 것 같지는 않다',\n",
              " '다만 평생 동안 새벽에 집에서 사무실까지 걸어서 출근했다',\n",
              " '그는 독일 바덴바덴의 올림픽 유치 홍보관 개관식에도 나타나지 않았다',\n",
              " '오직 정 회장만이 유치가 가능하다고 믿었다',\n",
              " '“대한민국 국민들이 모두 놀랐는데 특히 전두환 대통령이 놀랐다',\n",
              " '정 회장은 정부에 대해 할 말은 하는 사람이었다',\n",
              " '우리 정부가 사회주의 정부가 됐다’고 비판했다',\n",
              " '그러자 정 회장은 나는 흥분하지 않았다’며 목소리를 높였다',\n",
              " '노태우 대통령이 당선자 시절이었다',\n",
              " '그래서 전 대통령이 노태우 당선자에게 찾아 갔었다',\n",
              " '그러나 노 당선자는 참모진과 상의해보겠다’고 하면서 결국 거절했다고 한다',\n",
              " '무역협회대한상의중소기업중앙회는 법정 단체이므로 기업들이 의무적으로 가입하고 회비를 내야 한다',\n",
              " '이에 반해 전경련과 경총은 자율 단체이다',\n",
              " '“우선 민간 경제계의 목소리를 정부에 반영할 언로가 막혔다',\n",
              " '우리 경제인들이 한국 안보에 앞장 서야 한다',\n",
              " '정부는 나서기가 어렵다',\n",
              " '주한 미군 사령관인 베시 대장 싱글러브 참모장소장도 참석했다',\n",
              " '지금은 고인이 된 안기부 김영광 판단기획국장이 전경련을 찾아왔다',\n",
              " '안기부가 이 정보를 늦게 파악해 박정희 대통령이 노발대발했다고 한다',\n",
              " '그래서 정주영 회장을 찾아와 그 일을 맡아달라고 부탁했다',\n",
              " '정 회장이 이를 수락하고 우리에게 그 일을 하라고 지시했다',\n",
              " '그리고 북한의 계획을 무산시키는데 성공했다',\n",
              " '창업자 세대와 지금을 비교해 보면 어떨까',\n",
              " '국가경제에 대한 장기적인 발전 비전을 가져야 한다',\n",
              " '“첫째 국가 발전을 위한 장기적 안목을 가져야 한다',\n",
              " '둘째 사회 기여 정신을 가져야 한다',\n",
              " '“예전에는 기업과 정부간에 대화 창구가 열려 있었다',\n",
              " '기업인들이 언제나 정부 각료와 실무자들과 대화가 가능했다',\n",
              " '하지만 지금은 이런 대화 통로가 대부분 막혀 있는 느낌이다',\n",
              " '민간기업의 의견을 제대로 반영하는 통로가 없다',\n",
              " '그래서 요즘 기업인들은 정말 기업하기 어려워졌다고 한다',\n",
              " '삼성 거인’의 단점 스마트폰을 꺼내 시계를 들여다 봤다',\n",
              " '마무리 지어야 할 시간이다',\n",
              " '“김영삼 대통령 시절이다',\n",
              " '김영삼 정부 동안 현대그룹은 시련의 연속이었다',\n",
              " '그 때가 정 회장이 가장 어려웠던 시기가 아닌가 싶다',\n",
              " '“아무튼 나에게 여러차례 그렇게 말했다',\n",
              " '“성격이 급하고 남들에게 지는 것을 참지 못한다',\n",
              " '특히 분을 삭이지 못하는 성격이었다',\n",
              " '현행 방역조치가 장기간 유지되면서 자영업자들은 극심한 고통을 호소하고 있다',\n",
              " '윤씨는 형평성에 대해 불만을 토로하기도 했다',\n",
              " '쌓일대로 쌓인 적자를 견디지 못해 폐업한다는 사례도 잇따랐다',\n",
              " '영업일이 늘수록 적자만 쌓인다라고 한탄했다',\n",
              " '배신·도피·실종·갈등에 관한 이야기로 시간의 경계를 넘나들며 회전목마처럼 순환한다',\n",
              " '다양한 인물들이 화자로 번갈아 나온다',\n",
              " '자기 처지와 사연을 들려주면서 이야기의 밀도가 높아진다',\n",
              " '듣는 사람이 짧은 말을 더 잘 기억한다는 이유에서다',\n",
              " '중요한 말하기 순간을 여섯 가지로 나누고 맞춤형 전략을 설명한다',\n",
              " '짧게 말하는 건 장담하는데 남는 장사다',\n",
              " '각 지역의 문화·역사·공간보다 축제를 채우는 사람들에게 더 주목한다',\n",
              " '이들의 진심을 세세하게 읽어내며 축제의 본질에 다가간다',\n",
              " '너무 빨리 발표한 것 아니냐 이런 지적도 있는데요',\n",
              " '그런 부분들은 조사단계에서는 한계가 좀 있습니다',\n",
              " '그런 걸 조사 과정에서는 알 수가 없는 거거든요',\n",
              " '이런 의견 주셨습니다',\n",
              " '어쨌든 조사는 강제권한이 없기 때문에 한계가 있을 수밖에 없고요',\n",
              " '박영선 후보의 특검 제안에 대한 여야 입장까지 듣고 오시겠습니다',\n",
              " '왜냐하면 과연 중이 제 머리 깎을 수 있겠느냐',\n",
              " '혹시 또 제 식구 감싸기 논란이 벌어지는 거 아니겠느냐',\n",
              " '이런 차원에서 제안을 하신 거라고 봐요',\n",
              " '왜냐하면 야당 입장에서는 자꾸 그런 눈초리가 있을 수 있잖아요',\n",
              " '그런 논란을 없애기 위해서 그러는 건데요',\n",
              " '이런 것들은 국민적 합의와 요청이 있으면 하는 거예요',\n",
              " '그러니까 그런 요청들이 정말 점진하고 비대하다면 당연히 해야겠죠',\n",
              " '특검도 하고 유기적으로 협력이 돼야겠죠',\n",
              " '김근식 그러니까 박영선 지금 서울시장 후보잖아요',\n",
              " '길어도 한 달 이상은 걸린단 말이에요',\n",
              " '그러면 서울시장 보궐선거가 끝나는 겁니다',\n",
              " '앵커 그러면 특검카드에 대해서는 어떻게 보십니까',\n",
              " '조상호 당연히 후보니까 본인 선거가 신경이 쓰이겠죠',\n",
              " '그런데 국민들이 아까 어떻게 느끼냐는 겁니다',\n",
              " '검경 협력 수사는 당연한 거고요',\n",
              " '앵커 특검 발족까지는 얼마나 통상 걸립니까',\n",
              " '굉장히 믿을 수가 없다',\n",
              " '함께 듣고 오시죠',\n",
              " '그런데 그런 얘기 못 하고 뒤에 숨어있지 않습니까',\n",
              " '그런데 그런 자세는 저는 옳지 못하다고 생각하고 있습니다',\n",
              " '그러니까 검찰 관계자가 이른바 우리가 무당이냐고 그랬다는 거잖아요',\n",
              " '경찰은 못할 것이다',\n",
              " '이번에 처음 해 보는 겁니다',\n",
              " '그게 어떤 경찰만의 책임이라고 얘기할 수 있겠습니까',\n",
              " '이건 무슨 소리야 이렇게 되는 거거든요',\n",
              " 'LH 투기 의혹이 정치권으로도 번지고 있는 이런 상황입니다',\n",
              " '오늘도 여당은 국회가 먼저 솔선수범을 보이자 국민의힘을 압박하고 나섰고요',\n",
              " '국민의힘은 여당이 물타기를 하고 있다',\n",
              " '민주당 먼저 조사하면 우리도 스스로 전수조사하겠다 이런 입장을 밝혔습니다',\n",
              " '여야 입장 듣고 오시죠',\n",
              " '지체하지 말고 바로 행동에 옮겨야 합니다',\n",
              " '더불어민주당은 준비가 돼 있습니다',\n",
              " '앵커 이 문제는 김근식 실장님께 먼저 여쭤보겠습니다',\n",
              " '그놈이 그놈이라는 생각이 많지 않겠어요',\n",
              " '물타기라는 측면이 있겠죠',\n",
              " '그래서 그 의혹이 충분히 있기 때문에 보좌진도 해야 되고요',\n",
              " '여기도 여야를 가릴 필요가 있습니다',\n",
              " '그런데 국민들은 진짜 할까 이런 의문을 아직도 가지십니다',\n",
              " '조상호 계속 마지막 뉘앙스가 있거든요',\n",
              " '그러니까 의원들은 간단해요',\n",
              " '할 수 있는 일은 정말 되게 쉽습니다',\n",
              " '개인정보 제공 동의서만 써서 주시면 돼요',\n",
              " '이건 안 하겠다는 얘기거든요',\n",
              " '이런 모양새 아니냐',\n",
              " '이렇게 비판하시는 분도 있다는 거죠',\n",
              " '그래서 나중에 본인이 이른바 귀농이죠',\n",
              " '이게 꼭 투기지역도 아니고요',\n",
              " '현재 문제가 되고 있는 개발지역하고도 꽤 거리가 있거든요',\n",
              " '앵커 국민의힘도 뭔가 자체조사 분위기는 안에서 있습니까',\n",
              " '김근식 아직은 가시화되고 있지는 않고요',\n",
              " '왜냐하면 배우자가 땅을 어디에 갖고 있다',\n",
              " '어떻게 자진사퇴 쪽으로 무게가 실리는 겁니까',\n",
              " '그런데 그 이후의 대응이 되게 부적절했었거든요',\n",
              " '그리고 특히 본인은 여러 차례 청렴성을 강조해서 얘기했다',\n",
              " '그냥 계속해서 청렴성만 강조한다고 그게 청렴해지는 게 아니잖아요',\n",
              " '그게 본인이 해야 할 마지막 일이다',\n",
              " '앵커 그것까지 하고 거취를 결정해야 된다는 말씀이신가요',\n",
              " '조상호 그게 그렇게 오래 걸릴 거라고 생각하지 않습니다',\n",
              " '이런 목소리도 개별 의원들 중심으로 나오고 있는데 어떻게 보십니까',\n",
              " '그것도 일부 의원의 입장입니다',\n",
              " '그러다 보니까 땅장사라고 하는 기본적인 수익구조가 생길 수밖에 없어요',\n",
              " '토지공사와 주택공사가 결합하면서 사실 주택공사는 적자 투성이었습니다',\n",
              " '저희도 월요일부터 지금까지 계속 LH 얘기를 하고 있으니까요',\n",
              " '선거에 미칠 최대 이슈 여러분은 뭐라고 생각하십니까',\n",
              " '여기에 대한 여론조사를 한 부분이 있는데요',\n",
              " '그래픽 나올 때까지 제가 말로 설명을 드리겠습니다',\n",
              " '조상호 당연한 결과라고 생각합니다',\n",
              " '그리고 서울시를 둘러싸고 있는 수도권도 마찬가지고요',\n",
              " '이 부분도 얼마나 영향을 미칠까 주목해 봐야 될 부분인데요',\n",
              " '박영선 후보도 그런데 거리를 두지는 않는 모습인데요',\n",
              " '이 얘기 함께 듣고 오시겠습니다',\n",
              " '제가 실제로 확인하기도 했고요',\n",
              " '윤석열 변수가 재보궐선거에 영향이 어느 정도나 있을 거라고 보십니까',\n",
              " '김근식 굉장한 영향의 변수가 되죠',\n",
              " '앵커 부인하려 해도 부인할 수 없는 상수라고 보셨는데요',\n",
              " '조상호 부대변인은 어떻게 보십니까',\n",
              " '그러면 저희들 입장에서 나쁘지 않죠',\n",
              " '그래서 오히려 본인과는 가장 자연스럽게 연락할 수 있을 것이다',\n",
              " '특히 과거에 적폐수사 관련해서 진행하면서 서로 인연들이 있었거든요',\n",
              " '그러니까 편하게 통화할 거다 이 말을 한 것이라고 봅니다',\n",
              " '인터뷰는 한 시간 넘게 이어졌다',\n",
              " '“북한은 대미외교를 복원하기 위해 매우 애쓰고 있다',\n",
              " '당시 스웨덴 스톡홀름 실무회담도 이 부서에서 관리했다',\n",
              " '“최선희는 북미회담과 대미관계를 누구보다도 잘 아는 전문가다',\n",
              " '” 대북제재로 해외공관들도 많이 어려웠을 것 같다',\n",
              " '코로나가 풀리면 더 감축될 것이다',\n",
              " '대외영역이 그만큼 좁아진 것이다',\n",
              " '“한 마디로 제로다',\n",
              " '비핵화할 것 같았으면 왜 수많은 아사자를 내면서까지 핵에 집착했을까',\n",
              " '한반도 비핵화의 개념 자체를 명확히 해야한다',\n",
              " '이 정권의 최대 과제는 제재 해제와 핵 군축이다',\n",
              " '최대우방국이 한국과 수교한다는 걸 그때 처음으로 알게 됐다',\n",
              " '삐라 종이 질이 좋아서 태우려 해도 불도 붙지 않았다',\n",
              " '“생각보다 많이 의식한다',\n",
              " '하지만 통일을 포기하는 건 남북한 모두 발전을 포기하는 것이다',\n",
              " '시간을 가지고 차이를 줄여가길 바란다',\n",
              " '당연히 B기업이 더 비싼 기업입니다',\n",
              " '이커머스 기업은 어떨까요',\n",
              " '이제 본론으로 돌아와 쿠팡의 몸값을 들여다볼까요',\n",
              " '주가는 기대감으로 형성되니 과거가 아닌 미래의 실적 추정치를 활용합니다',\n",
              " '두 회사의 사업모델이 다르기 때문입니다',\n",
              " '쿠팡은 매출의 대부분을 직매입을 토대로 한 로켓배송으로 올리고 있습니다',\n",
              " '또 물리적인 물류 인프라 즉 창고와 배송인력도 확보해 왔습니다',\n",
              " '판매자들이 네이버 안에서 장사를 하도록 하는 플랫폼이죠',\n",
              " '네이버의 거래액을 구성하는 또 다른 사업은 네이버쇼핑입니다',\n",
              " '하지만 가격비교 플랫폼은 오픈마켓보다도 밸류에이션 멀티플이 낮습니다',\n",
              " '네이버와 쿠팡의 차이가 사라지고 있다',\n",
              " '쿠팡의 강점인 풀필먼트판매자의 배송 포장 재고관리 대행 서비스도 시작했어요',\n",
              " '심지어는 이륜차 물류 인프라도 묵묵히 확장해 왔습니다',\n",
              " '이밖에도 최근들어 이마트와의 지분 교환 얘기도 나오고 있네요',\n",
              " '네이버 소비자들이 느끼는 쿠팡의 가장 큰 장점은 빠른 배송입니다',\n",
              " '소비자 입장에서 느낄 차이가 점점 줄어들 것입니다',\n",
              " '차기 시즌은 아직 정해지지 않았다',\n",
              " '인용보도 시 출처를 밝혀주시기 바랍니다',\n",
              " '그런데 실체가 이게 다냐',\n",
              " '회의적인 시선이 많죠',\n",
              " '최경영  불행 중 다행이다',\n",
              " '그런데 다만 정부의 발표가 매우 부적절한 부분은 있습니다',\n",
              " '그런데 그런 발표 방식은 좀 문제가 있다고 생각하고 있습니다',\n",
              " '최경영  차명은 안 되잖아요',\n",
              " '최경영  뭐 친인척이다 부인이다 이런 경우는 확인이 됩니까',\n",
              " '최경영  그래야 되죠',\n",
              " '최경영  이게 법 때문에 그런 겁니까',\n",
              " '최경영  법 때문에 그런 겁니까',\n",
              " '개인정보보호법 때문에 개개인의 모두 정보 동의를 받아야 합니다',\n",
              " '이건 행정적인 조사입니다',\n",
              " '그리고 차명거래 이 부분은 사실상 강제수사 영역으로 넘어가요',\n",
              " '최경영  쏘쏘 님이라는 분이 이런 문자를 주셨는데요',\n",
              " '“법무사무실에 근무하는 사람입니다',\n",
              " '주민번호로 근저당권자가 LH나 국토부 직원인지 확인 가능하고요',\n",
              " '은행 대출인 경우 채무자가 실제 소유주일 확률이 높고요',\n",
              " '그것도 조사해야 합니다',\n",
              " '그런데 이런 것도 같이 조사를 정부가 했는지 모르겠습니다',\n",
              " '현재까지 밝혀진 내용은 거기까지입니다',\n",
              " '최경영  신도시 쪽에 토지뿐만 아니고 어떻게 생각하세요',\n",
              " '왜냐하면 재건축도 인허가를 해야 하지 않습니까',\n",
              " '공공 공직자가 재건축 재개발 지역에 들어가기는 쉽지 않습니다',\n",
              " '워낙 거기는 금액 단위가 큽니다',\n",
              " '훨씬 더 큰 영역이고요',\n",
              " '이런 계산이 나오거든요',\n",
              " '최경영  공직자는 LH 같은 경우에는 공사 직원이잖아요',\n",
              " '그러니까 공직자의 범위를 확대시키는 거죠',\n",
              " '최경영  그렇게 되는 겁니까',\n",
              " '그러면 앞으로 보완해야 할 입법 있지 않습니까',\n",
              " '최경영  앞으로 어떻게 해야 합니까',\n",
              " '최경영  헌법상 불가능해요',\n",
              " '그런데 투기 환수는 행정적으로 가능할 것 같아요',\n",
              " '최경영  또 다른 영역이다',\n",
              " '토지거래신고제는 어떻게 생각하십니까',\n",
              " '다 입법이 됐으면 좋겠습니다',\n",
              " '오늘 말씀 감사하고요',\n",
              " '민변 민생경제위원장 김태근 변호사와 이야기 나눠봤습니다',\n",
              " '전흥문은 근처의 풀밭으로 숨어들었고 강용휘는 금천교 쪽으로 도망쳤다',\n",
              " '그날 밤 정조의 암살 시도는 이렇게 실패로 끝나고 말았다',\n",
              " '하지만 이번에는 문을 통해 들어갈 수는 없었다',\n",
              " '궁궐로 통하는 모든 문의 경호가 강화됐기 때문이었다',\n",
              " '전흥문이 체포되자 정조는 심야에 친국을 명령했다',\n",
              " '장소는 창덕궁의 숙장문肅章門이었다',\n",
              " '하지만 노론이 오군영을 장악한 탓에 왕권을 위협했고 역모사건도 잇달았다',\n",
              " '즉위 후 정조는 군제개혁이 시급하다고 판단했다',\n",
              " '신도시 정책은 일정 구역을 지정하고 전면 정비하는 방식으로 이뤄진다',\n",
              " '때로는 이들의 반대로 사업이 차질을 빚기도 한다',\n",
              " '그 기준은 정부가 신도시를 발표하는 당일인 공람공고일이다',\n",
              " '그 이전부터 땅을 소유하고 있어야 보상 대상이 된다',\n",
              " '보상 방법에는 현금 보상 외에도 대토代土제도가 있다',\n",
              " '정부는 작년엔 협의양도인택지 대상자를 아파트 특별공급 대상으로도 편입시켜줬다',\n",
              " '이 외에도 각종 이주대책 생활대책 등이 이뤄진다',\n",
              " '변창흠 국토부 장관도 좋은 의견이라며 공감을 표시했다',\n",
              " '정부는 이 같은 방안을 다각도로 검토 중인 것으로 전해졌다',\n",
              " '일간스포츠 안민구 라면 업계가 때 이른 비빔면 경쟁에 돌입했다',\n",
              " '경쟁사보다 한발 앞선 신제품 출시로 시장점유율을 확대하겠다는 전략으로 풀이된다',\n",
              " '제품 이름은 세 가지 주재료의 앞글자를 따서 지었다',\n",
              " 'TV 광고는 이날부터 전파를 탄다',\n",
              " '좋은 이미지로 동네가 소문 나야지이게 뭔 꼴인지 모르겠다',\n",
              " '주로 쓰레기매립장 관리가 되지 않는 듯한 비닐하우스 등이 즐비했다',\n",
              " 'LH 때문 아니냐라고 말했다',\n",
              " '온라인 커뮤니티에서도 정부 발표를 믿을 수 없다는 분위기다',\n",
              " '이같은 결과는 어느 정도 예견됐다',\n",
              " '당연히 실명으로 이뤄진 거래만 대상이었다',\n",
              " '“정부조사는 믿을 수 없다”는 부정 여론만 자극한 꼴이 됐다',\n",
              " '나아가 토지중심’의 거래도 가능할 것으로 보인다',\n",
              " '그럼에도 입점 업체의 플랫폼에 대한 의존도는 상당히 컸다',\n",
              " '특히 해당 플랫폼 유형별로 답변 비율이 크게 갈렸다',\n",
              " '창업시 판로확보 수단으로서 플랫폼에 대한 의존도가 높아지는 것이다',\n",
              " '직접 재배할 수 없다면 다른 채소에 분산투자하는 것도 방법이다',\n",
              " '다행히 달래·냉이·참나물 등 향긋한 봄나물이 대안인 시절이니까',\n",
              " '“어렸을 때 스페인에 놀러 간 적이 있어요',\n",
              " '그게 스페인 대파칼솟인 걸 알고 깜짝 놀랐었어요',\n",
              " '말이 좋아 푸드 트럭이지 포장마차나 다름없었죠',\n",
              " '다행히 페이스북·트위터·인스타그램 등으로 소문이 나 장사가 잘됐죠',\n",
              " '이런 그에게 대파를 대신할 수 있는 대안 요리법을 물었다',\n",
              " '“참나물은 고기 특히 닭·오리 같은 가금류와 잘 어울려요',\n",
              " '참나물을 양념장에 버무리고 파닭처럼 닭과 함께 곁들여 먹으면 된다',\n",
              " '채 셰프는 냉이와 달래를 제안했다',\n",
              " '길쭉하고 아삭한 식감은 파와 비슷하면서도 더욱 향긋하다',\n",
              " '봄 기운 가득한 골뱅이를 만날 기회다',\n",
              " '최근에는 속초시 홍보대사까지 맡은 그의 이야기를 들어봤다',\n",
              " '너무 만족하고 속상해하는 팬분들이 많으셔서 걱정이에요',\n",
              " '감사하고 사랑한다고 팬들에게 꼭 말하고 싶어요',\n",
              " '사실 신승태는 어릴 때부터 될성부른 떡잎이었다',\n",
              " '특별한 이유 없이 그냥 춤과 노래가 좋았다고 했다',\n",
              " '어린 시절 사진을 보면 온통 춤추고 있는 모습뿐일 정도다',\n",
              " '그는 “무엇보다 갯마당 선생님들이 너무 멋있고 동경의 대상이었다',\n",
              " '신승태는 “트로트는 어릴 적부터도 하고 싶었던 음악이다',\n",
              " '신승태는 “사실 그리 모범적인 학생은 아니었던 것 같다',\n",
              " '이 인터뷰를 통해 너무너무 감사하다고 말씀 드리고 싶다”고 전했다',\n",
              " '강원도의 힘을 보여주겠다”고 했다',\n",
              " '한국토지주택공사LH 직원들의 땅 투기 의혹이다',\n",
              " '한두 번 해본 솜씨가 아니다',\n",
              " '그곳에는 생소한 왕버들 나무를 촘촘히 심어놨다',\n",
              " '토지개발에 따른 보상을 노린 것 말고 다른 설명은 불가능하다',\n",
              " '어쩌면 이번에 걸린 사람들은 상대적으로 순진했다고 해야 할지 모른다',\n",
              " '이번 LH 투기 의혹은 더 참을 수 없는 결정타였다',\n",
              " '그러니 정의로운 결과는 도저히 기대할 수 없게 됐다',\n",
              " '또다시 고양이에게 생선을 맡기는 어리석음을 범할 수는 없다',\n",
              " '비리를 저지른 다음에 부당한 이익을 환수하는 것만으로 충분하지 않다',\n",
              " '처음부터 비리를 저지를 틈을 주지 말아야 한다',\n",
              " '이번 기회에 LH의 역할과 기능을 진지하게 재검토해봐야 하는 이유다',\n",
              " 'LH의 핵심 사업 모델을 요약하면 땅장사’라고 할 수 있다',\n",
              " '심하게 말하면 이 법은 LH 특혜법’이라고 할 수 있다',\n",
              " '사유재산 침해나 보상가격을 둘러싼 논란이 끊이지 않았던 이유다',\n",
              " '이제는 최소화할 때가 됐다',\n",
              " '어쩌면 이번 LH 투기 의혹은 빙산의 일각에 불과할지 모른다',\n",
              " '변창흠 국토부 장관에겐 다시 한번 크게 실망했다',\n",
              " '집값 안정을 위해 대규모 주택공급이 필요하다는 것은 이해한다',\n",
              " '하지만 이 방법밖에 없는 건지 의문이다',\n",
              " '더 늦기 전에 문 대통령의 결단이 필요해 보인다',\n",
              " '이 장관을 보려고 산수유꽃 필 무렵이면 방방곡곡에서 상춘객이 몰려온다',\n",
              " '예년보다 한참 이르다는 화신花信을 듣고서였다',\n",
              " '산수유 마을 산수유나무는 잎보다 먼저 꽃을 피운다',\n",
              " '겉꽃잎이 먼저 열리고 속꽃잎이 불꽃놀이하듯 확 피어난다',\n",
              " '구례군 산동면의 마을들은 지리산 마루금을 병풍처럼 두른 산골 마을이다',\n",
              " '산동山洞이라는 이름이 산골 마을이라는 뜻이다',\n",
              " '이 개천이 서시천이다',\n",
              " '그러니까 산동의 마을들은 섬진강 지천 서시천 상류의 산골 마을이다',\n",
              " '이 마을들을 구례 산수유 마을’이라 부른다',\n",
              " '여느 마을의 소나무처럼 산수유나무가 흔한 마을들이다',\n",
              " '예부터 산수유 열매는 신비의 영약으로 통했다',\n",
              " '산동의 마을에서는 산수유 열매 팔아다 자식을 학교에 보냈다',\n",
              " '대학 나무’라는 별명이 그렇게 나왔다',\n",
              " '요즘에는 산수유 열매 시세가 예전 같지 않다',\n",
              " '값싼 중국산이 대거 수입되어서다',\n",
              " '대신 마을의 명성은 높아졌다',\n",
              " '이번엔 꽃 덕분이다',\n",
              " '산수유꽃은 가장 먼저 꽃망울을 터뜨리는 봄꽃이다',\n",
              " '산수유 마을이 노란 기운으로 아득하면 비로소 봄이 시작되었다는 뜻이다',\n",
              " '난감한 계절 올해는 산수유꽃이 일찍 피었다',\n",
              " '올봄 산수유꽃은 왜 일찍 피었을까',\n",
              " '겨울이 안 추웠다',\n",
              " '구례군청에 확인하니 겨울 평균기온이 예년보다 높았다',\n",
              " '반곡마을은 이미 상춘객으로 붐볐다',\n",
              " '십수 년 전에는 상위마을이 제일 먼저 알려졌었다',\n",
              " '노랗게 반짝이는 산촌과 고즈넉한 돌담길 풍경이 그윽했다',\n",
              " '상위마을 다음에는 현천마을이 유명세를 치렀다',\n",
              " '인증 사진 신흥 명소다',\n",
              " '서너 해 전부터는 상위마을 아랫동네인 반곡마을이 떴다',\n",
              " '올해도 산수유꽃 축제는 열리지 않는다',\n",
              " '행사장 출입을 막은 건 아니지만 시끌벅적한 이벤트는 다 취소했다',\n",
              " '외려 호젓하게 꽃놀이를 즐길 수 있겠다 싶기도 하다',\n",
              " '지역 상인에겐 타격이 크다',\n",
              " '전남 구례 산동 산수유 마을 올해도 봄이 오셨다',\n",
              " '새 계절이 시작되었다',\n",
              " '이 당연한 일이 새삼스럽게 느껴지는 건 왜일까',\n",
              " '봄이 돌아와 줘 고맙고 예전처럼 맞아주지 못해 미안하다',\n",
              " '전국적으로 집단감염이 이어지고 있는 영향이 큰데요',\n",
              " '아침까지는 내륙 일부 지방에서 안개가 끼어 교통안전에도 주의해야겠다',\n",
              " '강원영동과 경상동해안은 대체로 흐리겠다',\n",
              " '전남남해안과 일부 내륙에도 바람이 강하게 부는 곳이 있겠다',\n",
              " '전 LH전북본부장의 주거지에서는 유서 등이 발견됐다',\n",
              " '그는 전북에서 본부장으로 근무할 때 바람직하지 않은 일을 했다',\n",
              " '국민에게 죄송하다는 내용의 유서를 남긴 것으로 전해졌다',\n",
              " '투기 전모를 다 드러내야 한다고 말했다',\n",
              " '명운을 걸고 수사해야 한다고 주문했다',\n",
              " '미세먼지 농도는 수도권·세종·충남은 나쁨 그 밖의 권역은 좋음보통으로 예상된다',\n",
              " '그의 주거지에선 유서 등이 발견됐다',\n",
              " '다만 일부 상황에 대한 예외를 적용한다',\n",
              " '마켓컬리 마켓컬리를 운영하는 컬리가 IPO기업공개를 추진한다',\n",
              " '뉴욕 증시 상장 가능성도 열어뒀다',\n",
              " '이 과정에서 접촉자가 수백 명에 달했다',\n",
              " '김경수 경남지사가 브리핑을 하고 있다',\n",
              " '이후 변 장관은 문대통령에게 사의를 표명한 것으로 알려졌다',\n",
              " '강원 영동과 경상 동해안은 대체로 흐리겠다',\n",
              " '미세먼지와 안개가 겹쳐 시야도 짧아질 수 있다고 당부했다',\n",
              " '수도권·충남·제주권은 나쁨 수준의 대기질이 예상된다',\n",
              " '그밖의 권역은 보통’ 수준의 대기질을 보이겠다',\n",
              " '다만 일부 예외를 허용했다',\n",
              " '직계가족 모임 기준은 강화됐다',\n",
              " '가족 간 감염이 계속 발생하고 있어서다',\n",
              " '친환경 차량전기·하이브리드·수소차 등은 제외된다',\n",
              " '일부 시설과 모임에 대한 방역 수칙을 강화했다',\n",
              " '목욕탕에서 세신사와 대화하는 것도 금지된다',\n",
              " '대신 그동안 폐쇄했던 사우나·찜질시설은 운영할 수 있다',\n",
              " '인원 제한은 없었다',\n",
              " '돌잔치 전문점은 방역 수칙이 완화된다',\n",
              " '앞으로는 전문점에서 하는 돌잔치는 결혼·장례식과 같은 행사로 분류된다',\n",
              " '일반 음식점에서 하는 돌잔치는 인정되지 않는다',\n",
              " '시간에 상관없이 영업할 수 있다',\n",
              " '보급형 스마트폰들이 다시 대거 등장하고 있습니다',\n",
              " '국내에선 단연 삼성전자 제품들이 눈에 띕니다',\n",
              " '삼성전자는 다작으로 대응하고 있습니다',\n",
              " '전체적인 제품 크기는 기존 A시리즈들과 큰 차이는 없습니다',\n",
              " '삼성페이와 방수방진은 지원하지 않습니다',\n",
              " '간단한 웹서핑 중에도 반응 속도가 느립니다',\n",
              " 'GPS로 구동되는 내비게이션은 실행됩니다',\n",
              " '야간 환경에서도 제법 괜찮은 사진을 구현합니다',\n",
              " '성능 욕심 없이 저렴한 전화기를 원하는 소비자에게 필요한 상품이죠',\n",
              " '갤럭시 브랜드를 공유한다는 게 가장 큰 장점인 스마트폰입니다',\n",
              " '이는 서울 송파구 내 가장 높은 시세를 자랑한다',\n",
              " '법원에 들어가는 순간까지 반성의 모습은 전혀 보이지 않았다고 하는데요',\n",
              " '그리고 나흘 뒤 이곳 인천지법에서는 A씨에 대한 구속영장실질심사가 열렸습니다',\n",
              " 'A씨  살인 왜 하셨나요억울해서요',\n",
              " '피해자에게 하시고 싶은 말 있으세요',\n",
              " '그런데 교육청이 통합학급에 대해선 별다른 지침을 내놓지 않은 것이다',\n",
              " '교육당국의 부실한 운영 지침이 장애학생 교육의 사각지대를 만든다는 지적이다',\n",
              " '서울 여의도 순복음교회에서 대면 예배가 진행되고 있다',\n",
              " '특히 교인 간 모임을 통해 감염이 더욱 확산하고 있다',\n",
              " '하지만 예배 후 가진 모임이 문제가 됐다',\n",
              " '유통시장 점유율을 확대하는 데 공격적으로 나서겠다는 얘기다',\n",
              " '그러면서 “공격적이고 계획적으로 투자를 지속해야 한다”고 목소리를 높였다',\n",
              " '촘촘한 전국 당일배송망을 구축하려면 더 많은 기지가 필요하다는 이유에서다',\n",
              " '쿠팡 임직원도 상당한 차익을 거둘 전망이다',\n",
              " '전국민의 분노와 상대적 박탈감을 꾹꾹 눌러담은 표현입니다',\n",
              " '도 커지고 있습니다',\n",
              " '우선 정부기관인 금융위원회는 자본시장법과 공무원 행동강령의 적용을 받습니다',\n",
              " '공공기관에서 해제된 한국거래소는 어떨까요',\n",
              " '사기업은 좀 다를까 싶지만 그렇지도 않습니다',\n",
              " '그러나 조사는 진행되지 못했다',\n",
              " '이번에도 여야의 엇갈린 분위기 속 불발 조짐이 감지되고 있다',\n",
              " '김 대표 대행은\\xa0민주당은 전수조사가 준비돼 있다',\n",
              " '국민의힘도 참여해달라고 거듭 요청했다',\n",
              " '시민단체들은 이번 기회에 반드시 국회의원 전수조사가 이뤄져야 한다는 입장이다',\n",
              " '선호 공기업인 LH의 이미지가 심각하게 훼손됐기 때문이다',\n",
              " 'LH에 대한 혁신안도 이와 함께 논의할 전망이다',\n",
              " '경찰 수사도 가속도가 붙을 전망이다',\n",
              " '앵커 LH 투기 의혹으로나라 전체가 시끄럽죠',\n",
              " '공급 정책의 차질은다시 부동산 시장 불안으로 이어질 수 있죠',\n",
              " '관련 현안 한국투자증권 김규정 자산승계연구소장과 함께 얘기 나눠보겠습니다',\n",
              " '부동산 전문가이시니까 여러 가지 물어볼 게 많습니다',\n",
              " '생각보다는 맹탕조사가 아니냐 이런 비판도 있어요',\n",
              " '앵커 그런 곳이 전국에 산재해 있더라고요',\n",
              " '앵커 혹시 주변에 아는 분들 중에 LH 직원들 있었습니까',\n",
              " '거론되는 곳들이 몇 군데가 사실 있습니다',\n",
              " '이런 곳에서도 투기 정황이 드러나지 않을까 우려하는 목소리도 있거든요',\n",
              " '앵커 확대 조사할 필요가 있다',\n",
              " '김규정 그런 곳들을 의심해 볼 수 있습니다',\n",
              " '그게 가능할지는 사실 좀 현재로써는 의구심도 듭니다',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQDZ-X6iiT1U",
        "outputId": "bc7b0771-6f1c-4814-cfc7-e137b8e5cbe8"
      },
      "source": [
        "!pip install keybert\n",
        "!pip install sentence-transformers==0.3.0\n",
        "!pip install transformers==3.0.2"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keybert\n",
            "  Downloading https://files.pythonhosted.org/packages/5d/f4/65853bfc2ded495af3341a3f8938f17799d15a65be0150cb57774e1fb59f/keybert-0.2.0.tar.gz\n",
            "Collecting sentence-transformers>=0.3.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/aa/f672ce489063c4ee7a566ebac1b723c53ac0cea19d9e36599cc241d8ed56/sentence-transformers-1.0.4.tar.gz (74kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.7/dist-packages (from keybert) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from keybert) (1.19.5)\n",
            "Collecting transformers<5.0.0,>=3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 14.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (1.8.1+cu101)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 52.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2->keybert) (1.0.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 50.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 50.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (3.10.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers>=0.3.8->keybert) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (7.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (3.4.1)\n",
            "Building wheels for collected packages: keybert, sentence-transformers, sacremoses\n",
            "  Building wheel for keybert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keybert: filename=keybert-0.2.0-cp37-none-any.whl size=10599 sha256=a810321b6d78f41b67af0f8cd52019e1ee39e90210d672207524692302567a07\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/d7/16/04bab6677a4dfa9fd8ab2b350bac915d60f5378b83d6f5a372\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-1.0.4-cp37-none-any.whl size=114307 sha256=176b52a9e89be78c4880ae85c5e140053f021f66d6eef31e259ae762b5ef646e\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/ea/89/d0d2e013d951b6d23270aa9ca4018b82632ab7cd933c331316\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=6df7077fdf925403e6fa3ce54f2fd61e8e247b2ec3f2baef09d0eb220adde5a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n",
            "Successfully built keybert sentence-transformers sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers, sentencepiece, sentence-transformers, keybert\n",
            "Successfully installed keybert-0.2.0 sacremoses-0.0.44 sentence-transformers-1.0.4 sentencepiece-0.1.95 tokenizers-0.10.2 transformers-4.5.1\n",
            "Collecting sentence-transformers==0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/23/833e0620753a36cb2f18e2e4a4f72fd8c49c123c3f07744b69f8a592e083/sentence-transformers-0.3.0.tar.gz (61kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: transformers>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.3.0) (4.5.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.3.0) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.3.0) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.3.0) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.3.0) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.3.0) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.3.0) (3.2.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (0.0.44)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (0.10.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (3.10.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.1->sentence-transformers==0.3.0) (3.7.4.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers==0.3.0) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers==0.3.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.0.2->sentence-transformers==0.3.0) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers>=3.0.2->sentence-transformers==0.3.0) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers>=3.0.2->sentence-transformers==0.3.0) (3.4.1)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.3.0-cp37-none-any.whl size=86754 sha256=b5abd4ac22e2dfde21ffbc2362cfc6ec688db5c1e4fbb0400fab7a4332c90b9d\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/23/85/85d6a9a6c68f0625a1ecdaad903bb0a78df058c10cf74f9de4\n",
            "Successfully built sentence-transformers\n",
            "\u001b[31mERROR: keybert 0.2.0 has requirement sentence-transformers>=0.3.8, but you'll have sentence-transformers 0.3.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: sentence-transformers\n",
            "  Found existing installation: sentence-transformers 1.0.4\n",
            "    Uninstalling sentence-transformers-1.0.4:\n",
            "      Successfully uninstalled sentence-transformers-1.0.4\n",
            "Successfully installed sentence-transformers-0.3.0\n",
            "Collecting transformers==3.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (4.41.1)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.1.95)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (20.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (1.19.5)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/59/68c7e3833f535615fb97d33ffcb7b30bbf62bc7477a9c59cd19ad8535d72/tokenizers-0.8.1rc1-cp37-cp37m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 38.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.0.44)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.2) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2020.12.5)\n",
            "\u001b[31mERROR: keybert 0.2.0 has requirement sentence-transformers>=0.3.8, but you'll have sentence-transformers 0.3.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Found existing installation: tokenizers 0.10.2\n",
            "    Uninstalling tokenizers-0.10.2:\n",
            "      Successfully uninstalled tokenizers-0.10.2\n",
            "  Found existing installation: transformers 4.5.1\n",
            "    Uninstalling transformers-4.5.1:\n",
            "      Successfully uninstalled transformers-4.5.1\n",
            "Successfully installed tokenizers-0.8.1rc1 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ug1iNzVNQlU"
      },
      "source": [
        "## keyword 추출을 위한 keyBERT 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLXE7zoSieJg",
        "outputId": "250a0477-7a42-4d5e-915f-58aeb7341023"
      },
      "source": [
        "from keybert import KeyBERT\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "sentence_model = SentenceTransformer(\"xlm-r-large-en-ko-nli-ststb\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.80G/1.80G [01:33<00:00, 19.1MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7q8egfDPoTU"
      },
      "source": [
        "key_model = KeyBERT(model=sentence_model)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBN_3lM8wrqN"
      },
      "source": [
        "def extract_key(sentence):\n",
        "    #print(s)\n",
        "    top_n= int(len(sentence.split())/3)\n",
        "    keys = key_model.extract_keywords(sentence,top_n=top_n)\n",
        "    x= {}\n",
        "    for k,p in keys:\n",
        "        x[k] = sentence.lower().find(k)\n",
        "        #print(x[k],k)\n",
        "    keys = list(dict(sorted(x.items(), key=lambda item: item[1])).keys())\n",
        "    return keys\n",
        "\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZp15IHCzYTr",
        "outputId": "8a108be8-28ff-4d5b-f377-3f661d757b7c"
      },
      "source": [
        "i = 200\n",
        "[' '.join(extract_key(c_sentences[i])),c_sentences[i]]"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['한강야생조류생태공원', '한폭의 그림 같은 한강야생조류생태공원 모습']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaqtMJycNZqL"
      },
      "source": [
        "## 한국어 tokenizer 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9eKRmmQjr6z"
      },
      "source": [
        "# Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team and Jangwon Park\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\" Tokenization classes for KoBert model.\"\"\"\n",
        "\n",
        "\n",
        "import logging\n",
        "import os\n",
        "import unicodedata\n",
        "from shutil import copyfile\n",
        "\n",
        "from transformers import PreTrainedTokenizer\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "VOCAB_FILES_NAMES = {\"vocab_file\": \"tokenizer_78b3253a26.model\",\n",
        "                     \"vocab_txt\": \"vocab.txt\"}\n",
        "\n",
        "PRETRAINED_VOCAB_FILES_MAP = {\n",
        "    \"vocab_file\": {\n",
        "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/tokenizer_78b3253a26.model\",\n",
        "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/tokenizer_78b3253a26.model\",\n",
        "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/tokenizer_78b3253a26.model\"\n",
        "    },\n",
        "    \"vocab_txt\": {\n",
        "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/vocab.txt\",\n",
        "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/vocab.txt\",\n",
        "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/vocab.txt\"\n",
        "    }\n",
        "}\n",
        "\n",
        "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n",
        "    \"monologg/kobert\": 512,\n",
        "    \"monologg/kobert-lm\": 512,\n",
        "    \"monologg/distilkobert\": 512\n",
        "}\n",
        "\n",
        "PRETRAINED_INIT_CONFIGURATION = {\n",
        "    \"monologg/kobert\": {\"do_lower_case\": False},\n",
        "    \"monologg/kobert-lm\": {\"do_lower_case\": False},\n",
        "    \"monologg/distilkobert\": {\"do_lower_case\": False}\n",
        "}\n",
        "\n",
        "SPIECE_UNDERLINE = u'▁'\n",
        "\n",
        "\n",
        "class KoBertTokenizer(PreTrainedTokenizer):\n",
        "    \"\"\"\n",
        "        SentencePiece based tokenizer. Peculiarities:\n",
        "            - requires `SentencePiece <https://github.com/google/sentencepiece>`_\n",
        "    \"\"\"\n",
        "    vocab_files_names = VOCAB_FILES_NAMES\n",
        "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n",
        "    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n",
        "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            vocab_file,\n",
        "            vocab_txt,\n",
        "            do_lower_case=False,\n",
        "            remove_space=True,\n",
        "            keep_accents=False,\n",
        "            bos_token=\"[SOS]\",\n",
        "            eos_token=\"[EOS]\",\n",
        "            unk_token=\"[UNK]\",\n",
        "            sep_token=\"[SEP]\",\n",
        "            pad_token=\"[PAD]\",\n",
        "            cls_token=\"[CLS]\",\n",
        "            mask_token=\"[MASK]\",\n",
        "            **kwargs):\n",
        "        super().__init__(\n",
        "            bos_token=bos_token,\n",
        "            eos_token=eos_token,\n",
        "            unk_token=unk_token,\n",
        "            sep_token=sep_token,\n",
        "            pad_token=pad_token,\n",
        "            cls_token=cls_token,\n",
        "            mask_token=mask_token,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "        # Build vocab\n",
        "        self.token2idx = dict()\n",
        "        self.idx2token = []\n",
        "        with open(vocab_txt, 'r', encoding='utf-8') as f:\n",
        "            for idx, token in enumerate(f):\n",
        "                token = token.strip()\n",
        "                self.token2idx[token] = idx\n",
        "                self.idx2token.append(token)\n",
        "\n",
        "        try:\n",
        "            import sentencepiece as spm\n",
        "        except ImportError:\n",
        "            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n",
        "                           \"pip install sentencepiece\")\n",
        "\n",
        "        self.do_lower_case = do_lower_case\n",
        "        self.remove_space = remove_space\n",
        "        self.keep_accents = keep_accents\n",
        "        self.vocab_file = vocab_file\n",
        "        self.vocab_txt = vocab_txt\n",
        "\n",
        "        self.sp_model = spm.SentencePieceProcessor()\n",
        "        self.sp_model.Load(vocab_file)\n",
        "\n",
        "        self.token2idx[bos_token] = len(self.token2idx)\n",
        "        self.idx2token.append(bos_token)\n",
        "        self.token2idx[eos_token] = len(self.token2idx)\n",
        "        self.idx2token.append(eos_token)   \n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return len(self.idx2token)\n",
        "\n",
        "    def get_vocab(self):\n",
        "        return dict(self.token2idx, **self.added_tokens_encoder)\n",
        "\n",
        "    def __getstate__(self):\n",
        "        state = self.__dict__.copy()\n",
        "        state[\"sp_model\"] = None\n",
        "        return state\n",
        "\n",
        "    def __setstate__(self, d):\n",
        "        self.__dict__ = d\n",
        "        try:\n",
        "            import sentencepiece as spm\n",
        "        except ImportError:\n",
        "            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n",
        "                           \"pip install sentencepiece\")\n",
        "        self.sp_model = spm.SentencePieceProcessor()\n",
        "        self.sp_model.Load(self.vocab_file)\n",
        "\n",
        "    def preprocess_text(self, inputs):\n",
        "        if self.remove_space:\n",
        "            outputs = \" \".join(inputs.strip().split())\n",
        "        else:\n",
        "            outputs = inputs\n",
        "        outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')\n",
        "\n",
        "        if not self.keep_accents:\n",
        "            outputs = unicodedata.normalize('NFKD', outputs)\n",
        "            outputs = \"\".join([c for c in outputs if not unicodedata.combining(c)])\n",
        "        if self.do_lower_case:\n",
        "            outputs = outputs.lower()\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def _tokenize(self, text, return_unicode=True, sample=False):\n",
        "        \"\"\" Tokenize a string. \"\"\"\n",
        "        text = self.preprocess_text(text)\n",
        "\n",
        "        if not sample:\n",
        "            pieces = self.sp_model.EncodeAsPieces(text)\n",
        "        else:\n",
        "            pieces = self.sp_model.SampleEncodeAsPieces(text, 64, 0.1)\n",
        "        new_pieces = []\n",
        "        for piece in pieces:\n",
        "            if len(piece) > 1 and piece[-1] == str(\",\") and piece[-2].isdigit():\n",
        "                cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))\n",
        "                if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n",
        "                    if len(cur_pieces[0]) == 1:\n",
        "                        cur_pieces = cur_pieces[1:]\n",
        "                    else:\n",
        "                        cur_pieces[0] = cur_pieces[0][1:]\n",
        "                cur_pieces.append(piece[-1])\n",
        "                new_pieces.extend(cur_pieces)\n",
        "            else:\n",
        "                new_pieces.append(piece)\n",
        "\n",
        "        return new_pieces\n",
        "\n",
        "    def _convert_token_to_id(self, token):\n",
        "        \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\n",
        "        return self.token2idx.get(token, self.token2idx[self.unk_token])\n",
        "\n",
        "    def _convert_id_to_token(self, index, return_unicode=True):\n",
        "        \"\"\"Converts an index (integer) in a token (string/unicode) using the vocab.\"\"\"\n",
        "        return self.idx2token[index]\n",
        "\n",
        "    def convert_tokens_to_string(self, tokens):\n",
        "        \"\"\"Converts a sequence of tokens (strings for sub-words) in a single string.\"\"\"\n",
        "        out_string = \"\".join(tokens).replace(SPIECE_UNDERLINE, \" \").strip()\n",
        "        return out_string\n",
        "\n",
        "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
        "        \"\"\"\n",
        "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
        "        by concatenating and adding special tokens.\n",
        "        A KoBERT sequence has the following format:\n",
        "            single sequence: [CLS] X [SEP]\n",
        "            pair of sequences: [CLS] A [SEP] B [SEP]\n",
        "        \"\"\"\n",
        "        if token_ids_1 is None:\n",
        "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "        sep = [self.sep_token_id]\n",
        "        return cls + token_ids_0 + sep + token_ids_1 + sep\n",
        "\n",
        "    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n",
        "        \"\"\"\n",
        "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
        "        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n",
        "        Args:\n",
        "            token_ids_0: list of ids (must not contain special tokens)\n",
        "            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n",
        "                for sequence pairs\n",
        "            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n",
        "                special tokens for the model\n",
        "        Returns:\n",
        "            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\n",
        "        \"\"\"\n",
        "\n",
        "        if already_has_special_tokens:\n",
        "            if token_ids_1 is not None:\n",
        "                raise ValueError(\n",
        "                    \"You should not supply a second sequence if the provided sequence of \"\n",
        "                    \"ids is already formated with special tokens for the model.\"\n",
        "                )\n",
        "            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n",
        "\n",
        "        if token_ids_1 is not None:\n",
        "            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n",
        "        return [1] + ([0] * len(token_ids_0)) + [1]\n",
        "\n",
        "    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n",
        "        \"\"\"\n",
        "        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n",
        "        A KoBERT sequence pair mask has the following format:\n",
        "        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
        "        | first sequence    | second sequence\n",
        "        if token_ids_1 is None, only returns the first portion of the mask (0's).\n",
        "        \"\"\"\n",
        "        sep = [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "        if token_ids_1 is None:\n",
        "            return len(cls + token_ids_0 + sep) * [0]\n",
        "        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n",
        "\n",
        "    def save_vocabulary(self, save_directory):\n",
        "        \"\"\" Save the sentencepiece vocabulary (copy original file) and special tokens file\n",
        "            to a directory.\n",
        "        \"\"\"\n",
        "        if not os.path.isdir(save_directory):\n",
        "            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\n",
        "            return\n",
        "\n",
        "        # 1. Save sentencepiece model\n",
        "        out_vocab_model = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_file\"])\n",
        "\n",
        "        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_model):\n",
        "            copyfile(self.vocab_file, out_vocab_model)\n",
        "\n",
        "        # 2. Save vocab.txt\n",
        "        index = 0\n",
        "        out_vocab_txt = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_txt\"])\n",
        "        with open(out_vocab_txt, \"w\", encoding=\"utf-8\") as writer:\n",
        "            for token, token_index in sorted(self.token2idx.items(), key=lambda kv: kv[1]):\n",
        "                if index != token_index:\n",
        "                    logger.warning(\n",
        "                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n",
        "                        \" Please check that the vocabulary is not corrupted!\".format(out_vocab_txt)\n",
        "                    )\n",
        "                    index = token_index\n",
        "                writer.write(token + \"\\n\")\n",
        "                index += 1\n",
        "\n",
        "        return out_vocab_model, out_vocab_txt\n",
        "\n",
        "    def tokenize_wl(self,sentence,max_length=128):\n",
        "        tokens = ['[SOS]']+self.tokenize(sentence)+['[EOS]']\n",
        "        '''\n",
        "        if len(tokens) < max_length:\n",
        "            tokens += ['[PAD]' for i in range(max_length-len(tokens))]\n",
        "        else:\n",
        "            tokens = tokens[0:max_length]\n",
        "            tokens[max_length-1] = \"[EOS]\"\n",
        "        '''    \n",
        "        #print(len(tokens))\n",
        "        #print(tokens)\n",
        "        return tokens\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0,
          "referenced_widgets": [
            "6f8eddf7466744a5a16dfcfa3afc66fd",
            "1a47f0bfd2154353b2b9ca49594f766a",
            "58d05b397c284cc09b4b6654f5a99ffb",
            "f984bdb8ac014a5e85b8fc699f677ce7",
            "37a75b428eee4ef996924ea2ca42875b",
            "58c413a61a9542768ee1d9ed2dc5c1b1",
            "7956c222f15949a4a3a1aa7d43bdd189",
            "3624517e797e44d09f795757561cbcd8",
            "02bd54328b484332ac5cf3b22bbbc366",
            "da39b2fe0aab41f28e9d0ff6d31e6496",
            "20ee16c3a5f44b989a51e97f103b5fe6",
            "e25c4c51417349b290929a10d3f4b75a",
            "3be600671be14e7bbc4d564c3925ea59",
            "9b7f63b0a9894e1e92412c6c7e53a723",
            "7e7e61f0e1a441d6b3652319c4b451bb",
            "36cce342dd1541c2bbc63e60b41d9d4b"
          ]
        },
        "id": "DkzG9Nraj0Wz",
        "outputId": "32d29a02-e661-46c1-e0c1-419f1072153b"
      },
      "source": [
        "pretraoned_kobert_model_name='monologg/kobert'\n",
        "tokenizer = KoBertTokenizer.from_pretrained(pretraoned_kobert_model_name)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f8eddf7466744a5a16dfcfa3afc66fd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=371391.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "02bd54328b484332ac5cf3b22bbbc366",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=77779.0, style=ProgressStyle(descriptio…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPwSs7rsTAiu",
        "outputId": "1176a03a-bcc1-407c-8341-4c33879f9c16"
      },
      "source": [
        "print(tokenizer.get_vocab())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'[UNK]': 0, '[PAD]': 1, '[CLS]': 2, '[SEP]': 3, '[MASK]': 4, '!': 5, \"!'\": 6, '!”': 7, '\"': 8, '#': 9, '$': 10, '%': 11, '%)': 12, '&': 13, '&#34;': 14, \"'\": 15, \"'(\": 16, \"',\": 17, '(': 18, '(0': 19, '(1': 20, '(10': 21, '(12': 22, '(15': 23, '(17': 24, '(18': 25, '(19': 26, '(2': 27, '(20': 28, '(23': 29, '(24': 30, '(25': 31, '(3': 32, '(4': 33, '(5': 34, '(6': 35, '(7': 36, '(8': 37, '(9': 38, '(?)': 39, ')': 40, \")'\": 41, '),': 42, ')’': 43, '*': 44, '+': 45, ',': 46, '-': 47, '----------------': 48, '-1': 49, '-2': 50, '-20': 51, '-3': 52, '-4': 53, '.': 54, '...': 55, '...\"': 56, \"...'\": 57, '...”': 58, '/': 59, '0': 60, '0%': 61, '0%)': 62, '0.0': 63, '0.00': 64, '0.1': 65, '0.1%': 66, '0.2%': 67, '0.3': 68, '0.3%': 69, '0.4%': 70, '0.5': 71, '0.5%': 72, '0.6': 73, '0.6%': 74, '0.7': 75, '0.7%': 76, '0.8': 77, '0.8%': 78, '00': 79, '000.0': 80, '00000': 81, '01': 82, '02': 83, '02-': 84, '03': 85, '04': 86, '05': 87, '06': 88, '07': 89, '08': 90, '09': 91, '0:00:00': 92, '1': 93, '1%': 94, '1%)': 95, '1)': 96, '1,000': 97, '1.3%': 98, '1.4%': 99, '1.5%': 100, '1.6': 101, '1.6%': 102, '1.7%': 103, '1.8': 104, '10': 105, '100': 106, '1000': 107, '11': 108, '12': 109, '13': 110, '14': 111, '15': 112, '16': 113, '17': 114, '18': 115, '19': 116, '1⁄2': 117, '1⁄4': 118, '2': 119, '2%': 120, '2)': 121, '2.0': 122, '2.3%': 123, '2.5': 124, '2.5%': 125, '2.8%': 126, '20': 127, '200': 128, '2000': 129, '2011': 130, '2012': 131, '2013': 132, '21': 133, '22': 134, '23': 135, '24': 136, '25': 137, '26': 138, '27': 139, '28': 140, '29': 141, '3': 142, '3%': 143, '30': 144, '300': 145, '3000': 146, '31': 147, '32': 148, '33': 149, '34': 150, '35': 151, '36': 152, '37': 153, '38': 154, '39': 155, '3⁄4': 156, '4': 157, '4%': 158, '4%)': 159, '40': 160, '400': 161, '4000': 162, '41': 163, '42': 164, '43': 165, '44': 166, '45': 167, '46': 168, '47': 169, '48': 170, '49': 171, '5': 172, '5%': 173, '5%)': 174, '5,000': 175, '50': 176, '500': 177, '5000': 178, '51': 179, '52': 180, '53': 181, '54': 182, '55': 183, '56': 184, '57': 185, '58': 186, '59': 187, '6': 188, '6%': 189, '60': 190, '600': 191, '6000': 192, '61': 193, '62': 194, '63': 195, '64': 196, '65': 197, '66': 198, '67': 199, '68': 200, '69': 201, '7': 202, '7%': 203, '7%)': 204, '7.0': 205, '7.5%': 206, '70': 207, '700': 208, '7000': 209, '71': 210, '72': 211, '73': 212, '74': 213, '75': 214, '76': 215, '77': 216, '78': 217, '79': 218, '8': 219, '8%': 220, '8%)': 221, '80': 222, '800': 223, '8000': 224, '81': 225, '82': 226, '83': 227, '84': 228, '85': 229, '86': 230, '87': 231, '88': 232, '89': 233, '9': 234, '9%': 235, '9%)': 236, '90': 237, '900': 238, '9000': 239, '91': 240, '92': 241, '93': 242, '94': 243, '95': 244, '96': 245, '97': 246, '98': 247, '99': 248, ':': 249, '://': 250, ':00': 251, ';': 252, '<': 253, '=': 254, '=\"\"': 255, '=\"\">': 256, '>': 257, '?': 258, '?\"': 259, '??': 260, '???': 261, '????': 262, '?”': 263, 'A': 264, 'AM': 265, 'AP': 266, 'AR': 267, 'AS': 268, 'AT': 269, 'B': 270, 'BC': 271, 'BO': 272, 'BS': 273, 'C': 274, 'CC': 275, 'CD': 276, 'CI': 277, 'D': 278, 'DB': 279, 'DC': 280, 'DI': 281, 'E': 282, 'ER': 283, 'EU': 284, 'F': 285, 'FC': 286, 'FI': 287, 'FIFA': 288, 'FTA': 289, 'G': 290, 'GB': 291, 'GDP': 292, 'GM': 293, 'H': 294, 'HD': 295, 'I': 296, 'IA': 297, 'IB': 298, 'IC': 299, 'II': 300, 'IM': 301, 'IN': 302, 'IP': 303, 'IS': 304, 'IT': 305, 'J': 306, 'K': 307, 'KB': 308, 'KBS': 309, 'KT': 310, 'L': 311, 'LED': 312, 'LG': 313, 'LPGA': 314, 'LS': 315, 'M': 316, 'MBC': 317, 'MBN': 318, 'MC': 319, 'MOU': 320, 'MS': 321, 'N': 322, 'NA': 323, 'NE': 324, 'NLL': 325, 'NS': 326, 'NTSB': 327, 'New': 328, 'O': 329, 'OC': 330, 'OS': 331, 'OSEN': 332, 'P': 333, 'PC': 334, 'PD': 335, 'PGA': 336, 'PI': 337, 'PM': 338, 'POP': 339, 'PS': 340, 'Q': 341, 'R': 342, 'S': 343, 'SBS': 344, 'SI': 345, 'SK': 346, 'SNS': 347, 'SP': 348, 'SS': 349, 'ST': 350, 'T': 351, 'TI': 352, 'TS': 353, 'TV': 354, 'The': 355, 'U': 356, 'V': 357, 'W': 358, 'X': 359, 'Y': 360, 'Z': 361, '[': 362, ']': 363, '^': 364, '_': 365, '`': 366, 'a': 367, 'ab': 368, 'ac': 369, 'ad': 370, 'al': 371, 'all': 372, 'am': 373, 'an': 374, 'ar': 375, 'as': 376, 'at': 377, 'ation': 378, 'ay': 379, 'b': 380, 'bp': 381, 'c': 382, 'ch': 383, 'cm': 384, 'co': 385, 'com': 386, 'ct': 387, 'd': 388, 'e': 389, 'ed': 390, 'el': 391, 'en': 392, 'ent': 393, 'er': 394, 'es': 395, 'est': 396, 'et': 397, 'f': 398, 'g': 399, 'go': 400, 'h': 401, 'ha': 402, 'ho': 403, 'http': 404, 'i': 405, 'ic': 406, 'id': 407, 'il': 408, 'in': 409, 'ing': 410, 'ir': 411, 'is': 412, 'it': 413, 'j': 414, 'k': 415, 'kW': 416, 'kg': 417, 'km': 418, 'kr': 419, 'l': 420, 'le': 421, 'lo': 422, 'm': 423, 'mm': 424, 'n': 425, 'net': 426, 'o': 427, 'ol': 428, 'on': 429, 'or': 430, 'ow': 431, 'p': 432, 'q': 433, 'quot': 434, 'r': 435, 'ra': 436, 're': 437, 'ri': 438, 'ro': 439, 's': 440, 'st': 441, 't': 442, 'ter': 443, 'th': 444, 'tion': 445, 'u': 446, 'ul': 447, 'um': 448, 'un': 449, 'ur': 450, 'us': 451, 'ut': 452, 'v': 453, 'ver': 454, 'w': 455, 'www': 456, 'x': 457, 'y': 458, 'z': 459, '{': 460, '|': 461, '}': 462, '~': 463, '~1': 464, '~20': 465, '~3': 466, '~5': 467, '~6': 468, '~8': 469, '¡': 470, '¤': 471, '§': 472, '\\xad': 473, '®': 474, '°': 475, '±': 476, '¶': 477, '·': 478, '¿': 479, 'Æ': 480, '×': 481, 'Ø': 482, 'ß': 483, 'æ': 484, '÷': 485, 'ø': 486, '́': 487, '̧': 488, 'μ': 489, 'ᄀ': 490, 'ᄋ': 491, 'ᄏ': 492, 'ᄒ': 493, 'ᅵ': 494, 'ᆞ': 495, '―': 496, '‘': 497, '’': 498, '’(': 499, '’,': 500, '“': 501, '”': 502, '”,': 503, '′': 504, '※': 505, '⁄': 506, '↑': 507, '→': 508, '↓': 509, '↓]': 510, '∼': 511, '─': 512, '───': 513, '│': 514, '├': 515, '┼': 516, '▁': 517, '▁\"': 518, '▁&': 519, \"▁'\": 520, \"▁'2013\": 521, '▁(': 522, '▁*': 523, '▁-': 524, '▁/': 525, '▁0': 526, '▁0.2': 527, '▁00:0': 528, '▁1': 529, '▁1%': 530, '▁1,2': 531, '▁1.5': 532, '▁10': 533, '▁10%': 534, '▁100': 535, '▁100%': 536, '▁1000': 537, '▁11': 538, '▁12': 539, '▁120': 540, '▁13': 541, '▁14': 542, '▁15': 543, '▁150': 544, '▁16': 545, '▁17': 546, '▁18': 547, '▁19': 548, '▁1990': 549, '▁1997': 550, '▁1998': 551, '▁1~2': 552, '▁2': 553, '▁20': 554, '▁20%': 555, '▁200': 556, '▁2000': 557, '▁2001': 558, '▁2002': 559, '▁2003': 560, '▁2004': 561, '▁2005': 562, '▁2006': 563, '▁2007': 564, '▁2008': 565, '▁2009': 566, '▁2010': 567, '▁2011': 568, '▁2012': 569, '▁2013': 570, '▁2013-0': 571, '▁2013.0': 572, '▁2013.07.1': 573, '▁2013.07.2': 574, '▁2014': 575, '▁2015': 576, '▁2016': 577, '▁2017': 578, '▁21': 579, '▁22': 580, '▁23': 581, '▁24': 582, '▁25': 583, '▁26': 584, '▁27': 585, '▁28': 586, '▁29': 587, '▁2~3': 588, '▁3': 589, '▁3.0': 590, '▁3.3': 591, '▁30': 592, '▁30%': 593, '▁300': 594, '▁3000': 595, '▁31': 596, '▁32': 597, '▁33': 598, '▁34': 599, '▁35': 600, '▁36': 601, '▁37': 602, '▁38': 603, '▁39': 604, '▁4': 605, '▁40': 606, '▁40%': 607, '▁400': 608, '▁45': 609, '▁48': 610, '▁5': 611, '▁50': 612, '▁50%': 613, '▁500': 614, '▁5000': 615, '▁55': 616, '▁6': 617, '▁60': 618, '▁60%': 619, '▁65': 620, '▁7': 621, '▁70': 622, '▁70%': 623, '▁8': 624, '▁80': 625, '▁80%': 626, '▁9': 627, '▁90': 628, '▁:': 629, '▁<': 630, '▁=': 631, '▁>': 632, '▁?': 633, '▁??': 634, '▁A': 635, '▁APP': 636, '▁B': 637, '▁C': 638, '▁CCTV': 639, '▁CEO': 640, '▁CES': 641, '▁CGV': 642, '▁CJ': 643, '▁D': 644, '▁E': 645, '▁ELS': 646, '▁ETF': 647, '▁F': 648, '▁FA': 649, '▁G': 650, '▁GS': 651, '▁H': 652, '▁HOT': 653, '▁Home': 654, '▁I': 655, '▁IT': 656, '▁J': 657, '▁JTBC': 658, '▁K': 659, '▁KB': 660, '▁KBO': 661, '▁KBS': 662, '▁KDB': 663, '▁KIA': 664, '▁KT': 665, '▁L': 666, '▁LA': 667, '▁LG': 668, '▁LIG': 669, '▁LTE': 670, '▁M': 671, '▁MBC': 672, '▁MC': 673, '▁Mnet': 674, '▁N': 675, '▁NC': 676, '▁NH': 677, '▁NHN': 678, '▁NLL': 679, '▁O': 680, '▁P': 681, '▁PC': 682, '▁PD': 683, '▁Q': 684, '▁QPR': 685, '▁R': 686, '▁S': 687, '▁SBS': 688, '▁SK': 689, '▁SM': 690, '▁SNS': 691, '▁STX': 692, '▁T': 693, '▁TV': 694, '▁U': 695, '▁US': 696, '▁V': 697, '▁VIP': 698, '▁W': 699, '▁XML': 700, '▁YG': 701, '▁[': 702, '▁`': 703, '▁and': 704, '▁c': 705, '▁for': 706, '▁of': 707, '▁the': 708, '▁to': 709, '▁tvN': 710, '▁|': 711, '▁‘': 712, '▁‘2013': 713, '▁“': 714, '▁“‘': 715, '▁│': 716, '▁■': 717, '▁▦': 718, '▁▲': 719, '▁△': 720, '▁▷': 721, '▁◀': 722, '▁◆': 723, '▁◇': 724, '▁【': 725, '▁中': 726, '▁美': 727, '▁가격': 728, '▁가격은': 729, '▁가격이': 730, '▁가계': 731, '▁가계부채': 732, '▁가구': 733, '▁가까운': 734, '▁가까이': 735, '▁가는': 736, '▁가능': 737, '▁가능성': 738, '▁가능성도': 739, '▁가능성을': 740, '▁가능성이': 741, '▁가능하다': 742, '▁가능한': 743, '▁가동': 744, '▁가득': 745, '▁가량': 746, '▁가려': 747, '▁가로': 748, '▁가르': 749, '▁가리': 750, '▁가맹점': 751, '▁가방': 752, '▁가수': 753, '▁가스': 754, '▁가슴': 755, '▁가시': 756, '▁가운데': 757, '▁가입': 758, '▁가입자': 759, '▁가장': 760, '▁가전': 761, '▁가정': 762, '▁가져': 763, '▁가졌다': 764, '▁가족': 765, '▁가족들': 766, '▁가지': 767, '▁가지고': 768, '▁가진': 769, '▁가짜': 770, '▁가치': 771, '▁가치를': 772, '▁각': 773, '▁각각': 774, '▁각자': 775, '▁각종': 776, '▁간': 777, '▁간담회': 778, '▁간부': 779, '▁간사': 780, '▁갈': 781, '▁갈등': 782, '▁갈수록': 783, '▁감': 784, '▁감각': 785, '▁감독': 786, '▁감독님': 787, '▁감독은': 788, '▁감독의': 789, '▁감독이': 790, '▁감동': 791, '▁감면': 792, '▁감사': 793, '▁감사원': 794, '▁감성': 795, '▁감소': 796, '▁감소한': 797, '▁감소했다': 798, '▁감시': 799, '▁감안': 800, '▁감안하면': 801, '▁감염': 802, '▁감정': 803, '▁감추지': 804, '▁감축': 805, '▁갑자기': 806, '▁강': 807, '▁강남': 808, '▁강남구': 809, '▁강력': 810, '▁강력한': 811, '▁강렬한': 812, '▁강릉': 813, '▁강세': 814, '▁강원': 815, '▁강원도': 816, '▁강제': 817, '▁강조': 818, '▁강조했다': 819, '▁강하게': 820, '▁강한': 821, '▁강행': 822, '▁강호동': 823, '▁강화': 824, '▁갖': 825, '▁갖고': 826, '▁갖추': 827, '▁갖춘': 828, '▁갖춰': 829, '▁같': 830, '▁같다': 831, '▁같아': 832, '▁같은': 833, '▁같이': 834, '▁개': 835, '▁개그맨': 836, '▁개념': 837, '▁개막': 838, '▁개발': 839, '▁개방': 840, '▁개별': 841, '▁개봉': 842, '▁개선': 843, '▁개설': 844, '▁개성': 845, '▁개성공단': 846, '▁개인': 847, '▁개인정보': 848, '▁개입': 849, '▁개장': 850, '▁개정': 851, '▁개정안': 852, '▁개척': 853, '▁개최': 854, '▁개최한다': 855, '▁개통': 856, '▁개편': 857, '▁개편안': 858, '▁개혁': 859, '▁객관적': 860, '▁갤럭시': 861, '▁거': 862, '▁거두': 863, '▁거둔': 864, '▁거뒀다': 865, '▁거듭': 866, '▁거래': 867, '▁거래되고': 868, '▁거래량': 869, '▁거론': 870, '▁거리': 871, '▁거부': 872, '▁거의': 873, '▁거절': 874, '▁거주': 875, '▁거짓말': 876, '▁거쳐': 877, '▁거치': 878, '▁거친': 879, '▁걱정': 880, '▁건': 881, '▁건강': 882, '▁건립': 883, '▁건물': 884, '▁건설': 885, '▁건설사': 886, '▁건축': 887, '▁걷': 888, '▁걸': 889, '▁걸그룹': 890, '▁걸린': 891, '▁걸스데이': 892, '▁걸어': 893, '▁걸쳐': 894, '▁검': 895, '▁검거': 896, '▁검사': 897, '▁검색': 898, '▁검증': 899, '▁검찰': 900, '▁검찰에': 901, '▁검찰은': 902, '▁검토': 903, '▁겁니다': 904, '▁것': 905, '▁것과': 906, '▁것도': 907, '▁것에': 908, '▁것으로': 909, '▁것은': 910, '▁것을': 911, '▁것이': 912, '▁것이다': 913, '▁것이라고': 914, '▁것이라는': 915, '▁것이란': 916, '▁것인가': 917, '▁것인지': 918, '▁것입니다': 919, '▁것처럼': 920, '▁게': 921, '▁게다가': 922, '▁게스트': 923, '▁게시물': 924, '▁게시판에': 925, '▁게임': 926, '▁게재': 927, '▁게재했다': 928, '▁겨냥': 929, '▁겨울': 930, '▁격': 931, '▁격려': 932, '▁격차': 933, '▁겪': 934, '▁겪고': 935, '▁견': 936, '▁결': 937, '▁결과': 938, '▁결과를': 939, '▁결국': 940, '▁결론': 941, '▁결별': 942, '▁결승': 943, '▁결심': 944, '▁결정': 945, '▁결정했다': 946, '▁결제': 947, '▁결코': 948, '▁결합': 949, '▁결혼': 950, '▁결혼식': 951, '▁겸': 952, '▁경': 953, '▁경계': 954, '▁경고': 955, '▁경기': 956, '▁경기가': 957, '▁경기도': 958, '▁경기를': 959, '▁경기에서': 960, '▁경기침체': 961, '▁경남': 962, '▁경력': 963, '▁경매': 964, '▁경북': 965, '▁경비': 966, '▁경영': 967, '▁경우': 968, '▁경우가': 969, '▁경쟁': 970, '▁경쟁력': 971, '▁경제': 972, '▁경제성장률': 973, '▁경제적': 974, '▁경찰': 975, '▁경찰관': 976, '▁경찰에': 977, '▁경찰은': 978, '▁경험': 979, '▁계': 980, '▁계기가': 981, '▁계기로': 982, '▁계산': 983, '▁계속': 984, '▁계약': 985, '▁계약을': 986, '▁계열사': 987, '▁계절': 988, '▁계좌': 989, '▁계획': 990, '▁계획을': 991, '▁계획이다': 992, '▁고': 993, '▁고객': 994, '▁고객들': 995, '▁고객에게': 996, '▁고교': 997, '▁고급': 998, '▁고려': 999, '▁고려해': 1000, '▁고령': 1001, '▁고민': 1002, '▁고발': 1003, '▁고백': 1004, '▁고백했다': 1005, '▁고소': 1006, '▁고스란히': 1007, '▁고양': 1008, '▁고영욱': 1009, '▁고용': 1010, '▁고위': 1011, '▁고통': 1012, '▁곡': 1013, '▁곧': 1014, '▁곧바로': 1015, '▁골': 1016, '▁골든': 1017, '▁골키퍼': 1018, '▁골프': 1019, '▁골프장': 1020, '▁곳': 1021, '▁곳이': 1022, '▁공': 1023, '▁공간': 1024, '▁공감': 1025, '▁공개': 1026, '▁공개됐다': 1027, '▁공개된': 1028, '▁공개했다': 1029, '▁공격': 1030, '▁공격수': 1031, '▁공공': 1032, '▁공공기관': 1033, '▁공급': 1034, '▁공단': 1035, '▁공동': 1036, '▁공략': 1037, '▁공모': 1038, '▁공무원': 1039, '▁공방': 1040, '▁공백': 1041, '▁공부': 1042, '▁공사': 1043, '▁공시': 1044, '▁공시했다': 1045, '▁공식': 1046, '▁공약': 1047, '▁공연': 1048, '▁공연을': 1049, '▁공유': 1050, '▁공장': 1051, '▁공정': 1052, '▁공정위': 1053, '▁공포': 1054, '▁공항': 1055, '▁공화당': 1056, '▁과감': 1057, '▁과거': 1058, '▁과도한': 1059, '▁과세': 1060, '▁과시': 1061, '▁과시했다': 1062, '▁과연': 1063, '▁과장': 1064, '▁과정': 1065, '▁과정에서': 1066, '▁과정을': 1067, '▁과제': 1068, '▁과징금': 1069, '▁과태료': 1070, '▁과학': 1071, '▁곽': 1072, '▁관': 1073, '▁관객': 1074, '▁관객들': 1075, '▁관계': 1076, '▁관계자': 1077, '▁관계자는': 1078, '▁관계자들': 1079, '▁관광': 1080, '▁관광객': 1081, '▁관람': 1082, '▁관련': 1083, '▁관련된': 1084, '▁관련사진': 1085, '▁관련한': 1086, '▁관련해': 1087, '▁관리': 1088, '▁관심': 1089, '▁관심을': 1090, '▁관심이': 1091, '▁관중': 1092, '▁관측': 1093, '▁관한': 1094, '▁관행': 1095, '▁광': 1096, '▁광고': 1097, '▁광주': 1098, '▁광주시': 1099, '▁괜찮': 1100, '▁괴': 1101, '▁굉장히': 1102, '▁교': 1103, '▁교과부': 1104, '▁교류': 1105, '▁교사': 1106, '▁교수': 1107, '▁교수는': 1108, '▁교육': 1109, '▁교육부': 1110, '▁교체': 1111, '▁교통': 1112, '▁교통사고': 1113, '▁교환': 1114, '▁구': 1115, '▁구간': 1116, '▁구글': 1117, '▁구단': 1118, '▁구매': 1119, '▁구성': 1120, '▁구성된': 1121, '▁구속': 1122, '▁구속기소': 1123, '▁구입': 1124, '▁구자철': 1125, '▁구조': 1126, '▁구조조정': 1127, '▁구체적으로': 1128, '▁구체적인': 1129, '▁구축': 1130, '▁구현': 1131, '▁국': 1132, '▁국가': 1133, '▁국가기록원': 1134, '▁국가대표': 1135, '▁국가안보': 1136, '▁국가정보원': 1137, '▁국내': 1138, '▁국내에서': 1139, '▁국내외': 1140, '▁국립': 1141, '▁국무총리': 1142, '▁국무회의': 1143, '▁국민': 1144, '▁국민들': 1145, '▁국민연금': 1146, '▁국민은행': 1147, '▁국민의': 1148, '▁국방': 1149, '▁국방부': 1150, '▁국세청': 1151, '▁국정': 1152, '▁국정원': 1153, '▁국정조사': 1154, '▁국제': 1155, '▁국제사회': 1156, '▁국조': 1157, '▁국채': 1158, '▁국토교통부': 1159, '▁국토부': 1160, '▁국토해양부': 1161, '▁국회': 1162, '▁국회에서': 1163, '▁국회의원': 1164, '▁군': 1165, '▁군부': 1166, '▁군사': 1167, '▁굴': 1168, '▁궁금': 1169, '▁권': 1170, '▁권력': 1171, '▁권리': 1172, '▁권한': 1173, '▁귀': 1174, '▁귀국': 1175, '▁귀여운': 1176, '▁귀엽': 1177, '▁규명': 1178, '▁규모': 1179, '▁규모로': 1180, '▁규모의': 1181, '▁규정': 1182, '▁규제': 1183, '▁균형': 1184, '▁그': 1185, '▁그가': 1186, '▁그간': 1187, '▁그것이': 1188, '▁그냥': 1189, '▁그녀': 1190, '▁그는': 1191, '▁그대로': 1192, '▁그동안': 1193, '▁그때': 1194, '▁그래서': 1195, '▁그래픽': 1196, '▁그랬': 1197, '▁그러': 1198, '▁그러나': 1199, '▁그러면서': 1200, '▁그런': 1201, '▁그런데': 1202, '▁그럼에도': 1203, '▁그렇게': 1204, '▁그렇지': 1205, '▁그려': 1206, '▁그려졌다': 1207, '▁그룹': 1208, '▁그리': 1209, '▁그리고': 1210, '▁그린': 1211, '▁그림': 1212, '▁그만큼': 1213, '▁그의': 1214, '▁그쳤다': 1215, '▁극': 1216, '▁극대화': 1217, '▁극복': 1218, '▁극중': 1219, '▁극찬': 1220, '▁근': 1221, '▁근거': 1222, '▁근로': 1223, '▁근로자': 1224, '▁근무': 1225, '▁근본': 1226, '▁근육': 1227, '▁근절': 1228, '▁근처': 1229, '▁근황': 1230, '▁글': 1231, '▁글과': 1232, '▁글로벌': 1233, '▁글을': 1234, '▁금': 1235, '▁금감원': 1236, '▁금리': 1237, '▁금메달': 1238, '▁금액': 1239, '▁금융': 1240, '▁금융감독원': 1241, '▁금융권': 1242, '▁금융기관': 1243, '▁금융당국': 1244, '▁금융위기': 1245, '▁금융투자': 1246, '▁금융회사': 1247, '▁금지': 1248, '▁금품을': 1249, '▁급': 1250, '▁급격': 1251, '▁급등': 1252, '▁급락': 1253, '▁급여': 1254, '▁급증': 1255, '▁긍정적': 1256, '▁긍정적인': 1257, '▁기': 1258, '▁기간': 1259, '▁기계': 1260, '▁기관': 1261, '▁기기': 1262, '▁기념': 1263, '▁기능': 1264, '▁기능을': 1265, '▁기다리': 1266, '▁기대': 1267, '▁기대감': 1268, '▁기대된다': 1269, '▁기대를': 1270, '▁기대하고': 1271, '▁기대한다': 1272, '▁기록': 1273, '▁기록하고': 1274, '▁기록하며': 1275, '▁기록한': 1276, '▁기록했다': 1277, '▁기반': 1278, '▁기반으로': 1279, '▁기본': 1280, '▁기부': 1281, '▁기분': 1282, '▁기사': 1283, '▁기사본문': 1284, '▁기상': 1285, '▁기성용': 1286, '▁기소': 1287, '▁기소된': 1288, '▁기술': 1289, '▁기아차': 1290, '▁기억': 1291, '▁기업': 1292, '▁기업들': 1293, '▁기업의': 1294, '▁기여': 1295, '▁기온': 1296, '▁기울': 1297, '▁기재부': 1298, '▁기조': 1299, '▁기존': 1300, '▁기준': 1301, '▁기준금리': 1302, '▁기준으로': 1303, '▁기초': 1304, '▁기초자산': 1305, '▁기타': 1306, '▁기회': 1307, '▁기회를': 1308, '▁기획': 1309, '▁기획재정부': 1310, '▁긴': 1311, '▁긴급': 1312, '▁긴장': 1313, '▁긴장감': 1314, '▁길': 1315, '▁김': 1316, '▁김기': 1317, '▁김동': 1318, '▁김모': 1319, '▁김민': 1320, '▁김성': 1321, '▁김씨': 1322, '▁김씨는': 1323, '▁김연경': 1324, '▁김연아': 1325, '▁김영': 1326, '▁김용': 1327, '▁김용준': 1328, '▁김재': 1329, '▁김정': 1330, '▁김종': 1331, '▁김종학': 1332, '▁김진': 1333, '▁김태': 1334, '▁김태희': 1335, '▁김한길': 1336, '▁김현': 1337, '▁깊은': 1338, '▁깊이': 1339, '▁깔': 1340, '▁깜짝': 1341, '▁깨': 1342, '▁깨끗': 1343, '▁꺼': 1344, '▁꺾고': 1345, '▁꼬리': 1346, '▁꼭': 1347, '▁꼽았다': 1348, '▁꼽히': 1349, '▁꼽힌다': 1350, '▁꽃': 1351, '▁꽃미남': 1352, '▁꾸': 1353, '▁꾸준한': 1354, '▁꾸준히': 1355, '▁꿈': 1356, '▁꿈꾸': 1357, '▁끈다': 1358, '▁끊임없': 1359, '▁끌고': 1360, '▁끌어': 1361, '▁끌었다': 1362, '▁끝': 1363, '▁끝까지': 1364, '▁끝나': 1365, '▁끝난': 1366, '▁끝내': 1367, '▁끝에': 1368, '▁끼': 1369, '▁나': 1370, '▁나가': 1371, '▁나갈': 1372, '▁나누': 1373, '▁나눠': 1374, '▁나는': 1375, '▁나라': 1376, '▁나란히': 1377, '▁나로호': 1378, '▁나머지': 1379, '▁나쁜': 1380, '▁나서': 1381, '▁나선': 1382, '▁나선다': 1383, '▁나설': 1384, '▁나섰': 1385, '▁나섰다': 1386, '▁나아가': 1387, '▁나오': 1388, '▁나오고': 1389, '▁나오는': 1390, '▁나온': 1391, '▁나온다': 1392, '▁나올': 1393, '▁나와': 1394, '▁나왔': 1395, '▁나왔다': 1396, '▁나이': 1397, '▁나중에': 1398, '▁나타나': 1399, '▁나타난': 1400, '▁나타났다': 1401, '▁나타내': 1402, '▁나타냈다': 1403, '▁낙': 1404, '▁낙찰': 1405, '▁난': 1406, '▁날': 1407, '▁날씨': 1408, '▁남': 1409, '▁남겨': 1410, '▁남겼다': 1411, '▁남긴': 1412, '▁남녀': 1413, '▁남다른': 1414, '▁남부': 1415, '▁남북': 1416, '▁남북관계': 1417, '▁남북정상회담': 1418, '▁남성': 1419, '▁남아': 1420, '▁남아있': 1421, '▁남은': 1422, '▁남자': 1423, '▁남자친구': 1424, '▁남편': 1425, '▁납부': 1426, '▁납치': 1427, '▁납품': 1428, '▁낮': 1429, '▁낮아': 1430, '▁낮은': 1431, '▁낮추': 1432, '▁낳': 1433, '▁내': 1434, '▁내가': 1435, '▁내내': 1436, '▁내년': 1437, '▁내놓': 1438, '▁내놨다': 1439, '▁내다봤다': 1440, '▁내달': 1441, '▁내려': 1442, '▁내렸다': 1443, '▁내리': 1444, '▁내린': 1445, '▁내부': 1446, '▁내수': 1447, '▁내야': 1448, '▁내용': 1449, '▁내용은': 1450, '▁내용을': 1451, '▁내용의': 1452, '▁내용이': 1453, '▁낸': 1454, '▁냈다': 1455, '▁냉': 1456, '▁너': 1457, '▁너무': 1458, '▁넓': 1459, '▁넘': 1460, '▁넘게': 1461, '▁넘겨': 1462, '▁넘기': 1463, '▁넘는': 1464, '▁넘어': 1465, '▁넘치는': 1466, '▁넣': 1467, '▁넣어': 1468, '▁네': 1469, '▁네이버': 1470, '▁네트워크': 1471, '▁네티즌': 1472, '▁네티즌들': 1473, '▁네티즌들은': 1474, '▁넥센': 1475, '▁노': 1476, '▁노동': 1477, '▁노동자': 1478, '▁노래': 1479, '▁노량진': 1480, '▁노력': 1481, '▁노력을': 1482, '▁노력하겠다': 1483, '▁노무현': 1484, '▁노사': 1485, '▁노선': 1486, '▁노인': 1487, '▁노조': 1488, '▁노출': 1489, '▁노트북': 1490, '▁노하우': 1491, '▁노홍철': 1492, '▁노후': 1493, '▁녹': 1494, '▁녹색': 1495, '▁녹음': 1496, '▁녹화': 1497, '▁논': 1498, '▁논란': 1499, '▁논란이': 1500, '▁논리': 1501, '▁논의': 1502, '▁논쟁': 1503, '▁놀': 1504, '▁놀라': 1505, '▁놀라게': 1506, '▁농': 1507, '▁농업': 1508, '▁농촌': 1509, '▁농협': 1510, '▁높': 1511, '▁높게': 1512, '▁높다': 1513, '▁높아': 1514, '▁높아졌다': 1515, '▁높아지고': 1516, '▁높았다': 1517, '▁높여': 1518, '▁높였다': 1519, '▁높은': 1520, '▁높이': 1521, '▁놓': 1522, '▁놓고': 1523, '▁뇌': 1524, '▁뇌물': 1525, '▁누': 1526, '▁누가': 1527, '▁누구': 1528, '▁누구나': 1529, '▁누군가': 1530, '▁누리꾼들': 1531, '▁누리꾼들은': 1532, '▁누적': 1533, '▁누출': 1534, '▁눈': 1535, '▁눈길을': 1536, '▁눈물': 1537, '▁눈물을': 1538, '▁눈빛': 1539, '▁눈에': 1540, '▁뉴': 1541, '▁뉴스': 1542, '▁뉴욕': 1543, '▁느껴': 1544, '▁느꼈': 1545, '▁느끼': 1546, '▁느낀': 1547, '▁느낄': 1548, '▁느낌': 1549, '▁늘': 1550, '▁늘려': 1551, '▁늘리': 1552, '▁늘어': 1553, '▁늘어나': 1554, '▁늘어난': 1555, '▁늘어날': 1556, '▁늘어났다': 1557, '▁늘었다': 1558, '▁능력': 1559, '▁능력을': 1560, '▁늦어': 1561, '▁다': 1562, '▁다녀': 1563, '▁다니': 1564, '▁다룬': 1565, '▁다르다': 1566, '▁다른': 1567, '▁다리': 1568, '▁다만': 1569, '▁다문화': 1570, '▁다섯': 1571, '▁다소': 1572, '▁다수': 1573, '▁다시': 1574, '▁다양한': 1575, '▁다운로드': 1576, '▁다음': 1577, '▁다음날': 1578, '▁다음달': 1579, '▁다이어트': 1580, '▁다저스': 1581, '▁다저스는': 1582, '▁다짐': 1583, '▁다하겠다': 1584, '▁단': 1585, '▁단계': 1586, '▁단기': 1587, '▁단독': 1588, '▁단말기': 1589, '▁단속': 1590, '▁단순': 1591, '▁단장': 1592, '▁단지': 1593, '▁단체': 1594, '▁단축': 1595, '▁단행': 1596, '▁달': 1597, '▁달라': 1598, '▁달러': 1599, '▁달려': 1600, '▁달리': 1601, '▁달성': 1602, '▁달아': 1603, '▁달하는': 1604, '▁달한다': 1605, '▁닮은': 1606, '▁담': 1607, '▁담겨': 1608, '▁담긴': 1609, '▁담당': 1610, '▁담배': 1611, '▁담보': 1612, '▁담아': 1613, '▁담은': 1614, '▁답': 1615, '▁답변': 1616, '▁답했다': 1617, '▁당': 1618, '▁당국': 1619, '▁당기순이익': 1620, '▁당부했다': 1621, '▁당분간': 1622, '▁당사자': 1623, '▁당선': 1624, '▁당선인': 1625, '▁당시': 1626, '▁당연히': 1627, '▁당장': 1628, '▁당첨': 1629, '▁당초': 1630, '▁당했다': 1631, '▁당황': 1632, '▁대': 1633, '▁대거': 1634, '▁대결': 1635, '▁대구': 1636, '▁대규모': 1637, '▁대기': 1638, '▁대기업': 1639, '▁대다수': 1640, '▁대단': 1641, '▁대답': 1642, '▁대리점': 1643, '▁대만': 1644, '▁대법원': 1645, '▁대변인': 1646, '▁대부분': 1647, '▁대북': 1648, '▁대비': 1649, '▁대사': 1650, '▁대상': 1651, '▁대상으로': 1652, '▁대상자': 1653, '▁대선': 1654, '▁대신': 1655, '▁대안': 1656, '▁대외': 1657, '▁대우': 1658, '▁대응': 1659, '▁대전': 1660, '▁대중': 1661, '▁대중교통': 1662, '▁대책': 1663, '▁대처': 1664, '▁대체': 1665, '▁대출': 1666, '▁대통령': 1667, '▁대통령은': 1668, '▁대통령의': 1669, '▁대통령이': 1670, '▁대통령직': 1671, '▁대통령직인수위원회': 1672, '▁대폭': 1673, '▁대표': 1674, '▁대표는': 1675, '▁대표단': 1676, '▁대표이사': 1677, '▁대표적인': 1678, '▁대표팀': 1679, '▁대학': 1680, '▁대학생': 1681, '▁대한': 1682, '▁대한민국': 1683, '▁대한항공': 1684, '▁대해': 1685, '▁대해서': 1686, '▁대해서는': 1687, '▁대해서도': 1688, '▁대해선': 1689, '▁대형': 1690, '▁대형마트': 1691, '▁대화': 1692, '▁대화록': 1693, '▁대회': 1694, '▁대회에서': 1695, '▁댄스': 1696, '▁댓글': 1697, '▁더': 1698, '▁더불어': 1699, '▁더욱': 1700, '▁덕': 1701, '▁덕분에': 1702, '▁던지': 1703, '▁덜': 1704, '▁덧붙였다': 1705, '▁데': 1706, '▁데뷔': 1707, '▁데이터': 1708, '▁데이트': 1709, '▁도내': 1710, '▁도로': 1711, '▁도발': 1712, '▁도시': 1713, '▁도심': 1714, '▁도약': 1715, '▁도와': 1716, '▁도움': 1717, '▁도움을': 1718, '▁도움이': 1719, '▁도입': 1720, '▁도전': 1721, '▁도중': 1722, '▁도착': 1723, '▁도쿄': 1724, '▁독': 1725, '▁독립': 1726, '▁독일': 1727, '▁독자': 1728, '▁독특한': 1729, '▁돈': 1730, '▁돈을': 1731, '▁돌': 1732, '▁돌려': 1733, '▁돌아': 1734, '▁돌아가': 1735, '▁돌아오': 1736, '▁돌아온': 1737, '▁돌입': 1738, '▁돌파': 1739, '▁돕는': 1740, '▁동': 1741, '▁동결': 1742, '▁동기': 1743, '▁동남아': 1744, '▁동료': 1745, '▁동물': 1746, '▁동반': 1747, '▁동반성장': 1748, '▁동부': 1749, '▁동생': 1750, '▁동시에': 1751, '▁동아시안컵': 1752, '▁동아제약': 1753, '▁동안': 1754, '▁동양': 1755, '▁동영상': 1756, '▁동원': 1757, '▁동의': 1758, '▁동작': 1759, '▁동참': 1760, '▁돼': 1761, '▁됐다': 1762, '▁되': 1763, '▁되고': 1764, '▁되는': 1765, '▁되면': 1766, '▁되어': 1767, '▁되지': 1768, '▁되찾': 1769, '▁된': 1770, '▁된다': 1771, '▁될': 1772, '▁두': 1773, '▁두고': 1774, '▁두드러': 1775, '▁두산': 1776, '▁둔': 1777, '▁둔화': 1778, '▁둘': 1779, '▁둘러': 1780, '▁둘러싼': 1781, '▁둘째': 1782, '▁뒤': 1783, '▁뒤늦게': 1784, '▁뒤집': 1785, '▁뒷': 1786, '▁뒷받침': 1787, '▁드': 1788, '▁드라마': 1789, '▁드러나': 1790, '▁드러난': 1791, '▁드러났다': 1792, '▁드러내': 1793, '▁드러낸': 1794, '▁드러냈다': 1795, '▁드레스': 1796, '▁득점': 1797, '▁든다': 1798, '▁듣': 1799, '▁듣고': 1800, '▁들': 1801, '▁들고': 1802, '▁들려': 1803, '▁들어': 1804, '▁들어가': 1805, '▁들어간': 1806, '▁들어갈': 1807, '▁들어갔다': 1808, '▁들어서': 1809, '▁들어오': 1810, '▁들었다': 1811, '▁들여': 1812, '▁듯': 1813, '▁듯한': 1814, '▁등': 1815, '▁등과': 1816, '▁등도': 1817, '▁등록': 1818, '▁등록금': 1819, '▁등에': 1820, '▁등에서': 1821, '▁등으로': 1822, '▁등은': 1823, '▁등을': 1824, '▁등의': 1825, '▁등이': 1826, '▁등이다': 1827, '▁등장': 1828, '▁등판': 1829, '▁디': 1830, '▁디자인': 1831, '▁디지털': 1832, '▁따': 1833, '▁따뜻한': 1834, '▁따라': 1835, '▁따라서': 1836, '▁따로': 1837, '▁따르면': 1838, '▁따른': 1839, '▁딸': 1840, '▁땅': 1841, '▁땅볼': 1842, '▁땅에': 1843, '▁때': 1844, '▁때까지': 1845, '▁때는': 1846, '▁때마다': 1847, '▁때문': 1848, '▁때문에': 1849, '▁때문이다': 1850, '▁때부터': 1851, '▁떠': 1852, '▁떠나': 1853, '▁떠난': 1854, '▁떠오르': 1855, '▁떨': 1856, '▁떨어져': 1857, '▁떨어졌다': 1858, '▁떨어지': 1859, '▁떨어진': 1860, '▁또': 1861, '▁또는': 1862, '▁또다시': 1863, '▁또한': 1864, '▁똑같': 1865, '▁뚜렷': 1866, '▁뛰': 1867, '▁뛰어': 1868, '▁뛰어난': 1869, '▁뛰어넘': 1870, '▁뜨거운': 1871, '▁뜻': 1872, '▁뜻을': 1873, '▁띠고': 1874, '▁라': 1875, '▁라디오': 1876, '▁라이벌': 1877, '▁라이브': 1878, '▁라인업': 1879, '▁랭킹': 1880, '▁러시아': 1881, '▁런던': 1882, '▁런던올림픽': 1883, '▁레': 1884, '▁레드카펫': 1885, '▁레알': 1886, '▁레이': 1887, '▁레전드': 1888, '▁로그인': 1889, '▁로비': 1890, '▁로이킴': 1891, '▁로켓': 1892, '▁롯데': 1893, '▁롯데마트': 1894, '▁루': 1895, '▁루머': 1896, '▁류': 1897, '▁류현진': 1898, '▁류현진은': 1899, '▁리': 1900, '▁리그': 1901, '▁리더': 1902, '▁리더십': 1903, '▁리베이트': 1904, '▁리스크': 1905, '▁리포트': 1906, '▁마': 1907, '▁마감': 1908, '▁마감했다': 1909, '▁마드리드': 1910, '▁마련': 1911, '▁마련했다': 1912, '▁마리': 1913, '▁마무리': 1914, '▁마운드': 1915, '▁마을': 1916, '▁마음': 1917, '▁마음을': 1918, '▁마지막': 1919, '▁마지막으로': 1920, '▁마찬가지': 1921, '▁마쳤다': 1922, '▁마치': 1923, '▁마치고': 1924, '▁마친': 1925, '▁마케팅': 1926, '▁막': 1927, '▁막기': 1928, '▁막아': 1929, '▁막판': 1930, '▁만': 1931, '▁만기': 1932, '▁만나': 1933, '▁만난': 1934, '▁만날': 1935, '▁만남': 1936, '▁만났다': 1937, '▁만드는': 1938, '▁만든': 1939, '▁만들': 1940, '▁만들고': 1941, '▁만들기': 1942, '▁만들어': 1943, '▁만들었다': 1944, '▁만약': 1945, '▁만에': 1946, '▁만족': 1947, '▁만큼': 1948, '▁만한': 1949, '▁많고': 1950, '▁많다': 1951, '▁많아': 1952, '▁많았': 1953, '▁많았다': 1954, '▁많은': 1955, '▁많이': 1956, '▁많지': 1957, '▁말': 1958, '▁말까지': 1959, '▁말레이시아': 1960, '▁말씀': 1961, '▁말을': 1962, '▁말이': 1963, '▁말한다': 1964, '▁말해': 1965, '▁말했다': 1966, '▁맛': 1967, '▁망': 1968, '▁망명': 1969, '▁맞': 1970, '▁맞는': 1971, '▁맞대결': 1972, '▁맞서': 1973, '▁맞아': 1974, '▁맞았다': 1975, '▁맞추': 1976, '▁맞춘': 1977, '▁맞춤형': 1978, '▁맞춰': 1979, '▁맡': 1980, '▁맡고': 1981, '▁맡아': 1982, '▁맡았': 1983, '▁맡았다': 1984, '▁맡은': 1985, '▁매': 1986, '▁매각': 1987, '▁매년': 1988, '▁매달': 1989, '▁매도': 1990, '▁매력': 1991, '▁매력을': 1992, '▁매매': 1993, '▁매수': 1994, '▁매우': 1995, '▁매일': 1996, '▁매입': 1997, '▁매장': 1998, '▁매주': 1999, '▁매체': 2000, '▁매출': 2001, '▁매출액': 2002, '▁매출이': 2003, '▁맨': 2004, '▁맨유': 2005, '▁맺': 2006, '▁머': 2007, '▁머리': 2008, '▁머물': 2009, '▁먹': 2010, '▁먹고': 2011, '▁먼저': 2012, '▁멀티': 2013, '▁멀티미디어': 2014, '▁멋진': 2015, '▁메': 2016, '▁메뉴': 2017, '▁메시지': 2018, '▁메시지를': 2019, '▁메이저': 2020, '▁메이저리그': 2021, '▁메이크업': 2022, '▁메인': 2023, '▁멕시코': 2024, '▁멘토': 2025, '▁멜로': 2026, '▁멤버': 2027, '▁멤버들': 2028, '▁면': 2029, '▁면담': 2030, '▁면모를': 2031, '▁면접': 2032, '▁면제': 2033, '▁명': 2034, '▁명단': 2035, '▁명령': 2036, '▁명예': 2037, '▁명의': 2038, '▁명이': 2039, '▁명칭': 2040, '▁명품': 2041, '▁명확': 2042, '▁몇': 2043, '▁모': 2044, '▁모니터링': 2045, '▁모델': 2046, '▁모두': 2047, '▁모든': 2048, '▁모르': 2049, '▁모르겠다': 2050, '▁모른다': 2051, '▁모멘텀': 2052, '▁모바일': 2053, '▁모비스': 2054, '▁모색': 2055, '▁모습': 2056, '▁모습으로': 2057, '▁모습을': 2058, '▁모습이': 2059, '▁모습이다': 2060, '▁모아': 2061, '▁모았다': 2062, '▁모양': 2063, '▁모여': 2064, '▁모으고': 2065, '▁모자': 2066, '▁모집': 2067, '▁목': 2068, '▁목격': 2069, '▁목동': 2070, '▁목록': 2071, '▁목사': 2072, '▁목소리': 2073, '▁목소리가': 2074, '▁목숨': 2075, '▁목적': 2076, '▁목적으로': 2077, '▁목표': 2078, '▁목표로': 2079, '▁목표주가': 2080, '▁몰': 2081, '▁몰려': 2082, '▁몰아': 2083, '▁몸': 2084, '▁몸매': 2085, '▁못': 2086, '▁못하고': 2087, '▁못하는': 2088, '▁못한': 2089, '▁못한다': 2090, '▁못할': 2091, '▁못해': 2092, '▁못했다': 2093, '▁묘': 2094, '▁무': 2095, '▁무게': 2096, '▁무기': 2097, '▁무대': 2098, '▁무대를': 2099, '▁무대에': 2100, '▁무더위': 2101, '▁무려': 2102, '▁무료': 2103, '▁무료로': 2104, '▁무르시': 2105, '▁무릎': 2106, '▁무리': 2107, '▁무산': 2108, '▁무상': 2109, '▁무슨': 2110, '▁무승부': 2111, '▁무실점': 2112, '▁무엇보다': 2113, '▁무역': 2114, '▁무이자': 2115, '▁무제한': 2116, '▁무조건': 2117, '▁무죄': 2118, '▁묶': 2119, '▁문': 2120, '▁문서': 2121, '▁문의': 2122, '▁문자': 2123, '▁문재인': 2124, '▁문제': 2125, '▁문제가': 2126, '▁문제는': 2127, '▁문제로': 2128, '▁문제를': 2129, '▁문제에': 2130, '▁문제점': 2131, '▁문화': 2132, '▁문화체육관광부': 2133, '▁묻': 2134, '▁물': 2135, '▁물가': 2136, '▁물건': 2137, '▁물놀이': 2138, '▁물량': 2139, '▁물론': 2140, '▁물류': 2141, '▁물리': 2142, '▁물어': 2143, '▁물품': 2144, '▁뭐': 2145, '▁뭔가': 2146, '▁뮤지컬': 2147, '▁뮤직비디오': 2148, '▁미': 2149, '▁미국': 2150, '▁미국의': 2151, '▁미니': 2152, '▁미드필더': 2153, '▁미디어': 2154, '▁미래': 2155, '▁미래부': 2156, '▁미래창조과학부': 2157, '▁미뤄': 2158, '▁미리': 2159, '▁미만': 2160, '▁미모': 2161, '▁미사일': 2162, '▁미소': 2163, '▁미얀마': 2164, '▁미치는': 2165, '▁미치지': 2166, '▁미칠': 2167, '▁미투데이': 2168, '▁민': 2169, '▁민간': 2170, '▁민낯': 2171, '▁민생': 2172, '▁민원': 2173, '▁민족': 2174, '▁민주': 2175, '▁민주당': 2176, '▁민주당은': 2177, '▁민주주의': 2178, '▁민주통합당': 2179, '▁믿고': 2180, '▁밀': 2181, '▁밀려': 2182, '▁밀어': 2183, '▁및': 2184, '▁밑': 2185, '▁바': 2186, '▁바꾸': 2187, '▁바꿔': 2188, '▁바뀌': 2189, '▁바뀐': 2190, '▁바다': 2191, '▁바닥': 2192, '▁바라': 2193, '▁바란다': 2194, '▁바람': 2195, '▁바로': 2196, '▁바이러스': 2197, '▁바탕으로': 2198, '▁박': 2199, '▁박근혜': 2200, '▁박명수': 2201, '▁박수': 2202, '▁박인비': 2203, '▁박지성': 2204, '▁밖에': 2205, '▁밖으로': 2206, '▁반': 2207, '▁반대': 2208, '▁반도체': 2209, '▁반드시': 2210, '▁반등': 2211, '▁반면': 2212, '▁반박했다': 2213, '▁반발': 2214, '▁반복': 2215, '▁반영': 2216, '▁반응': 2217, '▁반응을': 2218, '▁반전': 2219, '▁받': 2220, '▁받게': 2221, '▁받고': 2222, '▁받기': 2223, '▁받는': 2224, '▁받는다': 2225, '▁받아': 2226, '▁받아들여': 2227, '▁받아들이': 2228, '▁받았': 2229, '▁받았다': 2230, '▁받으며': 2231, '▁받은': 2232, '▁받을': 2233, '▁받지': 2234, '▁발': 2235, '▁발견': 2236, '▁발견됐다': 2237, '▁발굴': 2238, '▁발급': 2239, '▁발매': 2240, '▁발목': 2241, '▁발사': 2242, '▁발생': 2243, '▁발생한': 2244, '▁발생할': 2245, '▁발생했다': 2246, '▁발언': 2247, '▁발언을': 2248, '▁발전': 2249, '▁발탁': 2250, '▁발표': 2251, '▁발표한': 2252, '▁발표했다': 2253, '▁발행': 2254, '▁발효': 2255, '▁발휘': 2256, '▁밝은': 2257, '▁밝혀': 2258, '▁밝혀졌다': 2259, '▁밝혔': 2260, '▁밝혔다': 2261, '▁밝혔습니다': 2262, '▁밝히': 2263, '▁밝힌': 2264, '▁밤': 2265, '▁밥': 2266, '▁방': 2267, '▁방문': 2268, '▁방문해': 2269, '▁방법': 2270, '▁방북': 2271, '▁방송': 2272, '▁방송되는': 2273, '▁방송된': 2274, '▁방송된다': 2275, '▁방송에서': 2276, '▁방식': 2277, '▁방식으로': 2278, '▁방안': 2279, '▁방안을': 2280, '▁방지': 2281, '▁방침': 2282, '▁방침이다': 2283, '▁방통위': 2284, '▁방향': 2285, '▁방향으로': 2286, '▁배': 2287, '▁배경': 2288, '▁배경으로': 2289, '▁배당': 2290, '▁배려': 2291, '▁배우': 2292, '▁배우들': 2293, '▁배제': 2294, '▁배출': 2295, '▁배치': 2296, '▁배터리': 2297, '▁백': 2298, '▁백악관': 2299, '▁백지영': 2300, '▁백화점': 2301, '▁버': 2302, '▁버냉키': 2303, '▁버디': 2304, '▁버스': 2305, '▁버전': 2306, '▁번': 2307, '▁번째': 2308, '▁벌': 2309, '▁벌금': 2310, '▁벌써': 2311, '▁벌어지': 2312, '▁벌어진': 2313, '▁벌였다': 2314, '▁벌이고': 2315, '▁벌이는': 2316, '▁벌인': 2317, '▁범': 2318, '▁범위': 2319, '▁범죄': 2320, '▁범행': 2321, '▁법': 2322, '▁법률': 2323, '▁법무부': 2324, '▁법안': 2325, '▁법원': 2326, '▁법인': 2327, '▁법적': 2328, '▁법정': 2329, '▁법칙': 2330, '▁벗': 2331, '▁벗어나': 2332, '▁베': 2333, '▁베이징': 2334, '▁베트남': 2335, '▁벤': 2336, '▁벤처': 2337, '▁벽': 2338, '▁변': 2339, '▁변경': 2340, '▁변동': 2341, '▁변동성': 2342, '▁변수': 2343, '▁변신': 2344, '▁변호사': 2345, '▁변화': 2346, '▁변화를': 2347, '▁별': 2348, '▁별도로': 2349, '▁별도의': 2350, '▁병': 2351, '▁병역': 2352, '▁병원': 2353, '▁병행': 2354, '▁보': 2355, '▁보건': 2356, '▁보건복지부': 2357, '▁보고': 2358, '▁보고서': 2359, '▁보관': 2360, '▁보급': 2361, '▁보기': 2362, '▁보내': 2363, '▁보낸': 2364, '▁보냈다': 2365, '▁보는': 2366, '▁보니': 2367, '▁보다': 2368, '▁보도': 2369, '▁보도자료': 2370, '▁보도했다': 2371, '▁보면': 2372, '▁보상': 2373, '▁보수': 2374, '▁보안': 2375, '▁보여': 2376, '▁보여주': 2377, '▁보여준': 2378, '▁보여줄': 2379, '▁보여줬다': 2380, '▁보였다': 2381, '▁보완': 2382, '▁보유': 2383, '▁보유하고': 2384, '▁보유한': 2385, '▁보육': 2386, '▁보이': 2387, '▁보이고': 2388, '▁보이는': 2389, '▁보이며': 2390, '▁보이지': 2391, '▁보인다': 2392, '▁보장': 2393, '▁보조금': 2394, '▁보증': 2395, '▁보통': 2396, '▁보험': 2397, '▁보험료': 2398, '▁보험사': 2399, '▁보호': 2400, '▁복': 2401, '▁복귀': 2402, '▁복무': 2403, '▁복수': 2404, '▁복잡': 2405, '▁복지': 2406, '▁복합': 2407, '▁본': 2408, '▁본격': 2409, '▁본격적으로': 2410, '▁본격적인': 2411, '▁본격화': 2412, '▁본다': 2413, '▁본사': 2414, '▁본인': 2415, '▁본회의': 2416, '▁볼': 2417, '▁봉': 2418, '▁봉사활동': 2419, '▁봐': 2420, '▁봤': 2421, '▁봤다': 2422, '▁부': 2423, '▁부각': 2424, '▁부과': 2425, '▁부담': 2426, '▁부담을': 2427, '▁부담이': 2428, '▁부당': 2429, '▁부대': 2430, '▁부동산': 2431, '▁부르': 2432, '▁부모': 2433, '▁부모님': 2434, '▁부문': 2435, '▁부부': 2436, '▁부분': 2437, '▁부분이': 2438, '▁부사장': 2439, '▁부산': 2440, '▁부산시': 2441, '▁부상': 2442, '▁부서': 2443, '▁부실': 2444, '▁부여': 2445, '▁부위원장': 2446, '▁부인': 2447, '▁부작용': 2448, '▁부정': 2449, '▁부정적': 2450, '▁부족': 2451, '▁부족한': 2452, '▁부지': 2453, '▁부진': 2454, '▁부채': 2455, '▁부처': 2456, '▁부총리': 2457, '▁부탁': 2458, '▁부품': 2459, '▁부활': 2460, '▁부회장': 2461, '▁북': 2462, '▁북미': 2463, '▁북측': 2464, '▁북한': 2465, '▁북한의': 2466, '▁북한이': 2467, '▁분': 2468, '▁분노': 2469, '▁분당': 2470, '▁분들': 2471, '▁분류': 2472, '▁분리': 2473, '▁분명': 2474, '▁분명히': 2475, '▁분석': 2476, '▁분석이다': 2477, '▁분석했다': 2478, '▁분야': 2479, '▁분야에서': 2480, '▁분양': 2481, '▁분위기': 2482, '▁분위기를': 2483, '▁분쟁': 2484, '▁불': 2485, '▁불가능': 2486, '▁불가피': 2487, '▁불공정': 2488, '▁불과': 2489, '▁불과하다': 2490, '▁불구속': 2491, '▁불구하고': 2492, '▁불러': 2493, '▁불리는': 2494, '▁불만을': 2495, '▁불법': 2496, '▁불안': 2497, '▁불투명': 2498, '▁불편': 2499, '▁불확실성': 2500, '▁불황': 2501, '▁붕괴': 2502, '▁붙': 2503, '▁붙잡': 2504, '▁브': 2505, '▁브라운': 2506, '▁브라질': 2507, '▁브랜드': 2508, '▁브리핑': 2509, '▁블랙': 2510, '▁블랙박스': 2511, '▁블로그': 2512, '▁블루': 2513, '▁비': 2514, '▁비가': 2515, '▁비공개': 2516, '▁비과세': 2517, '▁비교': 2518, '▁비교적': 2519, '▁비교해': 2520, '▁비난': 2521, '▁비대위': 2522, '▁비대위원장': 2523, '▁비롯': 2524, '▁비롯한': 2525, '▁비롯해': 2526, '▁비리': 2527, '▁비밀': 2528, '▁비상': 2529, '▁비서실': 2530, '▁비서실장': 2531, '▁비스트': 2532, '▁비슷': 2533, '▁비슷한': 2534, '▁비용': 2535, '▁비율': 2536, '▁비자금': 2537, '▁비정규직': 2538, '▁비중': 2539, '▁비중이': 2540, '▁비즈니스': 2541, '▁비판': 2542, '▁비판했다': 2543, '▁비해': 2544, '▁비행기': 2545, '▁빅': 2546, '▁빈': 2547, '▁빈소': 2548, '▁빌': 2549, '▁빌려': 2550, '▁빙': 2551, '▁빚': 2552, '▁빛': 2553, '▁빠': 2554, '▁빠르게': 2555, '▁빠르고': 2556, '▁빠른': 2557, '▁빠져': 2558, '▁빠졌다': 2559, '▁빠지': 2560, '▁빠진': 2561, '▁빨': 2562, '▁빨리': 2563, '▁빼': 2564, '▁빼앗': 2565, '▁뽐내': 2566, '▁뽐냈다': 2567, '▁뽑': 2568, '▁뽑아': 2569, '▁뿌리': 2570, '▁뿐': 2571, '▁뿐만': 2572, '▁사': 2573, '▁사건': 2574, '▁사건을': 2575, '▁사고': 2576, '▁사고가': 2577, '▁사고로': 2578, '▁사과': 2579, '▁사기': 2580, '▁사나이': 2581, '▁사는': 2582, '▁사라지': 2583, '▁사람': 2584, '▁사람들': 2585, '▁사람들이': 2586, '▁사람은': 2587, '▁사람의': 2588, '▁사람이': 2589, '▁사랑': 2590, '▁사랑을': 2591, '▁사례': 2592, '▁사로잡': 2593, '▁사로잡았다': 2594, '▁사망': 2595, '▁사망자': 2596, '▁사면': 2597, '▁사명': 2598, '▁사무': 2599, '▁사무실': 2600, '▁사무총장': 2601, '▁사법': 2602, '▁사상': 2603, '▁사실': 2604, '▁사실상': 2605, '▁사실을': 2606, '▁사실이': 2607, '▁사안': 2608, '▁사업': 2609, '▁사업을': 2610, '▁사업자': 2611, '▁사연': 2612, '▁사용': 2613, '▁사용자': 2614, '▁사용하는': 2615, '▁사용할': 2616, '▁사유': 2617, '▁사이': 2618, '▁사이버': 2619, '▁사이에': 2620, '▁사이에서': 2621, '▁사이트': 2622, '▁사장': 2623, '▁사장은': 2624, '▁사전': 2625, '▁사정': 2626, '▁사진': 2627, '▁사진을': 2628, '▁사진이': 2629, '▁사태': 2630, '▁사퇴': 2631, '▁사항': 2632, '▁사회': 2633, '▁사회공헌': 2634, '▁사회복지': 2635, '▁사회적': 2636, '▁사흘': 2637, '▁삭감': 2638, '▁삭제': 2639, '▁산': 2640, '▁산업': 2641, '▁산하': 2642, '▁살': 2643, '▁살아': 2644, '▁살인': 2645, '▁살펴': 2646, '▁살펴보면': 2647, '▁살해': 2648, '▁삶': 2649, '▁삼': 2650, '▁삼성': 2651, '▁삼성생명': 2652, '▁삼성전자': 2653, '▁삼성전자는': 2654, '▁삼성화재': 2655, '▁삼진': 2656, '▁삼청동': 2657, '▁상': 2658, '▁상담': 2659, '▁상당': 2660, '▁상당수': 2661, '▁상당의': 2662, '▁상당한': 2663, '▁상대': 2664, '▁상대로': 2665, '▁상대적으로': 2666, '▁상무': 2667, '▁상반기': 2668, '▁상생': 2669, '▁상승': 2670, '▁상승세': 2671, '▁상승세를': 2672, '▁상승한': 2673, '▁상승했다': 2674, '▁상위': 2675, '▁상임위': 2676, '▁상장': 2677, '▁상징': 2678, '▁상처': 2679, '▁상태': 2680, '▁상태다': 2681, '▁상태에서': 2682, '▁상품': 2683, '▁상하이': 2684, '▁상한가': 2685, '▁상향': 2686, '▁상호': 2687, '▁상환': 2688, '▁상황': 2689, '▁상황에': 2690, '▁상황에서': 2691, '▁상황을': 2692, '▁상황이': 2693, '▁상황이다': 2694, '▁새': 2695, '▁새누리당': 2696, '▁새누리당은': 2697, '▁새로': 2698, '▁새로운': 2699, '▁새롭게': 2700, '▁새벽': 2701, '▁새해': 2702, '▁샌프란시스코': 2703, '▁생': 2704, '▁생각': 2705, '▁생각을': 2706, '▁생각이': 2707, '▁생각한다': 2708, '▁생각해': 2709, '▁생기': 2710, '▁생긴': 2711, '▁생명': 2712, '▁생방송': 2713, '▁생산': 2714, '▁생존': 2715, '▁생태계': 2716, '▁생활': 2717, '▁서': 2718, '▁서귀포': 2719, '▁서로': 2720, '▁서류': 2721, '▁서민': 2722, '▁서부': 2723, '▁서비스': 2724, '▁서비스를': 2725, '▁서울': 2726, '▁서울대': 2727, '▁서울시': 2728, '▁서울중앙지검': 2729, '▁서울중앙지법': 2730, '▁서초구': 2731, '▁석': 2732, '▁석유': 2733, '▁선': 2734, '▁선거': 2735, '▁선고': 2736, '▁선고받': 2737, '▁선고했다': 2738, '▁선두': 2739, '▁선물': 2740, '▁선박': 2741, '▁선발': 2742, '▁선발투수': 2743, '▁선배': 2744, '▁선보여': 2745, '▁선보였다': 2746, '▁선보이며': 2747, '▁선보인': 2748, '▁선보인다': 2749, '▁선보일': 2750, '▁선사': 2751, '▁선생님': 2752, '▁선수': 2753, '▁선수가': 2754, '▁선수단': 2755, '▁선수들': 2756, '▁선수들이': 2757, '▁선언': 2758, '▁선예': 2759, '▁선임': 2760, '▁선정': 2761, '▁선정됐다': 2762, '▁선정된': 2763, '▁선제골': 2764, '▁선진국': 2765, '▁선출': 2766, '▁선택': 2767, '▁선호': 2768, '▁설': 2769, '▁설계': 2770, '▁설득': 2771, '▁설립': 2772, '▁설명': 2773, '▁설명이다': 2774, '▁설명했다': 2775, '▁설문조사': 2776, '▁설비': 2777, '▁설정': 2778, '▁설치': 2779, '▁섬': 2780, '▁성': 2781, '▁성격': 2782, '▁성공': 2783, '▁성공했다': 2784, '▁성과': 2785, '▁성과를': 2786, '▁성남': 2787, '▁성능': 2788, '▁성동일': 2789, '▁성매매': 2790, '▁성명을': 2791, '▁성장': 2792, '▁성장률': 2793, '▁성장세': 2794, '▁성적': 2795, '▁성추행': 2796, '▁성폭력': 2797, '▁성폭행': 2798, '▁성향': 2799, '▁성형': 2800, '▁세': 2801, '▁세계': 2802, '▁세계적인': 2803, '▁세금': 2804, '▁세대': 2805, '▁세력': 2806, '▁세무조사': 2807, '▁세미나': 2808, '▁세부': 2809, '▁세븐': 2810, '▁세상': 2811, '▁세상을': 2812, '▁세우': 2813, '▁세워': 2814, '▁세웠다': 2815, '▁세종': 2816, '▁세종시': 2817, '▁세트': 2818, '▁섹시': 2819, '▁셀카': 2820, '▁셈이다': 2821, '▁소': 2822, '▁소감을': 2823, '▁소개': 2824, '▁소개했다': 2825, '▁소녀': 2826, '▁소녀시대': 2827, '▁소득': 2828, '▁소리': 2829, '▁소방': 2830, '▁소방당국': 2831, '▁소비': 2832, '▁소비자': 2833, '▁소비자들': 2834, '▁소설': 2835, '▁소셜': 2836, '▁소속': 2837, '▁소속사': 2838, '▁소송': 2839, '▁소식에': 2840, '▁소식을': 2841, '▁소식통': 2842, '▁소요': 2843, '▁소유': 2844, '▁소장': 2845, '▁소재': 2846, '▁소중한': 2847, '▁소지섭': 2848, '▁소집': 2849, '▁소통': 2850, '▁소폭': 2851, '▁소프트웨어': 2852, '▁소형': 2853, '▁소화': 2854, '▁소환': 2855, '▁속': 2856, '▁속도': 2857, '▁속에': 2858, '▁속에서': 2859, '▁손': 2860, '▁손님': 2861, '▁손실': 2862, '▁손연재': 2863, '▁손을': 2864, '▁손해배상': 2865, '▁손흥민': 2866, '▁솔로': 2867, '▁솔루션': 2868, '▁송': 2869, '▁쇼': 2870, '▁쇼핑': 2871, '▁수': 2872, '▁수급': 2873, '▁수도': 2874, '▁수도권': 2875, '▁수립': 2876, '▁수많은': 2877, '▁수목드라마': 2878, '▁수밖에': 2879, '▁수비': 2880, '▁수비수': 2881, '▁수사': 2882, '▁수사를': 2883, '▁수상': 2884, '▁수석': 2885, '▁수석대표': 2886, '▁수수료': 2887, '▁수술': 2888, '▁수십': 2889, '▁수업': 2890, '▁수영': 2891, '▁수요': 2892, '▁수요가': 2893, '▁수용': 2894, '▁수원': 2895, '▁수익': 2896, '▁수익률': 2897, '▁수익성': 2898, '▁수익을': 2899, '▁수입': 2900, '▁수정': 2901, '▁수주': 2902, '▁수준': 2903, '▁수준으로': 2904, '▁수준이다': 2905, '▁수출': 2906, '▁수치': 2907, '▁수치다': 2908, '▁수행': 2909, '▁수혜': 2910, '▁숙': 2911, '▁순': 2912, '▁순간': 2913, '▁순매도': 2914, '▁순매수': 2915, '▁순위': 2916, '▁순이익': 2917, '▁술': 2918, '▁숨': 2919, '▁숨졌다': 2920, '▁숨진': 2921, '▁숫자': 2922, '▁쉬': 2923, '▁쉽게': 2924, '▁쉽지': 2925, '▁슈': 2926, '▁슈팅': 2927, '▁슈퍼': 2928, '▁스': 2929, '▁스노든': 2930, '▁스마트': 2931, '▁스마트폰': 2932, '▁스몰캡': 2933, '▁스스로': 2934, '▁스위스': 2935, '▁스케줄': 2936, '▁스크린': 2937, '▁스타': 2938, '▁스타일': 2939, '▁스태프': 2940, '▁스토리': 2941, '▁스트레스': 2942, '▁스페셜올림픽': 2943, '▁스페인': 2944, '▁스포츠': 2945, '▁스포츠조선닷컴': 2946, '▁스피드': 2947, '▁슬': 2948, '▁승': 2949, '▁승객': 2950, '▁승리': 2951, '▁승리를': 2952, '▁승무원': 2953, '▁승부': 2954, '▁승부주': 2955, '▁승용차': 2956, '▁승인': 2957, '▁승진': 2958, '▁시': 2959, '▁시가총액': 2960, '▁시각': 2961, '▁시간': 2962, '▁시간을': 2963, '▁시간이': 2964, '▁시기': 2965, '▁시나리오': 2966, '▁시너지': 2967, '▁시는': 2968, '▁시달리': 2969, '▁시대': 2970, '▁시도': 2971, '▁시리아': 2972, '▁시리즈': 2973, '▁시민': 2974, '▁시민단체': 2975, '▁시민들': 2976, '▁시범': 2977, '▁시사': 2978, '▁시상식': 2979, '▁시선을': 2980, '▁시설': 2981, '▁시스템': 2982, '▁시스템을': 2983, '▁시신': 2984, '▁시위': 2985, '▁시작': 2986, '▁시작된': 2987, '▁시작으로': 2988, '▁시작한': 2989, '▁시작했다': 2990, '▁시장': 2991, '▁시장에': 2992, '▁시장에서': 2993, '▁시장은': 2994, '▁시장의': 2995, '▁시절': 2996, '▁시점': 2997, '▁시즌': 2998, '▁시청률': 2999, '▁시청자': 3000, '▁시청자들': 3001, '▁시청자들의': 3002, '▁시카고': 3003, '▁시행': 3004, '▁시험': 3005, '▁식': 3006, '▁식당': 3007, '▁식사': 3008, '▁식품': 3009, '▁신': 3010, '▁신경': 3011, '▁신고': 3012, '▁신곡': 3013, '▁신규': 3014, '▁신동엽': 3015, '▁신뢰': 3016, '▁신문': 3017, '▁신분': 3018, '▁신설': 3019, '▁신세계': 3020, '▁신속': 3021, '▁신시내티': 3022, '▁신용': 3023, '▁신용등급': 3024, '▁신용카드': 3025, '▁신인': 3026, '▁신임': 3027, '▁신제품': 3028, '▁신중': 3029, '▁신청': 3030, '▁신한금융투자': 3031, '▁신한은행': 3032, '▁신호': 3033, '▁신화': 3034, '▁신흥국': 3035, '▁실': 3036, '▁실내': 3037, '▁실력': 3038, '▁실망': 3039, '▁실무': 3040, '▁실무회담': 3041, '▁실수': 3042, '▁실시': 3043, '▁실시간': 3044, '▁실시한': 3045, '▁실시한다': 3046, '▁실적': 3047, '▁실적이': 3048, '▁실제': 3049, '▁실제로': 3050, '▁실종': 3051, '▁실질적인': 3052, '▁실천': 3053, '▁실태': 3054, '▁실패': 3055, '▁실행': 3056, '▁실험': 3057, '▁실현': 3058, '▁싫어': 3059, '▁심': 3060, '▁심각': 3061, '▁심각한': 3062, '▁심경': 3063, '▁심리': 3064, '▁심사': 3065, '▁심사위원': 3066, '▁심지어': 3067, '▁심판': 3068, '▁심화': 3069, '▁싱가포르': 3070, '▁싱글': 3071, '▁싶다': 3072, '▁싶어': 3073, '▁싶었': 3074, '▁싶은': 3075, '▁싸': 3076, '▁싸이': 3077, '▁쌍용차': 3078, '▁쌓': 3079, '▁써': 3080, '▁써니': 3081, '▁썼다': 3082, '▁쏟아': 3083, '▁쓰': 3084, '▁쓰레기': 3085, '▁쓴': 3086, '▁쓸': 3087, '▁씨': 3088, '▁씨가': 3089, '▁씨는': 3090, '▁씨스타': 3091, '▁씨엔블루': 3092, '▁아': 3093, '▁아끼': 3094, '▁아나운서': 3095, '▁아내': 3096, '▁아니': 3097, '▁아니냐': 3098, '▁아니냐는': 3099, '▁아니다': 3100, '▁아니라': 3101, '▁아니면': 3102, '▁아니었다': 3103, '▁아니지만': 3104, '▁아닌': 3105, '▁아동': 3106, '▁아들': 3107, '▁아래': 3108, '▁아름': 3109, '▁아름다운': 3110, '▁아무': 3111, '▁아버지': 3112, '▁아베': 3113, '▁아빠': 3114, '▁아쉬움': 3115, '▁아시아': 3116, '▁아시아나': 3117, '▁아시아나항공': 3118, '▁아예': 3119, '▁아울러': 3120, '▁아이': 3121, '▁아이돌': 3122, '▁아이들': 3123, '▁아이디어': 3124, '▁아이유': 3125, '▁아이템': 3126, '▁아이폰': 3127, '▁아주': 3128, '▁아직': 3129, '▁아침': 3130, '▁아파트': 3131, '▁아픔': 3132, '▁악': 3133, '▁악화': 3134, '▁안': 3135, '▁안내': 3136, '▁안보': 3137, '▁안에': 3138, '▁안전': 3139, '▁안정': 3140, '▁안정적인': 3141, '▁안타': 3142, '▁안타를': 3143, '▁안팎': 3144, '▁앉아': 3145, '▁않': 3146, '▁않게': 3147, '▁않겠다': 3148, '▁않고': 3149, '▁않기': 3150, '▁않는': 3151, '▁않는다': 3152, '▁않다': 3153, '▁않도록': 3154, '▁않아': 3155, '▁않았': 3156, '▁않았다': 3157, '▁않았던': 3158, '▁않았지만': 3159, '▁않으': 3160, '▁않으면': 3161, '▁않은': 3162, '▁않을': 3163, '▁않을까': 3164, '▁않지만': 3165, '▁알': 3166, '▁알게': 3167, '▁알고': 3168, '▁알려': 3169, '▁알려져': 3170, '▁알려졌다': 3171, '▁알려진': 3172, '▁알렸다': 3173, '▁알리': 3174, '▁알아': 3175, '▁알제리': 3176, '▁암': 3177, '▁압': 3178, '▁압도적': 3179, '▁압력': 3180, '▁압류': 3181, '▁압박': 3182, '▁압수수색': 3183, '▁앞': 3184, '▁앞두고': 3185, '▁앞둔': 3186, '▁앞서': 3187, '▁앞선': 3188, '▁앞세워': 3189, '▁앞에': 3190, '▁앞에서': 3191, '▁앞으로': 3192, '▁앞장서': 3193, '▁애': 3194, '▁애널리스트': 3195, '▁애니메이션': 3196, '▁애리조나': 3197, '▁애정': 3198, '▁애초': 3199, '▁애플': 3200, '▁애플리케이션': 3201, '▁액션': 3202, '▁앤': 3203, '▁앨범': 3204, '▁앱': 3205, '▁야': 3206, '▁야간': 3207, '▁야구': 3208, '▁야당': 3209, '▁야외': 3210, '▁약': 3211, '▁약세': 3212, '▁약속': 3213, '▁양': 3214, '▁양국': 3215, '▁양성': 3216, '▁양적완화': 3217, '▁양측': 3218, '▁얘기': 3219, '▁어': 3220, '▁어깨': 3221, '▁어느': 3222, '▁어디': 3223, '▁어떤': 3224, '▁어떻게': 3225, '▁어려운': 3226, '▁어려울': 3227, '▁어려움': 3228, '▁어려움을': 3229, '▁어려워': 3230, '▁어렵': 3231, '▁어렵다': 3232, '▁어린': 3233, '▁어린이': 3234, '▁어린이집': 3235, '▁어머니': 3236, '▁어울리': 3237, '▁어제': 3238, '▁억울': 3239, '▁억제': 3240, '▁언': 3241, '▁언급': 3242, '▁언급했다': 3243, '▁언론': 3244, '▁언제': 3245, '▁얻고': 3246, '▁얻어': 3247, '▁얻었다': 3248, '▁얻은': 3249, '▁얻을': 3250, '▁얼굴': 3251, '▁얼마': 3252, '▁얼마나': 3253, '▁얼음': 3254, '▁엄': 3255, '▁엄격': 3256, '▁엄마': 3257, '▁엄청난': 3258, '▁엄태웅': 3259, '▁업': 3260, '▁업계': 3261, '▁업그레이드': 3262, '▁업데이트': 3263, '▁업무': 3264, '▁업무를': 3265, '▁업무보고': 3266, '▁업종': 3267, '▁업체': 3268, '▁업체들': 3269, '▁없': 3270, '▁없고': 3271, '▁없는': 3272, '▁없다': 3273, '▁없다고': 3274, '▁없다는': 3275, '▁없도록': 3276, '▁없애': 3277, '▁없어': 3278, '▁없었': 3279, '▁없었다': 3280, '▁없었던': 3281, '▁없을': 3282, '▁없이': 3283, '▁없지만': 3284, '▁에너지': 3285, '▁에릭': 3286, '▁에스': 3287, '▁에어컨': 3288, '▁에이': 3289, '▁에이스': 3290, '▁에피소드': 3291, '▁엔': 3292, '▁엔진': 3293, '▁엔화': 3294, '▁엘': 3295, '▁엠': 3296, '▁엠넷': 3297, '▁여': 3298, '▁여객기': 3299, '▁여건': 3300, '▁여기': 3301, '▁여기에': 3302, '▁여당': 3303, '▁여러': 3304, '▁여러분': 3305, '▁여론': 3306, '▁여름': 3307, '▁여름철': 3308, '▁여배우': 3309, '▁여부': 3310, '▁여부를': 3311, '▁여성': 3312, '▁여성들': 3313, '▁여신': 3314, '▁여야': 3315, '▁여유': 3316, '▁여의도': 3317, '▁여자': 3318, '▁여자친구': 3319, '▁여전히': 3320, '▁여행': 3321, '▁역': 3322, '▁역대': 3323, '▁역량': 3324, '▁역사': 3325, '▁역사적': 3326, '▁역시': 3327, '▁역을': 3328, '▁역전': 3329, '▁역할': 3330, '▁역할을': 3331, '▁연': 3332, '▁연간': 3333, '▁연결': 3334, '▁연계': 3335, '▁연구': 3336, '▁연구개발': 3337, '▁연구원은': 3338, '▁연극': 3339, '▁연금': 3340, '▁연기': 3341, '▁연기를': 3342, '▁연락': 3343, '▁연령': 3344, '▁연말': 3345, '▁연방': 3346, '▁연봉': 3347, '▁연세대': 3348, '▁연속': 3349, '▁연습': 3350, '▁연애': 3351, '▁연예': 3352, '▁연예병사': 3353, '▁연예인': 3354, '▁연인': 3355, '▁연장': 3356, '▁연출': 3357, '▁열': 3358, '▁열고': 3359, '▁열람': 3360, '▁열렸다': 3361, '▁열리': 3362, '▁열리는': 3363, '▁열린': 3364, '▁열린다': 3365, '▁열릴': 3366, '▁열심히': 3367, '▁열애': 3368, '▁열애설': 3369, '▁열어': 3370, '▁열었다': 3371, '▁열정': 3372, '▁열차': 3373, '▁염': 3374, '▁염두에': 3375, '▁영': 3376, '▁영광': 3377, '▁영국': 3378, '▁영등포': 3379, '▁영상': 3380, '▁영어': 3381, '▁영업': 3382, '▁영업이익': 3383, '▁영업이익은': 3384, '▁영업익': 3385, '▁영업정지': 3386, '▁영역': 3387, '▁영입': 3388, '▁영하': 3389, '▁영향': 3390, '▁영향력': 3391, '▁영향으로': 3392, '▁영향을': 3393, '▁영화': 3394, '▁옆': 3395, '▁예': 3396, '▁예고': 3397, '▁예금': 3398, '▁예능': 3399, '▁예능프로그램': 3400, '▁예방': 3401, '▁예비': 3402, '▁예쁘': 3403, '▁예산': 3404, '▁예상': 3405, '▁예상되는': 3406, '▁예상된다': 3407, '▁예상치': 3408, '▁예상했다': 3409, '▁예술': 3410, '▁예약': 3411, '▁예전': 3412, '▁예정': 3413, '▁예정이다': 3414, '▁예측': 3415, '▁옛': 3416, '▁오': 3417, '▁오는': 3418, '▁오늘': 3419, '▁오늘의': 3420, '▁오디션': 3421, '▁오래': 3422, '▁오랜': 3423, '▁오랫동안': 3424, '▁오르': 3425, '▁오른': 3426, '▁오름세': 3427, '▁오바마': 3428, '▁오빠': 3429, '▁오연서': 3430, '▁오전': 3431, '▁오픈': 3432, '▁오피스텔': 3433, '▁오후': 3434, '▁오히려': 3435, '▁옥': 3436, '▁온': 3437, '▁온라인': 3438, '▁올': 3439, '▁올라': 3440, '▁올랐': 3441, '▁올랐다': 3442, '▁올려': 3443, '▁올렸다': 3444, '▁올리': 3445, '▁올린': 3446, '▁올림픽': 3447, '▁올스타': 3448, '▁올스타전': 3449, '▁올해': 3450, '▁올해부터': 3451, '▁옮겨': 3452, '▁옮기': 3453, '▁옷': 3454, '▁완': 3455, '▁완공': 3456, '▁완료': 3457, '▁완벽': 3458, '▁완벽한': 3459, '▁완성': 3460, '▁완전': 3461, '▁완전히': 3462, '▁완화': 3463, '▁왔다': 3464, '▁왕': 3465, '▁왜': 3466, '▁왜곡': 3467, '▁외': 3468, '▁외교': 3469, '▁외교부': 3470, '▁외국': 3471, '▁외국인': 3472, '▁외모': 3473, '▁외부': 3474, '▁외에': 3475, '▁외에도': 3476, '▁외환': 3477, '▁외환은행': 3478, '▁왼쪽': 3479, '▁요': 3480, '▁요구': 3481, '▁요구하는': 3482, '▁요구했다': 3483, '▁요금': 3484, '▁요금제': 3485, '▁요리': 3486, '▁요소': 3487, '▁요인': 3488, '▁요즘': 3489, '▁요청': 3490, '▁요청했다': 3491, '▁욕': 3492, '▁욕심': 3493, '▁용': 3494, '▁용산': 3495, '▁용의자': 3496, '▁용인': 3497, '▁우': 3498, '▁우려': 3499, '▁우려가': 3500, '▁우리': 3501, '▁우리가': 3502, '▁우리나라': 3503, '▁우리는': 3504, '▁우리은행': 3505, '▁우리투자증권': 3506, '▁우선': 3507, '▁우수': 3508, '▁우승': 3509, '▁우승을': 3510, '▁우주': 3511, '▁우즈': 3512, '▁운': 3513, '▁운동': 3514, '▁운명': 3515, '▁운송': 3516, '▁운영': 3517, '▁운영하는': 3518, '▁운용': 3519, '▁운전': 3520, '▁운전자': 3521, '▁운항': 3522, '▁운행': 3523, '▁울': 3524, '▁울산': 3525, '▁움직이': 3526, '▁움직임': 3527, '▁웃': 3528, '▁웃음을': 3529, '▁워': 3530, '▁워낙': 3531, '▁워싱턴': 3532, '▁원': 3533, '▁원내대변인': 3534, '▁원내대표': 3535, '▁원내대표는': 3536, '▁원래': 3537, '▁원인': 3538, '▁원장': 3539, '▁원전': 3540, '▁원정': 3541, '▁원정경기': 3542, '▁원칙': 3543, '▁원하는': 3544, '▁원화': 3545, '▁원활': 3546, '▁월드': 3547, '▁월드컵': 3548, '▁월화드라마': 3549, '▁웨딩': 3550, '▁웹': 3551, '▁위': 3552, '▁위기': 3553, '▁위로': 3554, '▁위반': 3555, '▁위안부': 3556, '▁위원': 3557, '▁위원장': 3558, '▁위원장은': 3559, '▁위조': 3560, '▁위촉': 3561, '▁위축': 3562, '▁위치': 3563, '▁위치한': 3564, '▁위탁': 3565, '▁위한': 3566, '▁위해': 3567, '▁위해서': 3568, '▁위해서는': 3569, '▁위헌': 3570, '▁위험': 3571, '▁위협': 3572, '▁윌리엄': 3573, '▁유': 3574, '▁유가증권시장': 3575, '▁유감': 3576, '▁유나이티드': 3577, '▁유니폼': 3578, '▁유도': 3579, '▁유동성': 3580, '▁유럽': 3581, '▁유력': 3582, '▁유로존': 3583, '▁유리': 3584, '▁유명': 3585, '▁유사': 3586, '▁유엔': 3587, '▁유일': 3588, '▁유입': 3589, '▁유재석': 3590, '▁유족': 3591, '▁유지': 3592, '▁유출': 3593, '▁유치': 3594, '▁유통': 3595, '▁유튜브': 3596, '▁육': 3597, '▁육군': 3598, '▁육박': 3599, '▁육성': 3600, '▁윤': 3601, '▁윤씨': 3602, '▁윤창중': 3603, '▁은퇴': 3604, '▁은행': 3605, '▁음': 3606, '▁음반': 3607, '▁음성': 3608, '▁음식': 3609, '▁음악': 3610, '▁음원': 3611, '▁응': 3612, '▁응급': 3613, '▁응답': 3614, '▁응답자': 3615, '▁응시': 3616, '▁응원': 3617, '▁의견': 3618, '▁의견을': 3619, '▁의견이': 3620, '▁의결': 3621, '▁의도': 3622, '▁의뢰': 3623, '▁의료': 3624, '▁의류': 3625, '▁의무': 3626, '▁의문': 3627, '▁의미': 3628, '▁의사': 3629, '▁의상': 3630, '▁의식': 3631, '▁의심': 3632, '▁의약품': 3633, '▁의원': 3634, '▁의원들': 3635, '▁의원은': 3636, '▁의원이': 3637, '▁의장': 3638, '▁의존': 3639, '▁의지': 3640, '▁의지를': 3641, '▁의한': 3642, '▁의해': 3643, '▁의혹': 3644, '▁의혹을': 3645, '▁의회': 3646, '▁이': 3647, '▁이같은': 3648, '▁이같이': 3649, '▁이것': 3650, '▁이곳': 3651, '▁이끄는': 3652, '▁이끌': 3653, '▁이끌어': 3654, '▁이끌었다': 3655, '▁이날': 3656, '▁이내': 3657, '▁이는': 3658, '▁이달': 3659, '▁이대호': 3660, '▁이데일리': 3661, '▁이동': 3662, '▁이동통신': 3663, '▁이동흡': 3664, '▁이들': 3665, '▁이들은': 3666, '▁이들의': 3667, '▁이들이': 3668, '▁이라크': 3669, '▁이래': 3670, '▁이러한': 3671, '▁이런': 3672, '▁이렇게': 3673, '▁이례적': 3674, '▁이로써': 3675, '▁이루': 3676, '▁이뤄': 3677, '▁이뤄졌다': 3678, '▁이뤄지': 3679, '▁이뤄진': 3680, '▁이뤄질': 3681, '▁이르는': 3682, '▁이르면': 3683, '▁이른다': 3684, '▁이른바': 3685, '▁이를': 3686, '▁이름': 3687, '▁이름을': 3688, '▁이마트': 3689, '▁이메일': 3690, '▁이명박': 3691, '▁이미': 3692, '▁이미지': 3693, '▁이미지를': 3694, '▁이민': 3695, '▁이밖에': 3696, '▁이번': 3697, '▁이번에': 3698, '▁이벤트': 3699, '▁이벤트를': 3700, '▁이병헌': 3701, '▁이사장': 3702, '▁이사회': 3703, '▁이상': 3704, '▁이상의': 3705, '▁이서진': 3706, '▁이수': 3707, '▁이슈': 3708, '▁이스라엘': 3709, '▁이슬람': 3710, '▁이승': 3711, '▁이승엽': 3712, '▁이시영': 3713, '▁이야기': 3714, '▁이야기를': 3715, '▁이어': 3716, '▁이어가고': 3717, '▁이어갔다': 3718, '▁이어졌다': 3719, '▁이어지고': 3720, '▁이어지는': 3721, '▁이어진': 3722, '▁이어질': 3723, '▁이에': 3724, '▁이와': 3725, '▁이용': 3726, '▁이용자': 3727, '▁이용한': 3728, '▁이용할': 3729, '▁이용해': 3730, '▁이웃': 3731, '▁이유': 3732, '▁이유는': 3733, '▁이유로': 3734, '▁이유를': 3735, '▁이익': 3736, '▁이재': 3737, '▁이적': 3738, '▁이전': 3739, '▁이정': 3740, '▁이정현': 3741, '▁이제': 3742, '▁이종': 3743, '▁이종석': 3744, '▁이준': 3745, '▁이중': 3746, '▁이집트': 3747, '▁이처럼': 3748, '▁이탈리아': 3749, '▁이틀': 3750, '▁이하': 3751, '▁이해': 3752, '▁이행': 3753, '▁이혼': 3754, '▁이효리': 3755, '▁이후': 3756, '▁익': 3757, '▁인': 3758, '▁인간': 3759, '▁인구': 3760, '▁인권': 3761, '▁인근': 3762, '▁인기': 3763, '▁인기를': 3764, '▁인도': 3765, '▁인도네시아': 3766, '▁인력': 3767, '▁인물': 3768, '▁인사': 3769, '▁인사들': 3770, '▁인사를': 3771, '▁인사청문회': 3772, '▁인상': 3773, '▁인생': 3774, '▁인선': 3775, '▁인쇄': 3776, '▁인수': 3777, '▁인수위': 3778, '▁인수위원': 3779, '▁인수위원회': 3780, '▁인식': 3781, '▁인연': 3782, '▁인원': 3783, '▁인재': 3784, '▁인정': 3785, '▁인정받': 3786, '▁인증': 3787, '▁인증샷': 3788, '▁인질': 3789, '▁인천': 3790, '▁인천공항': 3791, '▁인천시': 3792, '▁인터': 3793, '▁인터넷': 3794, '▁인터뷰': 3795, '▁인터뷰에서': 3796, '▁인턴기자': 3797, '▁인프라': 3798, '▁인피니트': 3799, '▁인하': 3800, '▁인한': 3801, '▁인해': 3802, '▁일': 3803, '▁일각에서는': 3804, '▁일단': 3805, '▁일대': 3806, '▁일반': 3807, '▁일방적': 3808, '▁일본': 3809, '▁일본의': 3810, '▁일부': 3811, '▁일상': 3812, '▁일어나': 3813, '▁일어난': 3814, '▁일으키': 3815, '▁일으킨': 3816, '▁일을': 3817, '▁일이': 3818, '▁일자리': 3819, '▁일정': 3820, '▁일제히': 3821, '▁일주일': 3822, '▁일환으로': 3823, '▁읽': 3824, '▁잃은': 3825, '▁임': 3826, '▁임금': 3827, '▁임기': 3828, '▁임대': 3829, '▁임명': 3830, '▁임시': 3831, '▁임시국회': 3832, '▁임신': 3833, '▁임원': 3834, '▁임직원': 3835, '▁입': 3836, '▁입건': 3837, '▁입고': 3838, '▁입단': 3839, '▁입력': 3840, '▁입법': 3841, '▁입원': 3842, '▁입은': 3843, '▁입장': 3844, '▁입장을': 3845, '▁입장이다': 3846, '▁입주': 3847, '▁입증': 3848, '▁입지': 3849, '▁입찰': 3850, '▁입학': 3851, '▁잇': 3852, '▁잇따라': 3853, '▁있': 3854, '▁있게': 3855, '▁있겠': 3856, '▁있고': 3857, '▁있기': 3858, '▁있느냐': 3859, '▁있는': 3860, '▁있는데': 3861, '▁있다': 3862, '▁있다고': 3863, '▁있다는': 3864, '▁있던': 3865, '▁있도록': 3866, '▁있습니다': 3867, '▁있어': 3868, '▁있어서': 3869, '▁있어야': 3870, '▁있었': 3871, '▁있었는데': 3872, '▁있었다': 3873, '▁있었던': 3874, '▁있었지만': 3875, '▁있으나': 3876, '▁있으니': 3877, '▁있으며': 3878, '▁있으면': 3879, '▁있을': 3880, '▁있을까': 3881, '▁있을지': 3882, '▁있음': 3883, '▁있지': 3884, '▁있지만': 3885, '▁자': 3886, '▁자격': 3887, '▁자극': 3888, '▁자금': 3889, '▁자기': 3890, '▁자녀': 3891, '▁자동': 3892, '▁자동차': 3893, '▁자랑': 3894, '▁자료': 3895, '▁자료를': 3896, '▁자리': 3897, '▁자리를': 3898, '▁자리에': 3899, '▁자리에서': 3900, '▁자발적': 3901, '▁자본': 3902, '▁자본시장': 3903, '▁자사': 3904, '▁자산': 3905, '▁자살': 3906, '▁자세': 3907, '▁자세한': 3908, '▁자신': 3909, '▁자신감': 3910, '▁자신들': 3911, '▁자신을': 3912, '▁자신의': 3913, '▁자신이': 3914, '▁자아냈다': 3915, '▁자연': 3916, '▁자연스럽게': 3917, '▁자원': 3918, '▁자원봉사': 3919, '▁자유': 3920, '▁자율': 3921, '▁자전거': 3922, '▁자존심': 3923, '▁자주': 3924, '▁자체': 3925, '▁자체가': 3926, '▁자칫': 3927, '▁자택': 3928, '▁자회사': 3929, '▁작': 3930, '▁작가': 3931, '▁작곡': 3932, '▁작년': 3933, '▁작동': 3934, '▁작성': 3935, '▁작업': 3936, '▁작업을': 3937, '▁작용': 3938, '▁작은': 3939, '▁작품': 3940, '▁잔': 3941, '▁잘': 3942, '▁잘못': 3943, '▁잘못된': 3944, '▁잠': 3945, '▁잠시': 3946, '▁잠실': 3947, '▁잠재': 3948, '▁잠정': 3949, '▁잡': 3950, '▁잡고': 3951, '▁잡아': 3952, '▁잡았다': 3953, '▁장': 3954, '▁장관': 3955, '▁장관은': 3956, '▁장기': 3957, '▁장난': 3958, '▁장르': 3959, '▁장면': 3960, '▁장비': 3961, '▁장소': 3962, '▁장애': 3963, '▁장애인': 3964, '▁장윤정': 3965, '▁장점': 3966, '▁장착': 3967, '▁장학금': 3968, '▁재': 3969, '▁재가동': 3970, '▁재개': 3971, '▁재건축': 3972, '▁재계약': 3973, '▁재난': 3974, '▁재능': 3975, '▁재무': 3976, '▁재미': 3977, '▁재미있': 3978, '▁재발': 3979, '▁재발방지': 3980, '▁재벌': 3981, '▁재산': 3982, '▁재원': 3983, '▁재정': 3984, '▁재정절벽': 3985, '▁재판': 3986, '▁재판부는': 3987, '▁재활': 3988, '▁쟁점': 3989, '▁저': 3990, '▁저녁': 3991, '▁저렴': 3992, '▁저소득층': 3993, '▁저장': 3994, '▁저지른': 3995, '▁적': 3996, '▁적극': 3997, '▁적극적으로': 3998, '▁적극적인': 3999, '▁적립': 4000, '▁적발': 4001, '▁적시타': 4002, '▁적어': 4003, '▁적용': 4004, '▁적은': 4005, '▁적응': 4006, '▁적이': 4007, '▁적자': 4008, '▁적절': 4009, '▁적지': 4010, '▁적합': 4011, '▁전': 4012, '▁전개': 4013, '▁전국': 4014, '▁전기': 4015, '▁전날': 4016, '▁전날보다': 4017, '▁전남': 4018, '▁전년': 4019, '▁전년대비': 4020, '▁전년동기': 4021, '▁전달': 4022, '▁전담': 4023, '▁전두환': 4024, '▁전략': 4025, '▁전력': 4026, '▁전망': 4027, '▁전망된다': 4028, '▁전망이다': 4029, '▁전망치': 4030, '▁전망했다': 4031, '▁전면': 4032, '▁전문': 4033, '▁전문가': 4034, '▁전문가들': 4035, '▁전문가들은': 4036, '▁전반': 4037, '▁전반기': 4038, '▁전부터': 4039, '▁전북': 4040, '▁전세': 4041, '▁전세계': 4042, '▁전시': 4043, '▁전액': 4044, '▁전에': 4045, '▁전역': 4046, '▁전용': 4047, '▁전원': 4048, '▁전자': 4049, '▁전쟁': 4050, '▁전주': 4051, '▁전지현': 4052, '▁전지훈련': 4053, '▁전체': 4054, '▁전체회의': 4055, '▁전통': 4056, '▁전투': 4057, '▁전파': 4058, '▁전해': 4059, '▁전해졌다': 4060, '▁전했다': 4061, '▁전혀': 4062, '▁전형': 4063, '▁전화': 4064, '▁전환': 4065, '▁전후': 4066, '▁절': 4067, '▁절감': 4068, '▁절대': 4069, '▁절반': 4070, '▁절차': 4071, '▁절차를': 4072, '▁젊은': 4073, '▁점': 4074, '▁점검': 4075, '▁점도': 4076, '▁점수': 4077, '▁점에서': 4078, '▁점유율': 4079, '▁점을': 4080, '▁점이': 4081, '▁점이다': 4082, '▁점점': 4083, '▁점차': 4084, '▁점포': 4085, '▁접': 4086, '▁접근': 4087, '▁접속': 4088, '▁접수': 4089, '▁접촉': 4090, '▁접한': 4091, '▁정': 4092, '▁정권': 4093, '▁정규': 4094, '▁정규직': 4095, '▁정기': 4096, '▁정당': 4097, '▁정당공천': 4098, '▁정도': 4099, '▁정도로': 4100, '▁정리': 4101, '▁정말': 4102, '▁정보': 4103, '▁정보를': 4104, '▁정보통신': 4105, '▁정부': 4106, '▁정부가': 4107, '▁정부는': 4108, '▁정부와': 4109, '▁정부의': 4110, '▁정부조직': 4111, '▁정비': 4112, '▁정상': 4113, '▁정상화': 4114, '▁정상회담': 4115, '▁정식': 4116, '▁정신': 4117, '▁정착': 4118, '▁정책': 4119, '▁정책을': 4120, '▁정체': 4121, '▁정치': 4122, '▁정치권': 4123, '▁정치적': 4124, '▁정확한': 4125, '▁정확히': 4126, '▁정황': 4127, '▁제': 4128, '▁제거': 4129, '▁제공': 4130, '▁제공하고': 4131, '▁제공하는': 4132, '▁제공한다': 4133, '▁제기': 4134, '▁제기되고': 4135, '▁제대로': 4136, '▁제도': 4137, '▁제목으로': 4138, '▁제시': 4139, '▁제시한': 4140, '▁제시했다': 4141, '▁제안': 4142, '▁제약': 4143, '▁제외': 4144, '▁제외한': 4145, '▁제임스': 4146, '▁제작': 4147, '▁제작발표회': 4148, '▁제작진': 4149, '▁제재': 4150, '▁제조': 4151, '▁제조업': 4152, '▁제조업체': 4153, '▁제주': 4154, '▁제주도': 4155, '▁제출': 4156, '▁제치고': 4157, '▁제품': 4158, '▁제품을': 4159, '▁제한': 4160, '▁제휴': 4161, '▁조': 4162, '▁조건': 4163, '▁조금': 4164, '▁조금씩': 4165, '▁조기': 4166, '▁조달': 4167, '▁조례': 4168, '▁조만간': 4169, '▁조명': 4170, '▁조사': 4171, '▁조사결과': 4172, '▁조사됐다': 4173, '▁조사를': 4174, '▁조사하고': 4175, '▁조선': 4176, '▁조성': 4177, '▁조성민': 4178, '▁조심': 4179, '▁조언': 4180, '▁조율': 4181, '▁조작': 4182, '▁조절': 4183, '▁조정': 4184, '▁조종사': 4185, '▁조직': 4186, '▁조직개편': 4187, '▁조치': 4188, '▁조치를': 4189, '▁조합원': 4190, '▁조항': 4191, '▁존': 4192, '▁존재': 4193, '▁존중': 4194, '▁졸업': 4195, '▁좀': 4196, '▁종': 4197, '▁종교': 4198, '▁종로구': 4199, '▁종료': 4200, '▁종류': 4201, '▁종목': 4202, '▁종합': 4203, '▁좋': 4204, '▁좋겠다': 4205, '▁좋다': 4206, '▁좋아': 4207, '▁좋았': 4208, '▁좋은': 4209, '▁좋지': 4210, '▁좌': 4211, '▁죄': 4212, '▁주': 4213, '▁주가': 4214, '▁주가가': 4215, '▁주거': 4216, '▁주고': 4217, '▁주관': 4218, '▁주는': 4219, '▁주도': 4220, '▁주력': 4221, '▁주로': 4222, '▁주말': 4223, '▁주말드라마': 4224, '▁주목': 4225, '▁주목된다': 4226, '▁주문': 4227, '▁주민': 4228, '▁주변': 4229, '▁주식': 4230, '▁주식시장': 4231, '▁주연': 4232, '▁주요': 4233, '▁주요뉴스': 4234, '▁주인공': 4235, '▁주장': 4236, '▁주장했다': 4237, '▁주재': 4238, '▁주제': 4239, '▁주제로': 4240, '▁주주': 4241, '▁주최': 4242, '▁주택': 4243, '▁죽': 4244, '▁죽음': 4245, '▁준': 4246, '▁준공': 4247, '▁준다': 4248, '▁준비': 4249, '▁준수': 4250, '▁준우승': 4251, '▁줄': 4252, '▁줄어든': 4253, '▁줄어들': 4254, '▁줄었다': 4255, '▁줄여': 4256, '▁중': 4257, '▁중간': 4258, '▁중국': 4259, '▁중국의': 4260, '▁중단': 4261, '▁중반': 4262, '▁중소': 4263, '▁중소기업': 4264, '▁중소형주': 4265, '▁중순': 4266, '▁중심': 4267, '▁중심으로': 4268, '▁중앙': 4269, '▁중요': 4270, '▁중요성': 4271, '▁중요하다': 4272, '▁중요한': 4273, '▁중이다': 4274, '▁중인': 4275, '▁중장기': 4276, '▁중점': 4277, '▁중흥': 4278, '▁즉': 4279, '▁즉각': 4280, '▁즉시': 4281, '▁즐거운': 4282, '▁즐기': 4283, '▁즐길': 4284, '▁증': 4285, '▁증가': 4286, '▁증가율': 4287, '▁증가한': 4288, '▁증가했다': 4289, '▁증거': 4290, '▁증권': 4291, '▁증권사': 4292, '▁증명': 4293, '▁증시': 4294, '▁증액': 4295, '▁증인': 4296, '▁지': 4297, '▁지구': 4298, '▁지금': 4299, '▁지금까지': 4300, '▁지금은': 4301, '▁지급': 4302, '▁지나': 4303, '▁지난': 4304, '▁지난달': 4305, '▁지난해': 4306, '▁지내': 4307, '▁지낸': 4308, '▁지닌': 4309, '▁지도': 4310, '▁지도부': 4311, '▁지도자': 4312, '▁지동원': 4313, '▁지명': 4314, '▁지목': 4315, '▁지방': 4316, '▁지방자치단체': 4317, '▁지배': 4318, '▁지분': 4319, '▁지사': 4320, '▁지속': 4321, '▁지속될': 4322, '▁지속적으로': 4323, '▁지속적인': 4324, '▁지수': 4325, '▁지시': 4326, '▁지식': 4327, '▁지식경제부': 4328, '▁지역': 4329, '▁지연': 4330, '▁지원': 4331, '▁지원을': 4332, '▁지원하는': 4333, '▁지원한다': 4334, '▁지자체': 4335, '▁지적': 4336, '▁지적했다': 4337, '▁지점': 4338, '▁지정': 4339, '▁지지': 4340, '▁지출': 4341, '▁지켜': 4342, '▁지켜보': 4343, '▁지키': 4344, '▁지표': 4345, '▁지하': 4346, '▁지하철': 4347, '▁지휘': 4348, '▁직': 4349, '▁직구': 4350, '▁직무': 4351, '▁직업': 4352, '▁직원': 4353, '▁직원들': 4354, '▁직장': 4355, '▁직장인': 4356, '▁직전': 4357, '▁직접': 4358, '▁직후': 4359, '▁진': 4360, '▁진단': 4361, '▁진료': 4362, '▁진보': 4363, '▁진술': 4364, '▁진실': 4365, '▁진입': 4366, '▁진주의료원': 4367, '▁진짜': 4368, '▁진출': 4369, '▁진행': 4370, '▁진행됐다': 4371, '▁진행되는': 4372, '▁진행된': 4373, '▁진행된다': 4374, '▁진행될': 4375, '▁진행하고': 4376, '▁진행한다': 4377, '▁진화': 4378, '▁질': 4379, '▁질문': 4380, '▁질문에': 4381, '▁질병': 4382, '▁짐': 4383, '▁집': 4384, '▁집계': 4385, '▁집계됐다': 4386, '▁집권': 4387, '▁집단': 4388, '▁집중': 4389, '▁집행': 4390, '▁집행유예': 4391, '▁집회': 4392, '▁짓': 4393, '▁징계': 4394, '▁징역': 4395, '▁짜': 4396, '▁짧은': 4397, '▁쪽': 4398, '▁쪽으로': 4399, '▁찍': 4400, '▁찍은': 4401, '▁차': 4402, '▁차관': 4403, '▁차기': 4404, '▁차단': 4405, '▁차량': 4406, '▁차례': 4407, '▁차별': 4408, '▁차별화된': 4409, '▁차세대': 4410, '▁차원에서': 4411, '▁차원의': 4412, '▁차이': 4413, '▁차이가': 4414, '▁차지': 4415, '▁차지하는': 4416, '▁차지한': 4417, '▁차지했다': 4418, '▁차질': 4419, '▁착': 4420, '▁착공': 4421, '▁착륙': 4422, '▁착수': 4423, '▁착용': 4424, '▁찬': 4425, '▁찬성': 4426, '▁참': 4427, '▁참가': 4428, '▁참가자들': 4429, '▁참고': 4430, '▁참석': 4431, '▁참석한': 4432, '▁참석해': 4433, '▁참석했다': 4434, '▁참여': 4435, '▁참여한': 4436, '▁참의원': 4437, '▁창': 4438, '▁창단': 4439, '▁창업': 4440, '▁창원': 4441, '▁창조': 4442, '▁창출': 4443, '▁찾기': 4444, '▁찾는': 4445, '▁찾아': 4446, '▁찾았다': 4447, '▁찾은': 4448, '▁찾을': 4449, '▁찾지': 4450, '▁채': 4451, '▁채권': 4452, '▁채널': 4453, '▁채무': 4454, '▁채용': 4455, '▁채택': 4456, '▁책': 4457, '▁책임': 4458, '▁책임을': 4459, '▁챔피언십': 4460, '▁챙겨': 4461, '▁챙기': 4462, '▁챙긴': 4463, '▁처': 4464, '▁처리': 4465, '▁처벌': 4466, '▁처분': 4467, '▁처음': 4468, '▁처음으로': 4469, '▁처음이다': 4470, '▁천': 4471, '▁천안': 4472, '▁철': 4473, '▁철강': 4474, '▁철거': 4475, '▁철저한': 4476, '▁철저히': 4477, '▁철학': 4478, '▁철회': 4479, '▁첨단': 4480, '▁첫': 4481, '▁첫날': 4482, '▁청': 4483, '▁청구': 4484, '▁청년': 4485, '▁청문회': 4486, '▁청소년': 4487, '▁청약': 4488, '▁청와대': 4489, '▁청주': 4490, '▁체': 4491, '▁체결': 4492, '▁체결했다': 4493, '▁체계적': 4494, '▁체력': 4495, '▁체제': 4496, '▁체크': 4497, '▁체포': 4498, '▁체험': 4499, '▁첼시': 4500, '▁초': 4501, '▁초과': 4502, '▁초기': 4503, '▁초대': 4504, '▁초등학교': 4505, '▁초반': 4506, '▁초점을': 4507, '▁초청': 4508, '▁촉구': 4509, '▁촉구했다': 4510, '▁촉진': 4511, '▁총': 4512, '▁총괄': 4513, '▁총기': 4514, '▁총리': 4515, '▁총장': 4516, '▁총재': 4517, '▁촬영': 4518, '▁최': 4519, '▁최강': 4520, '▁최강희': 4521, '▁최고': 4522, '▁최고위원': 4523, '▁최고의': 4524, '▁최근': 4525, '▁최다': 4526, '▁최대': 4527, '▁최대주주': 4528, '▁최대한': 4529, '▁최선을': 4530, '▁최소': 4531, '▁최소화': 4532, '▁최신': 4533, '▁최악': 4534, '▁최우선': 4535, '▁최우수': 4536, '▁최저': 4537, '▁최종': 4538, '▁최초': 4539, '▁최초로': 4540, '▁추': 4541, '▁추가': 4542, '▁추가로': 4543, '▁추격': 4544, '▁추구': 4545, '▁추락': 4546, '▁추산': 4547, '▁추세': 4548, '▁추신수': 4549, '▁추신수는': 4550, '▁추억': 4551, '▁추적': 4552, '▁추정': 4553, '▁추정된다': 4554, '▁추진': 4555, '▁추진하고': 4556, '▁추진할': 4557, '▁추징금': 4558, '▁추천': 4559, '▁추첨': 4560, '▁축': 4561, '▁축구': 4562, '▁축구협회': 4563, '▁축소': 4564, '▁축제': 4565, '▁축하': 4566, '▁춘천': 4567, '▁출': 4568, '▁출구전략': 4569, '▁출국': 4570, '▁출근': 4571, '▁출동': 4572, '▁출발': 4573, '▁출범': 4574, '▁출산': 4575, '▁출석': 4576, '▁출시': 4577, '▁출신': 4578, '▁출연': 4579, '▁출연한': 4580, '▁출연해': 4581, '▁출연했다': 4582, '▁출입': 4583, '▁출장': 4584, '▁출전': 4585, '▁출처': 4586, '▁출판': 4587, '▁충': 4588, '▁충격': 4589, '▁충남': 4590, '▁충돌': 4591, '▁충북': 4592, '▁충분': 4593, '▁충분히': 4594, '▁충실': 4595, '▁충족': 4596, '▁충청': 4597, '▁취': 4598, '▁취급': 4599, '▁취득': 4600, '▁취득세': 4601, '▁취소': 4602, '▁취약': 4603, '▁취업': 4604, '▁취임': 4605, '▁취임식': 4606, '▁취재': 4607, '▁취재진': 4608, '▁취지': 4609, '▁취하고': 4610, '▁측': 4611, '▁측근': 4612, '▁측면': 4613, '▁측면에서': 4614, '▁측은': 4615, '▁측정': 4616, '▁치': 4617, '▁치러': 4618, '▁치료': 4619, '▁치료를': 4620, '▁치르': 4621, '▁치른': 4622, '▁치솟': 4623, '▁치열': 4624, '▁치열한': 4625, '▁친': 4626, '▁친구': 4627, '▁친구들': 4628, '▁친환경': 4629, '▁침': 4630, '▁침수': 4631, '▁침체': 4632, '▁침해': 4633, '▁칭찬': 4634, '▁카': 4635, '▁카드': 4636, '▁카드사': 4637, '▁카리스마': 4638, '▁카메라': 4639, '▁카카오': 4640, '▁카페': 4641, '▁칼': 4642, '▁캐': 4643, '▁캐나다': 4644, '▁캐릭터': 4645, '▁캐스팅': 4646, '▁캘리포니아': 4647, '▁캠페인': 4648, '▁캠프': 4649, '▁캠핑': 4650, '▁캡처': 4651, '▁커': 4652, '▁커뮤니티': 4653, '▁커지고': 4654, '▁커플': 4655, '▁커피': 4656, '▁컨트롤': 4657, '▁컬러': 4658, '▁컴백': 4659, '▁컴퓨터': 4660, '▁컸다': 4661, '▁케': 4662, '▁케이': 4663, '▁케이블': 4664, '▁코': 4665, '▁코너': 4666, '▁코넥스': 4667, '▁코리아': 4668, '▁코미디': 4669, '▁코스': 4670, '▁코스닥': 4671, '▁코스피': 4672, '▁코스피지수': 4673, '▁코치': 4674, '▁콘': 4675, '▁콘서트': 4676, '▁콘셉트': 4677, '▁콘텐츠': 4678, '▁콜': 4679, '▁쿠': 4680, '▁크': 4681, '▁크게': 4682, '▁크기': 4683, '▁크다': 4684, '▁크로스': 4685, '▁크루즈': 4686, '▁크리스': 4687, '▁큰': 4688, '▁클': 4689, '▁클라라': 4690, '▁클래식': 4691, '▁클럽': 4692, '▁키': 4693, '▁키스': 4694, '▁키우': 4695, '▁키워': 4696, '▁키워드': 4697, '▁타': 4698, '▁타격': 4699, '▁타고': 4700, '▁타선': 4701, '▁타율': 4702, '▁타이': 4703, '▁타이틀': 4704, '▁타이틀곡': 4705, '▁타자': 4706, '▁탄': 4707, '▁탄력': 4708, '▁탄생': 4709, '▁탄탄한': 4710, '▁탈': 4711, '▁탈락': 4712, '▁탈출': 4713, '▁탐': 4714, '▁탑승': 4715, '▁탑재': 4716, '▁탓': 4717, '▁탓에': 4718, '▁탔다': 4719, '▁태': 4720, '▁태국': 4721, '▁태도': 4722, '▁태블릿': 4723, '▁태양': 4724, '▁태양광': 4725, '▁태어났다': 4726, '▁택시': 4727, '▁터': 4728, '▁터치': 4729, '▁터키': 4730, '▁털어놓': 4731, '▁털어놨다': 4732, '▁테': 4733, '▁테러': 4734, '▁테마': 4735, '▁테스트': 4736, '▁토': 4737, '▁토대로': 4738, '▁토론회': 4739, '▁토지': 4740, '▁토크쇼': 4741, '▁톱스타': 4742, '▁통': 4743, '▁통계': 4744, '▁통과': 4745, '▁통보': 4746, '▁통산': 4747, '▁통상': 4748, '▁통신': 4749, '▁통일': 4750, '▁통일부': 4751, '▁통제': 4752, '▁통증': 4753, '▁통한': 4754, '▁통합': 4755, '▁통해': 4756, '▁통행': 4757, '▁통화': 4758, '▁통화정책': 4759, '▁퇴': 4760, '▁퇴직': 4761, '▁투': 4762, '▁투구': 4763, '▁투명': 4764, '▁투수': 4765, '▁투어': 4766, '▁투입': 4767, '▁투자': 4768, '▁투자의견': 4769, '▁투자자': 4770, '▁투자자들': 4771, '▁투표': 4772, '▁트': 4773, '▁트렌드': 4774, '▁트위터': 4775, '▁트위터에': 4776, '▁특': 4777, '▁특별': 4778, '▁특별사면': 4779, '▁특별한': 4780, '▁특사': 4781, '▁특성': 4782, '▁특수': 4783, '▁특위': 4784, '▁특유': 4785, '▁특정': 4786, '▁특집': 4787, '▁특징': 4788, '▁특징이다': 4789, '▁특허': 4790, '▁특혜': 4791, '▁특히': 4792, '▁틀': 4793, '▁티아라': 4794, '▁티저': 4795, '▁티켓': 4796, '▁팀': 4797, '▁팀의': 4798, '▁파': 4799, '▁파견': 4800, '▁파괴': 4801, '▁파악': 4802, '▁파워': 4803, '▁파트너': 4804, '▁판': 4805, '▁판결': 4806, '▁판단': 4807, '▁판단했다': 4808, '▁판매': 4809, '▁판매량': 4810, '▁판문점': 4811, '▁판사': 4812, '▁판정': 4813, '▁팔': 4814, '▁패': 4815, '▁패널': 4816, '▁패배': 4817, '▁패션': 4818, '▁패스': 4819, '▁패키지': 4820, '▁팬': 4821, '▁팬들': 4822, '▁팬들에게': 4823, '▁팬들의': 4824, '▁팽팽': 4825, '▁퍼': 4826, '▁퍼포먼스': 4827, '▁펀드': 4828, '▁페': 4829, '▁페널티': 4830, '▁페이스북': 4831, '▁편': 4832, '▁편리하게': 4833, '▁편성': 4834, '▁편의점': 4835, '▁편집': 4836, '▁펼쳐': 4837, '▁펼쳤다': 4838, '▁펼치고': 4839, '▁펼칠': 4840, '▁평': 4841, '▁평가': 4842, '▁평가를': 4843, '▁평가했다': 4844, '▁평균': 4845, '▁평균자책점': 4846, '▁평생': 4847, '▁평소': 4848, '▁평창': 4849, '▁평택': 4850, '▁평화': 4851, '▁폐': 4852, '▁폐기': 4853, '▁폐쇄': 4854, '▁폐지': 4855, '▁포': 4856, '▁포기': 4857, '▁포스코': 4858, '▁포인트': 4859, '▁포즈를': 4860, '▁포착': 4861, '▁포털': 4862, '▁포토': 4863, '▁포함': 4864, '▁포함돼': 4865, '▁포함됐다': 4866, '▁포함된': 4867, '▁포함한': 4868, '▁포함해': 4869, '▁포항': 4870, '▁폭': 4871, '▁폭력': 4872, '▁폭로': 4873, '▁폭발': 4874, '▁폭염': 4875, '▁폭우': 4876, '▁폭풍': 4877, '▁폭행': 4878, '▁폴': 4879, '▁표': 4880, '▁표명': 4881, '▁표시': 4882, '▁표정': 4883, '▁표준': 4884, '▁표현': 4885, '▁푸': 4886, '▁푸이그': 4887, '▁풀': 4888, '▁풀어': 4889, '▁풀이된다': 4890, '▁품': 4891, '▁품목': 4892, '▁품질': 4893, '▁풍': 4894, '▁풍부한': 4895, '▁프랑스': 4896, '▁프로': 4897, '▁프로그램': 4898, '▁프로그램을': 4899, '▁프로모션': 4900, '▁프로야구': 4901, '▁프로젝트': 4902, '▁프로축구': 4903, '▁프리': 4904, '▁프리미어리그': 4905, '▁프리미엄': 4906, '▁플랫폼': 4907, '▁플레이': 4908, '▁피': 4909, '▁피부': 4910, '▁피의자': 4911, '▁피해': 4912, '▁피해가': 4913, '▁피해를': 4914, '▁피해자': 4915, '▁필': 4916, '▁필리핀': 4917, '▁필수': 4918, '▁필요': 4919, '▁필요가': 4920, '▁필요성': 4921, '▁필요하다': 4922, '▁필요한': 4923, '▁하': 4924, '▁하겠다': 4925, '▁하고': 4926, '▁하기': 4927, '▁하나': 4928, '▁하나로': 4929, '▁하는': 4930, '▁하는데': 4931, '▁하다': 4932, '▁하락': 4933, '▁하락세': 4934, '▁하락한': 4935, '▁하락했다': 4936, '▁하루': 4937, '▁하며': 4938, '▁하면': 4939, '▁하면서': 4940, '▁하반기': 4941, '▁하이': 4942, '▁하자': 4943, '▁하정우': 4944, '▁하지': 4945, '▁하지만': 4946, '▁하향': 4947, '▁학': 4948, '▁학교': 4949, '▁학교폭력': 4950, '▁학부모': 4951, '▁학생': 4952, '▁학생들': 4953, '▁학습': 4954, '▁한': 4955, '▁한강': 4956, '▁한계': 4957, '▁한국': 4958, '▁한국거래소': 4959, '▁한국은': 4960, '▁한국은행': 4961, '▁한국의': 4962, '▁한국인': 4963, '▁한국전력': 4964, '▁한다': 4965, '▁한다고': 4966, '▁한다는': 4967, '▁한때': 4968, '▁한마디': 4969, '▁한반도': 4970, '▁한번': 4971, '▁한층': 4972, '▁한파': 4973, '▁한편': 4974, '▁한혜진': 4975, '▁한화': 4976, '▁할': 4977, '▁할리우드': 4978, '▁할머니': 4979, '▁할부': 4980, '▁할인': 4981, '▁함': 4982, '▁함께': 4983, '▁합': 4984, '▁합격': 4985, '▁합니다': 4986, '▁합동': 4987, '▁합류': 4988, '▁합리적': 4989, '▁합병': 4990, '▁합의': 4991, '▁항': 4992, '▁항공': 4993, '▁항공기': 4994, '▁항목': 4995, '▁항상': 4996, '▁항소심': 4997, '▁해': 4998, '▁해결': 4999, '▁해당': 5000, '▁해당하는': 5001, '▁해도': 5002, '▁해명': 5003, '▁해명했다': 5004, '▁해병대': 5005, '▁해상': 5006, '▁해서': 5007, '▁해석': 5008, '▁해소': 5009, '▁해야': 5010, '▁해양': 5011, '▁해외': 5012, '▁해운대': 5013, '▁핵': 5014, '▁핵실험': 5015, '▁핵심': 5016, '▁했': 5017, '▁했는데': 5018, '▁했다': 5019, '▁했던': 5020, '▁했지만': 5021, '▁행': 5022, '▁행동': 5023, '▁행보': 5024, '▁행복': 5025, '▁행사': 5026, '▁행사를': 5027, '▁행사에': 5028, '▁행위': 5029, '▁행정': 5030, '▁행진': 5031, '▁향': 5032, '▁향상': 5033, '▁향한': 5034, '▁향해': 5035, '▁향후': 5036, '▁허': 5037, '▁허가': 5038, '▁허리': 5039, '▁허용': 5040, '▁허위': 5041, '▁헌': 5042, '▁헌법': 5043, '▁헌법재판소장': 5044, '▁헌재': 5045, '▁헤': 5046, '▁헤어': 5047, '▁혁신': 5048, '▁현': 5049, '▁현금': 5050, '▁현대': 5051, '▁현대건설': 5052, '▁현대모비스': 5053, '▁현대자동차': 5054, '▁현대중공업': 5055, '▁현대차': 5056, '▁현대캐피탈': 5057, '▁현상': 5058, '▁현실': 5059, '▁현안': 5060, '▁현역': 5061, '▁현장': 5062, '▁현장에서': 5063, '▁현재': 5064, '▁현재까지': 5065, '▁현지': 5066, '▁현행': 5067, '▁현황': 5068, '▁혈': 5069, '▁혐의': 5070, '▁혐의로': 5071, '▁혐의를': 5072, '▁협력': 5073, '▁협력업체': 5074, '▁협박': 5075, '▁협상': 5076, '▁협약': 5077, '▁협업': 5078, '▁협의': 5079, '▁협조': 5080, '▁형': 5081, '▁형사': 5082, '▁형성': 5083, '▁형식': 5084, '▁형제': 5085, '▁형태': 5086, '▁형태로': 5087, '▁혜택': 5088, '▁혜택을': 5089, '▁호': 5090, '▁호소': 5091, '▁호응': 5092, '▁호조': 5093, '▁호주': 5094, '▁호텔': 5095, '▁호투': 5096, '▁호평': 5097, '▁호흡': 5098, '▁혹은': 5099, '▁혼': 5100, '▁혼란': 5101, '▁혼자': 5102, '▁홀': 5103, '▁홈': 5104, '▁홈경기': 5105, '▁홈런': 5106, '▁홈페이지': 5107, '▁홍': 5108, '▁홍명보': 5109, '▁홍보': 5110, '▁홍콩': 5111, '▁화': 5112, '▁화려한': 5113, '▁화면': 5114, '▁화보': 5115, '▁화성': 5116, '▁화이트': 5117, '▁화장실': 5118, '▁화장품': 5119, '▁화재': 5120, '▁화제': 5121, '▁화제다': 5122, '▁화학': 5123, '▁확': 5124, '▁확대': 5125, '▁확보': 5126, '▁확산': 5127, '▁확실한': 5128, '▁확실히': 5129, '▁확인': 5130, '▁확인됐다': 5131, '▁확인할': 5132, '▁확장': 5133, '▁확정': 5134, '▁확충': 5135, '▁환': 5136, '▁환경': 5137, '▁환영': 5138, '▁환율': 5139, '▁환자': 5140, '▁활동': 5141, '▁활동을': 5142, '▁활발': 5143, '▁활성화': 5144, '▁활약': 5145, '▁활용': 5146, '▁활용해': 5147, '▁활주로': 5148, '▁황': 5149, '▁황금': 5150, '▁황우여': 5151, '▁회': 5152, '▁회계': 5153, '▁회담': 5154, '▁회복': 5155, '▁회사': 5156, '▁회사채': 5157, '▁회원': 5158, '▁회원가입': 5159, '▁회의': 5160, '▁회의록': 5161, '▁회의를': 5162, '▁회장': 5163, '▁회장은': 5164, '▁회장의': 5165, '▁회장이': 5166, '▁획득': 5167, '▁횡령': 5168, '▁효': 5169, '▁효과': 5170, '▁효과가': 5171, '▁효과를': 5172, '▁효과적': 5173, '▁효율성': 5174, '▁효율적': 5175, '▁후': 5176, '▁후문이다': 5177, '▁후반': 5178, '▁후반기': 5179, '▁후보': 5180, '▁후보자': 5181, '▁후속': 5182, '▁후원': 5183, '▁훈련': 5184, '▁훌륭': 5185, '▁훔친': 5186, '▁훨씬': 5187, '▁훼손': 5188, '▁휘': 5189, '▁휩쓸': 5190, '▁휴': 5191, '▁휴가': 5192, '▁휴대전화': 5193, '▁휴대폰': 5194, '▁휴식': 5195, '▁흐': 5196, '▁흐름': 5197, '▁흑자': 5198, '▁흔들': 5199, '▁흔적': 5200, '▁흘러': 5201, '▁흘리': 5202, '▁흡수': 5203, '▁흡연': 5204, '▁흥국생명': 5205, '▁흥미': 5206, '▁흥행': 5207, '▁희망': 5208, '▁희생': 5209, '▁히트': 5210, '▁힘': 5211, '▁힘든': 5212, '▁힘들': 5213, '▁힘을': 5214, '▁힘입어': 5215, '▁힙합': 5216, '■': 5217, '□': 5218, '▦': 5219, '▲': 5220, '△': 5221, '▶': 5222, '▷': 5223, '▼': 5224, '▽': 5225, '◀': 5226, '◆': 5227, '◇': 5228, '◈': 5229, '○': 5230, '★': 5231, '☎': 5232, '〃': 5233, '〈': 5234, '〉': 5235, '「': 5236, '」': 5237, '『': 5238, '』': 5239, '【': 5240, '】': 5241, '一': 5242, '三': 5243, '上': 5244, '下': 5245, '不': 5246, '中': 5247, '亞': 5248, '京': 5249, '人': 5250, '企': 5251, '倍': 5252, '先': 5253, '公': 5254, '前': 5255, '北': 5256, '南': 5257, '反': 5258, '史': 5259, '國': 5260, '報': 5261, '外': 5262, '大': 5263, '天': 5264, '女': 5265, '子': 5266, '安': 5267, '家': 5268, '對': 5269, '小': 5270, '山': 5271, '島': 5272, '州': 5273, '市': 5274, '平': 5275, '年': 5276, '弗': 5277, '心': 5278, '性': 5279, '故': 5280, '文': 5281, '新': 5282, '日': 5283, '晋': 5284, '朴': 5285, '李': 5286, '東': 5287, '株': 5288, '檢': 5289, '母': 5290, '比': 5291, '民': 5292, '江': 5293, '海': 5294, '無': 5295, '獨': 5296, '王': 5297, '生': 5298, '田': 5299, '甲': 5300, '男': 5301, '百': 5302, '盧': 5303, '知': 5304, '硏': 5305, '社': 5306, '美': 5307, '習': 5308, '胎': 5309, '與': 5310, '英': 5311, '草': 5312, '行': 5313, '西': 5314, '證': 5315, '車': 5316, '軍': 5317, '近': 5318, '道': 5319, '重': 5320, '野': 5321, '金': 5322, '銀': 5323, '電': 5324, '靑': 5325, '非': 5326, '韓': 5327, '高': 5328, '鬼': 5329, '가': 5330, '가격': 5331, '가구': 5332, '가량': 5333, '가족': 5334, '가지': 5335, '각': 5336, '간': 5337, '간다': 5338, '갇': 5339, '갈': 5340, '감': 5341, '감독': 5342, '감사': 5343, '감을': 5344, '갑': 5345, '값': 5346, '갓': 5347, '갔': 5348, '갔다': 5349, '강': 5350, '강남스타일': 5351, '강심장': 5352, '갖': 5353, '같': 5354, '같은': 5355, '갚': 5356, '개': 5357, '개국': 5358, '개그콘서트': 5359, '개로': 5360, '개를': 5361, '개발': 5362, '개사': 5363, '개선': 5364, '개성공단': 5365, '개월': 5366, '개월간': 5367, '개의': 5368, '개혁': 5369, '객': 5370, '갤': 5371, '갭': 5372, '갯': 5373, '갱': 5374, '갸': 5375, '걀': 5376, '거': 5377, '거나': 5378, '거래': 5379, '거래소': 5380, '거래일': 5381, '거리': 5382, '걱': 5383, '건': 5384, '건강': 5385, '건설': 5386, '건으로': 5387, '건전성': 5388, '걷': 5389, '걸': 5390, '걸음': 5391, '검': 5392, '검사': 5393, '검색': 5394, '검증': 5395, '검찰': 5396, '겁': 5397, '것': 5398, '겉': 5399, '게': 5400, '게임': 5401, '겐': 5402, '겔': 5403, '겟': 5404, '겠': 5405, '겠다': 5406, '겠다고': 5407, '겠다는': 5408, '겠습니다': 5409, '겠지만': 5410, '겨': 5411, '격': 5412, '겪': 5413, '견': 5414, '결': 5415, '결과': 5416, '결정': 5417, '결제': 5418, '결혼': 5419, '겸': 5420, '겹': 5421, '겼': 5422, '겼다': 5423, '경': 5424, '경기': 5425, '경기에서': 5426, '경영': 5427, '경쟁': 5428, '경쟁력': 5429, '경제': 5430, '경찰': 5431, '경찰서': 5432, '경찰서는': 5433, '경찰청': 5434, '곁': 5435, '계': 5436, '계약': 5437, '계획': 5438, '고': 5439, '고객': 5440, '고등학교': 5441, '고속도로': 5442, '곡': 5443, '곤': 5444, '곧': 5445, '골': 5446, '골을': 5447, '골프': 5448, '곰': 5449, '곱': 5450, '곳': 5451, '공': 5452, '공간': 5453, '공개': 5454, '공급': 5455, '공단': 5456, '공동체': 5457, '공무원': 5458, '공사': 5459, '공약': 5460, '공업': 5461, '공연': 5462, '공원': 5463, '공장': 5464, '공항': 5465, '공화국': 5466, '곶': 5467, '과': 5468, '과정': 5469, '과정에서': 5470, '과학': 5471, '과학기술': 5472, '곽': 5473, '관': 5474, '관계': 5475, '관광': 5476, '관련': 5477, '관리': 5478, '괄': 5479, '괌': 5480, '광': 5481, '광고': 5482, '광역시': 5483, '괜': 5484, '괴': 5485, '괴물': 5486, '굉': 5487, '교': 5488, '교사': 5489, '교섭': 5490, '교육': 5491, '교육청': 5492, '교통': 5493, '교회': 5494, '구': 5495, '구나': 5496, '구단': 5497, '구를': 5498, '구역': 5499, '구장에서': 5500, '구조': 5501, '구청장': 5502, '국': 5503, '국가': 5504, '국내': 5505, '국내외': 5506, '국민': 5507, '국장': 5508, '국정원': 5509, '국제': 5510, '국회': 5511, '군': 5512, '군은': 5513, '군의': 5514, '굳': 5515, '굴': 5516, '굵': 5517, '굶': 5518, '굽': 5519, '굿': 5520, '궁': 5521, '궂': 5522, '궈': 5523, '권': 5524, '권을': 5525, '궐': 5526, '궜': 5527, '궤': 5528, '귀': 5529, '귀태': 5530, '귄': 5531, '규': 5532, '규모': 5533, '규제': 5534, '규칙': 5535, '균': 5536, '귤': 5537, '그': 5538, '그동안': 5539, '그래': 5540, '그런': 5541, '그룹': 5542, '극': 5543, '극본': 5544, '극장': 5545, '근': 5546, '글': 5547, '글로벌': 5548, '긁': 5549, '금': 5550, '금리': 5551, '금속': 5552, '금액': 5553, '금융': 5554, '금융그룹': 5555, '금융지주': 5556, '금을': 5557, '급': 5558, '긋': 5559, '긍': 5560, '기': 5561, '기가': 5562, '기간': 5563, '기관': 5564, '기구': 5565, '기금': 5566, '기념': 5567, '기는': 5568, '기능': 5569, '기도': 5570, '기로': 5571, '기록': 5572, '기를': 5573, '기사': 5574, '기술': 5575, '기아차': 5576, '기업': 5577, '기업은행': 5578, '기에': 5579, '기자': 5580, '기준': 5581, '기지': 5582, '기획': 5583, '긴': 5584, '길': 5585, '김': 5586, '깁': 5587, '깃': 5588, '깅': 5589, '깊': 5590, '까': 5591, '까지': 5592, '깍': 5593, '깎': 5594, '깐': 5595, '깔': 5596, '깜': 5597, '깜찍': 5598, '깝': 5599, '깡': 5600, '깥': 5601, '깨': 5602, '깬': 5603, '꺼': 5604, '꺾': 5605, '껄': 5606, '껌': 5607, '껍': 5608, '껏': 5609, '껑': 5610, '께': 5611, '께서': 5612, '껴': 5613, '꼈': 5614, '꼬': 5615, '꼭': 5616, '꼴': 5617, '꼼': 5618, '꼽': 5619, '꽁': 5620, '꽂': 5621, '꽃': 5622, '꽃보다': 5623, '꽉': 5624, '꽝': 5625, '꽤': 5626, '꾀': 5627, '꾸': 5628, '꾼': 5629, '꿀': 5630, '꿇': 5631, '꿈': 5632, '꿋': 5633, '꿔': 5634, '꿨': 5635, '꿰': 5636, '뀌': 5637, '뀐': 5638, '뀔': 5639, '끄': 5640, '끄러': 5641, '끈': 5642, '끊': 5643, '끌': 5644, '끓': 5645, '끔': 5646, '끗': 5647, '끝': 5648, '끼': 5649, '끼리': 5650, '끽': 5651, '낀': 5652, '낄': 5653, '낌': 5654, '나': 5655, '나갈': 5656, '나갔다': 5657, '나는': 5658, '나라': 5659, '나무': 5660, '낙': 5661, '낚': 5662, '난': 5663, '날': 5664, '낡': 5665, '남': 5666, '남북': 5667, '남자': 5668, '납': 5669, '낫': 5670, '났': 5671, '났다': 5672, '낭': 5673, '낮': 5674, '낯': 5675, '낱': 5676, '낳': 5677, '내': 5678, '내가': 5679, '내기': 5680, '낵': 5681, '낸': 5682, '낼': 5683, '냄': 5684, '냅': 5685, '냈': 5686, '냈다': 5687, '냉': 5688, '냐': 5689, '냥': 5690, '너': 5691, '너목들': 5692, '넉': 5693, '넋': 5694, '넌': 5695, '널': 5696, '넓': 5697, '넘': 5698, '넛': 5699, '넝': 5700, '넣': 5701, '네': 5702, '네요': 5703, '네트워크': 5704, '넥': 5705, '넨': 5706, '넬': 5707, '넷': 5708, '넸': 5709, '녀': 5710, '녁': 5711, '년': 5712, '년간': 5713, '년까지': 5714, '년대': 5715, '년만에': 5716, '년부터': 5717, '년생': 5718, '년에': 5719, '년에는': 5720, '년째': 5721, '념': 5722, '녔': 5723, '녕': 5724, '노': 5725, '노동': 5726, '노조': 5727, '노컷': 5728, '노트': 5729, '녹': 5730, '논': 5731, '놀': 5732, '놈': 5733, '농': 5734, '농협': 5735, '높': 5736, '놓': 5737, '놓고': 5738, '놔': 5739, '놨': 5740, '뇌': 5741, '뇨': 5742, '누': 5743, '눅': 5744, '눈': 5745, '눌': 5746, '눔': 5747, '눕': 5748, '눠': 5749, '눴': 5750, '뉘': 5751, '뉜': 5752, '뉴': 5753, '뉴스': 5754, '뉴시스': 5755, '늄': 5756, '느': 5757, '느냐': 5758, '늑': 5759, '는': 5760, '는데': 5761, '늘': 5762, '늙': 5763, '늠': 5764, '능': 5765, '능력': 5766, '늦': 5767, '늪': 5768, '늬': 5769, '니': 5770, '니까': 5771, '니다': 5772, '니스': 5773, '니아': 5774, '닉': 5775, '닌': 5776, '닐': 5777, '님': 5778, '닙': 5779, '닛': 5780, '닝': 5781, '다': 5782, '다른': 5783, '다면': 5784, '다운': 5785, '닥': 5786, '닦': 5787, '단': 5788, '단계': 5789, '단지': 5790, '단체': 5791, '닫': 5792, '달': 5793, '달라': 5794, '달러': 5795, '닭': 5796, '닮': 5797, '담': 5798, '담당': 5799, '담보대출': 5800, '답': 5801, '닷': 5802, '닷컴': 5803, '당': 5804, '당국': 5805, '당선인': 5806, '닿': 5807, '대': 5808, '대가': 5809, '대강': 5810, '대교': 5811, '대로': 5812, '대를': 5813, '대비': 5814, '대사관': 5815, '대상': 5816, '대우증권': 5817, '대의': 5818, '대책': 5819, '대출': 5820, '대통령': 5821, '대표': 5822, '대표팀': 5823, '대학': 5824, '대학교': 5825, '대학원': 5826, '대한': 5827, '대화록': 5828, '대회': 5829, '댁': 5830, '댄': 5831, '댈': 5832, '댐': 5833, '댓': 5834, '댔': 5835, '댜': 5836, '더': 5837, '더니': 5838, '더라': 5839, '더라도': 5840, '덕': 5841, '던': 5842, '덜': 5843, '덟': 5844, '덤': 5845, '덥': 5846, '덧': 5847, '덩': 5848, '덮': 5849, '데': 5850, '데다': 5851, '데일리': 5852, '덱': 5853, '덴': 5854, '델': 5855, '뎀': 5856, '뎁': 5857, '뎌': 5858, '도': 5859, '도로': 5860, '도록': 5861, '도서관': 5862, '도시': 5863, '도지사': 5864, '독': 5865, '돈': 5866, '돋': 5867, '돌': 5868, '돔': 5869, '돕': 5870, '돗': 5871, '동': 5872, '동맹': 5873, '동안': 5874, '동향': 5875, '돼': 5876, '돼야': 5877, '됐': 5878, '됐고': 5879, '됐다': 5880, '됐던': 5881, '됐습니다': 5882, '됐으나': 5883, '됐으며': 5884, '됐지만': 5885, '되': 5886, '되고': 5887, '되기': 5888, '되는': 5889, '되도록': 5890, '되며': 5891, '되면': 5892, '되면서': 5893, '되어': 5894, '되었': 5895, '되었다': 5896, '되자': 5897, '되지': 5898, '된': 5899, '된다': 5900, '된다면': 5901, '될': 5902, '됨': 5903, '됨에': 5904, '됩': 5905, '됩니다': 5906, '두': 5907, '둑': 5908, '둔': 5909, '둘': 5910, '둠': 5911, '둡': 5912, '둥': 5913, '둬': 5914, '뒀': 5915, '뒤': 5916, '뒷': 5917, '듀': 5918, '듈': 5919, '드': 5920, '드는': 5921, '드라마': 5922, '드래곤': 5923, '드리': 5924, '드립니다': 5925, '득': 5926, '득점': 5927, '든': 5928, '든지': 5929, '듣': 5930, '들': 5931, '들과': 5932, '들도': 5933, '들어': 5934, '들에': 5935, '들에게': 5936, '들은': 5937, '들을': 5938, '들의': 5939, '들이': 5940, '듬': 5941, '듭': 5942, '듯': 5943, '등': 5944, '등급': 5945, '등록': 5946, '디': 5947, '디스크': 5948, '디스플레이': 5949, '디지털': 5950, '딕': 5951, '딘': 5952, '딛': 5953, '딜': 5954, '딤': 5955, '딧': 5956, '딩': 5957, '딪': 5958, '따': 5959, '딱': 5960, '딴': 5961, '딸': 5962, '땀': 5963, '땅': 5964, '때': 5965, '땐': 5966, '땠': 5967, '땡': 5968, '떠': 5969, '떡': 5970, '떤': 5971, '떨': 5972, '떳': 5973, '떴': 5974, '떻': 5975, '떼': 5976, '뗀': 5977, '뗄': 5978, '뗐': 5979, '또': 5980, '똑': 5981, '똘': 5982, '똥': 5983, '뚜': 5984, '뚝': 5985, '뚫': 5986, '뚱': 5987, '뛰': 5988, '뛴': 5989, '뛸': 5990, '뜨': 5991, '뜩': 5992, '뜬': 5993, '뜯': 5994, '뜰': 5995, '뜸': 5996, '뜻': 5997, '띄': 5998, '띈': 5999, '띔': 6000, '띠': 6001, '띤': 6002, '라': 6003, '라고': 6004, '라는': 6005, '라도': 6006, '라디오스타': 6007, '라며': 6008, '라면': 6009, '라운드': 6010, '라이': 6011, '라이트': 6012, '라이프': 6013, '라인': 6014, '락': 6015, '란': 6016, '랄': 6017, '람': 6018, '랍': 6019, '랏': 6020, '랐': 6021, '랑': 6022, '래': 6023, '랙': 6024, '랜': 6025, '랜드': 6026, '랠': 6027, '램': 6028, '랩': 6029, '랫': 6030, '랬': 6031, '랭': 6032, '랴': 6033, '략': 6034, '량': 6035, '량이': 6036, '러': 6037, '러스': 6038, '럭': 6039, '런': 6040, '런닝맨': 6041, '럴': 6042, '럼': 6043, '럽': 6044, '럿': 6045, '렀': 6046, '렀다': 6047, '렁': 6048, '렇': 6049, '레': 6050, '레드': 6051, '레미제라블': 6052, '레스': 6053, '레이': 6054, '렉': 6055, '렌': 6056, '렐': 6057, '렘': 6058, '렛': 6059, '려': 6060, '려고': 6061, '려는': 6062, '려면': 6063, '력': 6064, '력을': 6065, '력이': 6066, '련': 6067, '렬': 6068, '렴': 6069, '렵': 6070, '렷': 6071, '렸': 6072, '렸고': 6073, '렸다': 6074, '렸던': 6075, '렸지만': 6076, '령': 6077, '례': 6078, '로': 6079, '로부터': 6080, '로서': 6081, '로운': 6082, '록': 6083, '론': 6084, '롤': 6085, '롬': 6086, '롭': 6087, '롭게': 6088, '롯': 6089, '롯데': 6090, '롱': 6091, '뢰': 6092, '료': 6093, '룡': 6094, '루': 6095, '루타': 6096, '룩': 6097, '룬': 6098, '룰': 6099, '룸': 6100, '룹': 6101, '룻': 6102, '룽': 6103, '뤄': 6104, '뤘': 6105, '뤼': 6106, '류': 6107, '륙': 6108, '륜': 6109, '률': 6110, '륨': 6111, '륭': 6112, '르': 6113, '르트': 6114, '른': 6115, '를': 6116, '름': 6117, '릅': 6118, '릇': 6119, '릉': 6120, '릎': 6121, '리': 6122, '리그': 6123, '리는': 6124, '리를': 6125, '리바운드': 6126, '리스': 6127, '리스트': 6128, '리아': 6129, '리조트': 6130, '리지': 6131, '릭': 6132, '린': 6133, '린다': 6134, '릴': 6135, '림': 6136, '립': 6137, '릿': 6138, '링': 6139, '링크': 6140, '마': 6141, '마다': 6142, '마을': 6143, '마음': 6144, '마저': 6145, '마케팅': 6146, '마켓': 6147, '마트': 6148, '막': 6149, '만': 6150, '만달러': 6151, '만명': 6152, '만원': 6153, '만원으로': 6154, '만원을': 6155, '만으로': 6156, '만큼': 6157, '많': 6158, '맏': 6159, '말': 6160, '맑': 6161, '맘': 6162, '맙': 6163, '맛': 6164, '망': 6165, '맞': 6166, '맡': 6167, '매': 6168, '매매': 6169, '매출': 6170, '매치': 6171, '맥': 6172, '맨': 6173, '맴': 6174, '맵': 6175, '맷': 6176, '맹': 6177, '맺': 6178, '머': 6179, '머니': 6180, '머니투데이': 6181, '머리': 6182, '먹': 6183, '먼': 6184, '먼트': 6185, '멀': 6186, '멈': 6187, '멋': 6188, '멍': 6189, '메': 6190, '메이': 6191, '멕': 6192, '멘': 6193, '멜': 6194, '멤': 6195, '멧': 6196, '며': 6197, '면': 6198, '면서': 6199, '면서도': 6200, '면세점': 6201, '멸': 6202, '몄': 6203, '명': 6204, '명과': 6205, '명에게': 6206, '명으로': 6207, '명은': 6208, '명을': 6209, '명의': 6210, '명이': 6211, '몇': 6212, '모': 6213, '모델': 6214, '모바일': 6215, '모씨': 6216, '목': 6217, '목표': 6218, '몫': 6219, '몬': 6220, '몰': 6221, '몸': 6222, '몹': 6223, '못': 6224, '몽': 6225, '뫼': 6226, '묘': 6227, '무': 6228, '무릎팍도사': 6229, '무역': 6230, '무한도전': 6231, '묵': 6232, '묶': 6233, '문': 6234, '문을': 6235, '문제': 6236, '문학': 6237, '문화': 6238, '문화예술': 6239, '묻': 6240, '물': 6241, '물을': 6242, '물질': 6243, '뭄': 6244, '뭇': 6245, '뭉': 6246, '뭐': 6247, '뭔': 6248, '뭘': 6249, '뮌': 6250, '뮤': 6251, '뮤직': 6252, '뮬': 6253, '므': 6254, '미': 6255, '미국': 6256, '미디어': 6257, '미래': 6258, '미술관': 6259, '미스터': 6260, '믹': 6261, '믹스': 6262, '민': 6263, '민주': 6264, '민주당': 6265, '믿': 6266, '밀': 6267, '밋': 6268, '밌': 6269, '밍': 6270, '및': 6271, '밑': 6272, '바': 6273, '바다': 6274, '바람': 6275, '바르셀로나': 6276, '바이': 6277, '바이오': 6278, '바퀴': 6279, '박': 6280, '박근혜': 6281, '박람회': 6282, '박물관': 6283, '밖': 6284, '밖에': 6285, '반': 6286, '반도체': 6287, '받': 6288, '받고': 6289, '받는': 6290, '받아': 6291, '받았다': 6292, '받은': 6293, '받을': 6294, '발': 6295, '발굴': 6296, '발언': 6297, '발전': 6298, '발전소': 6299, '발표': 6300, '밝': 6301, '밟': 6302, '밤': 6303, '밥': 6304, '방': 6305, '방송': 6306, '방식': 6307, '방안': 6308, '방위': 6309, '방향': 6310, '밭': 6311, '배': 6312, '배우': 6313, '백': 6314, '백화점': 6315, '밴': 6316, '밴드': 6317, '밸': 6318, '뱀': 6319, '뱃': 6320, '뱅': 6321, '뱉': 6322, '버': 6323, '버리': 6324, '버린': 6325, '버스': 6326, '벅': 6327, '번': 6328, '번째': 6329, '번호': 6330, '번홀': 6331, '벌': 6332, '범': 6333, '범죄': 6334, '법': 6335, '법원': 6336, '법을': 6337, '법인': 6338, '벗': 6339, '벙': 6340, '베': 6341, '베르': 6342, '베를린': 6343, '베이': 6344, '베이스': 6345, '벡': 6346, '벤': 6347, '벤처': 6348, '벨': 6349, '벨트': 6350, '벳': 6351, '벵': 6352, '벼': 6353, '벽': 6354, '변': 6355, '별': 6356, '별로': 6357, '별로는': 6358, '볍': 6359, '볐': 6360, '병': 6361, '병원': 6362, '볕': 6363, '보': 6364, '보건': 6365, '보고': 6366, '보고서': 6367, '보기': 6368, '보는': 6369, '보니': 6370, '보다': 6371, '보다는': 6372, '보드': 6373, '보이': 6374, '보장': 6375, '보증': 6376, '보험': 6377, '보호': 6378, '복': 6379, '복지': 6380, '복합': 6381, '볶': 6382, '본': 6383, '본부': 6384, '본부장': 6385, '볼': 6386, '볼넷': 6387, '볼륨': 6388, '봄': 6389, '봅': 6390, '봇': 6391, '봉': 6392, '봐': 6393, '봐야': 6394, '봤': 6395, '봤다': 6396, '뵙': 6397, '부': 6398, '부가': 6399, '부는': 6400, '부담': 6401, '부동산': 6402, '부르크': 6403, '부문': 6404, '부산': 6405, '부장': 6406, '부장검사': 6407, '부장판사': 6408, '부처': 6409, '부터': 6410, '부품': 6411, '북': 6412, '북방한계선': 6413, '북부': 6414, '북한': 6415, '분': 6416, '분과': 6417, '분기': 6418, '분기에': 6419, '분께': 6420, '분석': 6421, '분야': 6422, '분쯤': 6423, '불': 6424, '붉': 6425, '붐': 6426, '붓': 6427, '붕': 6428, '붙': 6429, '뷔': 6430, '뷰': 6431, '브': 6432, '브라': 6433, '브랜드': 6434, '브레이크': 6435, '브리': 6436, '븐': 6437, '블': 6438, '블랙': 6439, '블록': 6440, '비': 6441, '비가': 6442, '비를': 6443, '비서관': 6444, '비아': 6445, '비용': 6446, '비율': 6447, '비치': 6448, '빅': 6449, '빈': 6450, '빌': 6451, '빌딩': 6452, '빔': 6453, '빕': 6454, '빗': 6455, '빙': 6456, '빚': 6457, '빛': 6458, '빠': 6459, '빡': 6460, '빨': 6461, '빴': 6462, '빵': 6463, '빼': 6464, '빽': 6465, '뺀': 6466, '뺏': 6467, '뺐': 6468, '뺑': 6469, '뺨': 6470, '뻐': 6471, '뻑': 6472, '뻔': 6473, '뻗': 6474, '뻤': 6475, '뻥': 6476, '뼈': 6477, '뽀': 6478, '뽐': 6479, '뽑': 6480, '뽕': 6481, '뾰': 6482, '뿌': 6483, '뿐': 6484, '뿐만': 6485, '뿔': 6486, '뿜': 6487, '쁘': 6488, '쁜': 6489, '쁠': 6490, '쁨': 6491, '삐': 6492, '사': 6493, '사가': 6494, '사건': 6495, '사고': 6496, '사는': 6497, '사람': 6498, '사랑': 6499, '사를': 6500, '사무소': 6501, '사실': 6502, '사업': 6503, '사업본부': 6504, '사업부': 6505, '사업자': 6506, '사와': 6507, '사의': 6508, '사이드': 6509, '사이트': 6510, '사진': 6511, '사진제공': 6512, '사항': 6513, '사회': 6514, '삭': 6515, '산': 6516, '산업': 6517, '산업단지': 6518, '살': 6519, '삶': 6520, '삼': 6521, '삼성': 6522, '삼성전자': 6523, '삽': 6524, '삿': 6525, '샀': 6526, '상': 6527, '상공회의소': 6528, '상담': 6529, '상당히': 6530, '상을': 6531, '상태': 6532, '상품': 6533, '상황': 6534, '샅': 6535, '새': 6536, '새누리당': 6537, '색': 6538, '샌': 6539, '샐': 6540, '샘': 6541, '생': 6542, '생명': 6543, '생산': 6544, '생활': 6545, '샤': 6546, '샬': 6547, '샴': 6548, '샵': 6549, '샷': 6550, '샹': 6551, '섀': 6552, '서': 6553, '서는': 6554, '서비스': 6555, '서울': 6556, '석': 6557, '섞': 6558, '선': 6559, '선거': 6560, '선물': 6561, '선수': 6562, '선수권대회': 6563, '선을': 6564, '섣': 6565, '설': 6566, '설국열차': 6567, '설명회': 6568, '섬': 6569, '섭': 6570, '섯': 6571, '섰': 6572, '성': 6573, '성과': 6574, '성은': 6575, '성을': 6576, '성이': 6577, '성장': 6578, '세': 6579, '세가': 6580, '세계': 6581, '세계일보': 6582, '세대': 6583, '세력': 6584, '세를': 6585, '세요': 6586, '세이브': 6587, '세트': 6588, '세포': 6589, '섹': 6590, '센': 6591, '센스': 6592, '센터': 6593, '센터에서': 6594, '센트': 6595, '셀': 6596, '셈': 6597, '셉': 6598, '셋': 6599, '셍': 6600, '셔': 6601, '션': 6602, '셜': 6603, '셨': 6604, '셰': 6605, '셸': 6606, '소': 6607, '소득': 6608, '소리': 6609, '소방서': 6610, '소비자': 6611, '소송': 6612, '소연': 6613, '소프트': 6614, '속': 6615, '손': 6616, '손해보험': 6617, '솔': 6618, '솜': 6619, '솟': 6620, '송': 6621, '솥': 6622, '쇄': 6623, '쇠': 6624, '쇼': 6625, '숀': 6626, '숍': 6627, '숏': 6628, '수': 6629, '수가': 6630, '수는': 6631, '수록': 6632, '수를': 6633, '수사': 6634, '수석': 6635, '수수료': 6636, '수술': 6637, '수원': 6638, '수익': 6639, '수익률': 6640, '수지': 6641, '숙': 6642, '순': 6643, '순위': 6644, '술': 6645, '숨': 6646, '숨어있': 6647, '숫': 6648, '숭': 6649, '숱': 6650, '숲': 6651, '쉐': 6652, '쉬': 6653, '쉰': 6654, '쉴': 6655, '쉼': 6656, '쉽': 6657, '슈': 6658, '슈퍼': 6659, '슈퍼스타': 6660, '슐': 6661, '슘': 6662, '슛': 6663, '스': 6664, '스가': 6665, '스는': 6666, '스러운': 6667, '스러워': 6668, '스럽': 6669, '스럽게': 6670, '스럽다': 6671, '스를': 6672, '스마트': 6673, '스와': 6674, '스완지시티': 6675, '스의': 6676, '스카': 6677, '스캔들': 6678, '스케': 6679, '스코': 6680, '스쿨': 6681, '스크': 6682, '스키': 6683, '스타': 6684, '스탠': 6685, '스터': 6686, '스턴': 6687, '스테': 6688, '스토리': 6689, '스토어': 6690, '스트': 6691, '스티': 6692, '스페셜': 6693, '스포츠': 6694, '스포츠조선': 6695, '슨': 6696, '슬': 6697, '슴': 6698, '습': 6699, '습니까': 6700, '습니다': 6701, '슷': 6702, '승': 6703, '승을': 6704, '시': 6705, '시간': 6706, '시께': 6707, '시대': 6708, '시리즈': 6709, '시민': 6710, '시부터': 6711, '시설': 6712, '시스템': 6713, '시아': 6714, '시의회': 6715, '시장': 6716, '시장에서': 6717, '시즌': 6718, '시청': 6719, '시켜': 6720, '시켰다': 6721, '시키': 6722, '시키고': 6723, '시키기': 6724, '시키는': 6725, '시킨': 6726, '시킬': 6727, '시티': 6728, '시험': 6729, '식': 6730, '식을': 6731, '식품': 6732, '신': 6733, '신도시': 6734, '신문': 6735, '신청': 6736, '싣': 6737, '실': 6738, '실리콘': 6739, '실에서': 6740, '실장': 6741, '실적': 6742, '실점': 6743, '싫': 6744, '심': 6745, '심리': 6746, '심사': 6747, '심을': 6748, '십': 6749, '싱': 6750, '싶': 6751, '싸': 6752, '싸움': 6753, '싹': 6754, '싼': 6755, '쌀': 6756, '쌈': 6757, '쌌': 6758, '쌍': 6759, '쌓': 6760, '써': 6761, '썩': 6762, '썬': 6763, '썰': 6764, '썸': 6765, '썹': 6766, '썼': 6767, '썽': 6768, '쎄': 6769, '쏘': 6770, '쏙': 6771, '쏜': 6772, '쏟': 6773, '쏠': 6774, '쏴': 6775, '쐐': 6776, '쑤': 6777, '쑥': 6778, '쓰': 6779, '쓴': 6780, '쓸': 6781, '씀': 6782, '씁': 6783, '씌': 6784, '씨': 6785, '씨가': 6786, '씨는': 6787, '씨를': 6788, '씨에게': 6789, '씨와': 6790, '씨의': 6791, '씩': 6792, '씬': 6793, '씹': 6794, '씻': 6795, '씽': 6796, '아': 6797, '아버지': 6798, '아빠': 6799, '아시아': 6800, '아시아경제': 6801, '아시아투데이': 6802, '아웃': 6803, '아이': 6804, '아일랜드': 6805, '아직': 6806, '아카데미': 6807, '아트': 6808, '아파트': 6809, '아프리카': 6810, '악': 6811, '안': 6812, '안드로이드': 6813, '안보': 6814, '안을': 6815, '안전': 6816, '안정': 6817, '안타': 6818, '앉': 6819, '않': 6820, '알': 6821, '알리미': 6822, '앓': 6823, '암': 6824, '압': 6825, '앗': 6826, '았': 6827, '았다': 6828, '았던': 6829, '았지만': 6830, '앙': 6831, '앞': 6832, '앞으로': 6833, '애': 6834, '액': 6835, '액은': 6836, '앤': 6837, '앨': 6838, '앰': 6839, '앱': 6840, '앳': 6841, '앵': 6842, '앵커멘트': 6843, '야': 6844, '야구': 6845, '약': 6846, '약품': 6847, '얀': 6848, '얄': 6849, '얇': 6850, '얌': 6851, '얏': 6852, '양': 6853, '얘': 6854, '어': 6855, '어야': 6856, '어요': 6857, '억': 6858, '억달러': 6859, '억여원': 6860, '억원': 6861, '억원으로': 6862, '억원을': 6863, '억원의': 6864, '언': 6865, '언더파': 6866, '언론': 6867, '얹': 6868, '얻': 6869, '얼': 6870, '얽': 6871, '엄': 6872, '업': 6873, '업계': 6874, '업무': 6875, '업소': 6876, '업자': 6877, '업종': 6878, '업체': 6879, '업체들': 6880, '없': 6881, '없는': 6882, '없이': 6883, '엇': 6884, '었': 6885, '었고': 6886, '었는데': 6887, '었다': 6888, '었던': 6889, '었습니다': 6890, '었으나': 6891, '었으며': 6892, '었지만': 6893, '엉': 6894, '엎': 6895, '에': 6896, '에게': 6897, '에게는': 6898, '에너지': 6899, '에는': 6900, '에도': 6901, '에만': 6902, '에서': 6903, '에서는': 6904, '에서도': 6905, '에선': 6906, '엑': 6907, '엑스': 6908, '엔': 6909, '엔지니어링': 6910, '엔터테인먼트': 6911, '엘': 6912, '엠': 6913, '엣': 6914, '엥': 6915, '여': 6916, '여개': 6917, '여건': 6918, '여름': 6919, '여명': 6920, '여명의': 6921, '여명이': 6922, '여성': 6923, '여자': 6924, '여행': 6925, '역': 6926, '엮': 6927, '연': 6928, '연구': 6929, '연구소': 6930, '연구원': 6931, '연금': 6932, '연맹': 6933, '연수원': 6934, '연승': 6935, '연예': 6936, '연패': 6937, '연합': 6938, '연합회': 6939, '열': 6940, '염': 6941, '엽': 6942, '엿': 6943, '였': 6944, '였고': 6945, '였다': 6946, '였던': 6947, '였습니다': 6948, '였으며': 6949, '였지만': 6950, '영': 6951, '영상': 6952, '영업': 6953, '영화': 6954, '영화제': 6955, '옆': 6956, '예': 6957, '예방': 6958, '예산': 6959, '예술': 6960, '옌': 6961, '옐': 6962, '옛': 6963, '오': 6964, '오는': 6965, '오늘': 6966, '오른쪽': 6967, '오토': 6968, '오픈': 6969, '옥': 6970, '온': 6971, '온라인': 6972, '올': 6973, '올림픽': 6974, '올해': 6975, '옮': 6976, '옳': 6977, '옴': 6978, '옵': 6979, '옵션': 6980, '옷': 6981, '옹': 6982, '와': 6983, '와의': 6984, '완': 6985, '왈': 6986, '왑': 6987, '왓': 6988, '왔': 6989, '왔다': 6990, '왔던': 6991, '왕': 6992, '왜': 6993, '왠': 6994, '외': 6995, '외국인': 6996, '왼': 6997, '왼쪽': 6998, '요': 6999, '요금': 7000, '욕': 7001, '욘': 7002, '용': 7003, '용품': 7004, '우': 7005, '우는': 7006, '우리': 7007, '우스': 7008, '욱': 7009, '운': 7010, '운동': 7011, '운영': 7012, '울': 7013, '움': 7014, '웃': 7015, '웃음': 7016, '웅': 7017, '워': 7018, '웍': 7019, '원': 7020, '원에': 7021, '원에서': 7022, '원으로': 7023, '원은': 7024, '원을': 7025, '원의': 7026, '원이': 7027, '월': 7028, '월까지': 7029, '월드': 7030, '월드컵': 7031, '월말': 7032, '월부터': 7033, '월에': 7034, '월에는': 7035, '웠': 7036, '웠다': 7037, '웨': 7038, '웨어': 7039, '웨이': 7040, '웬': 7041, '웰': 7042, '웹': 7043, '위': 7044, '위권': 7045, '위기': 7046, '위는': 7047, '위로': 7048, '위를': 7049, '위안': 7050, '위에': 7051, '위원': 7052, '위원장': 7053, '위원회': 7054, '위원회는': 7055, '위험': 7056, '윅': 7057, '윈': 7058, '윌': 7059, '윔': 7060, '윗': 7061, '윙': 7062, '유': 7063, '유럽': 7064, '유통': 7065, '유플러스': 7066, '육': 7067, '윤': 7068, '율': 7069, '율은': 7070, '율을': 7071, '율이': 7072, '융': 7073, '으': 7074, '으나': 7075, '으니': 7076, '으려': 7077, '으로': 7078, '으로부터': 7079, '으로서': 7080, '으로써': 7081, '으며': 7082, '으면': 7083, '으면서': 7084, '윽': 7085, '은': 7086, '은행': 7087, '을': 7088, '음': 7089, '음악': 7090, '음에도': 7091, '음을': 7092, '읍': 7093, '응': 7094, '의': 7095, '이': 7096, '이기도': 7097, '이나': 7098, '이닝': 7099, '이다': 7100, '이라': 7101, '이라고': 7102, '이라는': 7103, '이라도': 7104, '이라며': 7105, '이라면서': 7106, '이란': 7107, '이며': 7108, '이번': 7109, '이상': 7110, '이어서': 7111, '이었다': 7112, '이었던': 7113, '이익': 7114, '이자': 7115, '이지만': 7116, '이하': 7117, '익': 7118, '인': 7119, '인데': 7120, '인사': 7121, '인수위': 7122, '인지': 7123, '인터내셔널': 7124, '인터넷': 7125, '일': 7126, '일간': 7127, '일까지': 7128, '일반': 7129, '일보': 7130, '일본': 7131, '일부터': 7132, '일에는': 7133, '읽': 7134, '잃': 7135, '임': 7136, '임을': 7137, '입': 7138, '입니다': 7139, '잇': 7140, '있': 7141, '있는': 7142, '있다': 7143, '잉': 7144, '잊': 7145, '잎': 7146, '자': 7147, '자가': 7148, '자금': 7149, '자는': 7150, '자동차': 7151, '자들': 7152, '자들은': 7153, '자들의': 7154, '자들이': 7155, '자로': 7156, '자료': 7157, '자를': 7158, '자리': 7159, '자마자': 7160, '자본': 7161, '자산': 7162, '자산운용': 7163, '자에게': 7164, '자와': 7165, '자원': 7166, '자의': 7167, '자인': 7168, '자치': 7169, '작': 7170, '작업': 7171, '잔': 7172, '잖': 7173, '잘': 7174, '잠': 7175, '잡': 7176, '잣': 7177, '장': 7178, '장과': 7179, '장관': 7180, '장비': 7181, '장애': 7182, '장애인': 7183, '장에서': 7184, '장으로': 7185, '장은': 7186, '장을': 7187, '장이': 7188, '장치': 7189, '잦': 7190, '재': 7191, '재단': 7192, '재료': 7193, '재산': 7194, '재정': 7195, '재판': 7196, '잭': 7197, '쟁': 7198, '저': 7199, '저축': 7200, '저축은행': 7201, '적': 7202, '적으로': 7203, '적이고': 7204, '적이다': 7205, '적인': 7206, '전': 7207, '전략': 7208, '전문': 7209, '전에': 7210, '전에서': 7211, '전우치': 7212, '전을': 7213, '전자': 7214, '전쟁': 7215, '전환': 7216, '절': 7217, '절차': 7218, '젊': 7219, '점': 7220, '점검': 7221, '점으로': 7222, '점을': 7223, '점이': 7224, '접': 7225, '젓': 7226, '정': 7227, '정글의': 7228, '정보': 7229, '정부': 7230, '정책': 7231, '정치': 7232, '젖': 7233, '제': 7234, '제도': 7235, '제를': 7236, '제약': 7237, '제주': 7238, '제품': 7239, '젝': 7240, '젠': 7241, '젤': 7242, '젬': 7243, '젯': 7244, '져': 7245, '져야': 7246, '젼': 7247, '졌': 7248, '졌고': 7249, '졌다': 7250, '졌습니다': 7251, '졌지만': 7252, '조': 7253, '조건': 7254, '조사': 7255, '조선': 7256, '조선해양': 7257, '조원': 7258, '조정': 7259, '조직': 7260, '조차': 7261, '조치': 7262, '족': 7263, '존': 7264, '졸': 7265, '좀': 7266, '좁': 7267, '종': 7268, '종목': 7269, '종합': 7270, '좋': 7271, '좋은': 7272, '좌': 7273, '죄': 7274, '죠': 7275, '주': 7276, '주가': 7277, '주기': 7278, '주년': 7279, '주는': 7280, '주민': 7281, '주세요': 7282, '주식': 7283, '주의': 7284, '주의보': 7285, '주택': 7286, '죽': 7287, '준': 7288, '준비': 7289, '준호': 7290, '준희': 7291, '줄': 7292, '줌': 7293, '줍': 7294, '중': 7295, '중견기업': 7296, '중공업': 7297, '중국': 7298, '중소기업': 7299, '중심': 7300, '중앙': 7301, '중인': 7302, '줘': 7303, '줘야': 7304, '줬': 7305, '줬다': 7306, '쥐': 7307, '쥔': 7308, '쥬': 7309, '즈': 7310, '즉': 7311, '즌': 7312, '즐': 7313, '즘': 7314, '즙': 7315, '증': 7316, '증권': 7317, '지': 7318, '지가': 7319, '지검': 7320, '지고': 7321, '지구': 7322, '지금': 7323, '지나치게': 7324, '지난': 7325, '지난해': 7326, '지는': 7327, '지도': 7328, '지를': 7329, '지만': 7330, '지면서': 7331, '지방': 7332, '지방경찰청': 7333, '지법': 7334, '지수': 7335, '지수는': 7336, '지역': 7337, '지원': 7338, '지원센터': 7339, '지지': 7340, '지훈': 7341, '직': 7342, '직원': 7343, '진': 7344, '진다': 7345, '진영': 7346, '진짜': 7347, '진흥': 7348, '진흥원': 7349, '질': 7350, '질서': 7351, '질환': 7352, '짐': 7353, '집': 7354, '집행': 7355, '짓': 7356, '징': 7357, '짖': 7358, '짙': 7359, '짚': 7360, '짜': 7361, '짜리': 7362, '짝': 7363, '짠': 7364, '짤': 7365, '짧': 7366, '짬': 7367, '짱': 7368, '째': 7369, '쨌': 7370, '쩌': 7371, '쩍': 7372, '쩔': 7373, '쩡': 7374, '쪼': 7375, '쪽': 7376, '쫄': 7377, '쫓': 7378, '쭈': 7379, '쭉': 7380, '쯔': 7381, '쯤': 7382, '찌': 7383, '찍': 7384, '찐': 7385, '찔': 7386, '찜': 7387, '찢': 7388, '차': 7389, '차량': 7390, '차례': 7391, '차익': 7392, '차전': 7393, '착': 7394, '찬': 7395, '찮': 7396, '찰': 7397, '참': 7398, '참여': 7399, '찹': 7400, '찼': 7401, '창': 7402, '창업': 7403, '찾': 7404, '채': 7405, '채권': 7406, '채널': 7407, '책': 7408, '책을': 7409, '책임': 7410, '챈': 7411, '챌': 7412, '챔': 7413, '챔피언': 7414, '챙': 7415, '처': 7416, '처럼': 7417, '처리': 7418, '처분': 7419, '처음': 7420, '척': 7421, '천': 7422, '천만': 7423, '천만원': 7424, '천명': 7425, '천억원': 7426, '철': 7427, '첨': 7428, '첩': 7429, '첫': 7430, '청': 7431, '청담동': 7432, '청사': 7433, '청소년': 7434, '청은': 7435, '체': 7436, '체계': 7437, '체육': 7438, '체제': 7439, '체험': 7440, '첸': 7441, '첼': 7442, '쳐': 7443, '쳤': 7444, '쳤다': 7445, '초': 7446, '초등학교': 7447, '촉': 7448, '촌': 7449, '촘': 7450, '촛': 7451, '총': 7452, '총리': 7453, '총회': 7454, '촨': 7455, '촬': 7456, '촬영': 7457, '최': 7458, '최고': 7459, '최근': 7460, '추': 7461, '추진': 7462, '축': 7463, '축구': 7464, '축구연맹': 7465, '축제': 7466, '춘': 7467, '출': 7468, '춤': 7469, '춥': 7470, '춧': 7471, '충': 7472, '춰': 7473, '췄': 7474, '췌': 7475, '취': 7476, '츄': 7477, '츠': 7478, '측': 7479, '측은': 7480, '츰': 7481, '층': 7482, '치': 7483, '치고': 7484, '치는': 7485, '치료': 7486, '치를': 7487, '칙': 7488, '친': 7489, '칠': 7490, '침': 7491, '칩': 7492, '칫': 7493, '칭': 7494, '카': 7495, '카드': 7496, '카페': 7497, '칵': 7498, '칸': 7499, '칼': 7500, '캄': 7501, '캉': 7502, '캐': 7503, '캐피탈': 7504, '캔': 7505, '캘': 7506, '캠': 7507, '캠퍼스': 7508, '캠프': 7509, '캡': 7510, '캣': 7511, '커': 7512, '컥': 7513, '컨': 7514, '컨설팅': 7515, '컫': 7516, '컬': 7517, '컴': 7518, '컴퍼니': 7519, '컵': 7520, '컷': 7521, '컸': 7522, '케': 7523, '케미칼': 7524, '케이': 7525, '켄': 7526, '켈': 7527, '켐': 7528, '켓': 7529, '켜': 7530, '켠': 7531, '켰': 7532, '코': 7533, '코드': 7534, '코리아': 7535, '코스닥': 7536, '코스피': 7537, '콕': 7538, '콘': 7539, '콘서트': 7540, '콘텐츠': 7541, '콜': 7542, '콤': 7543, '콥': 7544, '콧': 7545, '콩': 7546, '콰': 7547, '쾌': 7548, '쿄': 7549, '쿠': 7550, '쿠키': 7551, '쿡': 7552, '쿤': 7553, '쿨': 7554, '쿵': 7555, '쿼': 7556, '쿼터': 7557, '퀄': 7558, '퀘': 7559, '퀴': 7560, '퀵': 7561, '퀸': 7562, '큐': 7563, '큘': 7564, '크': 7565, '크레': 7566, '큰': 7567, '클': 7568, '클라우드': 7569, '클럽': 7570, '클리': 7571, '큼': 7572, '키': 7573, '키로': 7574, '킥': 7575, '킨': 7576, '킬': 7577, '킴': 7578, '킷': 7579, '킹': 7580, '타': 7581, '타를': 7582, '타수': 7583, '타운': 7584, '타워': 7585, '타임': 7586, '타자': 7587, '타점': 7588, '탁': 7589, '탄': 7590, '탈': 7591, '탈삼진': 7592, '탐': 7593, '탑': 7594, '탓': 7595, '탔': 7596, '탕': 7597, '태': 7598, '태영': 7599, '태평양': 7600, '택': 7601, '탠': 7602, '탤': 7603, '탬': 7604, '탭': 7605, '탰': 7606, '탱': 7607, '탱크': 7608, '터': 7609, '터미널': 7610, '턱': 7611, '턴': 7612, '털': 7613, '텀': 7614, '텁': 7615, '텃': 7616, '텅': 7617, '테': 7618, '테크': 7619, '텍': 7620, '텐': 7621, '텔': 7622, '텔레콤': 7623, '템': 7624, '텝': 7625, '텨': 7626, '톈': 7627, '토': 7628, '토록': 7629, '토크': 7630, '톡': 7631, '톤': 7632, '톨': 7633, '톰': 7634, '톱': 7635, '통': 7636, '통신': 7637, '통합': 7638, '통화': 7639, '퇴': 7640, '투': 7641, '투데이': 7642, '투수': 7643, '투어': 7644, '투자': 7645, '투자증권': 7646, '투표': 7647, '툭': 7648, '툰': 7649, '툴': 7650, '툼': 7651, '퉁': 7652, '퉈': 7653, '튀': 7654, '튕': 7655, '튜': 7656, '튠': 7657, '튬': 7658, '트': 7659, '트랙': 7660, '트로': 7661, '트리': 7662, '특': 7663, '특별': 7664, '특위': 7665, '특징주': 7666, '특히': 7667, '튼': 7668, '튿': 7669, '틀': 7670, '틈': 7671, '틋': 7672, '티': 7673, '티브': 7674, '틱': 7675, '틴': 7676, '틸': 7677, '팀': 7678, '팀장': 7679, '팁': 7680, '팅': 7681, '파': 7682, '파운드': 7683, '파이어': 7684, '파크': 7685, '팍': 7686, '팎': 7687, '판': 7688, '판매': 7689, '팔': 7690, '팜': 7691, '팝': 7692, '팟': 7693, '팠': 7694, '팡': 7695, '팥': 7696, '패': 7697, '패밀리': 7698, '패션': 7699, '팩': 7700, '팬': 7701, '팬들': 7702, '팰': 7703, '팸': 7704, '팽': 7705, '퍼': 7706, '펀': 7707, '펀드': 7708, '펄': 7709, '펌': 7710, '펑': 7711, '페': 7712, '페스티벌': 7713, '페이스': 7714, '펙': 7715, '펜': 7716, '펠': 7717, '펫': 7718, '펴': 7719, '편': 7720, '펼': 7721, '폄': 7722, '폈': 7723, '평': 7724, '평가': 7725, '평균': 7726, '폐': 7727, '포': 7728, '포럼': 7729, '포스트': 7730, '포인트': 7731, '포토': 7732, '포트': 7733, '폭': 7734, '폭력': 7735, '폰': 7736, '폴': 7737, '폴리': 7738, '폼': 7739, '퐁': 7740, '표': 7741, '표를': 7742, '푸': 7743, '푸드': 7744, '푹': 7745, '푼': 7746, '풀': 7747, '품': 7748, '풋': 7749, '풍': 7750, '퓨': 7751, '퓰': 7752, '프': 7753, '프라': 7754, '프로': 7755, '프로골프': 7756, '프로그램': 7757, '프로야구': 7758, '프리': 7759, '픈': 7760, '플': 7761, '플라이': 7762, '플랜트': 7763, '플러스': 7764, '플레이': 7765, '픔': 7766, '피': 7767, '피스': 7768, '피아': 7769, '피안타': 7770, '피해': 7771, '픽': 7772, '핀': 7773, '필': 7774, '필드': 7775, '필름': 7776, '핌': 7777, '핍': 7778, '핏': 7779, '핑': 7780, '핑크': 7781, '하': 7782, '하거나': 7783, '하게': 7784, '하겠다': 7785, '하겠다고': 7786, '하겠다는': 7787, '하고': 7788, '하기': 7789, '하기도': 7790, '하기로': 7791, '하나': 7792, '하느냐': 7793, '하는': 7794, '하는데': 7795, '하늘': 7796, '하니': 7797, '하다': 7798, '하다고': 7799, '하다는': 7800, '하더라도': 7801, '하던': 7802, '하도록': 7803, '하라': 7804, '하라고': 7805, '하려': 7806, '하려고': 7807, '하려는': 7808, '하려면': 7809, '하며': 7810, '하면': 7811, '하면서': 7812, '하면서도': 7813, '하세요': 7814, '하여': 7815, '하우스': 7816, '하이닉스': 7817, '하자': 7818, '하지': 7819, '하지만': 7820, '학': 7821, '학과': 7822, '학교': 7823, '학년': 7824, '학생': 7825, '학습': 7826, '학원': 7827, '한': 7828, '한국': 7829, '한국시간': 7830, '한다': 7831, '한다고': 7832, '한다는': 7833, '한다면': 7834, '한테': 7835, '할': 7836, '함': 7837, '함께': 7838, '함에': 7839, '함으로써': 7840, '함을': 7841, '합': 7842, '합니다': 7843, '핫': 7844, '핫이슈': 7845, '항': 7846, '항공': 7847, '해': 7848, '해달라': 7849, '해서': 7850, '해수욕장': 7851, '해야': 7852, '해온': 7853, '해왔다': 7854, '해졌다': 7855, '해주고': 7856, '해주는': 7857, '해진': 7858, '핵': 7859, '핸': 7860, '핸드': 7861, '햄': 7862, '햇': 7863, '했': 7864, '했고': 7865, '했기': 7866, '했는데': 7867, '했는지': 7868, '했다': 7869, '했다고': 7870, '했다는': 7871, '했던': 7872, '했습니다': 7873, '했어요': 7874, '했었다': 7875, '했으나': 7876, '했으며': 7877, '했으면': 7878, '했을': 7879, '했지만': 7880, '행': 7881, '행복': 7882, '행사': 7883, '행위': 7884, '행정': 7885, '향': 7886, '허': 7887, '허가': 7888, '헉': 7889, '헌': 7890, '헐': 7891, '험': 7892, '헛': 7893, '헝': 7894, '헤': 7895, '헨': 7896, '헬': 7897, '헷': 7898, '혀': 7899, '혁': 7900, '혁명': 7901, '혁신': 7902, '현': 7903, '현대': 7904, '현장': 7905, '현재': 7906, '현지시각': 7907, '현지시간': 7908, '혈': 7909, '혐': 7910, '협': 7911, '협동조합': 7912, '협력': 7913, '협상': 7914, '협의체': 7915, '협의회': 7916, '협정': 7917, '협회': 7918, '혔': 7919, '혔다': 7920, '형': 7921, '혜': 7922, '혜진': 7923, '혜택': 7924, '호': 7925, '호는': 7926, '호선': 7927, '호텔': 7928, '호텔에서': 7929, '혹': 7930, '혼': 7931, '홀': 7932, '홀딩스': 7933, '홀에서': 7934, '홈': 7935, '홈런': 7936, '홈쇼핑': 7937, '홉': 7938, '홍': 7939, '홍보': 7940, '화': 7941, '화를': 7942, '화학': 7943, '확': 7944, '확인': 7945, '환': 7946, '환경': 7947, '활': 7948, '활동': 7949, '활동을': 7950, '황': 7951, '황금어장': 7952, '회': 7953, '회담': 7954, '회를': 7955, '회말': 7956, '회사': 7957, '회의를': 7958, '회의에서': 7959, '회장': 7960, '회초': 7961, '획': 7962, '횟': 7963, '횡': 7964, '효': 7965, '효과': 7966, '효율': 7967, '후': 7968, '후보': 7969, '훈': 7970, '훈련': 7971, '훌': 7972, '훔': 7973, '훗': 7974, '훙': 7975, '훤': 7976, '훨': 7977, '훼': 7978, '휘': 7979, '휠': 7980, '휩': 7981, '휴': 7982, '흉': 7983, '흐': 7984, '흑': 7985, '흔': 7986, '흘': 7987, '흙': 7988, '흠': 7989, '흡': 7990, '흥': 7991, '흩': 7992, '희': 7993, '희망': 7994, '흰': 7995, '히': 7996, '힌': 7997, '힐': 7998, '힐링캠프': 7999, '힘': 8000, '힙': 8001, '[SOS]': 8002, '[EOS]': 8003}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SE-LC1TkCEb",
        "outputId": "d1beed1e-1e81-49bb-e96b-4ecb02593154"
      },
      "source": [
        "# Print the sentence split into tokens.\n",
        "toks = tokenizer.tokenize_wl(c_sentences[0])\n",
        "print('Tokenized: ', toks)\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(toks))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenized:  ['[SOS]', '▁그동안', '▁팔', '아', '치', '웠', '던', '▁삼성전자', '와', '▁SK', '하이닉스', '▁등', '▁반도체', '주', '를', '▁다시', '▁담', '기', '▁시작했다', '[EOS]']\n",
            "Token IDs:  [8002, 1193, 4814, 6797, 7483, 7036, 5842, 2653, 6983, 689, 7817, 1815, 2209, 7276, 6116, 1574, 1607, 5561, 2990, 8003]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eno9dC3RZpwL"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhZS9qUlQmc7"
      },
      "source": [
        "## dataset을 위한 Batch 구성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "CKdA9wbuCqD9",
        "outputId": "e88d343a-b354-4eea-d7e8-5f05f04852a9"
      },
      "source": [
        "class Batch:\n",
        "  src = []\n",
        "  trg = []\n",
        "max_length = 100\n",
        "batch_size = 128 #4 # 64, 128\n",
        "dataset_iterator = []\n",
        "batch_counter = 0\n",
        "\n",
        "sentences2 = c_sentences\n",
        "for i in range(0,len(sentences2),batch_size):\n",
        "#for i in range(0,1,batch_size):\n",
        "    nSRC = []\n",
        "    nTRG = []\n",
        "    src_max = 0\n",
        "    trg_max = 0\n",
        "    batch = Batch()\n",
        "    batch.src = []\n",
        "    batch.trg = []\n",
        "    \n",
        "    percent = (\"{0:.2f}\").format(100 * (i / float(len(sentences2))))\n",
        "    print(f'\\r {percent}% {i}/{str(len(sentences2))}', end=\"\", flush=True)\n",
        "    for j in range(i,i+batch_size):\n",
        "        try:\n",
        "            sentence = sentences2[j]+'.'\n",
        "            keys = extract_key(sentence)\n",
        "            srct = tokenizer.convert_tokens_to_ids(tokenizer.tokenize_wl((' '.join(keys)).strip()))\n",
        "            trgt = tokenizer.convert_tokens_to_ids(tokenizer.tokenize_wl(sentence))\n",
        "            if len(srct) < max_length and len(trgt) < max_length:\n",
        "                if len(trgt) > trg_max:\n",
        "                    trg_max = len(trgt)            \n",
        "                nTRG.append(trgt)\n",
        "                if len(srct) > src_max:\n",
        "                    src_max = len(srct)\n",
        "                nSRC.append(srct)\n",
        "\n",
        "        except Exception as e:\n",
        "            #print('Error:',e)\n",
        "            pass\n",
        "\n",
        "    for s in nSRC:\n",
        "        ss = s\n",
        "        if len(s) < src_max:\n",
        "            ss += [1 for i in range(src_max-len(s))]\n",
        "        batch.src.append(ss)\n",
        "\n",
        "    for t in nTRG:\n",
        "        tt = t\n",
        "        if len(t) < trg_max:        \n",
        "            tt += [1 for i in range(trg_max-len(t))]\n",
        "        batch.trg.append(tt)\n",
        "\n",
        "    #print(len(batch.src),len(batch.src[0]))\n",
        "    #print(len(batch.trg),len(batch.trg[0]))\n",
        "\n",
        "    batch.src = torch.tensor(batch.src).to(device)\n",
        "    batch.trg = torch.tensor(batch.trg).to(device)\n",
        "    dataset_iterator.append(batch)\n"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 25.71% 218496/849997"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-127-bfde073670ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0msrct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize_wl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mtrgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize_wl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-b3c1082d4d95>\u001b[0m in \u001b[0;36mextract_key\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#print(s)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtop_n\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_keywords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtop_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keybert/model.py\u001b[0m in \u001b[0;36mextract_keywords\u001b[0;34m(self, docs, keyphrase_ngram_range, stop_words, top_n, min_df, use_maxsum, use_mmr, diversity, nr_candidates, vectorizer)\u001b[0m\n\u001b[1;32m    115\u001b[0m                                                      \u001b[0mdiversity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                                                      \u001b[0mnr_candidates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                                                      vectorizer)\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             warnings.warn(\"Although extracting keywords for multiple documents is faster \"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keybert/model.py\u001b[0m in \u001b[0;36m_extract_keywords_single_doc\u001b[0;34m(self, doc, keyphrase_ngram_range, stop_words, top_n, use_maxsum, use_mmr, diversity, nr_candidates, vectorizer)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;31m# Extract Embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mdoc_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0mword_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m             \u001b[0;31m# doc_embedding = self.model.encode([doc])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;31m# word_embeddings = self.model.encode(words)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keybert/model.py\u001b[0m in \u001b[0;36m_extract_embeddings\u001b[0;34m(self, documents)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;31m# Infer embeddings with SentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;31m# Infer embeddings with Flair\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, is_pretokenized)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                 \u001b[0mout_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m                 \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput_value\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sentence_transformers/models/XLMRoBERTa.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;34m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m#RoBERTa does not use token_type_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0moutput_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlm_roberta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0moutput_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mcls_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# CLS token is first token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_extended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m         )\n\u001b[1;32m    764\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    437\u001b[0m                     \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m                 )\n\u001b[1;32m    441\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    369\u001b[0m     ):\n\u001b[1;32m    370\u001b[0m         self_attention_outputs = self.attention(\n\u001b[0;32m--> 371\u001b[0;31m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m         )\n\u001b[1;32m    373\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         )\n\u001b[0;32m--> 317\u001b[0;31m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add attentions if we output them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m                 self._forward_hooks.values()):\n\u001b[0m\u001b[1;32m    893\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhook_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqGxOzeKxjJQ",
        "outputId": "bb738e68-9969-48e5-f4d2-f9d1c99b8943",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(dataset_iterator)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1707"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abp5XMPnxoK3",
        "outputId": "599767ec-ae38-4f8e-a8de-17f13ffc7caf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dataset_iterator[1].src"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[8002, 3016, 1076,  ...,    1,    1,    1],\n",
              "        [8002, 3490, 6896,  ...,    1,    1,    1],\n",
              "        [8002,  760,  517,  ...,    1,    1,    1],\n",
              "        ...,\n",
              "        [8002, 3341, 5550,  ...,    1,    1,    1],\n",
              "        [8002, 1997, 6615,  ...,    1,    1,    1],\n",
              "        [8002, 2136, 6983,  ...,    1,    1,    1]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOZJjTgBR8PH"
      },
      "source": [
        "# dataset_iterator의 저장!\n",
        "\n",
        "import pickle\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/summary/dataset_iterator.pkl', 'wb') as f:\n",
        "    pickle.dump(dataset_iterator, f)"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiX6UBJSSP2B"
      },
      "source": [
        "# dataset_iterator를 읽어들임.\n",
        "\n",
        "import pickle\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/summary/dataset_iterator.pkl', 'rb') as f:\n",
        "    dataset_iterator2 = pickle.load(f)"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sStA4MqONtVK"
      },
      "source": [
        "# encoder / decoder 구성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpqrfqHRNxKm"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim,\n",
        "                 dropout, \n",
        "                 device,\n",
        "                 max_length = 100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim,\n",
        "                                                  dropout, \n",
        "                                                  device) \n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        \n",
        "        batch_size = src.shape[0]\n",
        "        src_len = src.shape[1]\n",
        "        \n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        \n",
        "        #pos = [batch size, src len]\n",
        "        \n",
        "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "            \n",
        "        #src = [batch size, src len, hid dim]\n",
        "            \n",
        "        return src"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thHItmM6N6Uu"
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim,  \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
        "                                                                     pf_dim, \n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        #src_mask = [batch size, 1, 1, src len] \n",
        "                \n",
        "        #self attention\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        #positionwise feedforward\n",
        "        _src = self.positionwise_feedforward(src)\n",
        "        \n",
        "        #dropout, residual and layer norm\n",
        "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        return src"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsZN2uo2OCC1"
      },
      "source": [
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        assert hid_dim % n_heads == 0\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = hid_dim // n_heads\n",
        "        \n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "        \n",
        "    def forward(self, query, key, value, mask = None):\n",
        "        \n",
        "        batch_size = query.shape[0]\n",
        "        \n",
        "        #query = [batch size, query len, hid dim]\n",
        "        #key = [batch size, key len, hid dim]\n",
        "        #value = [batch size, value len, hid dim]\n",
        "                \n",
        "        Q = self.fc_q(query)\n",
        "        K = self.fc_k(key)\n",
        "        V = self.fc_v(value)\n",
        "        \n",
        "        #Q = [batch size, query len, hid dim]\n",
        "        #K = [batch size, key len, hid dim]\n",
        "        #V = [batch size, value len, hid dim]\n",
        "                \n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        \n",
        "        #Q = [batch size, n heads, query len, head dim]\n",
        "        #K = [batch size, n heads, key len, head dim]\n",
        "        #V = [batch size, n heads, value len, head dim]\n",
        "                \n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "        \n",
        "        #energy = [batch size, n heads, query len, key len]\n",
        "        \n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\n",
        "        \n",
        "        attention = torch.softmax(energy, dim = -1)\n",
        "                \n",
        "        #attention = [batch size, n heads, query len, key len]\n",
        "                \n",
        "        x = torch.matmul(self.dropout(attention), V)\n",
        "        \n",
        "        #x = [batch size, n heads, query len, head dim]\n",
        "        \n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        \n",
        "        #x = [batch size, query len, n heads, head dim]\n",
        "        \n",
        "        x = x.view(batch_size, -1, self.hid_dim)\n",
        "        \n",
        "        #x = [batch size, query len, hid dim]\n",
        "        \n",
        "        x = self.fc_o(x)\n",
        "        \n",
        "        #x = [batch size, query len, hid dim]\n",
        "        \n",
        "        return x, attention"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZTi8NJZOJUK"
      },
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
        "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "        \n",
        "        #x = [batch size, seq len, pf dim]\n",
        "        \n",
        "        x = self.fc_2(x)\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SV8CVUlOPMM"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 output_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 dropout, \n",
        "                 device,\n",
        "                 max_length = 100):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim, \n",
        "                                                  dropout, \n",
        "                                                  device)\n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch size, trg len]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "                \n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "                            \n",
        "        #pos = [batch size, trg len]\n",
        "            \n",
        "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
        "                \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        output = self.fc_out(trg)\n",
        "        \n",
        "        #output = [batch size, trg len, output dim]\n",
        "            \n",
        "        return output, attention"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhO0Bjx0OWdk"
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
        "                                                                     pf_dim, \n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        \n",
        "        #self attention\n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
        "            \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "            \n",
        "        #encoder attention\n",
        "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
        "                    \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        \n",
        "        #positionwise feedforward\n",
        "        _trg = self.positionwise_feedforward(trg)\n",
        "        \n",
        "        #dropout, residual and layer norm\n",
        "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return trg, attention"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmUg6r57ObEf"
      },
      "source": [
        "##seq2seq 구성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIqJaJq1Odvy"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, \n",
        "                 encoder, \n",
        "                 decoder, \n",
        "                 src_pad_idx, \n",
        "                 trg_pad_idx, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "        \n",
        "    def make_src_mask(self, src):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        \n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "\n",
        "        return src_mask\n",
        "    \n",
        "    def make_trg_mask(self, trg):\n",
        "        \n",
        "        #trg = [batch size, trg len]\n",
        "        \n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        \n",
        "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
        "        \n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
        "        \n",
        "        #trg_sub_mask = [trg len, trg len]\n",
        "            \n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "        \n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        \n",
        "        return trg_mask\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #trg = [batch size, trg len]\n",
        "                \n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        \n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        \n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        \n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "                \n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        #output = [batch size, trg len, output dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return output, attention"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaaEpra3KEnS"
      },
      "source": [
        "# Building the Model --> 실행"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQntx79IdEGp"
      },
      "source": [
        "# Train 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Z8Z8q_fcL1B"
      },
      "source": [
        "INPUT_DIM = tokenizer.vocab_size\n",
        "OUTPUT_DIM = tokenizer.vocab_size\n",
        "#INPUT_DIM = len(tk.vocab)\n",
        "#OUTPUT_DIM = len(tk.vocab)\n",
        "HID_DIM = 256\n",
        "ENC_LAYERS = 3\n",
        "DEC_LAYERS = 3\n",
        "ENC_HEADS = 8\n",
        "DEC_HEADS = 8\n",
        "ENC_PF_DIM = 512\n",
        "DEC_PF_DIM = 512\n",
        "ENC_DROPOUT = 0.1\n",
        "DEC_DROPOUT = 0.1\n",
        "\n",
        "enc = Encoder(INPUT_DIM, \n",
        "              HID_DIM, \n",
        "              ENC_LAYERS, \n",
        "              ENC_HEADS, \n",
        "              ENC_PF_DIM, \n",
        "              ENC_DROPOUT, \n",
        "              device)\n",
        "\n",
        "dec = Decoder(OUTPUT_DIM, \n",
        "              HID_DIM, \n",
        "              DEC_LAYERS, \n",
        "              DEC_HEADS, \n",
        "              DEC_PF_DIM, \n",
        "              DEC_DROPOUT, \n",
        "              device)"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD8VBiXFc2uo"
      },
      "source": [
        "SRC_PAD_IDX = tokenizer.pad_token_id\n",
        "TRG_PAD_IDX = tokenizer.pad_token_id\n",
        "#SRC_PAD_IDX = tk.stoi['<pad>']\n",
        "#TRG_PAD_IDX = tk.stoi['<pad>']\n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyFRjDzpdJ76",
        "outputId": "16656cc4-5b52-4a11-fd08-b1f39f2a36c5"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 10,159,940 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QucNKWCBdUC0"
      },
      "source": [
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)\n",
        "\n",
        "model.apply(initialize_weights);"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNBgedENdeRY"
      },
      "source": [
        "LEARNING_RATE = 0.0005\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G6OgbLfdoJN"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        ##print(src.shape)\n",
        "        ##print(trg.shape)\n",
        "        output, _ = model(src, trg[:,:-1])\n",
        "        ##print(output.shape)\n",
        "        #output = [batch size, trg len - 1, output dim]\n",
        "        #trg = [batch size, trg len]\n",
        "            \n",
        "        output_dim = output.shape[-1]\n",
        "            \n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "                \n",
        "        #output = [batch size * trg len - 1, output dim]\n",
        "        #trg = [batch size * trg len - 1]\n",
        "            \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        ##print('loss',loss)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        ##print('epoch_loss',epoch_loss)\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdfEmlNHdvkX"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output, _ = model(src, trg[:,:-1])\n",
        "            \n",
        "            #output = [batch size, trg len - 1, output dim]\n",
        "            #trg = [batch size, trg len]\n",
        "            \n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "            \n",
        "            #output = [batch size * trg len - 1, output dim]\n",
        "            #trg = [batch size * trg len - 1]\n",
        "            \n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsdX1KJHd9LI"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEZ2o1g6dIh-"
      },
      "source": [
        "# Train !!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8ZsADe2e3av",
        "outputId": "a5ed409c-c81a-46e3-eb5a-d383a99dd944"
      },
      "source": [
        "bcnt = len(dataset_iterator)\n",
        "print(bcnt)\n",
        "train_iterator = dataset_iterator[0:bcnt-int(bcnt/10)]\n",
        "valid_iterator = dataset_iterator[bcnt-int(bcnt/10):bcnt]\n",
        "#train_iterator = dataset_iterator[0:bcnt-1]\n",
        "#valid_iterator = dataset_iterator[bcnt-1:bcnt]\n",
        "print('train',len(train_iterator))\n",
        "print('valid',len(valid_iterator))"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1707\n",
            "train 1537\n",
            "valid 170\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "po1OXs3sgmvI",
        "outputId": "65c92d7b-8d5b-4018-8acf-e3e2f37d304d"
      },
      "source": [
        "train_iterator[0].src"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[8002, 4814, 6797,  ..., 8003,    1,    1],\n",
              "        [8002, 2915, 7872,  ...,    1,    1,    1],\n",
              "        [8002, 2546, 7996,  ...,    1,    1,    1],\n",
              "        ...,\n",
              "        [8002, 1986, 5377,  ...,    1,    1,    1],\n",
              "        [8002, 1562, 6113,  ...,    1,    1,    1],\n",
              "        [8002, 2756, 5468,  ...,    1,    1,    1]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6lxOV5jd_id",
        "outputId": "dc8a2bc3-fdd1-4121-8537-151cf1253f59"
      },
      "source": [
        "\n",
        "N_EPOCHS = 100\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/summary/tut6-model.pt')\n",
        "        print('Save model!')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Save model!\n",
            "Epoch: 01 | Time: 1m 56s\n",
            "\tTrain Loss: 3.579 | Train PPL:  35.820\n",
            "\t Val. Loss: 3.537 |  Val. PPL:  34.355\n",
            "Save model!\n",
            "Epoch: 02 | Time: 1m 57s\n",
            "\tTrain Loss: 2.875 | Train PPL:  17.725\n",
            "\t Val. Loss: 3.214 |  Val. PPL:  24.877\n",
            "Save model!\n",
            "Epoch: 03 | Time: 1m 57s\n",
            "\tTrain Loss: 2.493 | Train PPL:  12.099\n",
            "\t Val. Loss: 3.017 |  Val. PPL:  20.434\n",
            "Save model!\n",
            "Epoch: 04 | Time: 1m 57s\n",
            "\tTrain Loss: 2.259 | Train PPL:   9.577\n",
            "\t Val. Loss: 2.918 |  Val. PPL:  18.497\n",
            "Save model!\n",
            "Epoch: 05 | Time: 1m 57s\n",
            "\tTrain Loss: 2.103 | Train PPL:   8.189\n",
            "\t Val. Loss: 2.861 |  Val. PPL:  17.472\n",
            "Save model!\n",
            "Epoch: 06 | Time: 1m 57s\n",
            "\tTrain Loss: 1.988 | Train PPL:   7.299\n",
            "\t Val. Loss: 2.820 |  Val. PPL:  16.778\n",
            "Save model!\n",
            "Epoch: 07 | Time: 1m 57s\n",
            "\tTrain Loss: 1.896 | Train PPL:   6.661\n",
            "\t Val. Loss: 2.793 |  Val. PPL:  16.324\n",
            "Save model!\n",
            "Epoch: 08 | Time: 1m 57s\n",
            "\tTrain Loss: 1.822 | Train PPL:   6.187\n",
            "\t Val. Loss: 2.773 |  Val. PPL:  16.010\n",
            "Save model!\n",
            "Epoch: 09 | Time: 1m 57s\n",
            "\tTrain Loss: 1.762 | Train PPL:   5.825\n",
            "\t Val. Loss: 2.762 |  Val. PPL:  15.827\n",
            "Epoch: 10 | Time: 1m 57s\n",
            "\tTrain Loss: 1.709 | Train PPL:   5.524\n",
            "\t Val. Loss: 2.763 |  Val. PPL:  15.847\n",
            "Save model!\n",
            "Epoch: 11 | Time: 1m 57s\n",
            "\tTrain Loss: 1.663 | Train PPL:   5.276\n",
            "\t Val. Loss: 2.747 |  Val. PPL:  15.596\n",
            "Epoch: 12 | Time: 1m 57s\n",
            "\tTrain Loss: 1.623 | Train PPL:   5.067\n",
            "\t Val. Loss: 2.750 |  Val. PPL:  15.650\n",
            "Epoch: 13 | Time: 1m 56s\n",
            "\tTrain Loss: 1.587 | Train PPL:   4.891\n",
            "\t Val. Loss: 2.755 |  Val. PPL:  15.713\n",
            "Epoch: 14 | Time: 1m 56s\n",
            "\tTrain Loss: 1.554 | Train PPL:   4.732\n",
            "\t Val. Loss: 2.759 |  Val. PPL:  15.783\n",
            "Epoch: 15 | Time: 1m 56s\n",
            "\tTrain Loss: 1.526 | Train PPL:   4.601\n",
            "\t Val. Loss: 2.755 |  Val. PPL:  15.722\n",
            "Epoch: 16 | Time: 1m 56s\n",
            "\tTrain Loss: 1.499 | Train PPL:   4.477\n",
            "\t Val. Loss: 2.763 |  Val. PPL:  15.841\n",
            "Epoch: 17 | Time: 1m 56s\n",
            "\tTrain Loss: 1.476 | Train PPL:   4.376\n",
            "\t Val. Loss: 2.752 |  Val. PPL:  15.676\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXsqseyMlPzB"
      },
      "source": [
        "def complete_sentence(sentence, model, device, max_len = 50):\n",
        "    \n",
        "    model.eval()\n",
        "    '''\n",
        "    if isinstance(sentence, str):\n",
        "        nlp = spacy.load('de_core_news_sm')\n",
        "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "    '''\n",
        "    tokens = tokenizer.tokenize_wl(sentence)\n",
        "    tokens = [tokenizer.bos_token] + tokens + [tokenizer.eos_token]\n",
        "    \n",
        "\n",
        "    src_indexes = [tokenizer.convert_tokens_to_ids(token) for token in tokens]\n",
        "    #src_indexes = get_stoi(tk,sentence)\n",
        "    print(src_indexes)\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "    print(src_tensor)\n",
        "    src_mask = model.make_src_mask(src_tensor)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\n",
        "\n",
        "    trg_indexes = [tokenizer.bos_token_id]\n",
        "    \n",
        "\n",
        "    for i in range(max_len):\n",
        "\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "        trg_mask = model.make_trg_mask(trg_tensor)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        pred_token = output.argmax(2)[:,-1].item()\n",
        "        \n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == tokenizer.eos_token_id: #tk.stoi['<eos>']: #.eos_token_id: # trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "            break\n",
        "    \n",
        "    trg_tokens = tokenizer.convert_ids_to_tokens(trg_indexes) # :[tk.itos[i] for i in trg_indexes]\n",
        "    \n",
        "    return trg_tokens[1:], attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guSK6Fasm8rC"
      },
      "source": [
        "trg, _ = complete_sentence('북한 미국 미사일',model,device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7bUE-sNnL97"
      },
      "source": [
        "print(''.join(trg))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1g-rZK5nkEE"
      },
      "source": [
        "extract_key(sentences[101])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alNmLf1bnnDc"
      },
      "source": [
        "trg, _ = complete_sentence('차갑게 공주의 일어서려던 그라시그',model,device)\n",
        "print(' '.join(trg))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}