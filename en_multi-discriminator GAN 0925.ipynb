{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "korean_frame_token_0_1.0_gamma_10.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dolmani38/Summary2/blob/main/en_multi-discriminator%20GAN%200925.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkQCxNatSIOk"
      },
      "source": [
        "# English Multi-Discriminator GAN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZjIW9VwyjDf"
      },
      "source": [
        "ABSTRACT\n",
        "\n",
        "----\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K87VNBbeRLFF"
      },
      "source": [
        "#4. Implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQZeBAf8NxAR"
      },
      "source": [
        "## 4.1 기본 설정..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAdXzWGuKSBT",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95964805-bc01-4fc5-8e20-062eed878798"
      },
      "source": [
        "if True:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "newO0mBXKVnE",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "822f28e7-6c11-4e78-884e-4f93274905ca"
      },
      "source": [
        "#!pip install keybert\n",
        "!pip3 install transformers\n",
        "!pip3 install sentence-transformers\n",
        "\n",
        "#!pip install sentence-transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.10.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.17)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.0.17)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.10.0+cu102)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.62.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.10.3)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.9.0+cu102)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.96)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.10.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (5.4.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.8.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.45)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmIxp0FnKXif",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62145966-03e2-45dc-9a5d-be82308eec4c"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# set seeds for reproducability\n",
        "from numpy.random import seed\n",
        "#seed(1)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string, os \n",
        "\n",
        "import urllib.request\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3J0n_lhKcgm",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae5701e1-7671-4b69-9be9-fc8e7a57b114"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "logout = True"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ue_4ZfdRKfdX",
        "trusted": true
      },
      "source": [
        "# Print iterations progress\n",
        "class ProgressBar:\n",
        "\n",
        "    def __init__(self,total=20, prefix = '', suffix = '', decimals = 1, length = 20, fill = '|', printEnd = \"\\r\"):\n",
        "        self.total = total\n",
        "        self.prefix = prefix\n",
        "        self.suffix = suffix\n",
        "        self.decimals = decimals\n",
        "        self.length = length\n",
        "        self.fill = fill\n",
        "        self.printEnd = printEnd\n",
        "        self.ite = 0\n",
        "        self.back_filledLength = 0\n",
        "\n",
        "    def printProgress(self,iteration, text):\n",
        "        self.ite += iteration\n",
        "        percent = (\"{0:.\" + str(self.decimals) + \"f}\").format(100 * (self.ite / float(self.total)))\n",
        "        filledLength = int(self.length * self.ite // self.total)\n",
        "        bar = self.fill * filledLength + '.' * (self.length - filledLength)\n",
        "        if filledLength > self.back_filledLength or percent == 100:\n",
        "            if logout:\n",
        "                print(f'\\r{self.prefix} |{bar}| {percent}% {self.suffix}  {text}', end=\"\", flush=True)\n",
        "            # Print New Line on Complete\n",
        "            if self.ite == self.total: \n",
        "                if logout:\n",
        "                    print()\n",
        "        self.back_filledLength = filledLength    "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNHI0G6JKc5h",
        "trusted": true
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Zsv-LVkKmfL"
      },
      "source": [
        "##4.2 Grammar Discriminator Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VQdGLciKc_y",
        "trusted": true
      },
      "source": [
        "from transformers import BertTokenizer, BertTokenizerFast,AutoTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset, random_split\n",
        "\n",
        "import time\n",
        "import random\n",
        "import datetime\n",
        "import pickle\n",
        "\n",
        "# 간단한 전처리\n",
        "def clean_text(txt):\n",
        "    txt = txt.replace('\\n',' ')\n",
        "    txt = txt.replace('\\r',' ')    \n",
        "    txt = txt.replace('=','')\n",
        "    txt = txt.replace('\\\"','')   \n",
        "    txt = txt.replace('\\'','')\n",
        "    #txt = txt.replace(',','')\n",
        "    txt = txt.replace('..','')\n",
        "    txt = txt.replace('...','')\n",
        "    txt = txt.replace(' .','.')\n",
        "    txt = txt.replace('.','. ')\n",
        "    txt = txt.replace('  ',' ')\n",
        "    txt = txt.replace('  ',' ')    \n",
        "    txt = txt.replace('  ',' ')   \n",
        "    txt = txt.replace('  ',' ')           \n",
        "    txt = txt.replace('  ',' ')\n",
        "    txt = txt.replace('  ',' ')    \n",
        "    txt = txt.replace('  ',' ')   \n",
        "    txt = txt.replace('  ',' ')             \n",
        "    return txt.strip()\n",
        "\n",
        "def shuffling(txt):\n",
        "    txt_list = txt.split(' ')\n",
        "    random.shuffle(txt_list)\n",
        "    return ' '.join(txt_list)\n",
        "\n",
        "def collect_training_dataset_for_grammar_discriminator(sentences_dataset):\n",
        "\n",
        "    sentences = []\n",
        "    labels = []\n",
        "\n",
        "    for txtss in sentences_dataset:\n",
        "        txtss = clean_text(txtss)\n",
        "        txts = txtss.strip().split('.')\n",
        "        for txt in txts:  \n",
        "            txt = txt.strip()\n",
        "            if len(txt) > 10:\n",
        "                #ko_grammar_dataset.append([txt,1])\n",
        "                txt = txt.replace('.','')\n",
        "                tf = random.choice([True,False])\n",
        "                # 정상 또는 비정상 둘중에 하나만 데이터셋에 추가\n",
        "                if (tf):\n",
        "                    sentences.append(txt) # '.'의 위치를 보고 True, False를 판단 하기 땜에...\n",
        "                    labels.append(1)\n",
        "                else:\n",
        "                    sentences.append(shuffling(txt))\n",
        "                    labels.append(0)\n",
        "\n",
        "    return sentences,labels\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "class Grammar_Discriminator:\n",
        "\n",
        "\n",
        "    def __init__(self, pretraoned_kobert_model_name='bert-base-v2', input_dir=None):\n",
        "\n",
        "        if input_dir is None:\n",
        "            self.tokenizer = BertTokenizerFast.from_pretrained(pretraoned_kobert_model_name)\n",
        "            self.discriminator = BertForSequenceClassification.from_pretrained(\n",
        "                                    pretraoned_kobert_model_name, # Use the 12-layer BERT model, with an uncased vocab.\n",
        "                                    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                                                    # You can increase this for multi-class tasks.   \n",
        "                                    output_attentions = False, # Whether the model returns attentions weights.\n",
        "                                    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "                                )            \n",
        "        else:\n",
        "            self.__load_model(input_dir)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def set_dataset(self, sentences,labels):\n",
        "        # Print the original sentence.\n",
        "        print(' Original: ', sentences[0])\n",
        "\n",
        "        # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "        input_ids = []\n",
        "        attention_masks = []\n",
        "\n",
        "        # For every sentence...\n",
        "        for i, sent in enumerate(sentences):\n",
        "            print(f'\\r Tokenize {i+1}/{len(sentences)}', end=\"\", flush=True)            \n",
        "            # `encode_plus` will:\n",
        "            #   (1) Tokenize the sentence.\n",
        "            #   (2) Prepend the `[CLS]` token to the start.\n",
        "            #   (3) Append the `[SEP]` token to the end.\n",
        "            #   (4) Map tokens to their IDs.\n",
        "            #   (5) Pad or truncate the sentence to `max_length`\n",
        "            #   (6) Create attention masks for [PAD] tokens.\n",
        "            encoded_dict = self.tokenizer.encode_plus(\n",
        "                                sent,                      # Sentence to encode.\n",
        "                                add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                                max_length = 64,           # Pad & truncate all sentences.\n",
        "                                pad_to_max_length = True,\n",
        "                                return_attention_mask = True,   # Construct attn. masks.\n",
        "                                return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                                truncation = True,\n",
        "                        )\n",
        "            \n",
        "            # Add the encoded sentence to the list.    \n",
        "            input_ids.append(encoded_dict['input_ids'])\n",
        "            \n",
        "            # And its attention mask (simply differentiates padding from non-padding).\n",
        "            attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "        # Convert the lists into tensors.\n",
        "        input_ids = torch.cat(input_ids, dim=0)\n",
        "        attention_masks = torch.cat(attention_masks, dim=0)\n",
        "        labels = torch.tensor(labels)\n",
        "\n",
        "        # Print sentence 0, now as a list of IDs.\n",
        "        print('Original: ', sentences[0])\n",
        "        print('Token IDs:', input_ids[0])\n",
        "\n",
        "        # Training & Validation Split\n",
        "        # Divide up our training set to use 90% for training and 10% for validation.\n",
        "\n",
        "        # Combine the training inputs into a TensorDataset.\n",
        "        dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "        # Create a 90-10 train-validation split.\n",
        "\n",
        "        # Calculate the number of samples to include in each set.\n",
        "        train_size = int(0.9 * len(dataset))\n",
        "        val_size = len(dataset) - train_size\n",
        "\n",
        "        # Divide the dataset by randomly selecting samples.\n",
        "        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "        print('{:>5,} training samples'.format(train_size))\n",
        "        print('{:>5,} validation samples'.format(val_size))\n",
        "\n",
        "        # The DataLoader needs to know our batch size for training, so we specify it \n",
        "        # here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "        # size of 16 or 32.\n",
        "        self.batch_size = 32\n",
        "\n",
        "        # Create the DataLoaders for our training and validation sets.\n",
        "        # We'll take training samples in random order. \n",
        "        self.train_dataloader = DataLoader(\n",
        "                    train_dataset,  # The training samples.\n",
        "                    sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "                    batch_size = self.batch_size # Trains with this batch size.\n",
        "                )\n",
        "\n",
        "        # For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "        self.validation_dataloader = DataLoader(\n",
        "                    val_dataset, # The validation samples.\n",
        "                    sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "                    batch_size = self.batch_size # Evaluate with this batch size.\n",
        "                )        \n",
        "\n",
        "\n",
        "    def train(self,epochs=4):\n",
        "        # Tell pytorch to run this model on the GPU.\n",
        "        self.discriminator.cuda()\n",
        "\n",
        "        # Get all of the model's parameters as a list of tuples.\n",
        "        params = list(self.discriminator.named_parameters())\n",
        "\n",
        "        print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "        print('==== Embedding Layer ====\\n')\n",
        "\n",
        "        for p in params[0:5]:\n",
        "            print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "        print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "        for p in params[5:21]:\n",
        "            print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "        print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "        for p in params[-4:]:\n",
        "            print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))  \n",
        "\n",
        "        # Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "        # I believe the 'W' stands for 'Weight Decay fix\"\n",
        "        self.optimizer = AdamW(self.discriminator.parameters(),\n",
        "                        lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                        eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                        )\n",
        "\n",
        "        # Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "        # We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "        # training data.\n",
        "        #epochs = 2\n",
        "\n",
        "        # Total number of training steps is [number of batches] x [number of epochs]. \n",
        "        # (Note that this is not the same as the number of training samples).\n",
        "        total_steps = len(self.train_dataloader) * epochs\n",
        "\n",
        "        # Create the learning rate scheduler.\n",
        "        scheduler = get_linear_schedule_with_warmup(self.optimizer, \n",
        "                                                    num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                                    num_training_steps = total_steps)\n",
        "            \n",
        "        # This training code is based on the `run_glue.py` script here:\n",
        "        # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "        # Set the seed value all over the place to make this reproducible.\n",
        "        seed_val = 42\n",
        "\n",
        "        random.seed(seed_val)\n",
        "        np.random.seed(seed_val)\n",
        "        torch.manual_seed(seed_val)\n",
        "        torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "        # We'll store a number of quantities such as training and validation loss, \n",
        "        # validation accuracy, and timings.\n",
        "        training_stats = []\n",
        "\n",
        "        # Measure the total training time for the whole run.\n",
        "        total_t0 = time.time()\n",
        "\n",
        "        # For each epoch...\n",
        "        for epoch_i in range(0, epochs):\n",
        "            \n",
        "            # ========================================\n",
        "            #               Training\n",
        "            # ========================================\n",
        "            \n",
        "            # Perform one full pass over the training set.\n",
        "\n",
        "            print(\"\")\n",
        "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "            print('Training...')\n",
        "\n",
        "            # Measure how long the training epoch takes.\n",
        "            t0 = time.time()\n",
        "\n",
        "            # Reset the total loss for this epoch.\n",
        "            total_train_loss = 0\n",
        "\n",
        "            # Put the model into training mode. Don't be mislead--the call to \n",
        "            # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "            # `dropout` and `batchnorm` layers behave differently during training\n",
        "            # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "            self.discriminator.train()\n",
        "\n",
        "            # For each batch of training data...\n",
        "            for step, batch in enumerate(self.train_dataloader):\n",
        "\n",
        "                # Progress update every 40 batches.\n",
        "                if step % 40 == 0 and not step == 0:\n",
        "                    # Calculate elapsed time in minutes.\n",
        "                    elapsed = format_time(time.time() - t0)\n",
        "                    \n",
        "                    # Report progress.\n",
        "                    print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(self.train_dataloader), elapsed))\n",
        "\n",
        "                # Unpack this training batch from our dataloader. \n",
        "                #\n",
        "                # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "                # `to` method.\n",
        "                #\n",
        "                # `batch` contains three pytorch tensors:\n",
        "                #   [0]: input ids \n",
        "                #   [1]: attention masks\n",
        "                #   [2]: labels \n",
        "                b_input_ids = batch[0].to(device)\n",
        "                b_input_mask = batch[1].to(device)\n",
        "                b_labels = batch[2].to(device)\n",
        "\n",
        "                # Always clear any previously calculated gradients before performing a\n",
        "                # backward pass. PyTorch doesn't do this automatically because \n",
        "                # accumulating the gradients is \"convenient while training RNNs\". \n",
        "                # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "                self.discriminator.zero_grad()        \n",
        "\n",
        "                # Perform a forward pass (evaluate the model on this training batch).\n",
        "                # The documentation for this `model` function is here: \n",
        "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "                # It returns different numbers of parameters depending on what arguments\n",
        "                # arge given and what flags are set. For our useage here, it returns\n",
        "                # the loss (because we provided labels) and the \"logits\"--the model\n",
        "                # outputs prior to activation.\n",
        "                outputs = self.discriminator(b_input_ids, \n",
        "                                    token_type_ids=None, \n",
        "                                    attention_mask=b_input_mask, \n",
        "                                    labels=b_labels)\n",
        "                loss, logits = outputs.loss, outputs.logits\n",
        "                # Accumulate the training loss over all of the batches so that we can\n",
        "                # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "                # single value; the `.item()` function just returns the Python value \n",
        "                # from the tensor.\n",
        "                total_train_loss += loss.item()\n",
        "\n",
        "                # Perform a backward pass to calculate the gradients.\n",
        "                loss.backward()\n",
        "\n",
        "                # Clip the norm of the gradients to 1.0.\n",
        "                # This is to help prevent the \"exploding gradients\" problem.\n",
        "                torch.nn.utils.clip_grad_norm_(self.discriminator.parameters(), 1.0)\n",
        "\n",
        "                # Update parameters and take a step using the computed gradient.\n",
        "                # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "                # modified based on their gradients, the learning rate, etc.\n",
        "                self.optimizer.step()\n",
        "\n",
        "                # Update the learning rate.\n",
        "                scheduler.step()\n",
        "\n",
        "            # Calculate the average loss over all of the batches.\n",
        "            avg_train_loss = total_train_loss / len(self.train_dataloader)            \n",
        "            \n",
        "            # Measure how long this epoch took.\n",
        "            training_time = format_time(time.time() - t0)\n",
        "\n",
        "            print(\"\")\n",
        "            print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "            print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "                \n",
        "            # ========================================\n",
        "            #               Validation\n",
        "            # ========================================\n",
        "            # After the completion of each training epoch, measure our performance on\n",
        "            # our validation set.\n",
        "\n",
        "            print(\"\")\n",
        "            print(\"Running Validation...\")\n",
        "\n",
        "            t0 = time.time()\n",
        "\n",
        "            # Put the model in evaluation mode--the dropout layers behave differently\n",
        "            # during evaluation.\n",
        "            self.discriminator.eval()\n",
        "\n",
        "            # Tracking variables \n",
        "            total_eval_accuracy = 0\n",
        "            total_eval_loss = 0\n",
        "            nb_eval_steps = 0\n",
        "\n",
        "            # Evaluate data for one epoch\n",
        "            for batch in self.validation_dataloader:\n",
        "                \n",
        "                # Unpack this training batch from our dataloader. \n",
        "                #\n",
        "                # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "                # the `to` method.\n",
        "                #\n",
        "                # `batch` contains three pytorch tensors:\n",
        "                #   [0]: input ids \n",
        "                #   [1]: attention masks\n",
        "                #   [2]: labels \n",
        "                b_input_ids = batch[0].to(device)\n",
        "                b_input_mask = batch[1].to(device)\n",
        "                b_labels = batch[2].to(device)\n",
        "                \n",
        "                # Tell pytorch not to bother with constructing the compute graph during\n",
        "                # the forward pass, since this is only needed for backprop (training).\n",
        "                with torch.no_grad():        \n",
        "\n",
        "                    # Forward pass, calculate logit predictions.\n",
        "                    # token_type_ids is the same as the \"segment ids\", which \n",
        "                    # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "                    # The documentation for this `model` function is here: \n",
        "                    # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "                    # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "                    # values prior to applying an activation function like the softmax.\n",
        "                    outputs = self.discriminator(b_input_ids, \n",
        "                                        token_type_ids=None, \n",
        "                                        attention_mask=b_input_mask,\n",
        "                                        labels=b_labels)\n",
        "                loss, logits = outputs.loss, outputs.logits\n",
        "                # Accumulate the validation loss.\n",
        "                total_eval_loss += loss.item()\n",
        "\n",
        "                # Move logits and labels to CPU\n",
        "                logits = logits.detach().cpu().numpy()\n",
        "                label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "                # Calculate the accuracy for this batch of test sentences, and\n",
        "                # accumulate it over all batches.\n",
        "                total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "                \n",
        "\n",
        "            # Report the final accuracy for this validation run.\n",
        "            avg_val_accuracy = total_eval_accuracy / len(self.validation_dataloader)\n",
        "            print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "            # Calculate the average loss over all of the batches.\n",
        "            avg_val_loss = total_eval_loss / len(self.validation_dataloader)\n",
        "            \n",
        "            # Measure how long the validation run took.\n",
        "            validation_time = format_time(time.time() - t0)\n",
        "            \n",
        "            print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "            print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "            # Record all statistics from this epoch.\n",
        "            training_stats.append(\n",
        "                {\n",
        "                    'epoch': epoch_i + 1,\n",
        "                    'Training Loss': avg_train_loss,\n",
        "                    'Valid. Loss': avg_val_loss,\n",
        "                    'Valid. Accur.': avg_val_accuracy,\n",
        "                    'Training Time': training_time,\n",
        "                    'Validation Time': validation_time\n",
        "                }\n",
        "            )\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"Training complete!\")\n",
        "\n",
        "        print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "            \n",
        "\n",
        "        return training_stats\n",
        "\n",
        "    def save_model(self, output_dir = './model_save/'):\n",
        "        # Create output directory if needed\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "\n",
        "        print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "        # They can then be reloaded using `from_pretrained()`\n",
        "        model_to_save = self.discriminator.module if hasattr(self.discriminator, 'module') else self.discriminator  # Take care of distributed/parallel training\n",
        "        model_to_save.save_pretrained(output_dir)\n",
        "        self.tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "        # Good practice: save your training arguments together with the trained model\n",
        "        # torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n",
        "\n",
        "    def __load_model(self, input_dir = './drive/MyDrive/Colab Notebooks/summary/en_grammar_check_model'):\n",
        "        print('Loading BERT tokenizer...')\n",
        "        self.tokenizer = BertTokenizerFast.from_pretrained(input_dir)\n",
        "        self.discriminator = BertForSequenceClassification.from_pretrained(input_dir)\n",
        "\n",
        "    def transfer_learning(self, sentences, train_for = True):\n",
        "        \n",
        "        input_ids = []\n",
        "        attention_masks = []\n",
        "\n",
        "        # For every sentence...\n",
        "        for sent in sentences:\n",
        "            # `encode_plus` will:\n",
        "            #   (1) Tokenize the sentence.\n",
        "            #   (2) Prepend the `[CLS]` token to the start.\n",
        "            #   (3) Append the `[SEP]` token to the end.\n",
        "            #   (4) Map tokens to their IDs.\n",
        "            #   (5) Pad or truncate the sentence to `max_length`\n",
        "            #   (6) Create attention masks for [PAD] tokens.\n",
        "            encoded_dict = self.tokenizer.encode_plus(\n",
        "                                sent,                      # Sentence to encode.\n",
        "                                add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                                max_length = 64,           # Pad & truncate all sentences.\n",
        "                                pad_to_max_length = True,\n",
        "                                return_attention_mask = True,   # Construct attn. masks.\n",
        "                                return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                                truncation = True,\n",
        "                        )\n",
        "            # Add the encoded sentence to the list.    \n",
        "            input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "            # And its attention mask (simply differentiates padding from non-padding).\n",
        "            attention_masks.append(encoded_dict['attention_mask'])\n",
        "        \n",
        "        if train_for:\n",
        "            b_labels = torch.ones(len(sentences),dtype=torch.long).to(device)\n",
        "        else:\n",
        "            b_labels = torch.zeros(len(sentences),dtype=torch.long).to(device)\n",
        "        #print(b_labels)\n",
        "        # Convert the lists into tensors.\n",
        "        input_ids = torch.cat(input_ids, dim=0).to(device)\n",
        "        attention_masks = torch.cat(attention_masks, dim=0).to(device)    \n",
        "        #if str(discriminator1.device) == 'cpu':\n",
        "        #    pass\n",
        "        #else:\n",
        "        #    input_ids = input_ids.to(device)\n",
        "        #    attention_masks = attention_masks.to(device)        \n",
        "\n",
        "        outputs = self.discriminator(input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=attention_masks, \n",
        "                                labels=b_labels)\n",
        "\n",
        "        #print(outputs)\n",
        "        #return torch.sigmoid(outputs[0][:,1])\n",
        "        #return outputs[0][:,1]\n",
        "        return outputs['loss'], outputs['logits']\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4zeEb0NR2QH"
      },
      "source": [
        "# 문법 discriminator 활용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7Zf2oRMMXmH",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2fb3499-916d-41bf-bfea-058cfbeac76c"
      },
      "source": [
        "g_discriminator = Grammar_Discriminator(input_dir = '/content/drive/MyDrive/Colab Notebooks/summary/en_grammar_model')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BERT tokenizer...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEXRsgqlXkpf",
        "outputId": "6e5f08a1-7316-4009-9110-0b48833d4c77"
      },
      "source": [
        "txt = ['Her friends sadly never heard from her after they parted company.','Her friends sadly never from her after heard they parted company.']\n",
        "g_discriminator.discriminator.to(device)\n",
        "g_discriminator.transfer_learning(txt)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(2.1402, device='cuda:0', grad_fn=<NllLossBackward>),\n",
              " tensor([[-5.8312,  5.9503],\n",
              "         [ 2.0386, -2.2279]], device='cuda:0', grad_fn=<AddmmBackward>))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d96kaCAHKuUc"
      },
      "source": [
        "##4.3 Static similarity discriminator class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZDpXe7XKxeg",
        "trusted": true
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import BertTokenizer\n",
        "from scipy.signal import find_peaks\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.misc import electrocardiogram\n",
        "import scipy\n",
        "\n",
        "\n",
        "class Similarity_Discriminator:\n",
        "    '''\n",
        "    _instance = None\n",
        "    _embedder = None\n",
        "    def __new__(cls,pre_trained_model_name='stsb-roberta-large'):\n",
        "        if cls._instance is None:\n",
        "            print('Creating Similarity_Discriminator object')\n",
        "            cls._instance = super(Similarity_Discriminator, cls).__new__(cls)\n",
        "            # Put any initialization here.\n",
        "            cls._embedder = SentenceTransformer(pre_trained_model_name)\n",
        "        return cls._instance\n",
        "\n",
        "    '''\n",
        "\n",
        "    def __init__(self,pre_trained_model_name='stsb-roberta-large'): #'roberta-large-nli-stsb-mean-tokens'):\n",
        "        print('Creating Similarity_Discriminator object')\n",
        "        # Put any initialization here.\n",
        "        self._embedder = SentenceTransformer(pre_trained_model_name,device=device)  \n",
        "        #self.cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "    def encode(self,texts):\n",
        "        return self._embedder.encode(texts,show_progress_bar=False)\n",
        "\n",
        "    def similarity(self, query_text, org_text_emb):\n",
        "        queries = nltk.sent_tokenize(query_text)\n",
        "        query_embeddings = self._embedder.encode(queries,show_progress_bar=False)\n",
        "        #query_embeddings = self._embedder.encode(queries,show_progress_bar=False)\n",
        "        #print(queries)\n",
        "        #print(org_text_emb)\n",
        "        \n",
        "        if len(query_embeddings) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        cos_scores = scipy.spatial.distance.cdist(query_embeddings, org_text_emb, \"cosine\")\n",
        "        similarity_score = 1.0 - np.min(np.max(cos_scores,axis=1))\n",
        "        '''\n",
        "        for query, query_embedding in zip(queries, query_embeddings):\n",
        "            distances = scipy.spatial.distance.cdist([query_embedding], [org_text_emb], \"cosine\")[0]\n",
        "            results = zip(range(len(distances)), distances)\n",
        "            for idx, distance in results:\n",
        "                scores.append(1-distance)\n",
        "        '''\n",
        "        return similarity_score  \n",
        " "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sQZ36GuMumP"
      },
      "source": [
        "###4.3.1 영어 문장 유사도 pre-trained model 적용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9Miao14Muww",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3f2fbc1-c96a-41cf-f32e-bb4ccc6ccfad"
      },
      "source": [
        "#del s_discriminator\n",
        "\n",
        "s_discriminator = Similarity_Discriminator()\n",
        "#s_discriminator = Similarity_Discriminator()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Similarity_Discriminator object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xnk9GsQ0K1t1"
      },
      "source": [
        "# 4.4 Document source class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYs__02JjKjT"
      },
      "source": [
        "## CDD/Daily mail Sample data 수집"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCOWg1jX-OKH"
      },
      "source": [
        "import pickle\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/summary/data/dnn_daily_mail_sample.bin\", \"rb\") as fp:\n",
        "    dt = pickle.load(fp)\n",
        "sentences_dataset = dt[0]\n",
        "gold_summary = dt[1]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnBm6RCvNIWG"
      },
      "source": [
        "## 4.4.2 source class 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYHIkaar2zb-"
      },
      "source": [
        "# 간단한 전처리\n",
        "def __clean_text(txt):\n",
        "    txt = txt.replace('\\n',' ')\n",
        "    txt = txt.replace('\\r',' ')    \n",
        "    #txt = txt.replace('=','')\n",
        "    #txt = txt.replace('\\\"','')   \n",
        "    #txt = txt.replace('\\'','')\n",
        "    #txt = txt.replace(',','')\n",
        "    #txt = txt.replace('..','')\n",
        "    #txt = txt.replace('...','')\n",
        "    txt = txt.replace('.',' ')\n",
        "    #txt = txt.replace('.','. ')\n",
        "    txt = txt.replace('  ',' ')\n",
        "    txt = txt.replace('  ',' ')    \n",
        "    txt = txt.replace('  ',' ')   \n",
        "    txt = txt.replace('  ',' ')           \n",
        "    txt = txt.replace('  ',' ')\n",
        "    txt = txt.replace('  ',' ')    \n",
        "    txt = txt.replace('  ',' ')   \n",
        "    txt = txt.replace('  ',' ')             \n",
        "    return txt.strip()\n",
        "\n",
        "def get_prepared_doc(txt):\n",
        "    docs = []\n",
        "    sentences = np.array(nltk.sent_tokenize(txt))\n",
        "    for sen in sentences:\n",
        "        docs.append(__clean_text(sen) +'.')\n",
        "    return (' '.join(docs)).strip()"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsJKbtc2K4xN",
        "trusted": true
      },
      "source": [
        "\n",
        "\n",
        "class Source:\n",
        "\n",
        "    def __init__(self,full_text,org_text,delete_ending = False,attendtion_rate=0.3):\n",
        "        self.full_text = full_text\n",
        "        self.org_text = org_text\n",
        "        self.delete_ending = delete_ending\n",
        "        self.attendtion_rate = attendtion_rate\n",
        "\n",
        "    def __crean_text(self, txt):\n",
        "        txt = txt.replace('\\n',' ')\n",
        "        txt = txt.replace('\\r',' ')    \n",
        "        txt = txt.replace('=','')\n",
        "        txt = txt.replace('\\\"','')   \n",
        "        txt = txt.replace('\\'','')\n",
        "        txt = txt.replace(',','')\n",
        "        txt = txt.replace('..','')\n",
        "        txt = txt.replace('...','')\n",
        "        txt = txt.replace(' .','.')\n",
        "        txt = txt.replace('.','. ')\n",
        "        txt = txt.replace('  ',' ')\n",
        "        txt = txt.replace('  ',' ')    \n",
        "        txt = txt.replace('  ',' ')   \n",
        "        txt = txt.replace('  ',' ')           \n",
        "        txt = txt.replace('  ',' ')\n",
        "        txt = txt.replace('  ',' ')    \n",
        "        txt = txt.replace('  ',' ')   \n",
        "        txt = txt.replace('  ',' ')           \n",
        "        return txt.strip()\n",
        "\n",
        "    def set_key_rate(self,s_discriminator):\n",
        "        # full_text에 대한 처리...\n",
        "        self.full_text = self.__crean_text(self.full_text.strip())\n",
        "        self.full_sentences = np.array(nltk.sent_tokenize(self.full_text))\n",
        "        self.s_discriminator = s_discriminator\n",
        "        self.full_text_emb = self.s_discriminator.encode(self.full_sentences)   \n",
        "\n",
        "        # original sentance, 즉 source sentence에 대한 처리\n",
        "        self.org_text = self.__crean_text(self.org_text.strip())\n",
        "        if logout:\n",
        "            print('-'*50)\n",
        "            print(self.org_text)\n",
        "            print('-'*50)  \n",
        "\n",
        "        # 두개 이상의 문장이 있는 경우, 중간 문자의 마침표를 지우고,\n",
        "        # ' and'를 넣어서, 연결한다.\n",
        "        self.org_sentences = np.array(nltk.sent_tokenize(self.org_text))\n",
        "        self.org_text_emb = self.s_discriminator.encode(self.org_sentences)\n",
        "        s = []\n",
        "        for i,sents in enumerate(self.org_sentences):\n",
        "            if sents.endswith('.') and i < len(self.org_sentences)-1:\n",
        "                s.append(sents[:-1].strip() + \" and\")\n",
        "            else:\n",
        "                s.append(sents)\n",
        "                #self.org_sentences[i] = sents[:-1].strip() + \" and\"\n",
        "                #print(self.org_sentences[i])\n",
        "        self.org_sentences = s\n",
        "        #print(s)\n",
        "        #print(self.org_sentences)\n",
        "        # 하나의 문장을 token 단위로 잘라서 {index:token} dict을 만든다.\n",
        "        # 또한, 각 token의 attention을 설정한다.\n",
        "        self.org_term_set = (' '.join(self.org_sentences)).strip().split(' ')\n",
        "        self.org_source_length = len(self.org_term_set)\n",
        "        self.term_table = {}\n",
        "\n",
        "        self.seps = []\n",
        "        self.bias_table = {}\n",
        "        #morp_table = {}\n",
        "        aw = 0.0\n",
        "        attentions = []\n",
        "        self_attentions = es.get_self_attention_weight(self.org_text)\n",
        "        self_attentions_max = 0\n",
        "        self_attentions_map = {}\n",
        "        for index, word in enumerate(self.org_term_set):\n",
        "            self.term_table[index] = word\n",
        "            #attention = cosine_similarity(self.full_text,word)\n",
        "            #attentions.append(attention)\n",
        "            sa = 0\n",
        "            for u in range(index,len(self_attentions)):\n",
        "                if word.lower().replace('.','') == self_attentions[u][1]:\n",
        "                    sa = self_attentions[u][2]\n",
        "                    if sa > self_attentions_max:\n",
        "                        self_attentions_max = sa\n",
        "\n",
        "            self_attentions_map[index] = sa\n",
        "            #print(f'{word} \\t\\t {attention:.4f} {sa:.4f}')\n",
        "            \n",
        "        #print('self_attentions_max',self_attentions_max)\n",
        "        attentions = list(self_attentions_map.values())\n",
        "        attentions.sort(reverse=True)\n",
        "        #문장 전체 token의 30%에 attention을 준다.\n",
        "        trs = attentions[int(len(attentions)*self.attendtion_rate + 0.5)]\n",
        "\n",
        "        for index, word in enumerate(self.org_term_set):\n",
        "            self.term_table[index] = word\n",
        "            #self.bias_table[index] = self_attentions_map[index] - self_attentions_max\n",
        "            attention = self_attentions_map[index] #cosine_similarity(self.full_text,word)\n",
        "\n",
        "            if attention >= trs or index == len(self.org_term_set)-1:\n",
        "                self.bias_table[index] = 0.0 #attention\n",
        "            else:\n",
        "                self.bias_table[index] = -1.0 #attention #-cosine_similarity(self.full_text,word)\n",
        "            \n",
        "            '''\n",
        "            if word.endswith(('.','?')):\n",
        "                self.seps.append(index)\n",
        "                if self.org_source_length - 1 == index:\n",
        "                    pass\n",
        "                else:\n",
        "                    self.term_table[index] = combine_sentence(word)\n",
        "            '''\n",
        "        #print(list(self.bias_table.values()))\n",
        "        # 또 다른 token 단위의 {index:token} dict을 만드는데, 이는 generator의 조합이\n",
        "        # 문법적으로 부실할 경우, corrector가 보정할때 '~~고'의 중간 연결문을 \n",
        "        # 부드럽게 만들기 위해 중간 문장의 '~다.'를 삭제한 dict에 해당한다.\n",
        "        self.combination_table = {}\n",
        "        for index, word in enumerate(self.org_term_set):\n",
        "            self.combination_table[index] = word\n",
        "            if index < len(self.org_term_set)-1: #중간 문장의 '~다.'를 삭제한다.\n",
        "                if self.org_term_set[index].endswith('다.'):\n",
        "                    self.combination_table[index] = word[0:len(word)-2]\n",
        "        if logout:\n",
        "            print('Length ------------------------------------|',len(self.term_table))\n",
        "            print(self.bias_table)\n",
        "        #print(self.combination_table)\n",
        "        if len(self.term_table) > 128:\n",
        "            raise Exception(\"Too much sentence length.\")\n",
        "\n",
        "    def get_org_sample(self, num):\n",
        "        return self.org_sentences[np.random.choice(len(self.org_sentences), num)]\n",
        "\n",
        "    def get_source_embedded_code(self):\n",
        "        return self.org_text_emb\n",
        "\n",
        "    def get_random_text(self,rate=0.5):\n",
        "        cnt = int(len(self.term_table) * rate)\n",
        "        a = list(self.term_table.keys())\n",
        "        b = np.random.choice(a, cnt)\n",
        "        c = [fruit for fruit in a if fruit not in b]\n",
        "        txt = []\n",
        "        for i in c:\n",
        "            txt.append(self.term_table[i])\n",
        "        return ' '.join(txt).strip(), hash(tuple(b))"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af5b1VF7poE2"
      },
      "source": [
        "## N-Gram Similarity Comparison\n",
        "\n",
        "https://gist.github.com/gaulinmp/da5825de975ed0ea6a24186434c24fe4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAfA5fHxBoGW",
        "outputId": "949f5fde-e1c8-4a41-c338-5e2ead253a52"
      },
      "source": [
        "# Get Tuple algorithms \n",
        "import re\n",
        "import math\n",
        "import numpy as np\n",
        "from itertools import chain\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.util import ngrams # This is the ngram magic.\n",
        "from textblob import TextBlob\n",
        "\n",
        "NGRAM = 4\n",
        "\n",
        "re_sent_ends_naive = re.compile(r'[.\\n]')\n",
        "re_stripper_alpha = re.compile('[^a-zA-Z]+')\n",
        "re_stripper_naive = re.compile('[^a-zA-Z\\.\\n]')\n",
        "\n",
        "splitter_naive = lambda x: re_sent_ends_naive.split(re_stripper_naive.sub(' ', x))\n",
        "\n",
        "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "def get_tuples_nosentences(txt):\n",
        "    \"\"\"Get tuples that ignores all punctuation (including sentences).\"\"\"\n",
        "    if not txt: return None\n",
        "    #ng = ngrams(re_stripper_alpha.sub(' ', txt).split(), NGRAM)\n",
        "    ng = ngrams(txt, NGRAM)\n",
        "    return list(ng)\n",
        "\n",
        "def get_tuples_manual_sentences(txt):\n",
        "    \"\"\"Naive get tuples that uses periods or newlines to denote sentences.\"\"\"\n",
        "    if not txt: return None\n",
        "    sentences = (x.split() for x in splitter_naive(txt) if x)\n",
        "    ng = (ngrams(x, NGRAM) for x in sentences if len(x) >= NGRAM)\n",
        "    return list(chain(*ng))\n",
        "\n",
        "def get_tuples_nltk_punkt_sentences(txt):\n",
        "    \"\"\"Get tuples that doesn't use textblob.\"\"\"\n",
        "    if not txt: return None\n",
        "    sentences = (re_stripper_alpha.split(x) for x in sent_detector.tokenize(txt) if x)\n",
        "    # Need to filter X because of empty 'words' from punctuation split\n",
        "    ng = (ngrams(filter(None, x), NGRAM) for x in sentences if len(x) >= NGRAM)\n",
        "    return list(chain(*ng))\n",
        "\n",
        "def get_tuples_textblob_sentences(txt):\n",
        "    \"\"\"New get_tuples that does use textblob.\"\"\"\n",
        "    if not txt: return None\n",
        "    tb = TextBlob(txt)\n",
        "    ng = (ngrams(x.words, NGRAM) for x in tb.sentences if len(x.words) > NGRAM)\n",
        "    return [item for sublist in ng for item in sublist]\n",
        "\n",
        "def jaccard_distance(a, b):\n",
        "    \"\"\"Calculate the jaccard distance between sets A and B\"\"\"\n",
        "    a = set(a)\n",
        "    b = set(b)\n",
        "    return 1.0 * len(a&b)/len(a|b)\n",
        "\n",
        "def cosine_similarity_ngrams(a, b):\n",
        "    vec1 = Counter(a)\n",
        "    vec2 = Counter(b)\n",
        "    \n",
        "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
        "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
        "\n",
        "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
        "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
        "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
        "\n",
        "    if not denominator:\n",
        "        return 0.0\n",
        "    return float(numerator) / denominator\n",
        "\n",
        "\n",
        "def test():\n",
        "    paragraph = \"\"\"It was the best of times, it was the worst of times.\n",
        "               It was the age of wisdom? It was the age of foolishness!\n",
        "               I first met Dr. Frankenstein in Munich; his monster was, presumably, at home.\"\"\"\n",
        "    print(paragraph)\n",
        "    _ = get_tuples_nosentences(paragraph);print(\"Number of N-grams (no sentences):\", len(_));_\n",
        "\n",
        "    _ = get_tuples_manual_sentences(paragraph);print(\"Number of N-grams (naive sentences):\", len(_));_\n",
        "\n",
        "    _ = get_tuples_nltk_punkt_sentences(paragraph);print(\"Number of N-grams (nltk sentences):\", len(_));_\n",
        "\n",
        "    _ = get_tuples_textblob_sentences(paragraph);print(\"Number of N-grams (TextBlob sentences):\", len(_));_\n",
        "\n",
        "    a = get_tuples_nosentences(\"It was the best of times.\")\n",
        "    b = get_tuples_nosentences(\"It was the worst of times.\")\n",
        "    print(\"Jaccard: {}   Cosine: {}\".format(jaccard_distance(a,b), cosine_similarity_ngrams(a,b)))\n",
        "\n",
        "    a = get_tuples_nosentences(\"Above is a bad example of four-gram similarity.\")\n",
        "    b = get_tuples_nosentences(\"This is a better example of four-gram similarity.\")\n",
        "    print(\"Jaccard: {}   Cosine: {}\".format(jaccard_distance(a,b), cosine_similarity_ngrams(a,b)))\n",
        "\n",
        "    a = get_tuples_nosentences(\"Jaccard Index ignores repetition repetition repetition repetition repetition.\")\n",
        "    b = get_tuples_nosentences(\"Cosine similarity weighs repetition repetition repetition repetition repetition.\")\n",
        "    print(\"Jaccard: {}   Cosine: {}\".format(jaccard_distance(a,b), cosine_similarity_ngrams(a,b)))\n",
        "test()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It was the best of times, it was the worst of times.\n",
            "               It was the age of wisdom? It was the age of foolishness!\n",
            "               I first met Dr. Frankenstein in Munich; his monster was, presumably, at home.\n",
            "Number of N-grams (no sentences): 214\n",
            "Number of N-grams (naive sentences): 25\n",
            "Number of N-grams (nltk sentences): 25\n",
            "Number of N-grams (TextBlob sentences): 25\n",
            "Jaccard: 0.6071428571428571   Cosine: 0.755742181606458\n",
            "Jaccard: 0.6071428571428571   Cosine: 0.755742181606458\n",
            "Jaccard: 0.23214285714285715   Cosine: 0.9208243668497166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akhPuNZHBx4w"
      },
      "source": [
        "def cosine_similarity(src_txt,trg_txt):\n",
        "    try:\n",
        "        if src_txt == None or src_txt.strip() == '':\n",
        "            return 0.0\n",
        "        if trg_txt == None or trg_txt.strip() == '':\n",
        "            return 0.0\n",
        "\n",
        "        a = get_tuples_nosentences(src_txt)\n",
        "        b = get_tuples_nosentences(trg_txt)\n",
        "        return cosine_similarity_ngrams(a,b)\n",
        "    except Exception as ex:\n",
        "        #print(src_txt,trg_txt)\n",
        "        return 0.0"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UAeFBYMMxKY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eda0da9a-1a7c-4bbb-fb8e-9894305e72c4"
      },
      "source": [
        "txt = \"\"\"\n",
        "The judge agreed with police that he would have been over the limit at the time his red Citroen hit Miss Titley’s blue Daihatsu Cuore on a road near Yarmouth, Isle of Wight, on October 11, 2013.\n",
        "His phone records showed he was also texting around the time of the crash.\n",
        "\"\"\"\n",
        "s = Source(txt,txt)\n",
        "s.set_key_rate(s_discriminator)\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "The judge agreed with police that he would have been over the limit at the time his red Citroen hit Miss Titley’s blue Daihatsu Cuore on a road near Yarmouth Isle of Wight on October 11 2013. His phone records showed he was also texting around the time of the crash.\n",
            "--------------------------------------------------\n",
            "The \t\t 0.0000 0.0952\n",
            "judge \t\t 0.0807 0.0642\n",
            "agreed \t\t 0.0989 0.0412\n",
            "with \t\t 0.0571 0.0220\n",
            "police \t\t 0.0989 0.1507\n",
            "that \t\t 0.0571 0.0306\n",
            "he \t\t 0.0000 0.2372\n",
            "would \t\t 0.0807 0.0633\n",
            "have \t\t 0.0571 0.0910\n",
            "been \t\t 0.0571 0.2784\n",
            "over \t\t 0.0571 0.3194\n",
            "the \t\t 0.0000 0.0952\n",
            "limit \t\t 0.0807 0.4084\n",
            "at \t\t 0.0000 0.0320\n",
            "the \t\t 0.0000 0.0952\n",
            "time \t\t 0.1141 0.0217\n",
            "his \t\t 0.0000 0.2955\n",
            "red \t\t 0.0000 0.1724\n",
            "Citroen \t\t 0.1141 0.0000\n",
            "hit \t\t 0.0000 0.3664\n",
            "Miss \t\t 0.0571 0.4336\n",
            "Titley’s \t\t 0.1276 0.0000\n",
            "blue \t\t 0.0571 0.1807\n",
            "Daihatsu \t\t 0.1276 0.0000\n",
            "Cuore \t\t 0.0807 0.0000\n",
            "on \t\t 0.0000 0.1166\n",
            "a \t\t 0.0000 0.1323\n",
            "road \t\t 0.0571 0.1636\n",
            "near \t\t 0.0571 0.1093\n",
            "Yarmouth \t\t 0.1276 0.2713\n",
            "Isle \t\t 0.0571 0.0761\n",
            "of \t\t 0.0000 0.0250\n",
            "Wight \t\t 0.0807 0.1073\n",
            "on \t\t 0.0000 0.1166\n",
            "October \t\t 0.1141 0.0968\n",
            "11 \t\t 0.0000 0.0547\n",
            "2013 \t\t 0.0571 0.1783\n",
            "and \t\t 0.0000 0.0000\n",
            "His \t\t 0.0000 0.2955\n",
            "phone \t\t 0.0807 0.0893\n",
            "records \t\t 0.1141 0.0756\n",
            "showed \t\t 0.0989 0.1856\n",
            "he \t\t 0.0000 0.2372\n",
            "was \t\t 0.0000 0.0921\n",
            "also \t\t 0.0571 0.1201\n",
            "texting \t\t 0.1141 0.0000\n",
            "around \t\t 0.0989 0.0698\n",
            "the \t\t 0.0000 0.0952\n",
            "time \t\t 0.1141 0.0217\n",
            "of \t\t 0.0000 0.0250\n",
            "the \t\t 0.0000 0.0952\n",
            "crash. \t\t 0.0989 0.3884\n",
            "[-0.3800386115908623, -0.4110163375735283, -0.43401921167969704, -0.4532301314175129, -0.3245444893836975, -0.44459403678774834, -0.2380419373512268, -0.41186240315437317, -0.3841570019721985, -0.196813702583313, -0.1558394432067871, -0.3800386115908623, -0.06676939129829407, -0.4432257078588009, -0.3800386115908623, -0.4534764848649502, -0.17973116040229797, -0.30284973978996277, -0.4752011001110077, -0.10876059532165527, -0.041623085737228394, -0.4752011001110077, -0.29447251558303833, -0.4752011001110077, -0.4752011001110077, -0.3585607260465622, -0.34287531673908234, -0.3115880489349365, -0.36595001071691513, -0.203903466463089, -0.3991253972053528, -0.4502161145210266, -0.3679448217153549, -0.3585607260465622, -0.3784070611000061, -0.42050599306821823, -0.29692552983760834, -0.4752011001110077, -0.17973116040229797, -0.3859023302793503, -0.39956874400377274, -0.2895888388156891, -0.2380419373512268, -0.38305673003196716, -0.35515056550502777, -0.4752011001110077, -0.4053828716278076, -0.3800386115908623, -0.4534764848649502, -0.4502161145210266, -0.3800386115908623, -0.08681043982505798]\n",
            "Length ------------------------------------| 52\n",
            "{0: -0.3800386115908623, 1: -0.4110163375735283, 2: -0.43401921167969704, 3: -0.4532301314175129, 4: -0.3245444893836975, 5: -0.44459403678774834, 6: -0.2380419373512268, 7: -0.41186240315437317, 8: -0.3841570019721985, 9: -0.196813702583313, 10: -0.1558394432067871, 11: -0.3800386115908623, 12: -0.06676939129829407, 13: -0.4432257078588009, 14: -0.3800386115908623, 15: -0.4534764848649502, 16: -0.17973116040229797, 17: -0.30284973978996277, 18: -0.4752011001110077, 19: -0.10876059532165527, 20: -0.041623085737228394, 21: -0.4752011001110077, 22: -0.29447251558303833, 23: -0.4752011001110077, 24: -0.4752011001110077, 25: -0.3585607260465622, 26: -0.34287531673908234, 27: -0.3115880489349365, 28: -0.36595001071691513, 29: -0.203903466463089, 30: -0.3991253972053528, 31: -0.4502161145210266, 32: -0.3679448217153549, 33: -0.3585607260465622, 34: -0.3784070611000061, 35: -0.42050599306821823, 36: -0.29692552983760834, 37: -0.4752011001110077, 38: -0.17973116040229797, 39: -0.3859023302793503, 40: -0.39956874400377274, 41: -0.2895888388156891, 42: -0.2380419373512268, 43: -0.38305673003196716, 44: -0.35515056550502777, 45: -0.4752011001110077, 46: -0.4053828716278076, 47: -0.3800386115908623, 48: -0.4534764848649502, 49: -0.4502161145210266, 50: -0.3800386115908623, 51: -0.08681043982505798}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XY59mdNK8ub"
      },
      "source": [
        "# 4.5 Generator class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5CLF3WcK6lp",
        "trusted": true
      },
      "source": [
        "from functools import reduce\n",
        "\n",
        "\n",
        "# custom weights initialization called on netG and netD\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Linear') != -1:\n",
        "        m.weight.data.normal_(0.01, 0.05)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.05)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "        Simple Generator w/ MLP\n",
        "    \"\"\"\n",
        "    '''\n",
        "    def __init__(self, input_size=1024):\n",
        "        super(Generator, self).__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(input_size, input_size*2),\n",
        "            nn.BatchNorm1d(input_size*2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(input_size*2, input_size*3),\n",
        "            nn.BatchNorm1d(input_size*3),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(input_size*3, input_size*3),\n",
        "            nn.BatchNorm1d(input_size*3),\n",
        "            nn.ReLU(True),            \n",
        "            nn.Linear(input_size*3, input_size*2),\n",
        "            nn.BatchNorm1d(input_size*2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(input_size*2, input_size),\n",
        "            #nn.BatchNorm1d(term_length*4),\n",
        "            nn.Tanh() # -1 ~ 1\n",
        "        )\n",
        "\n",
        "    \n",
        "    def __init__(self, input_size=1024):\n",
        "        super(Generator, self).__init__()\n",
        "        l1 = nn.Linear(input_size, input_size*4)\n",
        "        l1.weight.data.normal_(0.0, 0.01)\n",
        "        bn = nn.BatchNorm1d(input_size*4)\n",
        "        bn.weight.data.normal_(0.0, 0.01)\n",
        "        bn.bias.data.fill_(0)        \n",
        "        l2 = nn.Linear(input_size*4, input_size)\n",
        "        l2.weight.data.normal_(0.05, 0.01)\n",
        "        self.layer = nn.Sequential(\n",
        "            l1,\n",
        "            bn,\n",
        "            nn.ReLU(True), #nn.LeakyReLU(0.2),\n",
        "            l2,\n",
        "            #nn.BatchNorm1d(term_length*4),\n",
        "            nn.Tanh() # -1 ~ 1\n",
        "        )\n",
        "    '''\n",
        "\n",
        "    def __init__(self, input_size=1024):\n",
        "        super(Generator, self).__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(input_size, input_size*4),\n",
        "            nn.BatchNorm1d(input_size*4),\n",
        "            nn.ReLU(True), #nn.LeakyReLU(0.2),\n",
        "            nn.Linear(input_size*4, input_size),\n",
        "            #nn.BatchNorm1d(input_size*4),\n",
        "            #nn.ReLU(True), #nn.LeakyReLU(0.2),            \n",
        "            #nn.Linear(input_size*4, input_size),\n",
        "            #nn.BatchNorm1d(input_size),\n",
        "            #nn.ReLU(True), #nn.LeakyReLU(0.2),\n",
        "            nn.Tanh() # -1 ~ 1\n",
        "        )\n",
        "\n",
        "    def forward(self, x, bias):\n",
        "        #biased_noise = torch.randn(N,_NOISE_DIM)\n",
        "        # stroy peak에 해당하는 term에게 평균값에 해당하는 bias를 추가 한다.\n",
        "                 \n",
        "        y_ = self.layer(x)\n",
        "        y = torch.add(y_,bias)\n",
        "        #y = nn.Sigmoid()(y)\n",
        "\n",
        "        return y, y_\n"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myU75t4_4dni"
      },
      "source": [
        "# multi-discriminator에 대한 Adaptive discriminant factor 를 구하기 위한 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0RQOPpQgUTE"
      },
      "source": [
        "# SAM_Summarizer 학습기..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nd8GTS7HKz1H",
        "trusted": true
      },
      "source": [
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "from scipy.special import expit\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SAM_Summarizer:\n",
        "\n",
        "    def __init__(self,g_discriminator,s_discriminator):\n",
        "        self.g_discriminator = g_discriminator\n",
        "        #self.c_discriminator = c_discriminator\n",
        "        self.s_discriminator = s_discriminator\n",
        "        self.m = nn.Sigmoid()\n",
        "        self.with_bias = True\n",
        "\n",
        "    def ready(self,source):\n",
        "        self.source = source  \n",
        "        #self.source.analysis_frame_terms(self.s_discriminator)\n",
        "        self.generator = Generator(input_size=self.source.org_source_length)\n",
        "        self.generator.apply(weights_init)\n",
        "        return self\n",
        "\n",
        "    def summarize(self,epochs=10,batch_size=1,learning_rate=2e-4, display = False,comp_rate=1.0):\n",
        "        history = self.__train(epochs,batch_size,learning_rate,display,comp_rate)\n",
        "\n",
        "        if display and history is not None:\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            plt.plot(history['gen_g_loss'],label='grammar loss')\n",
        "            plt.plot(history['gen_l_loss'],label='compression loss')\n",
        "            plt.plot(history['gen_s_loss'],label='n-gram similarity loss')\n",
        "            #plt.plot(history['gen_c_loss'],label='context similarity loss')\n",
        "            #plt.plot(history['total loss'],label='total loss')\n",
        "            plt.plot(history['losses std'],label='standard deviation of losses')\n",
        "            \n",
        "            #if 'dis_loss' in history:\n",
        "            #    plt.plot(history['dis_loss'],label='discriminator grammar loss')\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "        return history\n",
        "\n",
        "    # text의 생성 for torch\n",
        "    def __text_gen2(self, p_txt, gen_length):\n",
        "        gtext = []\n",
        "        sorted_noise, i = torch.sort(p_txt, descending=True)\n",
        "        order, i = torch.sort(i[:gen_length], descending=False)\n",
        "        #print(len(order))\n",
        "        #print(gen_length)\n",
        "        assert len(order) == gen_length\n",
        "        order = order.cpu().detach().numpy()\n",
        "        for k in order:\n",
        "            gtext.append((self.source.term_table[k],k))\n",
        "        return gtext\n",
        "\n",
        "    def __text_gen3(self, p_txt):\n",
        "        gtext = []\n",
        "\n",
        "        for order,p in enumerate(p_txt):\n",
        "            if p > 0.0:\n",
        "                gtext.append(self.source.term_table[order])\n",
        "        return gtext\n",
        "\n",
        "    def __text_gen5(self, p_txt):\n",
        "        gtext = []\n",
        "\n",
        "        for order,p in enumerate(p_txt):\n",
        "            if p > 0.0:\n",
        "                gtext.append(self.source.combination_table[order])\n",
        "        return gtext\n",
        "\n",
        "    def __text_hash(self, p_txt):\n",
        "        b = []\n",
        "        #hash(tuple(b))\n",
        "        for order,p in enumerate(p_txt):\n",
        "            if p > 0.0:\n",
        "                b.append(order)\n",
        "        return hash(tuple(b))\n",
        "\n",
        "    def __text_gen4(self, p_txt):\n",
        "        gtext = \"\"\n",
        "        indexs = []\n",
        "        for order,p in enumerate(p_txt):\n",
        "            if p > 0.0:\n",
        "                gtext += self.source.term_table[order] + ' '\n",
        "                indexs.append(order)\n",
        "        return gtext.strip(),indexs\n",
        "\n",
        "\n",
        "    def __discrete_gradient(self,weights,use_gpu=False, verbose=0):\n",
        "        fake_gen_out = torch.zeros(weights.shape).to(device)\n",
        "        fake_cos_out = torch.zeros(weights.shape).to(device)\n",
        "        fake_sim_out = torch.zeros(weights.shape).to(device)\n",
        "        fake_len_out = torch.zeros(weights.shape).to(device) \n",
        "\n",
        "        #real_text = self.source.get_org_sample(weights.shape[0])\n",
        "        fake_outs = []\n",
        "        #real_outs = []\n",
        "        apply_order = []\n",
        "        for i, noise in enumerate(weights):\n",
        "            #gtext = self.__text_gen2(noise,gen_length)\n",
        "            gtext,tk = self.__text_gen4(noise)\n",
        "            fake_outs.append(gtext)\n",
        "            apply_order.append((i,tk))\n",
        "  \n",
        "        #print(fake_outs)\n",
        "\n",
        "        D_z_loss, fake_gmr_out=self.g_discriminator.transfer_learning(fake_outs,train_for = False)\n",
        "\n",
        "        o_sim_out = []\n",
        "        o_cos_out = []\n",
        "        o_len_out = []\n",
        "        for fake_text in fake_outs:\n",
        "            s1 = cosine_similarity(self.source.full_text,fake_text)  \n",
        "            #s1 = self.s_discriminator.similarity(fake_text,self.source.full_text_emb)\n",
        "            #s2 = 0.5 #cosine_similarity(self.source.full_text,fake_text)  #self.s_discriminator.similarity(fake_text,self.source.full_text_emb)\n",
        "            #s = ((s1+s2)/2 - 0.5) * 2\n",
        "            #s = (s1 - 0.5) * 2\n",
        "            #print(s)\n",
        "            #print(s)\n",
        "            cs = 0.5 #(s2/2 - 0.5) * 2\n",
        "            l = 1 - len(fake_text)/len(self.source.org_text)\n",
        "            #l = ((1 - len(fake_text.split(' '))/len(self.source.org_text.split(' ')))-0.5) * 2\n",
        "            #print(l)\n",
        "            o_sim_out.append(s1)\n",
        "            o_cos_out.append(cs)\n",
        "            o_len_out.append(l)\n",
        "            #print(1 - len(fake_text.split(' '))/self.source.org_source_length)\n",
        "            #o_len_out.append(-len(fake_text.split(' '))/self.source.org_source_length)\n",
        "        \n",
        "        \n",
        "        for j, (i,tk) in enumerate(apply_order):\n",
        "\n",
        "            try:\n",
        "                '''\n",
        "                a = torch.tanh( fake_gmr_out[j,1])\n",
        "                if a > 0 :\n",
        "                    fake_gen_out[:] = -0.5\n",
        "                    fake_gen_out[i,tk] = a\n",
        "                else:\n",
        "                    fake_gen_out[:] = 0.5\n",
        "                    fake_gen_out[i,tk] = a\n",
        "                '''\n",
        "                fake_gen_out[i,tk] = torch.tanh( fake_gmr_out[j,1])\n",
        "                fake_sim_out[i,tk] = o_sim_out[j]\n",
        "                fake_cos_out[i,tk] = o_cos_out[j]\n",
        "                #fake_len_out[i,tk] = o_len_out[j]\n",
        "                fake_len_out[:] = -o_len_out[j]\n",
        "                #fake_len_out[i,tk] = 0 #torch.tensor(fake_text_len/self.source.org_source_length).to(device)\n",
        "                #fake_len_out[:] = o_len_out[j] #torch.tensor(fake_text_len/self.source.org_source_length).to(device)\n",
        "                #print(o_len_out[j])\n",
        "            except Exception as ex:\n",
        "                print(j,i,tk)\n",
        "                print(fake_gmr_out)\n",
        "                raise ex\n",
        "\n",
        "        return fake_gen_out, fake_sim_out, fake_cos_out, fake_len_out #fake_com_out, fake_sim_out #, D_z_loss, D_x_loss\n",
        "\n",
        "\n",
        "    def __train(self, epochs=10,batch_size=10,learning_rate=2e-4,display = False,comp_rate=1.0):\n",
        "        # In the Deepmind paper they use RMSProp however then Adam optimizer\n",
        "        # improves training time\n",
        "        #generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "        # This method returns a helper function to compute cross entropy loss\n",
        "        #cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "        # Set the seed value all over the place to make this reproducible.\n",
        "        seed_val = int(random.random()*100)\n",
        "\n",
        "        random.seed(seed_val)\n",
        "        np.random.seed(seed_val)\n",
        "        torch.manual_seed(seed_val)\n",
        "        torch.cuda.manual_seed_all(seed_val)\n",
        "        \n",
        "        criterion = nn.MSELoss()\n",
        "        #D_opt = torch.optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "        G_opt = AdamW(self.generator.parameters(),\n",
        "                        lr = 2e-3, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                        eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                        )\n",
        "        # Create the learning rate scheduler.\n",
        "        scheduler = get_linear_schedule_with_warmup(G_opt, \n",
        "                                                    num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                                    num_training_steps = epochs)\n",
        "        \n",
        "        #gen_length = len(self.source.story_peaks) + int(len(self.source.story_peaks)*self.frame_expansion_ratio)\n",
        "        pb = ProgressBar(epochs,prefix='Train...')\n",
        "        gen_gmr_loss_history = []\n",
        "        gen_len_loss_history = []\n",
        "        gen_sim_loss_history = []\n",
        "        gen_cos_loss_history = []\n",
        "        dis_loss_history = []    \n",
        "        total_loss_history = []\n",
        "        losses_std_history = []\n",
        "\n",
        "        #model 들은 cuda로 보낸다.\n",
        "        self.g_discriminator.discriminator.to(device)\n",
        "        self.g_discriminator.discriminator.eval() # 학습하지 않는다...\n",
        "        #self.c_discriminator.discriminator.to(device)\n",
        "        #self.c_discriminator.discriminator.eval() # 학습하지 않는다...\n",
        "\n",
        "        self.generator.to(device)       \n",
        "        self.generator.train()\n",
        "\n",
        "        #self.bias_w = init_bias\n",
        "        initial_bias = 0\n",
        "        G_s_loss = torch.tensor(0)\n",
        "        #G_c_loss = torch.tensor(0)\n",
        "        G_g_loss = torch.tensor(0)\n",
        "\n",
        "\n",
        "        epsilon = 1 # Epsilon-greedy algorithm in initialized at 1 meaning every step is random at the start\n",
        "        max_epsilon = 1 # You can't explore more than 100% of the time\n",
        "        min_epsilon = 0.001 # At a minimum, we'll always explore 1% of the time\n",
        "        decay = 10/epochs\n",
        "        \n",
        "\n",
        "        dfs = torch.tensor([ 1.0, similarity, comp_rate], device=device, dtype=torch.float, requires_grad=True)\n",
        "        target = torch.tensor([list(self.source.bias_table.values()) for u in range(batch_size)],dtype=torch.float).to(device)\n",
        "        #print(target)\n",
        "        #noise = torch.randn(batch_size,self.source.org_source_length).to(device) \n",
        "        a_w = 1.0\n",
        "        for i in range(epochs):\n",
        "   \n",
        "            if True:\n",
        "                noise = torch.randn(batch_size,self.source.org_source_length).to(device) \n",
        "                '''\n",
        "                random_number = np.random.rand()\n",
        "                # 2. Explore using the Epsilon Greedy Exploration Strategy\n",
        "                if random_number <= epsilon:\n",
        "                    # Explore\n",
        "                    bias = torch.randn(batch_size,self.source.org_source_length).to(device) * epsilon\n",
        "                    #b = torch.tensor([list(self.source.bias_table.values()) for u in range(batch_size)]).to(device)\n",
        "                    #bias = torch.add(a,b)\n",
        "                    #noise = torch.randn(batch_size,self.source.org_source_length).to(device) \n",
        "                else:\n",
        "                    #bias = torch.tensor([list(self.source.bias_table.values()) for u in range(batch_size)]).to(device)\n",
        "                    bias = torch.zeros_like(noise).to(device)\n",
        "                '''\n",
        "                bias = torch.zeros_like(noise).to(device)\n",
        "\n",
        "\n",
        "                #if self.with_bias:\n",
        "                #    bias[:,noise.shape[1]-1] = 0.1\n",
        "                #bias[:,noise.shape[1]-1] = 0.5\n",
        "                #if i < epochs/4:\n",
        "                #bias = torch.randn(batch_size,self.source.org_source_length).to(device) / 4                 \n",
        "                #bias = torch.randn(batch_size,self.source.org_source_length).to(device) \n",
        "\n",
        "                sw, sw0 = self.generator(noise,bias)\n",
        "                #print(sw)\n",
        "                with torch.no_grad():                \n",
        "                    fake_gmr_out, fake_sim_out, fake_cos_out, fake_len_out = self.__discrete_gradient(sw)\n",
        "\n",
        "                #print(fake_len_out)\n",
        "                #print(fake_gmr_out)\n",
        "                sw2 = sw * fake_gmr_out\n",
        "                #print(sw2)\n",
        "                G_g_loss = -torch.mean(sw2)\n",
        "                #print(G_g_loss)\n",
        "                sw1 = sw * fake_sim_out\n",
        "                G_s_loss = -torch.mean(sw1)\n",
        "\n",
        "                sw4 = sw * fake_cos_out\n",
        "                G_c_loss = -torch.mean(sw4) \n",
        "\n",
        "                #sw3 = sw * fake_len_out\n",
        "                #G_l_loss = -torch.mean(sw3)\n",
        "\n",
        "                G_l_loss = (criterion(sw,target) - 1) #* (1-epsilon)\n",
        "\n",
        "                dsc_loss = torch.stack([G_g_loss,G_s_loss,G_l_loss])\n",
        "\n",
        "                G_loss = torch.dot(dfs,dsc_loss) + torch.std(dsc_loss)*std_factor\n",
        "                #G_loss =  G_g_loss  + G_s_loss + G_l_loss * comp_rate\n",
        "                #G_loss = G_l_loss\n",
        "\n",
        "                #print(G_loss)\n",
        "                \n",
        "                self.generator.zero_grad()\n",
        "                G_loss.backward()\n",
        "                #print('backward:')\n",
        "                G_opt.step()\n",
        "                scheduler.step()\n",
        "                '''\n",
        "                learning_rate = 0.02\n",
        "                with torch.no_grad():\n",
        "                    dfs += learning_rate * dfs.grad\n",
        "                    dfs.grad = None                    \n",
        "                    dfs[dfs < 0] = 0.1                \n",
        "                '''\n",
        "                #if G_g_loss == 0:# or (i > 100 and G_g_loss > 0):\n",
        "                #    return None\n",
        "\n",
        "                #if G_g_loss > 0:\n",
        "                #    a_w += 0.4\n",
        "\n",
        "            gen_gmr_loss_history.append(G_g_loss.cpu().detach().numpy())\n",
        "            gen_cos_loss_history.append(G_c_loss.cpu().detach().numpy())\n",
        "            gen_sim_loss_history.append(G_s_loss.cpu().detach().numpy())\n",
        "            #dis_loss_history.append(D_loss.cpu().detach().numpy())\n",
        "            gen_len_loss_history.append(G_l_loss.cpu().detach().numpy())\n",
        "\n",
        "            #pb.printProgress(+1,f'{i+1}/{epochs} epochs, beta:{dfs} Generator / grammar loss:{G_g_loss}  similarity loss:{G_s_loss}') #,   Discriminator grammar_loss:{D_loss}        ')\n",
        "            #pb.printProgress(+1,'{}/{} epochs, beta:{}, grammar loss:{:.4f}  similarity loss:{:.4f} length loss:{:.4f}'.format(i+1,epochs,dfs,G_g_loss,G_s_loss,G_l_loss)) #,   Discriminator grammar_loss:{D_loss}        ')\n",
        "            pb.printProgress(+1,'{}/{} epochs, e {:.5f} gl:{:.8f}  sl:{:.4f} ll:{:.4f}'.format(i+1,epochs,epsilon, G_g_loss,G_s_loss,G_l_loss)) #,   Discriminator grammar_loss:{D_loss}        ')\n",
        "            \n",
        "            total_loss_history.append(torch.sum(dsc_loss).item())\n",
        "            losses_std_history.append(torch.std(dsc_loss).item())\n",
        "\n",
        "            epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay * i)\n",
        "            \n",
        "        self.generator.eval()\n",
        "\n",
        "        if G_g_loss > -0.10:\n",
        "            return None\n",
        "        #self.g_discriminator.discriminator.eval()\n",
        "\n",
        "        if display:\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            xs = np.arange(self.source.org_source_length)\n",
        "            #plt.bar(xs+0.0,sw0[0].cpu().detach().numpy(),label='before activation weights',width=0.2)\n",
        "            plt.bar(xs+0.0,sw[0].cpu().detach().numpy(),label='generated value',width=0.2)\n",
        "            plt.bar(xs+0.2,list(self.source.bias_table.values()),label='-self_attention',width=0.2)         \n",
        "            plt.legend()        \n",
        "            plt.show()\n",
        "\n",
        "        return  {'gen_g_loss':gen_gmr_loss_history,'gen_s_loss':gen_sim_loss_history,'gen_c_loss':gen_cos_loss_history,'gen_l_loss':gen_len_loss_history,'total loss':total_loss_history,'losses std':losses_std_history} #,'dis_loss':dis_loss_history }\n",
        "    '''\n",
        "    def get_summary(self, count):\n",
        "        #texts = []\n",
        "        self.generator.cpu()\n",
        "        self.generator.eval()\n",
        "        #gen_length = len(self.source.story_peaks) + int(len(self.source.story_peaks)*self.frame_expansion_ratio)\n",
        "        noise = torch.randn(count,self.source.org_source_length)\n",
        "        bias = torch.zeros_like(noise)\n",
        "        if self.with_bias:\n",
        "            bias[:,noise.shape[1]-1] = 1\n",
        "        with torch.no_grad():\n",
        "            sw,sw0 = self.generator(noise,bias)\n",
        "            #sw,sw0 = self.generator(noise)\n",
        "\n",
        "        max_score = 0\n",
        "        max_sim = 0\n",
        "        comp_rate = 0\n",
        "        best_text = \"\"\n",
        "\n",
        "        for p_txt in sw:\n",
        "            gtext = self.__text_gen3(p_txt)\n",
        "            text = ' '.join(gtext)\n",
        "            \n",
        "            #print('>>',text)\n",
        "            sim_score = self.s_discriminator.similarity(text,self.source.full_text_emb)\n",
        "            if sim_score > max_sim:\n",
        "                best_text = text.strip()\n",
        "                loss, out=self.g_discriminator.transfer_learning([text],train_for = False)\n",
        "                max_score = out[0,1].item()\n",
        "                comp_rate = 1 - len(best_text)/len(self.source.org_text)\n",
        "                max_sim = sim_score\n",
        "            #texts.append([text.strip(),out,sim_score])\n",
        "        return best_text, max_score, max_sim, comp_rate\n",
        "    '''\n",
        "    def get_samples(self,count):\n",
        "        self.generator.cpu()\n",
        "        self.generator.eval()\n",
        "        noise = torch.randn(count,self.source.org_source_length)\n",
        "        bias = torch.zeros_like(noise)\n",
        "        #if self.with_bias:\n",
        "        #    bias[:,noise.shape[1]-1] = 1\n",
        "        with torch.no_grad():\n",
        "            sw,sw0 = self.generator(noise,bias)\n",
        "        #samples = []\n",
        "        best_p_txt = None\n",
        "        best_text = \"\"\n",
        "        best_grammar_score = 0\n",
        "        max_score = 0\n",
        "        second_best_text = \"\"\n",
        "        second_max_score = 0        \n",
        "        hash_list = []\n",
        "        for p_txt in sw:\n",
        "            gtext = self.__text_gen3(p_txt)\n",
        "            h = self.__text_hash(p_txt)\n",
        "            if h in hash_list:\n",
        "                pass\n",
        "            else:\n",
        "                hash_list.append(h)\n",
        "                text = (' '.join(gtext).strip())\n",
        "                loss, out=self.g_discriminator.transfer_learning([text],train_for = False)\n",
        "                #sim_score = self.s_discriminator.similarity(text,self.source.org_text_emb)\n",
        "\n",
        "                sim_score = cosine_similarity(self.source.org_text,text)    \n",
        "                comp_rate = 1 - len(text)/len(self.source.org_text)\n",
        "\n",
        "                #samples.append((text,out[0,1].item(),sim_score,comp_rate))\n",
        "                #score = out[0,1].item() + sim_score + comp_rate*2\n",
        "                score = out[0,1].item()/6 + sim_score * 2 + comp_rate\n",
        "                if logout:\n",
        "                    print('{:.4f},{:.4f},score:{:.4f},[{}]'.format(sim_score,comp_rate,score,text))\n",
        "                if max_score < score and (comp_rate > 0.4 and comp_rate < 0.6 ):\n",
        "                    best_p_txt = p_txt\n",
        "                    max_score = score\n",
        "                    best_text = text\n",
        "                    best_grammar_score = out[0,1].item()\n",
        "                if max_score < score:\n",
        "                    second_max_score = score\n",
        "                    second_best_text = text\n",
        "                    best_p_txt = p_txt\n",
        "                    best_grammar_score = out[0,1].item()\n",
        "            if max_score == 0:\n",
        "                max_score = second_max_score\n",
        "                best_text = second_best_text                           \n",
        "        #return [best_text for i in range(count)], max_score\n",
        "        \n",
        "        correct_best_text = sentence_correct(' '.join(self.__text_gen5(best_p_txt))) #' '.join(self.__text_gen5(best_p_txt))\n",
        "        loss, out=self.g_discriminator.transfer_learning([best_text],train_for = False)\n",
        "        best_grammar_score = out[0,1].item()\n",
        "        loss, out=self.g_discriminator.transfer_learning([correct_best_text],train_for = False)\n",
        "        correct_best_grammar_score = out[0,1].item()\n",
        "        if best_grammar_score < 5.0 and correct_best_grammar_score > best_grammar_score:\n",
        "            if logout:\n",
        "                print('correct_grammar_score:{:.4f} best_grammar_score:{:.4f}'.format(correct_best_grammar_score,best_grammar_score))\n",
        "                print(best_text)\n",
        "                print(correct_best_text)\n",
        "            best_text = correct_best_text\n",
        "            best_grammar_score = correct_best_grammar_score\n",
        "        \n",
        "        return best_text, max_score, best_grammar_score\n"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCdfO9iuLH6D"
      },
      "source": [
        "#5. Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YWDU3PbMo1T",
        "outputId": "8f589861-08db-4207-ec34-8183fd4a338b"
      },
      "source": [
        "txt = \"\"\"\n",
        "The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo Grand Forks and Jamestown to the hepatitis A virus in late September and early October.\n",
        "\"\"\"\n",
        "\n",
        "source = Source(get_prepared_doc(sentences_dataset[0]),txt)\n",
        "source.set_key_rate(s_discriminator)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo Grand Forks and Jamestown to the hepatitis A virus in late September and early October.\n",
            "--------------------------------------------------\n",
            "Length ------------------------------------| 34\n",
            "{0: -0.21044379472732544, 1: -0.07805800437927246, 2: -0.25383129715919495, 3: -0.21044379472732544, 4: -0.32429417222738266, 5: -0.08940473198890686, 6: -0.32979757338762283, 7: -0.28030624985694885, 8: -0.43506007455289364, 9: -0.4030472859740257, 10: -0.3340560272336006, 11: -0.1670476496219635, 12: -0.33512237668037415, 13: -0.304433673620224, 14: -0.25383129715919495, 15: -0.06337669491767883, 16: -0.24237361550331116, 17: -0.28030624985694885, 18: -0.32429417222738266, 19: -0.412482425570488, 20: -0.37476950883865356, 21: -0.2882152497768402, 22: -0.15138861536979675, 23: -0.18835493922233582, 24: -0.21044379472732544, 25: 0.0, 26: -0.3476515933871269, 27: -0.2077009379863739, 28: -0.28030624985694885, 29: -0.34136105328798294, 30: -0.32061411440372467, 31: -0.2882152497768402, 32: -0.38496625423431396, 33: -0.35287800431251526}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_eAwIPLb4aj"
      },
      "source": [
        "## 비교 대상 요약 알고리즘 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhcoXuPMGy09"
      },
      "source": [
        "def sam_wgan4(full_text,text, epochs=50, batch_size=100,display=False, retry = True, retry_count = 0,comp_rate=1.0):\n",
        "    if retry_count > 10:\n",
        "        raise Exception(\"Can't summarize the text\")\n",
        "\n",
        "    source = Source(full_text,text,delete_ending = False,attendtion_rate=atten_rate)\n",
        "    source.set_key_rate(s_discriminator)\n",
        "    summarizer = SAM_Summarizer(g_discriminator,s_discriminator)\n",
        "    summarizer.ready(source)\n",
        "    hist = summarizer.summarize(epochs,batch_size=2,learning_rate=5e-3,display=display,comp_rate=comp_rate)\n",
        "    if retry and hist == None and retry_count < 10:\n",
        "        print('\\n')\n",
        "        return sam_wgan4(full_text,text, epochs+10, batch_size,display=display,retry_count=retry_count+1)\n",
        "    samples, max_score, best_grammar_score = summarizer.get_samples(batch_size)\n",
        "    #print(samples)\n",
        "    \n",
        "    if retry and best_grammar_score < (1.0 - retry_count*0.1):\n",
        "        if logout:\n",
        "            print('max score:{} grammar:{} text:{}'.format(max_score,best_grammar_score,samples))\n",
        "        return sam_wgan4(full_text,text, epochs+10, batch_size,display=display,retry_count=retry_count+1)\n",
        "    \n",
        "    return samples, max_score, best_grammar_score"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "-pJUPNgwmSKB",
        "outputId": "ff1833d3-e571-42ac-d09d-a7903d2e122e"
      },
      "source": [
        "sentences_dataset[0]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"By . Associated Press . PUBLISHED: . 14:11 EST, 25 October 2013 . | . UPDATED: . 15:36 EST, 25 October 2013 . The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A virus in late September and early October. The state Health Department has issued an advisory of exposure for anyone who attended five churches and took communion. Bishop John Folda (pictured) of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A . State Immunization Program Manager Molly Howell says the risk is low, but officials feel it's important to alert people to the possible exposure. The diocese announced on Monday that Bishop John Folda is taking time off after being diagnosed with hepatitis A. The diocese says he contracted the infection through contaminated food while attending a conference for newly ordained bishops in Italy last month. Symptoms of hepatitis A include fever, tiredness, loss of appetite, nausea and abdominal discomfort. Fargo Catholic Diocese in North Dakota (pictured) is where the bishop is located .\""
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6v9sEryZOLa"
      },
      "source": [
        "# Sentence Corrector (EncoderDecoderModel)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUe3ZCSIz8N8"
      },
      "source": [
        "from transformers import EncoderDecoderModel, BertTokenizerFast\n",
        "import torch\n",
        "\n",
        "pre_trained_kobert_model_name='bert-base-uncased'\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained(pre_trained_kobert_model_name)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euuB9E5uZ1j2"
      },
      "source": [
        "from transformers import EncoderDecoderModel, BertTokenizerFast\n",
        "try:\n",
        "    del model\n",
        "    print('delete model')\n",
        "except Exception as ex:\n",
        "    pass\n",
        "model = EncoderDecoderModel.from_pretrained(\"/content/drive/MyDrive/GAN_ENDE/en_sentence_complete_model\")"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr_6N_CYaKAI"
      },
      "source": [
        "def sentence_correct(text):\n",
        "    text = text.strip().lower()\n",
        "    text = text.replace('!','')\n",
        "    w = text.split(' ')\n",
        "    last_token = w[-1]\n",
        "    if last_token.endswith(('.')):\n",
        "        last_token = w[-1][:-1]\n",
        "\n",
        "    last_character = w[len(w)-1][:-1]\n",
        "    inputs = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "    input_ids = inputs.input_ids.to(device)\n",
        "    attention_mask = inputs.attention_mask.to(device)\n",
        "    '''\n",
        "    v = torch.sum(attention_mask[0]).item()\n",
        "    c = random.sample([i for i in range(v)],int(v/2))\n",
        "    print(c)\n",
        "    #input_ids[0][c] = 0\n",
        "    attention_mask[0][c] = 0 #random.random()\n",
        "    attention_mask[0][0] = 1\n",
        "    attention_mask[0][v-1] = 1\n",
        "    \n",
        "    print(input_ids)    \n",
        "    print(attention_mask)\n",
        "    '''\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    outputs = model.generate(input_ids, attention_mask=attention_mask).cpu().detach().numpy()[0]\n",
        "    o=[]\n",
        "    for token in outputs:\n",
        "        if token == tokenizer.pad_token_id:\n",
        "            break\n",
        "        o.append(token)\n",
        "    output_str = tokenizer.batch_decode([o], skip_special_tokens=True)[0]\n",
        "    output_str = output_str.replace(' ’ ',\"'\") #' - '\n",
        "    output_str = output_str.replace(' - ',\"-\") #' - '\n",
        "    if logout:\n",
        "        print('raw:',output_str)\n",
        "    \n",
        "    eos = output_str.find('.')\n",
        "    real_eos =  eos\n",
        "    if last_character.endswith('다'):\n",
        "        eos2 = output_str.find(last_character) \n",
        "        if eos2 > 0 and eos2 < eos:\n",
        "            real_eos = eos2 + len(last_character)\n",
        "    # 끝에 token matching을 위해, 적어도 3글자 이상의 token에 대하여...\n",
        "    elif len(last_token) > 3:\n",
        "        eos2 = output_str.find(last_token) \n",
        "        if eos2 > 0 and eos2 < eos:\n",
        "            real_eos = eos2 + len(last_token) \n",
        "            tmp = output_str[0:real_eos] + '.'\n",
        "            # 적어도 5어절 이상은 되어야 인정해 준다.\n",
        "            if len(tmp.split(' ')) > 5:\n",
        "                output_str = tmp\n",
        "\n",
        "    if output_str.endswith('.'):\n",
        "        pass\n",
        "    else:\n",
        "        output_str += '.'\n",
        "    return output_str"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "GkdOT4m6Jlx9",
        "outputId": "6acbdaeb-b133-4ab1-d0e7-4000842ae2d5"
      },
      "source": [
        "txt = 'judge agreed with police that would have been over limit time Citroen Miss Titley’s blue Daihatsu Cuore road near Yarmouth Isle Wight October 2013 phone records showed also texting around time crash.'\n",
        "txt = 'internal Miami-Dade Police Department investigates Mata complaint airport transport Dominican.'\n",
        "sentence_correct(txt)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "raw: an internal miami-dade police department investigates mata complaint about airport transport in dominican, dominicans, and dominicans '.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'an internal miami-dade police department investigates mata complaint about airport transport in dominican.'"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_6X1JsSJLqi",
        "outputId": "0e8d3802-4751-4dc8-af0d-19f78c4a00ff"
      },
      "source": [
        "!pip3 install bert-extractive-summarizer"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bert-extractive-summarizer in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from bert-extractive-summarizer) (0.22.2.post1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from bert-extractive-summarizer) (2.2.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (from bert-extractive-summarizer) (4.10.3)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (2.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (3.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (4.62.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (0.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (57.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->bert-extractive-summarizer) (4.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy->bert-extractive-summarizer) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy->bert-extractive-summarizer) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (1.24.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (0.0.17)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (0.10.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (21.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (5.4.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (0.0.45)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers->bert-extractive-summarizer) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->bert-extractive-summarizer) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->bert-extractive-summarizer) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "porcPq1WJXF3"
      },
      "source": [
        "def besm(full_text,text,top_rank=2):\n",
        "\n",
        "    queries = nltk.sent_tokenize(text)\n",
        "    src_sentences = nltk.sent_tokenize(full_text)\n",
        "    query_embeddings = s_discriminator._embedder.encode(queries,show_progress_bar=False)\n",
        "    full_text_embeddings = s_discriminator._embedder.encode(src_sentences,show_progress_bar=False)\n",
        "    #print(queries)\n",
        "    #print(org_text_emb)\n",
        "    \n",
        "    if len(query_embeddings) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    cos_scores = scipy.spatial.distance.cdist(query_embeddings, full_text_embeddings, \"cosine\")\n",
        "    scores = np.max(cos_scores,axis=1)\n",
        "    orderd = [(o,s) for o,s in enumerate(scores)]\n",
        "    orderd.sort(key=lambda e: e[1],reverse=True)\n",
        "    a = [orderd[i][0] for i in range(0,top_rank)]\n",
        "    a.sort()\n",
        "    summ_text = \" \".join([queries[i] for i in a])\n",
        "\n",
        "    return summ_text\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TN3dYRJ84j3Y"
      },
      "source": [
        "def besm2(full_text,text,top_rank=2):\n",
        "    scores = []\n",
        "    queries = nltk.sent_tokenize(text)\n",
        "    for sen in queries:\n",
        "        s = cosine_similarity(sen,full_text)\n",
        "        scores.append(s)\n",
        "        #print(s,sen)\n",
        "    orderd = [(o,s) for o,s in enumerate(scores)]\n",
        "    orderd.sort(key=lambda e: e[1],reverse=True)\n",
        "    a = [orderd[i][0] for i in range(0,top_rank)]\n",
        "    a.sort()\n",
        "    summ_text = \" \".join([queries[i] for i in a])\n",
        "\n",
        "    return summ_text\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClV7dqUAFmit",
        "outputId": "ee96c1f6-cd23-4275-de5b-877906b4435c"
      },
      "source": [
        "from summarizer import Summarizer\n",
        "\n",
        "\n",
        "model1 = Summarizer()\n",
        "\n",
        "def besm(full_text,num_sentences=3):\n",
        "    result = model1(full_text, num_sentences=num_sentences)\n",
        "    return \"\".join(result)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LKIH0nG31yx"
      },
      "source": [
        "\n",
        "def similarity3(full_text,org_text):\n",
        "    sentences = nltk.sent_tokenize(full_text)\n",
        "    #print(\"Num sentences:\", len(sentences))\n",
        "    querys = nltk.sent_tokenize(org_text)\n",
        "    #print(\"Num querys:\", len(querys))\n",
        "\n",
        "    #Compute the sentence embeddings\n",
        "    org_embeddings = s_discriminator._embedder.encode(sentences,show_progress_bar=False)\n",
        "    query_embeddings = s_discriminator._embedder.encode(querys,show_progress_bar=False)\n",
        "\n",
        "    #Compute the pair-wise cosine similarities\n",
        "    cos_scores = scipy.spatial.distance.cdist(query_embeddings, org_embeddings, \"cosine\")\n",
        "    similarity_score = 1.0 - np.mean(np.min(cos_scores,axis=0))\n",
        "\n",
        "    return similarity_score"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "uMBrB8mWF2TG",
        "outputId": "94540bab-5a7e-4ee2-94a9-df5f292d0445"
      },
      "source": [
        "full_text = get_prepared_doc(sentences_dataset[0])\n",
        "besm(full_text)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A virus in late September and early October. The state Health Department has issued an advisory of exposure for anyone who attended five churches and took communion. Bishop John Folda (pictured) of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A. State Immunization Program Manager Molly Howell says the risk is low, but officials feel it's important to alert people to the possible exposure. Symptoms of hepatitis A include fever, tiredness, loss of appetite, nausea and abdominal discomfort.\""
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NGqfC0qL1JU"
      },
      "source": [
        "## ExtactiveSummarizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-NxbjyjT9k-"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import re\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import BertTokenizer, BertModel"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpwkLTqaUApn"
      },
      "source": [
        "class ExtactiveSummarizer:\n",
        "    # 한국어의 경우, 'kykim/bert-kor-base'\n",
        "    def __init__(self,model_name='bert-base-uncased'):\n",
        "\n",
        "        #nltk.download('stopwords')\n",
        "        nltk.download('punkt')\n",
        "\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "        self.model = BertModel.from_pretrained(model_name, return_dict=True, output_attentions=True)\n",
        "        self.cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "        # If there's a GPU available...\n",
        "        if torch.cuda.is_available():    \n",
        "            # Tell PyTorch to use the GPU.    \n",
        "            self.device = torch.device(\"cuda\")\n",
        "        # If not...\n",
        "        else:\n",
        "            self.device = torch.device(\"cpu\")\n",
        "\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def read_article(self,text):        \n",
        "        sentences =[]        \n",
        "        sentences = sent_tokenize(text)    \n",
        "        for sentence in sentences:        \n",
        "            sentence.replace(\"[^a-zA-Z0-9]\",\" \")     \n",
        "        return sentences\n",
        "\n",
        "    def sentence_similarity(self,sent1,sent2):\n",
        "        tok_sent1 = self.tokenizer(sent1, return_tensors=\"pt\")\n",
        "        tok_sent2 = self.tokenizer(sent2, return_tensors=\"pt\")\n",
        "        tok_sent1.to(self.device)\n",
        "        tok_sent2.to(self.device)\n",
        "        outputs = self.model(**tok_sent1)\n",
        "        sent_1_pooler_output = outputs.pooler_output\n",
        "\n",
        "        outputs = self.model(**tok_sent2)\n",
        "        sent_2_pooler_output = outputs.pooler_output\n",
        "        return self.cos(sent_1_pooler_output, sent_2_pooler_output).cpu().detach().numpy()\n",
        "\n",
        "    def get_self_attention_weight(self,sentence):\n",
        "        tok_sent = self.tokenizer(sentence, return_tensors=\"pt\")\n",
        "        tok_sent.to(self.device)\n",
        "        outputs = self.model(**tok_sent)\n",
        "        \n",
        "        attentions = torch.stack(outputs.attentions)\n",
        "        #attention = outputs[-1]  # Output includes attention weights when output_attentions=True\n",
        "        last_attentions = attentions[11][0][11]\n",
        "        #print(last_attentions.shape)\n",
        "        tokens = [self.tokenizer.convert_ids_to_tokens(s) for s in tok_sent['input_ids'].tolist()[0]]\n",
        "        #print(tokens)\n",
        "        attention_map = []\n",
        "        for i,token in enumerate(tokens):\n",
        "            attention_map.append((i,token,torch.sum(last_attentions[:,i]).item()))\n",
        "        return attention_map\n",
        "\n",
        "    # Create similarity matrix among all sentences\n",
        "    def build_similarity_matrix(self,sentences):\n",
        "        #create an empty similarity matrix\n",
        "        similarity_matrix = np.zeros((len(sentences),len(sentences)))\n",
        "        \n",
        "        for idx1 in range(len(sentences)):\n",
        "            for idx2 in range(len(sentences)):\n",
        "                if idx1!=idx2:\n",
        "                    similarity_matrix[idx1][idx2] = self.sentence_similarity(sentences[idx1],sentences[idx2])\n",
        "                    \n",
        "        return similarity_matrix\n",
        "\n",
        "    # Generate and return text summary\n",
        "    def generate_summary(self,text,top_n):\n",
        "        \n",
        "        #stop_words = stopwords.words('english')\n",
        "        summarize_text = []\n",
        "        \n",
        "        # Step1: read text and tokenize\n",
        "        sentences = self.read_article(text)\n",
        "        \n",
        "        # Steo2: generate similarity matrix across sentences\n",
        "        sentence_similarity_matrix = self.build_similarity_matrix(sentences)\n",
        "        \n",
        "        # Step3: Rank sentences in similarirty matrix\n",
        "        sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_matrix)\n",
        "        scores = nx.pagerank(sentence_similarity_graph)\n",
        "        \n",
        "        #Step4: sort the rank and place top sentences\n",
        "        ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)),reverse=True)\n",
        "        \n",
        "        #print(ranked_sentences)\n",
        "        # Step 5: get the top n number of sentences based on rank    \n",
        "        for i in range(top_n if top_n < len(ranked_sentences) else len(ranked_sentences)):\n",
        "            summarize_text.append(ranked_sentences[i][1])\n",
        "        \n",
        "        # Step 6 : outpur the summarized version\n",
        "        return \" \".join(summarize_text),len(sentences)   "
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVsoAq3hz3mr",
        "outputId": "c8307075-2726-44a6-d694-27d1059799aa"
      },
      "source": [
        "es = ExtactiveSummarizer()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVWEePCD0VTr",
        "outputId": "da7a6ff2-3c63-430c-cf89-f0d7c30fc63a"
      },
      "source": [
        "es.sentence_similarity('This IS expected if you are BertModel from the checkpoint','This IS expected if you are initializing BertModel from the checkpoint')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.9934667], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qG4aL4oRz8mu",
        "outputId": "631ad067-8e51-4dca-aedb-86be2eadcea5"
      },
      "source": [
        "es.get_self_attention_weight('The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo Grand Forks and Jamestown to the hepatitis A virus in late September and early October.')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, '[CLS]', 0.18557289242744446),\n",
              " (1, 'the', 0.07899213582277298),\n",
              " (2, 'bishop', 0.3666439354419708),\n",
              " (3, 'of', 0.05223364382982254),\n",
              " (4, 'the', 0.10034431517124176),\n",
              " (5, 'fargo', 0.0550147145986557),\n",
              " (6, 'catholic', 0.3552972078323364),\n",
              " (7, 'diocese', 0.11490436643362045),\n",
              " (8, 'in', 0.12254498898983002),\n",
              " (9, 'north', 0.009641865268349648),\n",
              " (10, 'dakota', 0.04165465384721756),\n",
              " (11, 'has', 0.11064591258764267),\n",
              " (12, 'exposed', 0.2776542901992798),\n",
              " (13, 'potentially', 0.10957956314086914),\n",
              " (14, 'hundreds', 0.1402682662010193),\n",
              " (15, 'of', 0.19087064266204834),\n",
              " (16, 'church', 0.38132524490356445),\n",
              " (17, 'members', 0.20232832431793213),\n",
              " (18, 'in', 0.20666250586509705),\n",
              " (19, 'fargo', 0.12040776759386063),\n",
              " (20, 'grand', 0.03221951425075531),\n",
              " (21, 'forks', 0.06993243098258972),\n",
              " (22, 'and', 0.13787443935871124),\n",
              " (23, 'jamestown', 0.29331332445144653),\n",
              " (24, 'to', 0.25634700059890747),\n",
              " (25, 'the', 0.23425814509391785),\n",
              " (26, 'hepatitis', 0.4447019398212433),\n",
              " (27, 'a', 0.09705034643411636),\n",
              " (28, 'virus', 0.23700100183486938),\n",
              " (29, 'in', 0.16439568996429443),\n",
              " (30, 'late', 0.10334088653326035),\n",
              " (31, 'september', 0.12408782541751862),\n",
              " (32, 'and', 0.15648669004440308),\n",
              " (33, 'early', 0.05973568558692932),\n",
              " (34, 'october', 0.09182393550872803),\n",
              " (35, '.', 17.814111709594727),\n",
              " (36, '[SEP]', 13.460731506347656)]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VadbdJnzL8Lq"
      },
      "source": [
        "## 실험 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFJ5v8fia6VD"
      },
      "source": [
        "def summary(ft,text,steps=4,top_rank=2,comp_rate=1.0):\n",
        "    org_sentences = np.array(nltk.sent_tokenize(text.strip()))\n",
        "    summary_text = []\n",
        "    g = []\n",
        "    sm = []\n",
        "    for i in range(0,len(org_sentences),steps):\n",
        "        txt = ''\n",
        "        cnt = 0\n",
        "        for s in range(i,i+steps):\n",
        "            if s < len(org_sentences):\n",
        "                txt +=  ' ' + org_sentences[s]\n",
        "                cnt +=1\n",
        "        #print(cnt,top_rank)\n",
        "        txt = txt.strip()\n",
        "        if cnt > top_rank:\n",
        "            txt = besm2(ft,txt,top_rank=top_rank)\n",
        "\n",
        "        t,score, grammar = sam_wgan4(ft,txt.strip(),epochs=300,display=logout,comp_rate=comp_rate)\n",
        "        if logout:\n",
        "            print('-'*50)\n",
        "            print(t,score,grammar)\n",
        "        #t = sentence_correct(t)\n",
        "        #print(t)\n",
        "        summary_text.append(t)\n",
        "        g.append(grammar)\n",
        "        sm.append(similarity3(ft,t))\n",
        "\n",
        "    return ' '.join(summary_text).strip(),np.tanh(np.mean(g)),np.mean(sm)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irjx-TOnPBL4"
      },
      "source": [
        "## Main 실험"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FstAHWGQ8KR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 995
        },
        "outputId": "7c0fec39-b614-4082-c163-dea059728e11"
      },
      "source": [
        "logout = True\n",
        "atten_rate = 0.2\n",
        "similarity = 1.0\n",
        "std_factor = 3.0\n",
        "txt = \"\"\"\n",
        "The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo Grand Forks and Jamestown to the hepatitis A virus in late September and early October. The state Health Department has issued an advisory of exposure for anyone who attended five churches and took communion. Bishop John Folda (pictured) of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo Grand Forks and Jamestown to the hepatitis A.\n",
        "\"\"\"\n",
        "#The diocese says he contracted the infection through contaminated food while attending a conference for newly ordained bishops in Italy last month. Symptoms of hepatitis A include fever, tiredness, loss of appetite, nausea and abdominal discomfort. Fargo Catholic Diocese in North Dakota (pictured) is where the bishop is located.\n",
        "#The judge agreed with police that he would have been over the limit at the time his red Citroen hit Miss Titley’s blue Daihatsu Cuore on a road near Yarmouth, Isle of Wight, on October 11, 2013. His phone records showed he was also texting around the time of the crash.\n",
        "\n",
        "sam_wgan4(get_prepared_doc(sentences_dataset[0]),txt,epochs=300,display= True,retry = False,comp_rate= 1.5)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo Grand Forks and Jamestown to the hepatitis A virus in late September and early October. The state Health Department has issued an advisory of exposure for anyone who attended five churches and took communion. Bishop John Folda (pictured) of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo Grand Forks and Jamestown to the hepatitis A.\n",
            "--------------------------------------------------\n",
            "Length ------------------------------------| 84\n",
            "{0: -1.0, 1: 0.0, 2: -1.0, 3: -1.0, 4: -1.0, 5: -1.0, 6: -1.0, 7: 0.0, 8: -1.0, 9: -1.0, 10: -1.0, 11: 0.0, 12: -1.0, 13: -1.0, 14: -1.0, 15: -1.0, 16: -1.0, 17: 0.0, 18: -1.0, 19: -1.0, 20: -1.0, 21: -1.0, 22: 0.0, 23: 0.0, 24: -1.0, 25: 0.0, 26: -1.0, 27: 0.0, 28: 0.0, 29: -1.0, 30: -1.0, 31: -1.0, 32: -1.0, 33: -1.0, 34: -1.0, 35: -1.0, 36: -1.0, 37: 0.0, 38: -1.0, 39: -1.0, 40: -1.0, 41: -1.0, 42: -1.0, 43: -1.0, 44: -1.0, 45: -1.0, 46: -1.0, 47: -1.0, 48: 0.0, 49: -1.0, 50: 0.0, 51: -1.0, 52: -1.0, 53: 0.0, 54: -1.0, 55: 0.0, 56: -1.0, 57: -1.0, 58: -1.0, 59: -1.0, 60: -1.0, 61: -1.0, 62: -1.0, 63: -1.0, 64: 0.0, 65: -1.0, 66: -1.0, 67: -1.0, 68: 0.0, 69: -1.0, 70: -1.0, 71: -1.0, 72: -1.0, 73: -1.0, 74: 0.0, 75: -1.0, 76: -1.0, 77: -1.0, 78: -1.0, 79: 0.0, 80: 0.0, 81: -1.0, 82: 0.0, 83: 0.0}\n",
            "Train... |||||||||||||||||||||| 100.0%   300/300 epochs, e 0.00105 gl:-0.27735081  sl:-0.1983 ll:-0.2752\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7016,0.4545,score:2.0608,[bishop in North has exposed hundreds in Grand and Jamestown to hepatitis virus in late and October Health has advisory of attended churches communion and Bishop John Folda of Catholic Diocese in Dakota has exposed hundreds church members in Forks and Jamestown to hepatitis A.]\n",
            "raw: a bishop in north dakota has exposed hundreds of priests in grand rapids and williamsburg to the hepatitis virus in late august and october health officials have issued an advisory list of priests attended by churches like communion and bishop john folda of the catholic diocese of north dakota which has exposed nearly hundreds of church members in forks and detention in williamsburg to hepatitis a.\n",
            "correct_grammar_score:5.7929 best_grammar_score:1.2191\n",
            "bishop in North has exposed hundreds in Grand and Jamestown to hepatitis virus in late and October Health has advisory of attended churches communion and Bishop John Folda of Catholic Diocese in Dakota has exposed hundreds church members in Forks and Jamestown to hepatitis A.\n",
            "a bishop in north dakota has exposed hundreds of priests in grand rapids and williamsburg to the hepatitis virus in late august and october health officials have issued an advisory list of priests attended by churches like communion and bishop john folda of the catholic diocese of north dakota which has exposed nearly hundreds of church members in forks and detention in williamsburg to hepatitis a.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('a bishop in north dakota has exposed hundreds of priests in grand rapids and williamsburg to the hepatitis virus in late august and october health officials have issued an advisory list of priests attended by churches like communion and bishop john folda of the catholic diocese of north dakota which has exposed nearly hundreds of church members in forks and detention in williamsburg to hepatitis a.',\n",
              " 2.0608434643054316,\n",
              " 5.792886734008789)"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "di_qtu_05Zcf"
      },
      "source": [
        "# 측정 도구..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFgYYBgh5b-P",
        "outputId": "c2c15eb3-ac09-4786-e918-6d027539f68d"
      },
      "source": [
        "!pip install rouge-score"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.7/dist-packages (0.0.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.15.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge-score) (3.2.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge-score) (0.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqwC0uZrFAVo"
      },
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeL'], use_stemmer=True)"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_AzsvMX6KEq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09c99110-227a-4e96-ec29-27d4e502b576"
      },
      "source": [
        "import io\n",
        "R1 = []\n",
        "R2 = []\n",
        "RL = []\n",
        "Similarity = []\n",
        "Grammar = []\n",
        "\n",
        "result_data = {}\n",
        "result_data['comp_rate'] = []\n",
        "result_data['R1'] = []\n",
        "result_data['R2'] = []\n",
        "result_data['RL'] = []\n",
        "result_data['Similarity'] = []\n",
        "result_data['Grammar'] = []\n",
        "\n",
        "atten_rate = 0.2\n",
        "similarity = 1.0\n",
        "std_factor = 3.0\n",
        "\n",
        "es = ExtactiveSummarizer()\n",
        "\n",
        "logout = False\n",
        "for rate in range(0,1):\n",
        "    comp_rate=1.5\n",
        "    result_data['comp_rate'].append(comp_rate)\n",
        "    for try_count in range(100):\n",
        "        \n",
        "        full_text = get_prepared_doc(sentences_dataset[try_count])\n",
        "        '''\n",
        "        try:\n",
        "            del model\n",
        "            print('delete model')\n",
        "        except Exception as ex:\n",
        "            pass\n",
        "        model = EncoderDecoderModel.from_pretrained(\"/content/drive/MyDrive/GAN_ENDE/en_sentence_complete_model\")\n",
        "        '''\n",
        "        #try:\n",
        "\n",
        "        #org_text = besm(full_text,num_sentences=9)\n",
        "        org_text = es.generate_summary(full_text,top_n=9)[0]\n",
        "        \n",
        "        org_sentences = np.array(nltk.sent_tokenize(org_text.strip()))\n",
        "        if logout:\n",
        "            print('BESM Summary sentance length:',len(org_sentences))\n",
        "        org_text,grammar,simil = summary(full_text,org_text,steps=3,top_rank=3,comp_rate=comp_rate)\n",
        "        \n",
        "        gsmry = gold_summary[try_count]\n",
        "        \n",
        "        print('='*50 + ' Original Document ' + str(try_count) + '='*50)\n",
        "        print(full_text)\n",
        "        print('-'*50 + ' Summary ' + str(try_count) + '-'*50)\n",
        "        for txt in np.array(nltk.sent_tokenize(org_text.strip())):\n",
        "            print(txt)\n",
        "        print('-'*50 + ' Ground truth' + '-'*50)\n",
        "        print(gsmry)\n",
        "        print('-'*120)\n",
        "\n",
        "        with io.open('/content/drive/MyDrive/GAN_ENDE/CNN_Daily_summary_result.txt','a',encoding='utf8') as f:\n",
        "            f.write(org_text + '\\r\\n\\r\\n')\n",
        "        smry = org_text\n",
        "        scores = scorer.score(gsmry,smry)\n",
        "        print(scores['rouge1'].fmeasure)\n",
        "        if scores['rouge1'].fmeasure > 0.1:\n",
        "            print('rouge1', scores['rouge1'].fmeasure)\n",
        "            print('rouge2', scores['rouge2'].fmeasure)\n",
        "            print('rougeL', scores['rougeL'].fmeasure)\n",
        "            print('Similarity',simil)\n",
        "            print('Grammar',grammar)\n",
        "\n",
        "            R1.append(scores['rouge1'].fmeasure)\n",
        "            R2.append(scores['rouge2'].fmeasure)\n",
        "            RL.append(scores['rougeL'].fmeasure)\n",
        "            Similarity.append(simil)\n",
        "            Grammar.append(grammar)\n",
        "            print()\n",
        "            print('Mean R1',np.mean(R1))\n",
        "            print('Mean R2',np.mean(R2))\n",
        "            print('Mean RL',np.mean(RL))\n",
        "            print('Similarity',np.mean(Similarity))\n",
        "            print('Grammar',np.mean(Grammar))\n",
        "        print()               \n",
        "        #except Exception as ex:\n",
        "        #    print(ex)\n",
        "\n",
        "    print()\n",
        "    print()\n",
        "    print()\n",
        "    print(\"=\"*50 + \"Compression rate:\" + str(comp_rate) + \"=\" * 50)\n",
        "    print('R1',np.mean(R1))\n",
        "    print('R2',np.mean(R2))\n",
        "    print('RL',np.mean(RL))\n",
        "    print('Similarity',np.mean(Similarity))\n",
        "    print('Grammar',np.mean(Grammar))    \n",
        "    print()\n",
        "    print()\n",
        "    print()\n",
        "    result_data['R1'].append(np.mean(R1))\n",
        "    result_data['R2'].append(np.mean(R2))\n",
        "    result_data['RL'].append(np.mean(RL))\n",
        "    result_data['Similarity'].append(np.mean(Similarity))\n",
        "    result_data['Grammar'].append(np.mean(Grammar))\n",
        "df_result_data = pd.DataFrame(result_data)\n",
        "df_result_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}