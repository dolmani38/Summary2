{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "korean_frame_token_0_1.0_gamma_10.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0c7a13599148468c8a07d3bc085210cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9956ad7af56c4add9533462fd832757f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8df25b5b726840cfa400b8a78a1d5b7a",
              "IPY_MODEL_820a1ebe60d644a5a20075433fa9046c"
            ]
          }
        },
        "9956ad7af56c4add9533462fd832757f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8df25b5b726840cfa400b8a78a1d5b7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_055aee5192b3423ab1215ebc874ca8d0",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 434,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 434,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3f844ba88420437ea5a4933c83193bd6"
          }
        },
        "820a1ebe60d644a5a20075433fa9046c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_29058fd2d4ec4f94b69f0469bd6edd3d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 434/434 [00:27&lt;00:00, 15.6B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e5b9d736be814592a1c5fd2d42a9c609"
          }
        },
        "055aee5192b3423ab1215ebc874ca8d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3f844ba88420437ea5a4933c83193bd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "29058fd2d4ec4f94b69f0469bd6edd3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e5b9d736be814592a1c5fd2d42a9c609": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9068e6c83e584024a44275b0fe832350": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_66b2998258ea4505b27378335b562aa7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d070c067d1fc4defa18f560be78a2b76",
              "IPY_MODEL_1cd2a9a1e7b244ec92d690347054184c"
            ]
          }
        },
        "66b2998258ea4505b27378335b562aa7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d070c067d1fc4defa18f560be78a2b76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3fda91dc4c25482db3eff80195047663",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1344997306,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1344997306,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2cb63e40757445149c3dccd2cb00652d"
          }
        },
        "1cd2a9a1e7b244ec92d690347054184c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_01f7226bda27447380de090a4eab7dfb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.34G/1.34G [00:22&lt;00:00, 59.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_38d97695f22043dfbd2843669153ab48"
          }
        },
        "3fda91dc4c25482db3eff80195047663": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2cb63e40757445149c3dccd2cb00652d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "01f7226bda27447380de090a4eab7dfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "38d97695f22043dfbd2843669153ab48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9e0c84afa8ab4d0a9f2ac49ce1eb4da9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_60782b1b9b02436ca4a6a55df17d1b57",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_37223d9428e94b6abd20f3ed76464584",
              "IPY_MODEL_a66c848be955443185fec1a10c962e78"
            ]
          }
        },
        "60782b1b9b02436ca4a6a55df17d1b57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "37223d9428e94b6abd20f3ed76464584": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e65098e6bea24e7b9421e01faa17e58d",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_83d4e28d7ee34108989be64bb135270b"
          }
        },
        "a66c848be955443185fec1a10c962e78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d0554a5520564a6b8ff006e18104f0f5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 798kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_660cb282966b475bba9690f9768d4e8b"
          }
        },
        "e65098e6bea24e7b9421e01faa17e58d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "83d4e28d7ee34108989be64bb135270b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d0554a5520564a6b8ff006e18104f0f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "660cb282966b475bba9690f9768d4e8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5a23562870cd447898b7fdfc5f56e29e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c48eb29505b54fd3a92a98d4021386b5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ca157a4cfe6f4acdbeac9bcc089968f1",
              "IPY_MODEL_505fbc1a1fa94d55ac24a2084ac5d270"
            ]
          }
        },
        "c48eb29505b54fd3a92a98d4021386b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ca157a4cfe6f4acdbeac9bcc089968f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c0449b7329e241f08e440fac1d31cd2e",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 426,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 426,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e6bc669ac021423aa9f342a3f3e1f14c"
          }
        },
        "505fbc1a1fa94d55ac24a2084ac5d270": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_da36499eb20b4c80b51044267bb8ef23",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 426/426 [00:00&lt;00:00, 460B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b70d62f11b4447cb9640526c04a04d8c"
          }
        },
        "c0449b7329e241f08e440fac1d31cd2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e6bc669ac021423aa9f342a3f3e1f14c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "da36499eb20b4c80b51044267bb8ef23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b70d62f11b4447cb9640526c04a04d8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cd0fbe93c5c94b00a20c2204ac40b04b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_382b240cdf784fc3a19fbb68ffd94f9d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_df97922dc1e0414a96afcabbafab0ec1",
              "IPY_MODEL_2afe6907b10e4d28914f28a11ce96610"
            ]
          }
        },
        "382b240cdf784fc3a19fbb68ffd94f9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "df97922dc1e0414a96afcabbafab0ec1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8eabe11ee69d49c6ae0d3ce767f79292",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 77779,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 77779,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b8e8ab587bd04a3b95b99de909773a7a"
          }
        },
        "2afe6907b10e4d28914f28a11ce96610": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d74b3fcdb038469c98294aaa52e28c75",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 77.8k/77.8k [00:01&lt;00:00, 48.6kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e8e588931da14f99b576d3a9186d552c"
          }
        },
        "8eabe11ee69d49c6ae0d3ce767f79292": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b8e8ab587bd04a3b95b99de909773a7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d74b3fcdb038469c98294aaa52e28c75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e8e588931da14f99b576d3a9186d552c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a23196285bdb4205b8642028fbf1e092": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e12c52f295284cf2a17b179051c6fa76",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6a4d973f70c044ee8651f46f03b77ea4",
              "IPY_MODEL_3e30ad42ad4a44d4b6e362f5eb9b7197"
            ]
          }
        },
        "e12c52f295284cf2a17b179051c6fa76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6a4d973f70c044ee8651f46f03b77ea4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_99288b64736a4c55a99d42929b9a9e92",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 51,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 51,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_72dedb2fae35415999123f27fa6dd7a8"
          }
        },
        "3e30ad42ad4a44d4b6e362f5eb9b7197": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9113b8b6e9d84b2f9486c4f0bdb81016",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 51.0/51.0 [00:00&lt;00:00, 150B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_68e32144b52144258fd258580772594c"
          }
        },
        "99288b64736a4c55a99d42929b9a9e92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "72dedb2fae35415999123f27fa6dd7a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9113b8b6e9d84b2f9486c4f0bdb81016": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "68e32144b52144258fd258580772594c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "abce3174c791495681215b3e1cea6956": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_49460bef094f4ac8acdb6633390d2bf0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2f799e10b4b542f7b24b67f341acf8a1",
              "IPY_MODEL_87d91565eb2c410982a81c1a30878c7c"
            ]
          }
        },
        "49460bef094f4ac8acdb6633390d2bf0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2f799e10b4b542f7b24b67f341acf8a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e352de75553f46668c47e170f3779148",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 368792146,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 368792146,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_17a2c32438454254b76593737cbb8b7d"
          }
        },
        "87d91565eb2c410982a81c1a30878c7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c0d8b1e6e9f44111ab0dacc00fcfad8b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 369M/369M [00:06&lt;00:00, 57.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4a1b357341944f62974fd918fedb0c71"
          }
        },
        "e352de75553f46668c47e170f3779148": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "17a2c32438454254b76593737cbb8b7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c0d8b1e6e9f44111ab0dacc00fcfad8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4a1b357341944f62974fd918fedb0c71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dolmani38/Summary2/blob/main/korean_frame_token_0_1.0_gamma_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkQCxNatSIOk"
      },
      "source": [
        "# A hybrid summarization methods using adaptive discriminant GAN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZjIW9VwyjDf"
      },
      "source": [
        "ABSTRACT\n",
        "\n",
        "Recently, a breakthrough has been made in the NLP field by the BERT and Transformer techniques. Machine summaries based on Transformers have almost reached the human level, but large datasets with high-quality human-generated summaries are indispensable. Preparing these datasets takes a lot of effort and time. For this reason, there is no dataset in non-mainstream languages. Also, the mechanical summarization has a problem of isthmus that does not include the overall contents of the original text. These summaries are not suitable for fairy tales or novels that should reflect the entire story. In this paper, we propose a hybrid summarization method that does not require a large amount of summary dataset and overcomes the isthmus problem through GAN training using two adaptive discriminators. We evaluate our model on the CNN/Daily Mail dataset. And the experimental results in Korean show that our model leads to isthmus improvements and higher similarity without paired dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K87VNBbeRLFF"
      },
      "source": [
        "#4. Implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQZeBAf8NxAR"
      },
      "source": [
        "## 4.1 기본 설정..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAdXzWGuKSBT",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49e04fbd-fd67-48ca-bf1a-1f2e0c3cbade"
      },
      "source": [
        "if True:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "newO0mBXKVnE",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2454fd19-46dd-492d-df78-6ae4d5d47499"
      },
      "source": [
        "#!pip install keybert\n",
        "!pip install sentence-transformers==0.3.0\n",
        "!pip install transformers==3.0.2\n",
        "\n",
        "#!pip install sentence-transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers==0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/23/833e0620753a36cb2f18e2e4a4f72fd8c49c123c3f07744b69f8a592e083/sentence-transformers-0.3.0.tar.gz (61kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.0MB/s \n",
            "\u001b[?25hCollecting transformers>=3.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/d5/f4157a376b8a79489a76ce6cfe147f4f3be1e029b7144fa7b8432e8acb26/transformers-4.4.2-py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.3.0) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.3.0) (1.8.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.3.0) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.3.0) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.3.0) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.3.0) (3.2.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 48.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 55.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (3.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.1->sentence-transformers==0.3.0) (3.7.4.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers==0.3.0) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers==0.3.0) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (2020.12.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.0.2->sentence-transformers==0.3.0) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers>=3.0.2->sentence-transformers==0.3.0) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers>=3.0.2->sentence-transformers==0.3.0) (3.4.1)\n",
            "Building wheels for collected packages: sentence-transformers, sacremoses\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.3.0-cp37-none-any.whl size=86754 sha256=5bd529ce71a617679b30139291b303395aec66aab7871dd25ac5664e5d37e367\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/23/85/85d6a9a6c68f0625a1ecdaad903bb0a78df058c10cf74f9de4\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=717baa30ed362f490a00e6e964e0a34aacb47e3c636afbb41f3887693cfa8535\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sentence-transformers sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers, sentence-transformers\n",
            "Successfully installed sacremoses-0.0.43 sentence-transformers-0.3.0 tokenizers-0.10.1 transformers-4.4.2\n",
            "Collecting transformers==3.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 7.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (4.41.1)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 15.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.0.43)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (1.19.5)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/59/68c7e3833f535615fb97d33ffcb7b30bbf62bc7477a9c59cd19ad8535d72/tokenizers-0.8.1rc1-cp37-cp37m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 36.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (3.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.2) (2.4.7)\n",
            "Installing collected packages: sentencepiece, tokenizers, transformers\n",
            "  Found existing installation: tokenizers 0.10.1\n",
            "    Uninstalling tokenizers-0.10.1:\n",
            "      Successfully uninstalled tokenizers-0.10.1\n",
            "  Found existing installation: transformers 4.4.2\n",
            "    Uninstalling transformers-4.4.2:\n",
            "      Successfully uninstalled transformers-4.4.2\n",
            "Successfully installed sentencepiece-0.1.95 tokenizers-0.8.1rc1 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmIxp0FnKXif",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b996561-5d79-4541-dfd0-69e65cb10f04"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# set seeds for reproducability\n",
        "from numpy.random import seed\n",
        "seed(1)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string, os \n",
        "\n",
        "import urllib.request\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1yL4NtUKaRn",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8289ad24-7fc2-4b4b-8f3c-39034beeaa48"
      },
      "source": [
        "import tensorflow as tf\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    print('GPU device not found')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3J0n_lhKcgm",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77c51f51-1c64-4e44-8b14-0fe7f35e61ab"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MJy2UYyLAoO",
        "trusted": true
      },
      "source": [
        "import random\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ue_4ZfdRKfdX",
        "trusted": true
      },
      "source": [
        "# Print iterations progress\n",
        "class ProgressBar:\n",
        "\n",
        "    def __init__(self,total=20, prefix = '', suffix = '', decimals = 1, length = 20, fill = '|', printEnd = \"\\r\"):\n",
        "        self.total = total\n",
        "        self.prefix = prefix\n",
        "        self.suffix = suffix\n",
        "        self.decimals = decimals\n",
        "        self.length = length\n",
        "        self.fill = fill\n",
        "        self.printEnd = printEnd\n",
        "        self.ite = 0\n",
        "        self.back_filledLength = 0\n",
        "\n",
        "    def printProgress(self,iteration, text):\n",
        "        self.ite += iteration\n",
        "        percent = (\"{0:.\" + str(self.decimals) + \"f}\").format(100 * (self.ite / float(self.total)))\n",
        "        filledLength = int(self.length * self.ite // self.total)\n",
        "        bar = self.fill * filledLength + '.' * (self.length - filledLength)\n",
        "        if filledLength > self.back_filledLength or percent == 100:\n",
        "            print(f'\\r{self.prefix} |{bar}| {percent}% {self.suffix}  {text}', end=\"\", flush=True)\n",
        "            # Print New Line on Complete\n",
        "            if self.ite == self.total: \n",
        "                print()\n",
        "        self.back_filledLength = filledLength    "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNHI0G6JKc5h",
        "trusted": true
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Zsv-LVkKmfL"
      },
      "source": [
        "##4.2 Grammar Discriminator Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "MGVvJrn3lEuO"
      },
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team and Jangwon Park\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\" Tokenization classes for KoBert model.\"\"\"\n",
        "\n",
        "\n",
        "import logging\n",
        "import os\n",
        "import unicodedata\n",
        "from shutil import copyfile\n",
        "\n",
        "from transformers import PreTrainedTokenizer\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "VOCAB_FILES_NAMES = {\"vocab_file\": \"tokenizer_78b3253a26.model\",\n",
        "                     \"vocab_txt\": \"vocab.txt\"}\n",
        "\n",
        "PRETRAINED_VOCAB_FILES_MAP = {\n",
        "    \"vocab_file\": {\n",
        "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/tokenizer_78b3253a26.model\",\n",
        "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/tokenizer_78b3253a26.model\",\n",
        "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/tokenizer_78b3253a26.model\"\n",
        "    },\n",
        "    \"vocab_txt\": {\n",
        "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/vocab.txt\",\n",
        "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/vocab.txt\",\n",
        "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/vocab.txt\"\n",
        "    }\n",
        "}\n",
        "\n",
        "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n",
        "    \"monologg/kobert\": 512,\n",
        "    \"monologg/kobert-lm\": 512,\n",
        "    \"monologg/distilkobert\": 512\n",
        "}\n",
        "\n",
        "PRETRAINED_INIT_CONFIGURATION = {\n",
        "    \"monologg/kobert\": {\"do_lower_case\": False},\n",
        "    \"monologg/kobert-lm\": {\"do_lower_case\": False},\n",
        "    \"monologg/distilkobert\": {\"do_lower_case\": False}\n",
        "}\n",
        "\n",
        "SPIECE_UNDERLINE = u'▁'\n",
        "\n",
        "\n",
        "class KoBertTokenizer(PreTrainedTokenizer):\n",
        "    \"\"\"\n",
        "        SentencePiece based tokenizer. Peculiarities:\n",
        "            - requires `SentencePiece <https://github.com/google/sentencepiece>`_\n",
        "    \"\"\"\n",
        "    vocab_files_names = VOCAB_FILES_NAMES\n",
        "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n",
        "    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n",
        "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            vocab_file,\n",
        "            vocab_txt,\n",
        "            do_lower_case=False,\n",
        "            remove_space=True,\n",
        "            keep_accents=False,\n",
        "            unk_token=\"[UNK]\",\n",
        "            sep_token=\"[SEP]\",\n",
        "            pad_token=\"[PAD]\",\n",
        "            cls_token=\"[CLS]\",\n",
        "            mask_token=\"[MASK]\",\n",
        "            **kwargs):\n",
        "        super().__init__(\n",
        "            unk_token=unk_token,\n",
        "            sep_token=sep_token,\n",
        "            pad_token=pad_token,\n",
        "            cls_token=cls_token,\n",
        "            mask_token=mask_token,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "        # Build vocab\n",
        "        self.token2idx = dict()\n",
        "        self.idx2token = []\n",
        "        with open(vocab_txt, 'r', encoding='utf-8') as f:\n",
        "            for idx, token in enumerate(f):\n",
        "                token = token.strip()\n",
        "                self.token2idx[token] = idx\n",
        "                self.idx2token.append(token)\n",
        "\n",
        "        try:\n",
        "            import sentencepiece as spm\n",
        "        except ImportError:\n",
        "            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n",
        "                           \"pip install sentencepiece\")\n",
        "\n",
        "        self.do_lower_case = do_lower_case\n",
        "        self.remove_space = remove_space\n",
        "        self.keep_accents = keep_accents\n",
        "        self.vocab_file = vocab_file\n",
        "        self.vocab_txt = vocab_txt\n",
        "\n",
        "        self.sp_model = spm.SentencePieceProcessor()\n",
        "        self.sp_model.Load(vocab_file)\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return len(self.idx2token)\n",
        "\n",
        "    def get_vocab(self):\n",
        "        return dict(self.token2idx, **self.added_tokens_encoder)\n",
        "\n",
        "    def __getstate__(self):\n",
        "        state = self.__dict__.copy()\n",
        "        state[\"sp_model\"] = None\n",
        "        return state\n",
        "\n",
        "    def __setstate__(self, d):\n",
        "        self.__dict__ = d\n",
        "        try:\n",
        "            import sentencepiece as spm\n",
        "        except ImportError:\n",
        "            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n",
        "                           \"pip install sentencepiece\")\n",
        "        self.sp_model = spm.SentencePieceProcessor()\n",
        "        self.sp_model.Load(self.vocab_file)\n",
        "\n",
        "    def preprocess_text(self, inputs):\n",
        "        if self.remove_space:\n",
        "            outputs = \" \".join(inputs.strip().split())\n",
        "        else:\n",
        "            outputs = inputs\n",
        "        outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')\n",
        "\n",
        "        if not self.keep_accents:\n",
        "            outputs = unicodedata.normalize('NFKD', outputs)\n",
        "            outputs = \"\".join([c for c in outputs if not unicodedata.combining(c)])\n",
        "        if self.do_lower_case:\n",
        "            outputs = outputs.lower()\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def _tokenize(self, text, return_unicode=True, sample=False):\n",
        "        \"\"\" Tokenize a string. \"\"\"\n",
        "        text = self.preprocess_text(text)\n",
        "\n",
        "        if not sample:\n",
        "            pieces = self.sp_model.EncodeAsPieces(text)\n",
        "        else:\n",
        "            pieces = self.sp_model.SampleEncodeAsPieces(text, 64, 0.1)\n",
        "        new_pieces = []\n",
        "        for piece in pieces:\n",
        "            if len(piece) > 1 and piece[-1] == str(\",\") and piece[-2].isdigit():\n",
        "                cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))\n",
        "                if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n",
        "                    if len(cur_pieces[0]) == 1:\n",
        "                        cur_pieces = cur_pieces[1:]\n",
        "                    else:\n",
        "                        cur_pieces[0] = cur_pieces[0][1:]\n",
        "                cur_pieces.append(piece[-1])\n",
        "                new_pieces.extend(cur_pieces)\n",
        "            else:\n",
        "                new_pieces.append(piece)\n",
        "\n",
        "        return new_pieces\n",
        "\n",
        "    def _convert_token_to_id(self, token):\n",
        "        \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\n",
        "        return self.token2idx.get(token, self.token2idx[self.unk_token])\n",
        "\n",
        "    def _convert_id_to_token(self, index, return_unicode=True):\n",
        "        \"\"\"Converts an index (integer) in a token (string/unicode) using the vocab.\"\"\"\n",
        "        return self.idx2token[index]\n",
        "\n",
        "    def convert_tokens_to_string(self, tokens):\n",
        "        \"\"\"Converts a sequence of tokens (strings for sub-words) in a single string.\"\"\"\n",
        "        out_string = \"\".join(tokens).replace(SPIECE_UNDERLINE, \" \").strip()\n",
        "        return out_string\n",
        "\n",
        "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
        "        \"\"\"\n",
        "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
        "        by concatenating and adding special tokens.\n",
        "        A KoBERT sequence has the following format:\n",
        "            single sequence: [CLS] X [SEP]\n",
        "            pair of sequences: [CLS] A [SEP] B [SEP]\n",
        "        \"\"\"\n",
        "        if token_ids_1 is None:\n",
        "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "        sep = [self.sep_token_id]\n",
        "        return cls + token_ids_0 + sep + token_ids_1 + sep\n",
        "\n",
        "    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n",
        "        \"\"\"\n",
        "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
        "        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n",
        "        Args:\n",
        "            token_ids_0: list of ids (must not contain special tokens)\n",
        "            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n",
        "                for sequence pairs\n",
        "            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n",
        "                special tokens for the model\n",
        "        Returns:\n",
        "            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\n",
        "        \"\"\"\n",
        "\n",
        "        if already_has_special_tokens:\n",
        "            if token_ids_1 is not None:\n",
        "                raise ValueError(\n",
        "                    \"You should not supply a second sequence if the provided sequence of \"\n",
        "                    \"ids is already formated with special tokens for the model.\"\n",
        "                )\n",
        "            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n",
        "\n",
        "        if token_ids_1 is not None:\n",
        "            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n",
        "        return [1] + ([0] * len(token_ids_0)) + [1]\n",
        "\n",
        "    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n",
        "        \"\"\"\n",
        "        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n",
        "        A KoBERT sequence pair mask has the following format:\n",
        "        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
        "        | first sequence    | second sequence\n",
        "        if token_ids_1 is None, only returns the first portion of the mask (0's).\n",
        "        \"\"\"\n",
        "        sep = [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "        if token_ids_1 is None:\n",
        "            return len(cls + token_ids_0 + sep) * [0]\n",
        "        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n",
        "\n",
        "    def save_vocabulary(self, save_directory):\n",
        "        \"\"\" Save the sentencepiece vocabulary (copy original file) and special tokens file\n",
        "            to a directory.\n",
        "        \"\"\"\n",
        "        if not os.path.isdir(save_directory):\n",
        "            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\n",
        "            return\n",
        "\n",
        "        # 1. Save sentencepiece model\n",
        "        out_vocab_model = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_file\"])\n",
        "\n",
        "        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_model):\n",
        "            copyfile(self.vocab_file, out_vocab_model)\n",
        "\n",
        "        # 2. Save vocab.txt\n",
        "        index = 0\n",
        "        out_vocab_txt = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_txt\"])\n",
        "        with open(out_vocab_txt, \"w\", encoding=\"utf-8\") as writer:\n",
        "            for token, token_index in sorted(self.token2idx.items(), key=lambda kv: kv[1]):\n",
        "                if index != token_index:\n",
        "                    logger.warning(\n",
        "                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n",
        "                        \" Please check that the vocabulary is not corrupted!\".format(out_vocab_txt)\n",
        "                    )\n",
        "                    index = token_index\n",
        "                writer.write(token + \"\\n\")\n",
        "                index += 1\n",
        "\n",
        "        return out_vocab_model, out_vocab_txt"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VQdGLciKc_y",
        "trusted": true
      },
      "source": [
        "from transformers import BertTokenizer, AutoTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset, random_split\n",
        "\n",
        "import time\n",
        "import random\n",
        "import datetime\n",
        "\n",
        "# 간단한 전처리\n",
        "def clean_text(txt):\n",
        "    txt = txt.replace('\\n',' ')\n",
        "    txt = txt.replace('\\r',' ')    \n",
        "    txt = txt.replace('=','')\n",
        "    txt = txt.replace('\\\"','')   \n",
        "    txt = txt.replace('\\'','')\n",
        "    #txt = txt.replace(',','')\n",
        "    txt = txt.replace('..','')\n",
        "    txt = txt.replace('...','')\n",
        "    txt = txt.replace(' .','.')\n",
        "    txt = txt.replace('.','. ')\n",
        "    txt = txt.replace('  ',' ')\n",
        "    txt = txt.replace('  ',' ')    \n",
        "    txt = txt.replace('  ',' ')   \n",
        "    txt = txt.replace('  ',' ')           \n",
        "    txt = txt.replace('  ',' ')\n",
        "    txt = txt.replace('  ',' ')    \n",
        "    txt = txt.replace('  ',' ')   \n",
        "    txt = txt.replace('  ',' ')             \n",
        "    return txt.strip()\n",
        "\n",
        "def shuffling(txt):\n",
        "    txt_list = txt.split(' ')\n",
        "    random.shuffle(txt_list)\n",
        "    return ' '.join(txt_list)\n",
        "\n",
        "def collect_training_dataset_for_grammar_discriminator(sentences_dataset):\n",
        "\n",
        "    sentences = []\n",
        "    labels = []\n",
        "\n",
        "    for txtss in sentences_dataset:\n",
        "        txtss = clean_text(txtss)\n",
        "        txts = txtss.strip().split('.')\n",
        "        for txt in txts:  \n",
        "            txt = txt.strip()\n",
        "            if len(txt) > 40:\n",
        "                #ko_grammar_dataset.append([txt,1])\n",
        "                txt = txt.replace('.','')\n",
        "                tf = random.choice([True,False])\n",
        "                # 정상 또는 비정상 둘중에 하나만 데이터셋에 추가\n",
        "                if (tf):\n",
        "                    sentences.append(txt) # '.'의 위치를 보고 True, False를 판단 하기 땜에...\n",
        "                    labels.append(1)\n",
        "                else:\n",
        "                    sentences.append(shuffling(txt))\n",
        "                    labels.append(0)\n",
        "\n",
        "    return sentences,labels\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "class Grammar_Discriminator:\n",
        "\n",
        "\n",
        "    def __init__(self, pretraoned_kobert_model_name='monologg/kobert', input_dir=None):\n",
        "\n",
        "        if input_dir is None:\n",
        "            self.tokenizer = KoBertTokenizer.from_pretrained(pretraoned_kobert_model_name)\n",
        "            self.discriminator = BertForSequenceClassification.from_pretrained(\n",
        "                                    pretraoned_kobert_model_name, # Use the 12-layer BERT model, with an uncased vocab.\n",
        "                                    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                                                    # You can increase this for multi-class tasks.   \n",
        "                                    output_attentions = False, # Whether the model returns attentions weights.\n",
        "                                    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "                                )            \n",
        "        else:\n",
        "            self.__load_model(input_dir)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def set_dataset(self, sentences,labels):\n",
        "        # Print the original sentence.\n",
        "        print(' Original: ', sentences[0])\n",
        "\n",
        "        # Print the sentence split into tokens.\n",
        "        print('Tokenized: ', self.tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "        # Print the sentence mapped to token ids.\n",
        "        print('Token IDs: ', self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(sentences[0])))   \n",
        "\n",
        "        # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "        input_ids = []\n",
        "        attention_masks = []\n",
        "\n",
        "        # For every sentence...\n",
        "        for sent in sentences:\n",
        "            # `encode_plus` will:\n",
        "            #   (1) Tokenize the sentence.\n",
        "            #   (2) Prepend the `[CLS]` token to the start.\n",
        "            #   (3) Append the `[SEP]` token to the end.\n",
        "            #   (4) Map tokens to their IDs.\n",
        "            #   (5) Pad or truncate the sentence to `max_length`\n",
        "            #   (6) Create attention masks for [PAD] tokens.\n",
        "            encoded_dict = self.tokenizer.encode_plus(\n",
        "                                sent,                      # Sentence to encode.\n",
        "                                add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                                max_length = 64,           # Pad & truncate all sentences.\n",
        "                                pad_to_max_length = True,\n",
        "                                return_attention_mask = True,   # Construct attn. masks.\n",
        "                                return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                                truncation = True,\n",
        "                        )\n",
        "            \n",
        "            # Add the encoded sentence to the list.    \n",
        "            input_ids.append(encoded_dict['input_ids'])\n",
        "            \n",
        "            # And its attention mask (simply differentiates padding from non-padding).\n",
        "            attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "        # Convert the lists into tensors.\n",
        "        input_ids = torch.cat(input_ids, dim=0)\n",
        "        attention_masks = torch.cat(attention_masks, dim=0)\n",
        "        labels = torch.tensor(labels)\n",
        "\n",
        "        # Print sentence 0, now as a list of IDs.\n",
        "        print('Original: ', sentences[0])\n",
        "        print('Token IDs:', input_ids[0])\n",
        "\n",
        "        # Training & Validation Split\n",
        "        # Divide up our training set to use 90% for training and 10% for validation.\n",
        "\n",
        "        # Combine the training inputs into a TensorDataset.\n",
        "        dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "        # Create a 90-10 train-validation split.\n",
        "\n",
        "        # Calculate the number of samples to include in each set.\n",
        "        train_size = int(0.9 * len(dataset))\n",
        "        val_size = len(dataset) - train_size\n",
        "\n",
        "        # Divide the dataset by randomly selecting samples.\n",
        "        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "        print('{:>5,} training samples'.format(train_size))\n",
        "        print('{:>5,} validation samples'.format(val_size))\n",
        "\n",
        "        # The DataLoader needs to know our batch size for training, so we specify it \n",
        "        # here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "        # size of 16 or 32.\n",
        "        self.batch_size = 32\n",
        "\n",
        "        # Create the DataLoaders for our training and validation sets.\n",
        "        # We'll take training samples in random order. \n",
        "        self.train_dataloader = DataLoader(\n",
        "                    train_dataset,  # The training samples.\n",
        "                    sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "                    batch_size = self.batch_size # Trains with this batch size.\n",
        "                )\n",
        "\n",
        "        # For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "        self.validation_dataloader = DataLoader(\n",
        "                    val_dataset, # The validation samples.\n",
        "                    sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "                    batch_size = self.batch_size # Evaluate with this batch size.\n",
        "                )        \n",
        "\n",
        "\n",
        "\n",
        "    def train(self,epochs=4):\n",
        "        # Tell pytorch to run this model on the GPU.\n",
        "        self.discriminator.cuda()\n",
        "\n",
        "        # Get all of the model's parameters as a list of tuples.\n",
        "        params = list(self.discriminator.named_parameters())\n",
        "\n",
        "        print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "        print('==== Embedding Layer ====\\n')\n",
        "\n",
        "        for p in params[0:5]:\n",
        "            print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "        print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "        for p in params[5:21]:\n",
        "            print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "        print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "        for p in params[-4:]:\n",
        "            print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))  \n",
        "\n",
        "        # Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "        # I believe the 'W' stands for 'Weight Decay fix\"\n",
        "        self.optimizer = AdamW(self.discriminator.parameters(),\n",
        "                        lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                        eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                        )\n",
        "\n",
        "        # Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "        # We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "        # training data.\n",
        "        #epochs = 2\n",
        "\n",
        "        # Total number of training steps is [number of batches] x [number of epochs]. \n",
        "        # (Note that this is not the same as the number of training samples).\n",
        "        total_steps = len(self.train_dataloader) * epochs\n",
        "\n",
        "        # Create the learning rate scheduler.\n",
        "        scheduler = get_linear_schedule_with_warmup(self.optimizer, \n",
        "                                                    num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                                    num_training_steps = total_steps)\n",
        "            \n",
        "        # This training code is based on the `run_glue.py` script here:\n",
        "        # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "        # Set the seed value all over the place to make this reproducible.\n",
        "        seed_val = 42\n",
        "\n",
        "        random.seed(seed_val)\n",
        "        np.random.seed(seed_val)\n",
        "        torch.manual_seed(seed_val)\n",
        "        torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "        # We'll store a number of quantities such as training and validation loss, \n",
        "        # validation accuracy, and timings.\n",
        "        training_stats = []\n",
        "\n",
        "        # Measure the total training time for the whole run.\n",
        "        total_t0 = time.time()\n",
        "\n",
        "        # For each epoch...\n",
        "        for epoch_i in range(0, epochs):\n",
        "            \n",
        "            # ========================================\n",
        "            #               Training\n",
        "            # ========================================\n",
        "            \n",
        "            # Perform one full pass over the training set.\n",
        "\n",
        "            print(\"\")\n",
        "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "            print('Training...')\n",
        "\n",
        "            # Measure how long the training epoch takes.\n",
        "            t0 = time.time()\n",
        "\n",
        "            # Reset the total loss for this epoch.\n",
        "            total_train_loss = 0\n",
        "\n",
        "            # Put the model into training mode. Don't be mislead--the call to \n",
        "            # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "            # `dropout` and `batchnorm` layers behave differently during training\n",
        "            # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "            self.discriminator.train()\n",
        "\n",
        "            # For each batch of training data...\n",
        "            for step, batch in enumerate(self.train_dataloader):\n",
        "\n",
        "                # Progress update every 40 batches.\n",
        "                if step % 40 == 0 and not step == 0:\n",
        "                    # Calculate elapsed time in minutes.\n",
        "                    elapsed = format_time(time.time() - t0)\n",
        "                    \n",
        "                    # Report progress.\n",
        "                    print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(self.train_dataloader), elapsed))\n",
        "\n",
        "                # Unpack this training batch from our dataloader. \n",
        "                #\n",
        "                # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "                # `to` method.\n",
        "                #\n",
        "                # `batch` contains three pytorch tensors:\n",
        "                #   [0]: input ids \n",
        "                #   [1]: attention masks\n",
        "                #   [2]: labels \n",
        "                b_input_ids = batch[0].to(device)\n",
        "                b_input_mask = batch[1].to(device)\n",
        "                b_labels = batch[2].to(device)\n",
        "\n",
        "                # Always clear any previously calculated gradients before performing a\n",
        "                # backward pass. PyTorch doesn't do this automatically because \n",
        "                # accumulating the gradients is \"convenient while training RNNs\". \n",
        "                # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "                self.discriminator.zero_grad()        \n",
        "\n",
        "                # Perform a forward pass (evaluate the model on this training batch).\n",
        "                # The documentation for this `model` function is here: \n",
        "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "                # It returns different numbers of parameters depending on what arguments\n",
        "                # arge given and what flags are set. For our useage here, it returns\n",
        "                # the loss (because we provided labels) and the \"logits\"--the model\n",
        "                # outputs prior to activation.\n",
        "                loss, logits = self.discriminator(b_input_ids, \n",
        "                                    token_type_ids=None, \n",
        "                                    attention_mask=b_input_mask, \n",
        "                                    labels=b_labels)\n",
        "\n",
        "                # Accumulate the training loss over all of the batches so that we can\n",
        "                # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "                # single value; the `.item()` function just returns the Python value \n",
        "                # from the tensor.\n",
        "                total_train_loss += loss.item()\n",
        "\n",
        "                # Perform a backward pass to calculate the gradients.\n",
        "                loss.backward()\n",
        "\n",
        "                # Clip the norm of the gradients to 1.0.\n",
        "                # This is to help prevent the \"exploding gradients\" problem.\n",
        "                torch.nn.utils.clip_grad_norm_(self.discriminator.parameters(), 1.0)\n",
        "\n",
        "                # Update parameters and take a step using the computed gradient.\n",
        "                # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "                # modified based on their gradients, the learning rate, etc.\n",
        "                self.optimizer.step()\n",
        "\n",
        "                # Update the learning rate.\n",
        "                scheduler.step()\n",
        "\n",
        "            # Calculate the average loss over all of the batches.\n",
        "            avg_train_loss = total_train_loss / len(self.train_dataloader)            \n",
        "            \n",
        "            # Measure how long this epoch took.\n",
        "            training_time = format_time(time.time() - t0)\n",
        "\n",
        "            print(\"\")\n",
        "            print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "            print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "                \n",
        "            # ========================================\n",
        "            #               Validation\n",
        "            # ========================================\n",
        "            # After the completion of each training epoch, measure our performance on\n",
        "            # our validation set.\n",
        "\n",
        "            print(\"\")\n",
        "            print(\"Running Validation...\")\n",
        "\n",
        "            t0 = time.time()\n",
        "\n",
        "            # Put the model in evaluation mode--the dropout layers behave differently\n",
        "            # during evaluation.\n",
        "            self.discriminator.eval()\n",
        "\n",
        "            # Tracking variables \n",
        "            total_eval_accuracy = 0\n",
        "            total_eval_loss = 0\n",
        "            nb_eval_steps = 0\n",
        "\n",
        "            # Evaluate data for one epoch\n",
        "            for batch in self.validation_dataloader:\n",
        "                \n",
        "                # Unpack this training batch from our dataloader. \n",
        "                #\n",
        "                # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "                # the `to` method.\n",
        "                #\n",
        "                # `batch` contains three pytorch tensors:\n",
        "                #   [0]: input ids \n",
        "                #   [1]: attention masks\n",
        "                #   [2]: labels \n",
        "                b_input_ids = batch[0].to(device)\n",
        "                b_input_mask = batch[1].to(device)\n",
        "                b_labels = batch[2].to(device)\n",
        "                \n",
        "                # Tell pytorch not to bother with constructing the compute graph during\n",
        "                # the forward pass, since this is only needed for backprop (training).\n",
        "                with torch.no_grad():        \n",
        "\n",
        "                    # Forward pass, calculate logit predictions.\n",
        "                    # token_type_ids is the same as the \"segment ids\", which \n",
        "                    # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "                    # The documentation for this `model` function is here: \n",
        "                    # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "                    # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "                    # values prior to applying an activation function like the softmax.\n",
        "                    (loss, logits) = self.discriminator(b_input_ids, \n",
        "                                        token_type_ids=None, \n",
        "                                        attention_mask=b_input_mask,\n",
        "                                        labels=b_labels)\n",
        "                    \n",
        "                # Accumulate the validation loss.\n",
        "                total_eval_loss += loss.item()\n",
        "\n",
        "                # Move logits and labels to CPU\n",
        "                logits = logits.detach().cpu().numpy()\n",
        "                label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "                # Calculate the accuracy for this batch of test sentences, and\n",
        "                # accumulate it over all batches.\n",
        "                total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "                \n",
        "\n",
        "            # Report the final accuracy for this validation run.\n",
        "            avg_val_accuracy = total_eval_accuracy / len(self.validation_dataloader)\n",
        "            print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "            # Calculate the average loss over all of the batches.\n",
        "            avg_val_loss = total_eval_loss / len(self.validation_dataloader)\n",
        "            \n",
        "            # Measure how long the validation run took.\n",
        "            validation_time = format_time(time.time() - t0)\n",
        "            \n",
        "            print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "            print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "            # Record all statistics from this epoch.\n",
        "            training_stats.append(\n",
        "                {\n",
        "                    'epoch': epoch_i + 1,\n",
        "                    'Training Loss': avg_train_loss,\n",
        "                    'Valid. Loss': avg_val_loss,\n",
        "                    'Valid. Accur.': avg_val_accuracy,\n",
        "                    'Training Time': training_time,\n",
        "                    'Validation Time': validation_time\n",
        "                }\n",
        "            )\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"Training complete!\")\n",
        "\n",
        "        print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "            \n",
        "\n",
        "        return training_stats\n",
        "\n",
        "    def save_model(self, output_dir = './model_save/'):\n",
        "        # Create output directory if needed\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "\n",
        "        print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "        # They can then be reloaded using `from_pretrained()`\n",
        "        model_to_save = self.discriminator.module if hasattr(self.discriminator, 'module') else self.discriminator  # Take care of distributed/parallel training\n",
        "        model_to_save.save_pretrained(output_dir)\n",
        "        self.tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "        # Good practice: save your training arguments together with the trained model\n",
        "        # torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n",
        "\n",
        "    def __load_model(self, input_dir = './drive/MyDrive/Colab Notebooks/summary/en_grammar_check_model'):\n",
        "        print('Loading BERT tokenizer...')\n",
        "        self.tokenizer = KoBertTokenizer.from_pretrained(input_dir)\n",
        "        self.discriminator = BertForSequenceClassification.from_pretrained(input_dir)\n",
        "\n",
        "    def transfer_learning(self, sentences, train_for = True):\n",
        "        \n",
        "        input_ids = []\n",
        "        attention_masks = []\n",
        "\n",
        "        # For every sentence...\n",
        "        for sent in sentences:\n",
        "            # `encode_plus` will:\n",
        "            #   (1) Tokenize the sentence.\n",
        "            #   (2) Prepend the `[CLS]` token to the start.\n",
        "            #   (3) Append the `[SEP]` token to the end.\n",
        "            #   (4) Map tokens to their IDs.\n",
        "            #   (5) Pad or truncate the sentence to `max_length`\n",
        "            #   (6) Create attention masks for [PAD] tokens.\n",
        "            encoded_dict = self.tokenizer.encode_plus(\n",
        "                                sent,                      # Sentence to encode.\n",
        "                                add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                                max_length = 64,           # Pad & truncate all sentences.\n",
        "                                pad_to_max_length = True,\n",
        "                                return_attention_mask = True,   # Construct attn. masks.\n",
        "                                return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                                truncation = True,\n",
        "                        )\n",
        "            # Add the encoded sentence to the list.    \n",
        "            input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "            # And its attention mask (simply differentiates padding from non-padding).\n",
        "            attention_masks.append(encoded_dict['attention_mask'])\n",
        "        \n",
        "        if train_for:\n",
        "            b_labels = torch.ones(len(sentences),dtype=torch.long).to(device)\n",
        "        else:\n",
        "            b_labels = torch.zeros(len(sentences),dtype=torch.long).to(device)\n",
        "        #print(b_labels)\n",
        "        # Convert the lists into tensors.\n",
        "        input_ids = torch.cat(input_ids, dim=0).to(device)\n",
        "        attention_masks = torch.cat(attention_masks, dim=0).to(device)    \n",
        "        #if str(discriminator1.device) == 'cpu':\n",
        "        #    pass\n",
        "        #else:\n",
        "        #    input_ids = input_ids.to(device)\n",
        "        #    attention_masks = attention_masks.to(device)        \n",
        "\n",
        "        loss, logits = self.discriminator(input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=attention_masks, \n",
        "                                labels=b_labels)\n",
        "        #return torch.sigmoid(outputs[0][:,1])\n",
        "        #return outputs[0][:,1]\n",
        "        return loss, logits\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "U9Jmf98ilEup"
      },
      "source": [
        "urls = ['https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-ABC%20%EC%82%B4%EC%9D%B8%EC%82%AC%EA%B1%B4.txt',\n",
        "        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EA%B7%B8%EB%A6%AC%EA%B3%A0%20%EC%95%84%EB%AC%B4%EB%8F%84%20%EC%97%86%EC%97%88%EB%8B%A4.txt',\n",
        "        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EB%82%98%EC%9D%BC%EA%B0%95%EC%9D%98%20%EC%A3%BD%EC%9D%8C.txt',\n",
        "        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EB%A7%8C%EC%B0%AC%ED%9A%8C%EC%9D%98%2013%EC%9D%B8.txt',\n",
        "        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EB%A9%94%EC%86%8C%ED%8F%AC%ED%83%80%EB%AF%B8%EC%95%84%EC%9D%98%20%EC%A3%BD%EC%9D%8C.txt',\n",
        "        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EB%AA%A9%EC%82%AC%EA%B4%80%EC%82%B4%EC%9D%B8.txt',\n",
        "        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EB%B2%99%EC%96%B4%EB%A6%AC%20%EB%AA%A9%EA%B2%A9%EC%9E%90.txt',\n",
        "        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EB%B9%84%EB%B0%80%20%EC%84%9C%EB%A5%98%EB%A5%BC%20%EB%85%B8%EB%A0%A4%EB%9D%BC.txt',\n",
        "        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EC%8A%A4%ED%8E%98%EC%9D%B8%EA%B6%A4%EC%A7%9D%EC%9D%98%20%EB%B9%84%EB%B0%80.txt',\n",
        "        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EC%95%84%ED%8C%8C%ED%8A%B8%EC%97%90%20%EB%82%98%ED%83%80%EB%82%9C%20%EC%9A%94%EC%A0%95.txt',\n",
        "        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EC%95%A0%ED%81%AC%EB%A1%9C%EC%9D%B4%EB%93%9C%20%EC%82%B4%EC%9D%B8%20%EC%82%AC%EA%B1%B4.txt',\n",
        "        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EC%98%88%EA%B3%A0%20%EC%82%B4%EC%9D%B8.txt',\n",
        "        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EC%A5%90%EB%8D%AB.txt',\n",
        "        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EC%BB%A4%ED%8A%BC.txt',\n",
        "        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%ED%81%AC%EB%A6%AC%EC%8A%A4%EB%A7%88%EC%8A%A4%20%ED%91%B8%EB%94%A9%EC%9D%98%20%EB%AA%A8%ED%97%98.txt',\n",
        "        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%ED%91%B8%EB%A5%B8%EC%97%B4%EC%B0%A8%EC%9D%98%EC%A3%BD%EC%9D%8C.txt',\n",
        "        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%ED%99%94%EC%9A%94%EC%9D%BC%20%ED%81%B4%EB%9F%BD%EC%9D%98%20%EC%82%B4%EC%9D%B8.txt']\n",
        "\n",
        "ko_sentences_dataset = []\n",
        "for url in urls:\n",
        "    raw_text = urllib.request.urlopen(url).read().decode('utf-8')\n",
        "    ko_sentences_dataset += nltk.sent_tokenize(clean_text(raw_text))\n",
        "    "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "v8jnd-LvlEuq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19176e7f-d003-4e96-a165-28e0d5c9c8f9"
      },
      "source": [
        "len(ko_sentences_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "95889"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "2YphAI73lEur"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/summary/korean_sentences.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "11I136pFlEus"
      },
      "source": [
        "ko_sentences_dataset += list(df['sentence'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "PJM1wxnxlEut",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "594bf674-c45d-49e5-9bd8-655570427a04"
      },
      "source": [
        "len(ko_sentences_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "503444"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7Zf2oRMMXmH",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6d55750-3658-428a-d1bf-b12d456d50d1"
      },
      "source": [
        "use_pretrained_model = True\n",
        "\n",
        "if use_pretrained_model:\n",
        "    #g_discriminator = Grammar_Discriminator(input_dir = '/content/drive/MyDrive/Colab Notebooks/summary/model_save')\n",
        "    g_discriminator = Grammar_Discriminator(input_dir = '/content/drive/MyDrive/Colab Notebooks/summary/ko_grammar_model')\n",
        "else:\n",
        "    sentences,labels = collect_training_dataset_for_grammar_discriminator(ko_sentences_dataset)\n",
        "    print(len(sentences))\n",
        "    g_discriminator = Grammar_Discriminator()\n",
        "    g_discriminator.set_dataset(sentences,labels)\n",
        "    g_discriminator.train(epochs=1)\n",
        "    g_discriminator.save_model(output_dir='/content/drive/MyDrive/Colab Notebooks/summary/ko_grammar_model')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbGhj6JGuFab",
        "trusted": true
      },
      "source": [
        "if False: ## 추가적인 fine-tuning\n",
        "    #sentences,labels = collect_training_dataset_for_grammar_discriminator(ko_sentences_dataset)\n",
        "    #print(len(sentences))\n",
        "    #g_discriminator = Grammar_Discriminator()\n",
        "    #g_discriminator.set_dataset(sentences,labels)\n",
        "    g_discriminator.train(epochs=1)\n",
        "    g_discriminator.save_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d96kaCAHKuUc"
      },
      "source": [
        "##4.3 Static similarity discriminator class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZDpXe7XKxeg",
        "trusted": true
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import BertTokenizer\n",
        "from scipy.signal import find_peaks\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.misc import electrocardiogram\n",
        "import scipy\n",
        "\n",
        "\n",
        "class Similarity_Discriminator:\n",
        "    '''\n",
        "    _instance = None\n",
        "    _embedder = None\n",
        "    def __new__(cls,pre_trained_model_name='stsb-roberta-large'):\n",
        "        if cls._instance is None:\n",
        "            print('Creating Similarity_Discriminator object')\n",
        "            cls._instance = super(Similarity_Discriminator, cls).__new__(cls)\n",
        "            # Put any initialization here.\n",
        "            cls._embedder = SentenceTransformer(pre_trained_model_name)\n",
        "        return cls._instance\n",
        "\n",
        "    '''\n",
        "\n",
        "    def __init__(self,pre_trained_model_name='xlm-r-large-en-ko-nli-ststb'):\n",
        "        print('Creating Similarity_Discriminator object')\n",
        "        # Put any initialization here.\n",
        "        self._embedder = SentenceTransformer(pre_trained_model_name)  \n",
        "        #self.cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "    def encode(self,texts):\n",
        "        return self._embedder.encode(texts,show_progress_bar=False)\n",
        "\n",
        "    def similarity(self, query_text, org_text_emb):\n",
        "        queries = nltk.sent_tokenize(query_text)\n",
        "        query_embeddings = self._embedder.encode(queries,show_progress_bar=False)\n",
        "        #query_embeddings = self._embedder.encode(queries,show_progress_bar=False)\n",
        "        #print(queries)\n",
        "        #print(org_text_emb)\n",
        "        \n",
        "        if len(query_embeddings) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        cos_scores = scipy.spatial.distance.cdist(query_embeddings, org_text_emb, \"cosine\")\n",
        "        similarity_score = 1.0 - np.mean(np.min(cos_scores,axis=0))\n",
        "        '''\n",
        "        for query, query_embedding in zip(queries, query_embeddings):\n",
        "            distances = scipy.spatial.distance.cdist([query_embedding], [org_text_emb], \"cosine\")[0]\n",
        "            results = zip(range(len(distances)), distances)\n",
        "            for idx, distance in results:\n",
        "                scores.append(1-distance)\n",
        "        '''\n",
        "        return similarity_score  \n",
        " "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sQZ36GuMumP"
      },
      "source": [
        "###4.3.1 한국어 문장 유사도 pre-trained model 적용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9Miao14Muww",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29ef219a-fb55-458c-c054-753ed9821160"
      },
      "source": [
        "s_discriminator = Similarity_Discriminator()\n",
        "#s_discriminator = Similarity_Discriminator()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating Similarity_Discriminator object\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.80G/1.80G [01:30<00:00, 19.8MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xnk9GsQ0K1t1"
      },
      "source": [
        "##4.4 Document source class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_ztc0q3M_4F"
      },
      "source": [
        "###4.4.1 keyBERT를 위한 pre-trained model의 적재"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "dEc9R82hi1gO"
      },
      "source": [
        "#!pip install keybert"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJFTjWlwK3Uz",
        "trusted": true
      },
      "source": [
        "#from keybert import KeyBERT\n",
        "#key_model = KeyBERT('distilbert-base-nli-mean-tokens')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnBm6RCvNIWG"
      },
      "source": [
        "###4.4.2 frame term 추출을 위한 source class 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsJKbtc2K4xN",
        "trusted": true
      },
      "source": [
        "\n",
        "\n",
        "class Source:\n",
        "\n",
        "    def __init__(self,org_text):\n",
        "        self.org_text = org_text\n",
        "\n",
        "    def __crean_text(self, txt):\n",
        "        txt = txt.replace('\\n',' ')\n",
        "        txt = txt.replace('\\r',' ')    \n",
        "        txt = txt.replace('=','')\n",
        "        txt = txt.replace('\\\"','')   \n",
        "        txt = txt.replace('\\'','')\n",
        "        #txt = txt.replace(',','')\n",
        "        txt = txt.replace('..','')\n",
        "        txt = txt.replace('...','')\n",
        "        txt = txt.replace(' .','.')\n",
        "        txt = txt.replace('.','. ')\n",
        "        txt = txt.replace('  ',' ')\n",
        "        txt = txt.replace('  ',' ')    \n",
        "        txt = txt.replace('  ',' ')   \n",
        "        txt = txt.replace('  ',' ')           \n",
        "        txt = txt.replace('  ',' ')\n",
        "        txt = txt.replace('  ',' ')    \n",
        "        txt = txt.replace('  ',' ')   \n",
        "        txt = txt.replace('  ',' ')           \n",
        "        return txt.strip()\n",
        "\n",
        "\n",
        "    def extract_keywords(self,s_discriminator,key_model,comp_rate=0.2):\n",
        "        self.org_text = self.__crean_text(self.org_text.strip())\n",
        "        print('------------------------------------------------------------------')\n",
        "        print(self.org_text)\n",
        "        print('------------------------------------------------------------------')\n",
        "        self.org_sentences = np.array(nltk.sent_tokenize(self.org_text))\n",
        "        self.org_term_set = (' ' + self.org_text + ' ').split(' ')\n",
        "        self.org_source_length = len(self.org_term_set)\n",
        "        self.term_table = {}\n",
        "        #morp_table = {}\n",
        "        index_table = {}\n",
        "        for index, word in zip(range(len(self.org_term_set)),self.org_term_set):\n",
        "            self.term_table[index] = word\n",
        "        '''\n",
        "        print('Token table of origin text')\n",
        "        print('---------------------------------------------')\n",
        "        print(' Code     Token     ')\n",
        "        for k in self.term_table.keys():\n",
        "            print( f'  {str(k).ljust(5)}     {self.term_table[k]}')\n",
        "        print('---------------------------------------------')\n",
        "        '''\n",
        "        self.s_discriminator = s_discriminator\n",
        "        # 원문의 embedding...\n",
        "        self.org_text_emb = self.s_discriminator.encode(self.org_sentences)\n",
        "        '''\n",
        "        # weight 들의 초기화\n",
        "        terms = np.array(list(self.term_table.values()))\n",
        "\n",
        "        word_filters=np.array([[0]])\n",
        "\n",
        "        story_weights = np.zeros(self.org_source_length,)\n",
        "        word_weights = np.zeros(self.org_source_length,)\n",
        "\n",
        "        #terms = np.array(list(self.term_table.values()))\n",
        "\n",
        "        # story에 지배적인 word를 찾는다.\n",
        "        # 먼저 word의 강세 분석\n",
        "        for filter in word_filters:\n",
        "            #print(filter)\n",
        "            last_idx = len(terms)-(max(filter)+1)\n",
        "            pb = ProgressBar(last_idx,prefix='Frame token scan:')\n",
        "            for conv in range(last_idx,0,-1):\n",
        "                pb.printProgress(+1,f'filer:{filter} {conv}/{last_idx}       ')\n",
        "                t = np.array(filter) + conv\n",
        "                part_sen = ' '.join(terms[t]) \n",
        "                #print('\\n part_sen:',part_sen)\n",
        "                score = self.s_discriminator.similarity(part_sen.strip(),self.org_text_emb)\n",
        "                word_weights[t] += score \n",
        "\n",
        "        # story의 강세 분석\n",
        "        for filter in story_filters:\n",
        "            #print(filter)\n",
        "            last_idx = len(terms)-(max(filter)+1)\n",
        "            pb = ProgressBar(last_idx,prefix='Frame token scan:')\n",
        "            for conv in range(last_idx,0,-1):\n",
        "                pb.printProgress(+1,f'filer:{filter} {conv}/{last_idx}       ')\n",
        "                t = np.array(filter) + conv\n",
        "                part_sen = ' '.join(terms[t]) \n",
        "                score = self.s_discriminator.similarity(part_sen.strip(),self.org_text_emb)\n",
        "                story_weights[t] += score\n",
        "\n",
        "        #각각의 peak를 산출\n",
        "        word_peaks, _ = find_peaks(word_weights, height=0)\n",
        "        story_peaks, _ = find_peaks(story_weights, height=0)\n",
        "\n",
        "        #두개의 peak가 겹치는 word에 대해 한개 word가 유사도에 미치는 영향이 큰것으로 간주\n",
        "        #해당 word를 유사도 판단 필터에서 제외하고 다시 필터링...\n",
        "        #이를 통해 story에 대한 word를 최대한 추출 한다.\n",
        "\n",
        "        dup_order = []\n",
        "        for i in range(self.org_source_length):\n",
        "            #lst = \"\"\n",
        "            if (i in word_peaks) and (i in story_peaks):\n",
        "                if terms[i].endswith('.'):\n",
        "                    pass\n",
        "                else:\n",
        "                    dup_order.append(i)\n",
        "                    \n",
        "        # Story에 대한 weight을 추출하기 위해, word에 유독 강세가 있는 term을 제외 시킨다.\n",
        "        print('Negative tokens:',terms[dup_order])\n",
        "        '''\n",
        "\n",
        "        top_n = int(len(self.term_table) * comp_rate)\n",
        "\n",
        "        self.story_peaks = []\n",
        "        keywords = key_model.extract_keywords(self.org_text,top_n=top_n)\n",
        "        #print('keywords len',len(keywords))\n",
        "        #print('keywords',keywords)\n",
        "        for keyword,p in keywords:\n",
        "            for k in self.term_table.keys():\n",
        "                if self.term_table[k] == keyword: # and k not in dup_order:\n",
        "                    self.story_peaks.append(k)\n",
        "\n",
        "        self.story_peaks.append(len(self.term_table)-2)\n",
        "        self.story_peaks = np.sort(np.asarray(self.story_peaks))\n",
        "        print('story_peaks:',self.story_peaks)\n",
        "        print('Peak count:',len(self.story_peaks))          \n",
        "\n",
        "\n",
        "        # story skeleton 추출\n",
        "        self.frame_text = \"\"\n",
        "        for k in self.story_peaks:\n",
        "            #print(k,term_weight[k],word_table[k])\n",
        "            self.frame_text += self.term_table[k]+' '  \n",
        "\n",
        "        print('Frame tokens:',self.frame_text)\n",
        "        print('')\n",
        "        print(f'Similarity : {self.s_discriminator.similarity(self.frame_text.strip(),self.org_text_emb)}')    \n",
        "\n",
        "    def set_key_rate(self,s_discriminator,comp_rate=0.2):\n",
        "        self.org_text = self.__crean_text(self.org_text.strip())\n",
        "        print('------------------------------------------------------------------')\n",
        "        print(self.org_text)\n",
        "        print('------------------------------------------------------------------')\n",
        "        self.org_sentences = np.array(nltk.sent_tokenize(self.org_text))\n",
        "        self.org_term_set = (' ' + self.org_text + ' ').split(' ')\n",
        "        self.org_source_length = len(self.org_term_set)\n",
        "        self.term_table = {}\n",
        "        #morp_table = {}\n",
        "\n",
        "        for index, word in zip(range(len(self.org_term_set)),self.org_term_set):\n",
        "            self.term_table[index] = word\n",
        "\n",
        "        self.s_discriminator = s_discriminator\n",
        "        # 원문의 embedding...\n",
        "        self.org_text_emb = self.s_discriminator.encode(self.org_sentences)\n",
        "        top_n = int(len(self.term_table) * comp_rate)\n",
        "        #print('top_n',top_n)\n",
        "        self.story_peaks = [i+1 for i in range(top_n)]\n",
        "\n",
        "    def analysis_frame_terms(self,s_discriminator,story_filters=np.array([[0,1],[0,1,2],[0,1,2,3]]),peak_base_line = 0.0,comp_rate=0.2,except_key=True,display=False):\n",
        "\n",
        "        self.org_text = self.__crean_text(self.org_text.strip())\n",
        "        print('------------------------------------------------------------------')\n",
        "        print(self.org_text)\n",
        "        print('------------------------------------------------------------------')\n",
        "        self.org_sentences = np.array(nltk.sent_tokenize(self.org_text))\n",
        "        self.org_term_set = (' ' + self.org_text + ' ').split(' ')\n",
        "        self.org_source_length = len(self.org_term_set)\n",
        "        self.term_table = {}\n",
        "        #morp_table = {}\n",
        "\n",
        "        for index, word in zip(range(len(self.org_term_set)),self.org_term_set):\n",
        "            self.term_table[index] = word\n",
        "        '''\n",
        "        print('Token table of origin text')\n",
        "        print('---------------------------------------------')\n",
        "        print(' Code     Token     ')\n",
        "        for k in self.term_table.keys():\n",
        "            print( f'  {str(k).ljust(5)}     {self.term_table[k]}')\n",
        "        print('---------------------------------------------')\n",
        "        '''\n",
        "\n",
        "        self.s_discriminator = s_discriminator\n",
        "        # 원문의 embedding...\n",
        "        self.org_text_emb = self.s_discriminator.encode(self.org_sentences)\n",
        "\n",
        "        # weight 들의 초기화\n",
        "        terms = np.array(list(self.term_table.values()))\n",
        "\n",
        "        word_filters=np.array([[0]])\n",
        "\n",
        "        story_weights = np.zeros(self.org_source_length,)\n",
        "        word_weights = np.zeros(self.org_source_length,)\n",
        "\n",
        "        #terms = np.array(list(self.term_table.values()))\n",
        "\n",
        "        if except_key:\n",
        "            # story에 지배적인 word를 찾는다.\n",
        "            # 먼저 word의 강세 분석\n",
        "            for filter in word_filters:\n",
        "                #print(filter)\n",
        "                last_idx = len(terms)-(max(filter)+1)\n",
        "                pb = ProgressBar(last_idx,prefix='Frame token scan:')\n",
        "                for conv in range(last_idx,0,-1):\n",
        "                    pb.printProgress(+1,f'filer:{filter} {conv}/{last_idx}       ')\n",
        "                    t = np.array(filter) + conv\n",
        "                    part_sen = ' '.join(terms[t]) \n",
        "                    score = self.s_discriminator.similarity(part_sen.strip(),self.org_text_emb)\n",
        "                    word_weights[t] += score \n",
        "\n",
        "            # story의 강세 분석\n",
        "            for filter in story_filters:\n",
        "                #print(filter)\n",
        "                last_idx = len(terms)-(max(filter)+1)\n",
        "                pb = ProgressBar(last_idx,prefix='Frame token scan:')\n",
        "                for conv in range(last_idx,0,-1):\n",
        "                    pb.printProgress(+1,f'filer:{filter} {conv}/{last_idx}       ')\n",
        "                    t = np.array(filter) + conv\n",
        "                    part_sen = ' '.join(terms[t]) \n",
        "                    score = self.s_discriminator.similarity(part_sen.strip(),self.org_text_emb)\n",
        "                    story_weights[t] += score\n",
        "\n",
        "            #각각의 peak를 산출\n",
        "            word_peaks, _ = find_peaks(word_weights, height=0)\n",
        "            story_peaks, _ = find_peaks(story_weights, height=0)\n",
        "\n",
        "            #두개의 peak가 겹치는 word에 대해 한개 word가 유사도에 미치는 영향이 큰것으로 간주\n",
        "            #해당 word를 유사도 판단 필터에서 제외하고 다시 필터링...\n",
        "            #이를 통해 story에 대한 word를 최대한 추출 한다.\n",
        "\n",
        "            dup_order = []\n",
        "            for i in range(self.org_source_length):\n",
        "                #lst = \"\"\n",
        "                if (i in word_peaks) and (i in story_peaks):\n",
        "                    if terms[i].endswith('.'):\n",
        "                        pass\n",
        "                    else:\n",
        "                        dup_order.append(i)\n",
        "                        \n",
        "            # Story에 대한 weight을 추출하기 위해, word에 유독 강세가 있는 term을 제외 시킨다.\n",
        "            print('Negative tokens:',terms[dup_order])\n",
        "            if except_key:\n",
        "                terms[dup_order] = '---'\n",
        "        '''\n",
        "        print('Token table of origin text')\n",
        "        print('---------------------------------------------')\n",
        "        print(' Code         Token      ')\n",
        "        print('')\n",
        "        for index, word in zip(range(len(terms)),terms):\n",
        "            print( f'  {str(index).ljust(8)}    {word}')\n",
        "        print('---------------------------------------------')\n",
        "        '''\n",
        "        self.story_weights = np.zeros(self.org_source_length,)\n",
        "        # 그리고 다시 story 분석 스캔\n",
        "        for filter in story_filters:\n",
        "            #print(filter)\n",
        "            last_idx = len(terms)-(max(filter)+1)\n",
        "            pb = ProgressBar(last_idx,prefix='Frame token scan:')\n",
        "            for conv in range(last_idx):\n",
        "                pb.printProgress(+1,f'filer:{filter} {conv}/{last_idx}       ')\n",
        "                t = np.array(filter) + conv\n",
        "                part_sen = ' '.join(terms[t]) \n",
        "                #part_sen = part_sen.replace('소녀','---')\n",
        "                score = self.s_discriminator.similarity(part_sen.strip(),self.org_text_emb)\n",
        "                self.story_weights[t] += score        \n",
        "\n",
        "\n",
        "        # base line\n",
        "        base_line = peak_base_line\n",
        "        # 다시 peak 추출\n",
        "        story_peaks, _ = find_peaks(self.story_weights, height=base_line)\n",
        "\n",
        "        top_n = int(len(self.term_table) * comp_rate)\n",
        "\n",
        "        if len(story_peaks) > top_n:\n",
        "            peak_dict = {}\n",
        "            for i,peak in zip(range(len(story_peaks)),story_peaks):\n",
        "                peak_dict[peak] = self.story_weights[peak]\n",
        "            #print(peak_dict)\n",
        "            peaks = sorted(peak_dict, key=peak_dict.get, reverse=True)\n",
        "            #print(peaks)\n",
        "            peaks = peaks[:top_n]\n",
        "            #print(peaks)\n",
        "            peaks.sort()\n",
        "            story_peaks = peaks\n",
        "            #print(story_peaks)\n",
        "\n",
        "        #print('top_n:',top_n,'story_peaks:',len(story_peaks))\n",
        "        #print(story_peaks)\n",
        "        \n",
        "        self.story_peaks = np.append(story_peaks,len(story_weights)-2)\n",
        "        #print(self.story_peaks)\n",
        "        # story density 표출\n",
        "        if display:\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            plt.plot(self.story_weights)\n",
        "            plt.plot(self.story_peaks, self.story_weights[self.story_peaks], \"x\")\n",
        "            plt.plot(np.zeros_like(self.story_weights)+base_line, \"--\", color=\"gray\")\n",
        "            plt.show() \n",
        "        print('Peak count:',len(self.story_peaks))          \n",
        "\n",
        "\n",
        "        # story skeleton 추출\n",
        "        self.frame_text = \"\"\n",
        "        for k in self.story_peaks:\n",
        "            #print(k,term_weight[k],word_table[k])\n",
        "            self.frame_text += self.term_table[k]+' '  \n",
        "\n",
        "        print('Frame tokens:',self.frame_text)\n",
        "        print('')\n",
        "        print(f'Similarity : {self.s_discriminator.similarity(self.frame_text.strip(),self.org_text_emb)}')      \n",
        "        ''' \n",
        "        for index, word in zip(range(len(self.org_term_set)),self.org_term_set):\n",
        "            self.term_table[index] = word\n",
        "   \n",
        "        print('Token table of origin text')\n",
        "        print('---------------------------------------------')\n",
        "        print(' Code     Score        Token              ')\n",
        "        print('')\n",
        "        for k in self.term_table.keys(): \n",
        "            print( f'  {str(k).ljust(5)}   {str(round(self.story_weights[k],4)).ljust(8)}  {self.term_table[k]}')\n",
        "\n",
        "        print('---------------------------------------------') \n",
        "        '''\n",
        "    def get_org_sample(self, num):\n",
        "        return self.org_sentences[np.random.choice(len(self.org_sentences), num)]\n",
        "\n",
        "    def get_source_embedded_code(self):\n",
        "        return self.org_text_emb"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XY59mdNK8ub"
      },
      "source": [
        "##4.5 Generator class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5CLF3WcK6lp",
        "trusted": true
      },
      "source": [
        "from functools import reduce\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "        Simple Generator w/ MLP\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size=1024):\n",
        "        super(Generator, self).__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(input_size, input_size*2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(input_size*2, input_size*3),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(input_size*3, input_size*3),\n",
        "            nn.LeakyReLU(0.2),            \n",
        "            nn.Linear(input_size*3, input_size*2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(input_size*2, input_size),\n",
        "            #nn.BatchNorm1d(term_length*4),\n",
        "            nn.Tanh() # -1 ~ 1\n",
        "        )\n",
        "    '''\n",
        "    def forward(self, x, story_peaks, bias):\n",
        "        #biased_noise = torch.randn(N,_NOISE_DIM)\n",
        "        # stroy peak에 해당하는 term에게 평균값에 해당하는 bias를 추가 한다.\n",
        "                 \n",
        "        y_ = self.layer(x)\n",
        "        y_[:,story_peaks] += bias\n",
        "        y_ = nn.Sigmoid()(y_)\n",
        "        #reduce(torch.add, [y_,bias]) / 2\n",
        "        return y_\n",
        "    '''\n",
        "\n",
        "    \n",
        "    def forward(self, x, bias):\n",
        "        #biased_noise = torch.randn(N,_NOISE_DIM)\n",
        "        # stroy peak에 해당하는 term에게 평균값에 해당하는 bias를 추가 한다.\n",
        "                 \n",
        "        y_ = self.layer(x)\n",
        "        y = torch.add(y_,bias)\n",
        "        #y = nn.Sigmoid()(y)\n",
        "\n",
        "        return y, y_\n",
        "\n",
        "    '''    \n",
        "    def forward(self, x):\n",
        "        #biased_noise = torch.randn(N,_NOISE_DIM)\n",
        "        # stroy peak에 해당하는 term에게 평균값에 해당하는 bias를 추가 한다.\n",
        "                 \n",
        "        y_ = self.layer(x)\n",
        "        #y = torch.add(y_,bias)\n",
        "        y = nn.Sigmoid()(y_)\n",
        "\n",
        "        return y, y_    \n",
        "    '''    "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLVVmQdxLBHZ"
      },
      "source": [
        "##4.6 Summarizer class (GAN training)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nd8GTS7HKz1H",
        "trusted": true
      },
      "source": [
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "from scipy.special import expit\n",
        "\n",
        "class SAM_Summarizer:\n",
        "\n",
        "    def __init__(self,g_discriminator,s_discriminator):\n",
        "        self.g_discriminator = g_discriminator\n",
        "        self.s_discriminator = s_discriminator\n",
        "        self.m = nn.Sigmoid()\n",
        "\n",
        "    def ready(self,source):\n",
        "        self.source = source  \n",
        "        #self.source.analysis_frame_terms(self.s_discriminator)\n",
        "        self.generator = Generator(input_size=self.source.org_source_length)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def summarize(self,epochs=10,batch_size=2,frame_expansion_ratio = 0.8,init_bias = 1.0,learning_rate=2e-4, display = False):\n",
        "        self.frame_expansion_ratio = frame_expansion_ratio\n",
        "        history = self.__train(epochs,batch_size,init_bias,learning_rate,display)\n",
        "        if display:\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            plt.plot(history['gen_g_loss'],label='generator grammar loss')\n",
        "            plt.plot(history['gen_s_loss'],label='generator similarity loss')\n",
        "            #if 'dis_loss' in history:\n",
        "            #    plt.plot(history['dis_loss'],label='discriminator grammar loss')\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "        return self\n",
        "\n",
        "    # text의 생성 for torch\n",
        "    def __text_gen2(self, noise, gen_length):\n",
        "        gtext = []\n",
        "        sorted_noise, i = torch.sort(noise, descending=True)\n",
        "        order, i = torch.sort(i[:gen_length], descending=False)\n",
        "        #print(len(order))\n",
        "        assert len(order) == gen_length\n",
        "        order = order.cpu().detach().numpy()\n",
        "        for k in order:\n",
        "            gtext.append((self.source.term_table[k],k))\n",
        "        return gtext\n",
        "\n",
        "    def __discrete_gradient(self,weights,gen_length,beta,use_gpu=False, verbose=0):\n",
        "        fake_gen_out = torch.zeros(weights.shape).to(device)\n",
        "        fake_sim_out = torch.zeros(weights.shape).to(device)\n",
        "\n",
        "        real_text = self.source.get_org_sample(weights.shape[0])\n",
        "        fake_outs = []\n",
        "        real_outs = []\n",
        "        apply_order = []\n",
        "        for i, noise in enumerate(weights):\n",
        "            gtext = self.__text_gen2(noise,gen_length)\n",
        "            tw = \"\"\n",
        "            tk = []\n",
        "            fake_scores = []\n",
        "            for (w,k) in gtext:\n",
        "                tw += w + ' '\n",
        "                tk.append(k)\n",
        "                if w.endswith('.'):\n",
        "                    fake_outs.append(tw.strip())\n",
        "                    real_outs.append(real_text[i])\n",
        "                    apply_order.append((i,tk))\n",
        "                    tw = \"\"\n",
        "                    tk = []\n",
        "                    \n",
        "            if len(tk) > 0:\n",
        "                fake_outs.append(tw.strip())\n",
        "                real_outs.append(real_text[i])\n",
        "                apply_order.append((i,tk))\n",
        "\n",
        "        D_z_loss, fake_gmr_out=self.g_discriminator.transfer_learning(fake_outs,train_for = False)\n",
        "        D_x_loss, real_gmr_out=self.g_discriminator.transfer_learning(real_outs,train_for = True)   # not use of 'real_gmr_out'\n",
        "\n",
        "        f_sim_out = []\n",
        "        for fake_text in fake_outs:\n",
        "            f_sim_out.append(self.s_discriminator.similarity(fake_text,self.source.org_text_emb))\n",
        "\n",
        "        #if use_gpu:\n",
        "        #    apply_order = torch.FloatTensor(apply_order).to(device)  \n",
        "        \n",
        "        #print(fake_dis_out)\n",
        "        \n",
        "        for j, (i,tk) in enumerate(apply_order):\n",
        "            #fake_gen_out[i,tk] += fake_dis_out[j].numpy() --> 이거는 tf 용...\n",
        "            #fake_gen_out[i,tk] += fake_dis_out[j] #.cpu().detach().numpy()\n",
        "            # \n",
        "            try:\n",
        "                #print('fake_gmr_out:',fake_gmr_out[j,1])\n",
        "                #print('real_gmr_out:',real_gmr_out[j,1])\n",
        "                #fake_gen_out[i,tk] += torch.sigmoid(fake_gmr_out[j,1])\n",
        "\n",
        "                fake_gen_out[i,tk] += torch.tanh( fake_gmr_out[j,1])\n",
        "                fake_sim_out[i,tk] += f_sim_out[j] * beta\n",
        "                \n",
        "            except Exception as ex:\n",
        "                print(j,i,tk)\n",
        "                print(fake_gmr_out)\n",
        "                raise ex\n",
        "\n",
        "        return fake_gen_out, fake_sim_out, D_z_loss, D_x_loss\n",
        "\n",
        "\n",
        "    def __train(self, epochs=10,batch_size=10,init_bias = 1.0,learning_rate=2e-4, display = False):\n",
        "        # In the Deepmind paper they use RMSProp however then Adam optimizer\n",
        "        # improves training time\n",
        "        #generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "        # This method returns a helper function to compute cross entropy loss\n",
        "        #cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "        # Set the seed value all over the place to make this reproducible.\n",
        "        seed_val = 10\n",
        "\n",
        "        random.seed(seed_val)\n",
        "        np.random.seed(seed_val)\n",
        "        torch.manual_seed(seed_val)\n",
        "        torch.cuda.manual_seed_all(seed_val)\n",
        "        \n",
        "        criterion = nn.BCELoss()\n",
        "        #D_opt = torch.optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "        G_opt = torch.optim.Adam(self.generator.parameters(), lr=learning_rate)\n",
        "        D1_opt = AdamW(self.g_discriminator.discriminator.parameters(),\n",
        "                        lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                        eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                        )\n",
        "\n",
        "        \n",
        "        gen_length = len(self.source.story_peaks) + int(len(self.source.story_peaks)*self.frame_expansion_ratio)\n",
        "        pb = ProgressBar(epochs,prefix='Train...')\n",
        "        gen_gmr_loss_history = []\n",
        "        gen_sim_loss_history = []\n",
        "        dis_loss_history = []    \n",
        "\n",
        "        #model 들은 cuda로 보낸다.\n",
        "        self.g_discriminator.discriminator.to(device)\n",
        "        self.g_discriminator.discriminator.eval() # 학습하지 않는다...\n",
        "\n",
        "        self.generator.to(device)       \n",
        "        self.generator.train()\n",
        "\n",
        "        self.bias_w = init_bias\n",
        "        initial_bias = 0\n",
        "        G_s_loss = torch.tensor(0)\n",
        "        G_g_loss = torch.tensor(0)\n",
        "\n",
        "        beta = 1\n",
        "\n",
        "        for i in range(epochs):\n",
        "            '''\n",
        "            noise = torch.randn(batch_size,self.source.org_source_length).to(device)\n",
        "            bias = torch.zeros_like(noise).to(device)\n",
        "            bias[:,self.source.story_peaks] += self.bias_w \n",
        "            with torch.no_grad():        \n",
        "                sw, sw0 = self.generator(noise,bias)\n",
        "\n",
        "            self.g_discriminator.discriminator.train()          #discriminator는 evaluation 모드로 전환\n",
        "            fake_gmr_out, fake_sim_out, D_z_loss, D_x_loss = self.__discrete_gradient(sw,gen_length)\n",
        "            \n",
        "            D_loss = D_x_loss + D_z_loss      \n",
        "\n",
        "            self.g_discriminator.discriminator.zero_grad()\n",
        "            D_loss.backward()\n",
        "            D1_opt.step()\n",
        "            self.g_discriminator.discriminator.eval()\n",
        "            '''\n",
        "            if True:\n",
        "                noise = torch.randn(batch_size,self.source.org_source_length).to(device)\n",
        "                bias = torch.zeros_like(noise).to(device)\n",
        "                bias[:,self.source.story_peaks] += self.bias_w\n",
        "\n",
        "                sw, sw0 = self.generator(noise,bias)\n",
        "\n",
        "                with torch.no_grad():                \n",
        "                    fake_gmr_out, fake_sim_out, D_z_loss, D_x_loss = self.__discrete_gradient(sw,gen_length,beta)\n",
        "                \n",
        "                '''\n",
        "                if int(i/10)%2 == 0:  # grammar와 similarity를 각각 한번씩 교대로 학습한다?\n",
        "                    sw1 = sw * fake_sim_out\n",
        "                    G_s_loss = -torch.mean(sw1)\n",
        "                    G_loss = G_s_loss    \n",
        "                else: #if i%2 == 1:\n",
        "                    sw1 = sw * fake_gmr_out\n",
        "                    G_g_loss = -torch.mean(sw1)\n",
        "                    G_loss = G_g_loss\n",
        "                '''\n",
        "                sw1 = sw * fake_sim_out\n",
        "                G_s_loss = -torch.mean(sw1)\n",
        "                sw2 = sw * fake_gmr_out\n",
        "                G_g_loss = -torch.mean(sw2)\n",
        "\n",
        "                G_loss =  G_g_loss + G_s_loss\n",
        "                \n",
        "                self.generator.zero_grad()\n",
        "                G_loss.backward()\n",
        "                #print('backward:')\n",
        "                G_opt.step()\n",
        "                #self.generator.eval()\n",
        "            #print('step:')\n",
        "            gen_gmr_loss_history.append(G_g_loss.cpu().detach().numpy())\n",
        "            gen_sim_loss_history.append(G_s_loss.cpu().detach().numpy())\n",
        "            #dis_loss_history.append(D_loss.cpu().detach().numpy())\n",
        "\n",
        "            beta = self.m(-(G_g_loss-G_s_loss)*10) * 4\n",
        "\n",
        "            if math.isnan(beta) or beta > 5:\n",
        "                beta = 1\n",
        "\n",
        "            pb.printProgress(+1,f'{i+1}/{epochs} epochs, beta:{beta} Generator / grammar loss:{G_g_loss}   similarity loss:{G_s_loss}') #,   Discriminator grammar_loss:{D_loss}        ')\n",
        "            \n",
        "            \n",
        "        self.generator.eval()\n",
        "        self.g_discriminator.discriminator.eval()\n",
        "        if display:\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            plt.plot(sw0[0].cpu().detach().numpy(),label='before activation weights')\n",
        "            plt.plot(sw[0].cpu().detach().numpy(),label='after activation weights')\n",
        "            plt.plot(bias[0].cpu().detach().numpy(),label='bias weights')\n",
        "            plt.legend()        \n",
        "            plt.show()\n",
        "\n",
        "        return  {'gen_g_loss':gen_gmr_loss_history,'gen_s_loss':gen_sim_loss_history} #,'dis_loss':dis_loss_history }\n",
        "\n",
        "    def get_summary(self, count):\n",
        "        texts = []\n",
        "        self.generator.cpu()\n",
        "        self.generator.eval()\n",
        "        gen_length = len(self.source.story_peaks) + int(len(self.source.story_peaks)*self.frame_expansion_ratio)\n",
        "        noise = torch.randn(count,self.source.org_source_length)\n",
        "        bias = torch.zeros_like(noise)\n",
        "        #bias = torch.randn(1,self.source.org_source_length)\n",
        "        bias[:,self.source.story_peaks] += self.bias_w #self.last_bias_max.cpu().detach().numpy()\n",
        "        #bias = 0\n",
        "        with torch.no_grad():\n",
        "            sw,sw0 = self.generator(noise,bias)\n",
        "            #sw,sw0 = self.generator(noise)\n",
        "\n",
        "        for noise in sw:\n",
        "            gtext = self.__text_gen2(noise,gen_length)\n",
        "            text = ' '.join([w for (w,k) in gtext])\n",
        "            #print(text)\n",
        "            texts.append(text)\n",
        "        return texts"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCdfO9iuLH6D"
      },
      "source": [
        "#5. Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_eAwIPLb4aj"
      },
      "source": [
        "## 비교 대상 요약 알고리즘 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7Ty6_5gb_zR",
        "trusted": true
      },
      "source": [
        "\n",
        "def similarity(query_text, org_text):\n",
        "    sentences = nltk.sent_tokenize(org_text)\n",
        "    #print(\"Num sentences:\", len(sentences))\n",
        "    querys = nltk.sent_tokenize(query_text)\n",
        "    #print(\"Num querys:\", len(querys))\n",
        "\n",
        "    #Compute the sentence embeddings\n",
        "    org_embeddings = s_discriminator._embedder.encode(sentences,show_progress_bar=False)\n",
        "    query_embeddings = s_discriminator._embedder.encode(querys,show_progress_bar=False)\n",
        "\n",
        "    #Compute the pair-wise cosine similarities\n",
        "    cos_scores = scipy.spatial.distance.cdist(query_embeddings, org_embeddings, \"cosine\")\n",
        "    similarity_score = 1.0 - np.mean(np.min(cos_scores,axis=0))\n",
        "\n",
        "    return similarity_score\n",
        "\n",
        "def grammarity(text):\n",
        "    \n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    sentences = np.asarray(nltk.sent_tokenize(text))\n",
        "    # For every sentence...\n",
        "    for sent in sentences:\n",
        "        # `encode_plus` will:\n",
        "        #   (1) Tokenize the sentence.\n",
        "        #   (2) Prepend the `[CLS]` token to the start.\n",
        "        #   (3) Append the `[SEP]` token to the end.\n",
        "        #   (4) Map tokens to their IDs.\n",
        "        #   (5) Pad or truncate the sentence to `max_length`\n",
        "        #   (6) Create attention masks for [PAD] tokens.\n",
        "        encoded_dict = g_discriminator.tokenizer.encode_plus(\n",
        "                            sent,                      # Sentence to encode.\n",
        "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                            max_length = 64,           # Pad & truncate all sentences.\n",
        "                            pad_to_max_length = True,\n",
        "                            return_attention_mask = True,   # Construct attn. masks.\n",
        "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                            truncation = True,\n",
        "                       )\n",
        "        # Add the encoded sentence to the list.    \n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "        # And its attention mask (simply differentiates padding from non-padding).\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    # Convert the lists into tensors.\n",
        "    input_ids = torch.cat(input_ids, dim=0).to(device)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0).to(device)\n",
        "    g_discriminator.discriminator.to(device)\n",
        "    #if str(discriminator1.device) == 'cpu':\n",
        "    #    pass\n",
        "    #else:\n",
        "    #    input_ids = input_ids.to(device)\n",
        "    #    attention_masks = attention_masks.to(device)        \n",
        "\n",
        "    with torch.no_grad():        \n",
        "        outputs = g_discriminator.discriminator(input_ids, \n",
        "                               token_type_ids=None, \n",
        "                               attention_mask=attention_masks)\n",
        "    #return torch.sigmoid(outputs[0][:,1])\n",
        "    return torch.mean(outputs[0][:,1]).detach().cpu().numpy()\n",
        "    #return outputs"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLbWuwKXcMyk",
        "trusted": true
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def evaluate(method_name, text, g_summ, org_text_1,org_text_2,org_text_3):\n",
        "    result = {}\n",
        "    result['method'] = [method_name]\n",
        "    org_text = org_text_1 + ' ' + org_text_2 + ' ' + org_text_3\n",
        "    result['comp ratio'] = [len(text)/len(org_text)]\n",
        "    result['intro'] = [similarity(text,org_text_1)]\n",
        "    result['body'] = [similarity(text,org_text_2)]\n",
        "    result['ending'] = [similarity(text,org_text_3)]\n",
        "    result['var'] = [np.var([result['intro'][0],result['body'][0],result['ending'][0]])]\n",
        "    result['total'] = [similarity(text,org_text)]\n",
        "    result['grammar'] = [np.tanh(float(grammarity(text)))]\n",
        "    #scores = scorer.score(g_summ,text)\n",
        "    #result['R1'] = [scores['rouge1'].fmeasure]\n",
        "    #result['R2'] = [scores['rouge2'].fmeasure]\n",
        "    #result['RL'] = [scores['rougeL'].fmeasure]\n",
        "    return pd.DataFrame(result),result"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utotZ2vLcSSO",
        "trusted": true
      },
      "source": [
        "\"\"\"\n",
        "LexRank implementation\n",
        "Source: https://github.com/crabcamp/lexrank/tree/dev\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from scipy.sparse.csgraph import connected_components\n",
        "\n",
        "def degree_centrality_scores(\n",
        "    similarity_matrix,\n",
        "    threshold=None,\n",
        "    increase_power=True,\n",
        "):\n",
        "    if not (\n",
        "        threshold is None\n",
        "        or isinstance(threshold, float)\n",
        "        and 0 <= threshold < 1\n",
        "    ):\n",
        "        raise ValueError(\n",
        "            '\\'threshold\\' should be a floating-point number '\n",
        "            'from the interval [0, 1) or None',\n",
        "        )\n",
        "\n",
        "    if threshold is None:\n",
        "        markov_matrix = create_markov_matrix(similarity_matrix)\n",
        "\n",
        "    else:\n",
        "        markov_matrix = create_markov_matrix_discrete(\n",
        "            similarity_matrix,\n",
        "            threshold,\n",
        "        )\n",
        "\n",
        "    scores = stationary_distribution(\n",
        "        markov_matrix,\n",
        "        increase_power=increase_power,\n",
        "        normalized=False,\n",
        "    )\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "def _power_method(transition_matrix, increase_power=True):\n",
        "    eigenvector = np.ones(len(transition_matrix))\n",
        "\n",
        "    if len(eigenvector) == 1:\n",
        "        return eigenvector\n",
        "\n",
        "    transition = transition_matrix.transpose()\n",
        "\n",
        "    while True:\n",
        "        eigenvector_next = np.dot(transition, eigenvector)\n",
        "\n",
        "        if np.allclose(eigenvector_next, eigenvector):\n",
        "            return eigenvector_next\n",
        "\n",
        "        eigenvector = eigenvector_next\n",
        "\n",
        "        if increase_power:\n",
        "            transition = np.dot(transition, transition)\n",
        "\n",
        "\n",
        "def connected_nodes(matrix):\n",
        "    _, labels = connected_components(matrix)\n",
        "\n",
        "    groups = []\n",
        "\n",
        "    for tag in np.unique(labels):\n",
        "        group = np.where(labels == tag)[0]\n",
        "        groups.append(group)\n",
        "\n",
        "    return groups\n",
        "\n",
        "\n",
        "def create_markov_matrix(weights_matrix):\n",
        "    n_1, n_2 = weights_matrix.shape\n",
        "    if n_1 != n_2:\n",
        "        raise ValueError('\\'weights_matrix\\' should be square')\n",
        "\n",
        "    row_sum = weights_matrix.sum(axis=1, keepdims=True)\n",
        "\n",
        "    return weights_matrix / row_sum\n",
        "\n",
        "\n",
        "def create_markov_matrix_discrete(weights_matrix, threshold):\n",
        "    discrete_weights_matrix = np.zeros(weights_matrix.shape)\n",
        "    ixs = np.where(weights_matrix >= threshold)\n",
        "    discrete_weights_matrix[ixs] = 1\n",
        "\n",
        "    return create_markov_matrix(discrete_weights_matrix)\n",
        "\n",
        "\n",
        "def graph_nodes_clusters(transition_matrix, increase_power=True):\n",
        "    clusters = connected_nodes(transition_matrix)\n",
        "    clusters.sort(key=len, reverse=True)\n",
        "\n",
        "    centroid_scores = []\n",
        "\n",
        "    for group in clusters:\n",
        "        t_matrix = transition_matrix[np.ix_(group, group)]\n",
        "        eigenvector = _power_method(t_matrix, increase_power=increase_power)\n",
        "        centroid_scores.append(eigenvector / len(group))\n",
        "\n",
        "    return clusters, centroid_scores\n",
        "\n",
        "\n",
        "def stationary_distribution(\n",
        "    transition_matrix,\n",
        "    increase_power=True,\n",
        "    normalized=True,\n",
        "):\n",
        "    n_1, n_2 = transition_matrix.shape\n",
        "    if n_1 != n_2:\n",
        "        raise ValueError('\\'transition_matrix\\' should be square')\n",
        "\n",
        "    distribution = np.zeros(n_1)\n",
        "\n",
        "    grouped_indices = connected_nodes(transition_matrix)\n",
        "\n",
        "    for group in grouped_indices:\n",
        "        t_matrix = transition_matrix[np.ix_(group, group)]\n",
        "        eigenvector = _power_method(t_matrix, increase_power=increase_power)\n",
        "        distribution[group] = eigenvector\n",
        "\n",
        "    if normalized:\n",
        "        distribution /= n_1\n",
        "\n",
        "    return distribution"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M25NP9gOeX15"
      },
      "source": [
        "\n",
        "* Hands-on Guide To Extractive Text Summarization With BERTSum<br>\n",
        "https://analyticsindiamag.com/hands-on-guide-to-extractive-text-summarization-with-bertsum/ <br>\n",
        "https://pypi.org/project/bert-extractive-summarizer/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9oEW5wyeI9C",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7144f4e9-ba57-4060-93c1-293010a42111"
      },
      "source": [
        "!pip install bert-extractive-summarizer"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-extractive-summarizer\n",
            "  Downloading https://files.pythonhosted.org/packages/1a/07/fdb05f9e18b6f641499ef56737126fbd2fafe1cdc1a04ba069d5aa205901/bert_extractive_summarizer-0.7.1-py3-none-any.whl\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (from bert-extractive-summarizer) (3.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from bert-extractive-summarizer) (0.22.2.post1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from bert-extractive-summarizer) (2.2.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (2.23.0)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (0.8.1rc1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (3.0.12)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (0.1.95)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (0.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (4.41.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (3.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (0.8.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (2.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (54.1.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (0.4.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->bert-extractive-summarizer) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->bert-extractive-summarizer) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->bert-extractive-summarizer) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->bert-extractive-summarizer) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->bert-extractive-summarizer) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->bert-extractive-summarizer) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers->bert-extractive-summarizer) (2.4.7)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->bert-extractive-summarizer) (3.7.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->bert-extractive-summarizer) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->bert-extractive-summarizer) (3.4.1)\n",
            "Installing collected packages: bert-extractive-summarizer\n",
            "Successfully installed bert-extractive-summarizer-0.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJb2hLPRPp7Q",
        "trusted": true
      },
      "source": [
        "def bert_lexrank_sum(g_summ,org_text,n_top=4):\n",
        "    input_text = org_text[0] + org_text[1] + org_text[2]\n",
        "    #Split the document into sentences\n",
        "    sentences = nltk.sent_tokenize(input_text)\n",
        "    #print(\"Num sentences:\", len(sentences))\n",
        "\n",
        "    #Compute the sentence embeddings\n",
        "    embeddings = s_discriminator._embedder.encode(sentences,show_progress_bar=False)\n",
        "\n",
        "    #Compute the pair-wise cosine similarities\n",
        "    cos_scores = scipy.spatial.distance.cdist(embeddings, embeddings, \"cosine\")\n",
        "    #util.pytorch_cos_sim(embeddings, embeddings).numpy()\n",
        "    #print(cos_scores)\n",
        "    #Compute the centrality for each sentence\n",
        "    centrality_scores = degree_centrality_scores(cos_scores, threshold=None)\n",
        "\n",
        "    #We argsort so that the first element is the sentence with the highest score\n",
        "    most_central_sentence_indices = np.argsort(-centrality_scores)\n",
        "\n",
        "    #Print the 5 sentences with the highest scores\n",
        "    summary_text = \"\"\n",
        "    for idx in most_central_sentence_indices[0:n_top]:\n",
        "        summary_text += sentences[idx].strip()\n",
        "    print('bert_lexrank summary:')\n",
        "    print(summary_text)\n",
        "    print('-'*50)\n",
        "    df,arr = evaluate('BERT+LexRank',summary_text,g_summ,org_text[0],org_text[1],org_text[2])\n",
        "    return df,arr\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcDVXR4XQZAh",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166,
          "referenced_widgets": [
            "0c7a13599148468c8a07d3bc085210cc",
            "9956ad7af56c4add9533462fd832757f",
            "8df25b5b726840cfa400b8a78a1d5b7a",
            "820a1ebe60d644a5a20075433fa9046c",
            "055aee5192b3423ab1215ebc874ca8d0",
            "3f844ba88420437ea5a4933c83193bd6",
            "29058fd2d4ec4f94b69f0469bd6edd3d",
            "e5b9d736be814592a1c5fd2d42a9c609",
            "9068e6c83e584024a44275b0fe832350",
            "66b2998258ea4505b27378335b562aa7",
            "d070c067d1fc4defa18f560be78a2b76",
            "1cd2a9a1e7b244ec92d690347054184c",
            "3fda91dc4c25482db3eff80195047663",
            "2cb63e40757445149c3dccd2cb00652d",
            "01f7226bda27447380de090a4eab7dfb",
            "38d97695f22043dfbd2843669153ab48",
            "9e0c84afa8ab4d0a9f2ac49ce1eb4da9",
            "60782b1b9b02436ca4a6a55df17d1b57",
            "37223d9428e94b6abd20f3ed76464584",
            "a66c848be955443185fec1a10c962e78",
            "e65098e6bea24e7b9421e01faa17e58d",
            "83d4e28d7ee34108989be64bb135270b",
            "d0554a5520564a6b8ff006e18104f0f5",
            "660cb282966b475bba9690f9768d4e8b"
          ]
        },
        "outputId": "b7f78a95-f370-46b8-d0b9-7c18db854aae"
      },
      "source": [
        "\n",
        "from summarizer import Summarizer\n",
        "\n",
        "\n",
        "model1 = Summarizer()\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c7a13599148468c8a07d3bc085210cc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=434.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9068e6c83e584024a44275b0fe832350",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1344997306.0, style=ProgressStyle(descr…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e0c84afa8ab4d0a9f2ac49ce1eb4da9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wqR1PeSIR4V"
      },
      "source": [
        "\n",
        "def besm(g_summ,org_text):\n",
        "    result = model1(org_text[0] + org_text[1] + org_text[2], num_sentences=2)\n",
        "    summary_text = \"\".join(result)\n",
        "    print('besm summary:')\n",
        "    print(summary_text)\n",
        "    print('-'*50)    \n",
        "    df,arr = evaluate('BESM',summary_text,g_summ,org_text[0],org_text[1],org_text[2])\n",
        "    return df,arr"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "T24x_-DPi1gV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216,
          "referenced_widgets": [
            "5a23562870cd447898b7fdfc5f56e29e",
            "c48eb29505b54fd3a92a98d4021386b5",
            "ca157a4cfe6f4acdbeac9bcc089968f1",
            "505fbc1a1fa94d55ac24a2084ac5d270",
            "c0449b7329e241f08e440fac1d31cd2e",
            "e6bc669ac021423aa9f342a3f3e1f14c",
            "da36499eb20b4c80b51044267bb8ef23",
            "b70d62f11b4447cb9640526c04a04d8c",
            "cd0fbe93c5c94b00a20c2204ac40b04b",
            "382b240cdf784fc3a19fbb68ffd94f9d",
            "df97922dc1e0414a96afcabbafab0ec1",
            "2afe6907b10e4d28914f28a11ce96610",
            "8eabe11ee69d49c6ae0d3ce767f79292",
            "b8e8ab587bd04a3b95b99de909773a7a",
            "d74b3fcdb038469c98294aaa52e28c75",
            "e8e588931da14f99b576d3a9186d552c",
            "a23196285bdb4205b8642028fbf1e092",
            "e12c52f295284cf2a17b179051c6fa76",
            "6a4d973f70c044ee8651f46f03b77ea4",
            "3e30ad42ad4a44d4b6e362f5eb9b7197",
            "99288b64736a4c55a99d42929b9a9e92",
            "72dedb2fae35415999123f27fa6dd7a8",
            "9113b8b6e9d84b2f9486c4f0bdb81016",
            "68e32144b52144258fd258580772594c",
            "abce3174c791495681215b3e1cea6956",
            "49460bef094f4ac8acdb6633390d2bf0",
            "2f799e10b4b542f7b24b67f341acf8a1",
            "87d91565eb2c410982a81c1a30878c7c",
            "e352de75553f46668c47e170f3779148",
            "17a2c32438454254b76593737cbb8b7d",
            "c0d8b1e6e9f44111ab0dacc00fcfad8b",
            "4a1b357341944f62974fd918fedb0c71"
          ]
        },
        "outputId": "48bbe5f9-e691-4ab5-c81b-c243c50fe61d"
      },
      "source": [
        "from transformers import AutoConfig,AutoTokenizer,AutoModel\n",
        "\n",
        "SQUAD_MODEL = \"monologg/kobert\"\n",
        "\n",
        "#SQUAD_MODEL = \"bert-large-uncased\"\n",
        "# Load model, model config and tokenizer via Transformers\n",
        "custom_config = AutoConfig.from_pretrained(SQUAD_MODEL)\n",
        "custom_config.output_hidden_states=True\n",
        "custom_tokenizer = AutoTokenizer.from_pretrained(SQUAD_MODEL)\n",
        "custom_model = AutoModel.from_pretrained(SQUAD_MODEL, config=custom_config)\n",
        "\n",
        "model2 = Summarizer(custom_model=custom_model, custom_tokenizer=custom_tokenizer)\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a23562870cd447898b7fdfc5f56e29e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=426.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd0fbe93c5c94b00a20c2204ac40b04b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=77779.0, style=ProgressStyle(descriptio…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a23196285bdb4205b8642028fbf1e092",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=51.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "abce3174c791495681215b3e1cea6956",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=368792146.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6xQHXrUZ9K_"
      },
      "source": [
        "def besm_bert(g_summ,org_text):\n",
        "    result = model2(org_text[0].lower() + org_text[1].lower() + org_text[2].lower(), num_sentences=2)\n",
        "    summary_text = \"\".join(result)\n",
        "    print('besm_bert summary:')\n",
        "    print(summary_text)\n",
        "    print('-'*50)      \n",
        "    df,arr = evaluate('BESM+kobert',summary_text,g_summ,org_text[0],org_text[1],org_text[2])\n",
        "    return df,arr"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "0S301yeelEvG"
      },
      "source": [
        "org_text_1 = \"\"\"\n",
        "옛날 어느 집에 귀여운 여자 아기가 태어났어요.\n",
        "아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요.\n",
        "그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요.\n",
        "소녀의 아버지는 홀로 남은 소녀가 걱정되었어요.\n",
        "그래서 얼마 후 새어머니를 맞이했어요.\n",
        "새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요.\n",
        "그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요.\n",
        "새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요.\n",
        "그런데 이번에는 아버지마저 돌아가셨어요.\n",
        "소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요.\n",
        "해도 해도 끝이 없는 집안일이 힘들어 지칠때면\n",
        "난롯가에 앉아서 잠시 쉬곤 했지요.\n",
        "\"\"\"\n",
        "\n",
        "org_text_2 = \"\"\"\n",
        "어느 날, 왕궁에서 무도회가 열렸어요.\n",
        "신데렐라의 집에도 초대장이 왔어요.\n",
        "새어머니는 언니들을 데리고 무도회장으로 떠났어요.\n",
        "신데렐라도 무도회에 가고 싶었어요.\n",
        "혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요.\n",
        "신데렐라, 너도 무도회에 가고 싶니?\n",
        "신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요.\n",
        "내가 너를 무도회에 보내주마 호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴.\n",
        "마법사 할머니가 주문을 외웠어요.\n",
        "그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요.\n",
        "이번에는 생쥐와 도마뱀을 건드렸어요.\n",
        "그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다.\n",
        "신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요.\n",
        "신데렐라, 발을 내밀어 보거라.\n",
        "할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요.\n",
        "신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼. 그러니까 반드시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지?\n",
        "왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요.\n",
        "왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,신데렐라하고만 춤을 추었어요.\n",
        "신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요.\n",
        "땡, 땡, 땡...... 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요.\n",
        "신데렐라가 허둥지둥 왕궁을 빠져나가는데,\n",
        "유리 구두 한 짝이 벗겨졌어요.\n",
        "하지만 구두를 주울 틈이 없었어요.\n",
        "신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요.\n",
        "왕자님은 유리 구두를 가지고 임금님께 가서 말했어요.\n",
        "이 유리 구두의 주인과 결혼하겠어요.\n",
        "\"\"\"\n",
        "\n",
        "org_text_3 = \"\"\"\n",
        "그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요.\n",
        "언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리 구두는 너무 작았어요.\n",
        "그때, 신데렐라가 조용히 다가와 말했어요.\n",
        "저도 한번 신어 볼 수 있나요?\n",
        "신데렐라는 신하게 건넨 유리 구두를 신었어요,\n",
        "유리 구두는 신데렐라의 발에 꼭 맞았어요.\n",
        "신하들은 신데렐라를 왕궁으로 데리고 갔어요.\n",
        "그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\n",
        "\"\"\""
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "tRZIwu73lEvH",
        "outputId": "05ec310a-bc76-4823-d2e8-825b492df378",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 기,승,전,결  모두 비슷한 내용이 반복되니... 지협성의 문제가 나타나지 않는다.\n",
        "ko_sentences_dataset2 = []\n",
        "\n",
        "if True:\n",
        "    urls = ['https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-ABC%20%EC%82%B4%EC%9D%B8%EC%82%AC%EA%B1%B4.txt',\n",
        "            'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EA%B7%B8%EB%A6%AC%EA%B3%A0%20%EC%95%84%EB%AC%B4%EB%8F%84%20%EC%97%86%EC%97%88%EB%8B%A4.txt',\n",
        "            'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EB%82%98%EC%9D%BC%EA%B0%95%EC%9D%98%20%EC%A3%BD%EC%9D%8C.txt',\n",
        "            'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EB%A7%8C%EC%B0%AC%ED%9A%8C%EC%9D%98%2013%EC%9D%B8.txt',\n",
        "            'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EB%A9%94%EC%86%8C%ED%8F%AC%ED%83%80%EB%AF%B8%EC%95%84%EC%9D%98%20%EC%A3%BD%EC%9D%8C.txt',\n",
        "            'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EB%AA%A9%EC%82%AC%EA%B4%80%EC%82%B4%EC%9D%B8.txt',\n",
        "            'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EB%B2%99%EC%96%B4%EB%A6%AC%20%EB%AA%A9%EA%B2%A9%EC%9E%90.txt',\n",
        "            'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EB%B9%84%EB%B0%80%20%EC%84%9C%EB%A5%98%EB%A5%BC%20%EB%85%B8%EB%A0%A4%EB%9D%BC.txt',\n",
        "            'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EC%8A%A4%ED%8E%98%EC%9D%B8%EA%B6%A4%EC%A7%9D%EC%9D%98%20%EB%B9%84%EB%B0%80.txt',\n",
        "            'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EC%95%84%ED%8C%8C%ED%8A%B8%EC%97%90%20%EB%82%98%ED%83%80%EB%82%9C%20%EC%9A%94%EC%A0%95.txt',\n",
        "            'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EC%95%A0%ED%81%AC%EB%A1%9C%EC%9D%B4%EB%93%9C%20%EC%82%B4%EC%9D%B8%20%EC%82%AC%EA%B1%B4.txt',\n",
        "            'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EC%98%88%EA%B3%A0%20%EC%82%B4%EC%9D%B8.txt',\n",
        "            'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EC%A5%90%EB%8D%AB.txt',\n",
        "            'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EC%BB%A4%ED%8A%BC.txt',\n",
        "            'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%ED%81%AC%EB%A6%AC%EC%8A%A4%EB%A7%88%EC%8A%A4%20%ED%91%B8%EB%94%A9%EC%9D%98%20%EB%AA%A8%ED%97%98.txt',\n",
        "            'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%ED%91%B8%EB%A5%B8%EC%97%B4%EC%B0%A8%EC%9D%98%EC%A3%BD%EC%9D%8C.txt',\n",
        "            'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%ED%99%94%EC%9A%94%EC%9D%BC%20%ED%81%B4%EB%9F%BD%EC%9D%98%20%EC%82%B4%EC%9D%B8.txt']\n",
        "\n",
        "    \n",
        "    for url in urls:\n",
        "        raw_text = urllib.request.urlopen(url).read().decode('utf-8')\n",
        "        ko_sentences_dataset2 += nltk.sent_tokenize(clean_text(raw_text))\n",
        "    #random.shuffle(ko_sentences_dataset2)\n",
        "else:\n",
        "    # 각 문장이 유사하지 않도록 구성한다. --> 그래야 지협성의 문제 대두??\n",
        "    df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/summary/korean_sentences.csv')\n",
        "    ko_sentences_dataset2 += list(df['sentence'])\n",
        "    random.shuffle(ko_sentences_dataset2)\n",
        "    \n",
        "len(ko_sentences_dataset2)    "
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "95889"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3run5EVvMSiB",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4aa085c7-5bed-464e-8988-7d85de923ee6"
      },
      "source": [
        "document = []\n",
        "offset = 0\n",
        "#document += [[org_text_1,org_text_2,org_text_3]]\n",
        "while (offset < len(ko_sentences_dataset2)):\n",
        "    intro_cnt = 2*2 #random.choice([5,8,10,13])\n",
        "    body_cnt = 5*2 #random.choice([10,15,18,20,25])\n",
        "    conclu_cnt = 3*2 #random.choice([5,8,10,13])\n",
        "    intro = ' '.join(ko_sentences_dataset2[offset:offset+intro_cnt])\n",
        "    body = ' '.join(ko_sentences_dataset2[offset+intro_cnt:offset+intro_cnt+body_cnt])\n",
        "    conclu = ' '.join(ko_sentences_dataset2[offset+intro_cnt+body_cnt:offset+intro_cnt+body_cnt+conclu_cnt])\n",
        "    offset = offset+intro_cnt+body_cnt+conclu_cnt\n",
        "    document.append([intro,body,conclu])\n",
        "\n",
        "print(len(document))\n",
        "\n",
        "document[0]"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4795\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['이 이야기에서는 내가 직접 입회한 사건이나 장면만을 이야기하는 전의 내 방법을 바꿔 보았다. 그래서 몇몇 장은 3인칭으로 씌어 있다. 이제부터의 각 장에서 이야기되는 사건들은 모두 내가 확증 할 수 있었던 것임을 밝혀둔다. 여러 인물들의 생각이나 감정을 서술하는 데 있어 얼마쯤 내가 시인의 특권을 행사했다 해도 그것은 아주 정확을 기해서 한 일이다.',\n",
              " '또한 그것들은 모두 내 친구 에르큘 포아로의 검토를 받았음을 덧붙여 둔다. 끝으로, 나는 이 이상한 연쇄 범죄의 결과로서 일어나는 부차적인 인간관계에 대해 너무 많은 이야기를 했는지도 모른다. 하지만 인간적, 개인적 요소란 빠뜨려선 안 되는 것이다. 에르큘 포아로가 언젠가 과장된 몸짓으로 나에게 가르쳐 준 일이 있다. 로맨스란 범죄의 부산물일 경우가 있다고. ABC 수수께끼의 해결에 대해 말한다면, 에르큘 포아로는 이제까지 그가 다뤄 온 어느 사건과도 다른 방법으로 문제에 뛰어들어 그 진정한 천재성을 발휘했다고 말해도 좋으리라. < 편지 > 1935년 6월, 나는 남아메리카의 내 농장에서 떠나 여섯 달쯤 머무를 예정으로 귀국했다. 그때는 어려웠던 시대로, 다른 사람들과 마찬가지로 우리 역시 세계적인 불황에 어려움을 겪고 있었다. 영국에서 나 자신이 손대지 않으면 도저히 잘되어 나가지 않을 것 같은 볼일이 여러 가지 있었다. 농장 관리를 위해 아내가 뒤에 남았다.',\n",
              " '영국에 와 닿아 내가 맨 먼저 한 일의 하나는 말할 나위도 없이 오랜 친구인 에르큘 포아로를 찾아간 것이었다. 그는 런던의 어떤 최신형 아파트에 살고 있었다. 내가 그것을 지적하며, 그가 이 특별한 건물을 고른 것은 완전히 그 기하학적이 겉모습과 넓이 때문일 거라고 말하자 그는 고개를 끄덕였다. “그러나 아주 기분 좋게 균형이 잡혀 있지. 그렇게 생각되지 않나?” 나는 좀 너무 모난 것같이 생각된다고 말했다. 그리고 오래된 농담이 생각나 이 아파트에서는 암탉에게 네모난 달걀을 낳게 할 수 있을 듯하다고 말했다.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "rcvSSH_Ri1gZ"
      },
      "source": [
        "def sam_wgan(g_summ,org_text,init_bias=0.0, display = False):\n",
        "    source = Source(org_text[0] + org_text[1] + org_text[2])\n",
        "    comp_rate=0.05\n",
        "    if init_bias > 0:\n",
        "        source.analysis_frame_terms(s_discriminator,comp_rate=comp_rate,except_key=True,display=display)\n",
        "    else:\n",
        "        #source.extract_keywords(s_discriminator,key_model, comp_rate=0.1)\n",
        "        source.set_key_rate(s_discriminator,comp_rate=comp_rate)\n",
        "    summarizer = SAM_Summarizer(g_discriminator,s_discriminator)\n",
        "    summarizer.ready(source)\n",
        "    summarizer.summarize(epochs=200,batch_size=1,frame_expansion_ratio = 2.0, init_bias=init_bias,learning_rate=5e-5,display=display)\n",
        "    summary_text = summarizer.get_summary(3)[0]\n",
        "    print('-'*50)\n",
        "    print('gold summary:')\n",
        "    print(g_summ)    \n",
        "    print('-'*50)\n",
        "    print('sam_wgan summary:')\n",
        "    print(summary_text)\n",
        "    print('-'*50)\n",
        "    df,arr = evaluate('SAM+WGAN',summary_text,g_summ,org_text[0],org_text[1],org_text[2])\n",
        "    return df,arr"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCxKl8YVgF6F"
      },
      "source": [
        "Test용 Data 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "r9HG9E8blEvJ"
      },
      "source": [
        "def seeding(seed):\n",
        "\n",
        "    SEED = seed\n",
        "\n",
        "    random.seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDK19hrh-TeV"
      },
      "source": [
        "## 신데렐라 Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvigjmChLVPb",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "outputId": "042901d8-c194-4742-e927-eeac37a3219c"
      },
      "source": [
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "\n",
        "\n",
        "\n",
        "num = 0\n",
        "\n",
        "for i in range(10):\n",
        "    for j in range(5):\n",
        "        seeding(i+j)\n",
        "        df1,_ = sam_wgan('',document[num],init_bias=0.1*i,display= False)\n",
        "        #df2,_ = bert_lexrank_sum('',[org_text_1,org_text_2,org_text_3])\n",
        "        #df3,_ = besm('',[org_text_1,org_text_2,org_text_3])\n",
        "        #df4,_ = besm_bert('',[org_text_1,org_text_2,org_text_3])\n",
        "        #df5,_ = abstract_method_1(g_summ,[org_text_1,org_text_2,org_text_3])\n",
        "        #df6,_ = abstract_method_2(g_summ,[org_text_1,org_text_2,org_text_3])\n",
        "        #result = pd.concat([df1, df2, df3, df4], ignore_index=True)\n",
        "        #result\n",
        "        print(df1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------\n",
            "옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요. 그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요. 소녀의 아버지는 홀로 남은 소녀가 걱정되었어요. 그래서 얼마 후 새어머니를 맞이했어요. 새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요. 그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요. 새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요. 그런데 이번에는 아버지마저 돌아가셨어요. 소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시 쉬곤 했지요. 어느 날, 왕궁에서 무도회가 열렸어요. 신데렐라의 집에도 초대장이 왔어요. 새어머니는 언니들을 데리고 무도회장으로 떠났어요. 신데렐라도 무도회에 가고 싶었어요. 혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요. 신데렐라, 너도 무도회에 가고 싶니? 신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요. 내가 너를 무도회에 보내주마 호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴. 마법사 할머니가 주문을 외웠어요. 그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요. 이번에는 생쥐와 도마뱀을 건드렸어요. 그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다. 신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요. 신데렐라, 발을 내밀어 보거라. 할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요. 신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼. 그러니까 반드시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지? 왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요. 왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,신데렐라하고만 춤을 추었어요. 신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요. 땡, 땡, 땡 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요. 신데렐라가 허둥지둥 왕궁을 빠져나가는데, 유리 구두 한 짝이 벗겨졌어요. 하지만 구두를 주울 틈이 없었어요. 신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요. 왕자님은 유리 구두를 가지고 임금님께 가서 말했어요. 이 유리 구두의 주인과 결혼하겠어요. 그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요. 언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리 구두는 너무 작았어요. 그때, 신데렐라가 조용히 다가와 말했어요. 저도 한번 신어 볼 수 있나요? 신데렐라는 신하게 건넨 유리 구두를 신었어요, 유리 구두는 신데렐라의 발에 꼭 맞았어요. 신하들은 신데렐라를 왕궁으로 데리고 갔어요. 그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\n",
            "------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-d98a95a8a359>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mseeding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mdf1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msam_wgan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minit_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;31m#df2,_ = bert_lexrank_sum('',[org_text_1,org_text_2,org_text_3])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m#df3,_ = besm('',[org_text_1,org_text_2,org_text_3])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-fa8aca854f49>\u001b[0m in \u001b[0;36msam_wgan\u001b[0;34m(g_summ, org_text, init_bias, display)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0msummarizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSAM_Summarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_discriminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms_discriminator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0msummarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0msummarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mframe_expansion_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0msummary_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-832cf7329490>\u001b[0m in \u001b[0;36msummarize\u001b[0;34m(self, epochs, batch_size, frame_expansion_ratio, init_bias, learning_rate, display)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mframe_expansion_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minit_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_expansion_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe_expansion_ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minit_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-832cf7329490>\u001b[0m in \u001b[0;36m__train\u001b[0;34m(self, epochs, batch_size, init_bias, learning_rate, display)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m                     \u001b[0mfake_gmr_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_sim_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_z_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_x_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__discrete_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgen_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                 '''\n",
            "\u001b[0;32m<ipython-input-24-832cf7329490>\u001b[0m in \u001b[0;36m__discrete_gradient\u001b[0;34m(self, weights, gen_length, beta, use_gpu, verbose)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mf_sim_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfake_text\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfake_outs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mf_sim_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_discriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg_text_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m#if use_gpu:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-fe99af6fcd77>\u001b[0m in \u001b[0;36msimilarity\u001b[0;34m(self, query_text, org_text_emb)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morg_text_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mqueries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mquery_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embedder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;31m#query_embeddings = self._embedder.encode(queries,show_progress_bar=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m#print(queries)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, is_pretokenized)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                 \u001b[0mout_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m                 \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput_value\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sentence_transformers/models/XLMRoBERTa.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;34m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m#RoBERTa does not use token_type_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0moutput_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlm_roberta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0moutput_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mcls_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# CLS token is first token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_extended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m         )\n\u001b[1;32m    764\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    437\u001b[0m                     \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m                 )\n\u001b[1;32m    441\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    369\u001b[0m     ):\n\u001b[1;32m    370\u001b[0m         self_attention_outputs = self.attention(\n\u001b[0;32m--> 371\u001b[0;31m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m         )\n\u001b[1;32m    373\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    313\u001b[0m     ):\n\u001b[1;32m    314\u001b[0m         self_outputs = self.self(\n\u001b[0;32m--> 315\u001b[0;31m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m         )\n\u001b[1;32m    317\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0mmixed_value_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0mquery_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmixed_query_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0mkey_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmixed_key_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mvalue_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmixed_value_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mtranspose_for_scores\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mnew_x_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_attention_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_x_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     def forward(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8z4bfSR-atB"
      },
      "source": [
        "## 한국어 Sample Test (No frame token)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "OhUFVVRki1ga"
      },
      "source": [
        "import sys\n",
        "\n",
        "def get_features(dct1):\n",
        "    return [dct1['comp ratio'][0],dct1['intro'][0],dct1['body'][0],dct1['ending'][0],dct1['var'][0],dct1['total'][0],dct1['grammar'][0]]\n",
        "\n",
        "test_result = {}\n",
        "test_result['SAM+WGAN']=[]\n",
        "test_result['BERT+LexRank']=[]\n",
        "test_result['BESM']=[]\n",
        "test_result['BESM+kobert']=[]\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "R0yLlIo3i1ga"
      },
      "source": [
        "def get_test_statistics(test_result):\n",
        "    df_data = {}\n",
        "    df_data['method'] = []\n",
        "    df_data['comp rate'] = []\n",
        "    df_data['intro'] = []\n",
        "    df_data['body'] = []\n",
        "    df_data['conclusion'] = []\n",
        "    df_data['isthmus'] = []\n",
        "    df_data['simlirality'] = []\n",
        "    df_data['grammarity'] = []\n",
        "\n",
        "    for key in test_result:\n",
        "        df_data['method'].append(key)\n",
        "        data = np.asarray(test_result[key])\n",
        "        df_data['comp rate'].append(np.mean(data[:,0]))\n",
        "        df_data['intro'].append(np.mean(data[:,1]))\n",
        "        df_data['body'].append(np.mean(data[:,2]))\n",
        "        df_data['conclusion'].append(np.mean(data[:,3]))\n",
        "        df_data['isthmus'].append(np.mean(data[:,4]))\n",
        "        df_data['simlirality'].append(np.mean(data[:,5]))\n",
        "        df_data['grammarity'].append(np.mean(data[:,6]))\n",
        "\n",
        "\n",
        "    df = pd.DataFrame(df_data)\n",
        "    return df"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "8E5m2SVti1gb"
      },
      "source": [
        "def prepare_data(offset,length):\n",
        "    return document[offset:offset+length]\n",
        "\n",
        "\n",
        "ko_docs = prepare_data(0,70)\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "wOfI-ih2i1gb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "378f4863-e0a3-4433-f007-801de5b84f16"
      },
      "source": [
        "ko_docs[0]"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['이 이야기에서는 내가 직접 입회한 사건이나 장면만을 이야기하는 전의 내 방법을 바꿔 보았다. 그래서 몇몇 장은 3인칭으로 씌어 있다. 이제부터의 각 장에서 이야기되는 사건들은 모두 내가 확증 할 수 있었던 것임을 밝혀둔다. 여러 인물들의 생각이나 감정을 서술하는 데 있어 얼마쯤 내가 시인의 특권을 행사했다 해도 그것은 아주 정확을 기해서 한 일이다.',\n",
              " '또한 그것들은 모두 내 친구 에르큘 포아로의 검토를 받았음을 덧붙여 둔다. 끝으로, 나는 이 이상한 연쇄 범죄의 결과로서 일어나는 부차적인 인간관계에 대해 너무 많은 이야기를 했는지도 모른다. 하지만 인간적, 개인적 요소란 빠뜨려선 안 되는 것이다. 에르큘 포아로가 언젠가 과장된 몸짓으로 나에게 가르쳐 준 일이 있다. 로맨스란 범죄의 부산물일 경우가 있다고. ABC 수수께끼의 해결에 대해 말한다면, 에르큘 포아로는 이제까지 그가 다뤄 온 어느 사건과도 다른 방법으로 문제에 뛰어들어 그 진정한 천재성을 발휘했다고 말해도 좋으리라. < 편지 > 1935년 6월, 나는 남아메리카의 내 농장에서 떠나 여섯 달쯤 머무를 예정으로 귀국했다. 그때는 어려웠던 시대로, 다른 사람들과 마찬가지로 우리 역시 세계적인 불황에 어려움을 겪고 있었다. 영국에서 나 자신이 손대지 않으면 도저히 잘되어 나가지 않을 것 같은 볼일이 여러 가지 있었다. 농장 관리를 위해 아내가 뒤에 남았다.',\n",
              " '영국에 와 닿아 내가 맨 먼저 한 일의 하나는 말할 나위도 없이 오랜 친구인 에르큘 포아로를 찾아간 것이었다. 그는 런던의 어떤 최신형 아파트에 살고 있었다. 내가 그것을 지적하며, 그가 이 특별한 건물을 고른 것은 완전히 그 기하학적이 겉모습과 넓이 때문일 거라고 말하자 그는 고개를 끄덕였다. “그러나 아주 기분 좋게 균형이 잡혀 있지. 그렇게 생각되지 않나?” 나는 좀 너무 모난 것같이 생각된다고 말했다. 그리고 오래된 농담이 생각나 이 아파트에서는 암탉에게 네모난 달걀을 낳게 할 수 있을 듯하다고 말했다.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBEVNQsqBy7w",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e1205b0d-0620-4d8e-edcf-311f83782af4"
      },
      "source": [
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        " \n",
        "step = 0\n",
        "for intro,body,end in ko_docs:\n",
        "    step += 1\n",
        "    print(\"=\" * 50)\n",
        "    print(str(step),\"/\",len(ko_docs))\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    org_text_1 = intro\n",
        "    org_text_2 = body\n",
        "    org_text_3 = end\n",
        " \n",
        "    try:\n",
        "        df1,dct1 = sam_wgan('',[org_text_1,org_text_2,org_text_3],init_bias=0.0,display= False)\n",
        "        if dct1['grammar'][0] > 0.0:\n",
        "            df2,dct2 = bert_lexrank_sum('',[org_text_1,org_text_2,org_text_3])\n",
        "            df3,dct3 = besm('',[org_text_1,org_text_2,org_text_3])\n",
        "            df4,dct4 = besm_bert('',[org_text_1,org_text_2,org_text_3])\n",
        "            #df5,dct5 = abstract_method_1(g_summ,[org_text_1,org_text_2,org_text_3])\n",
        "            #df6,dct6 = abstract_method_2(g_summ,[org_text_1,org_text_2,org_text_3])\n",
        " \n",
        "            test_result['SAM+WGAN'].append(get_features(dct1))\n",
        "            test_result['BERT+LexRank'].append(get_features(dct2))\n",
        "            test_result['BESM'].append(get_features(dct3))\n",
        "            test_result['BESM+kobert'].append(get_features(dct4))\n",
        "            #test_result['Transformer'].append(get_features(dct5))\n",
        "            #test_result['T5'].append(get_features(dct6))\n",
        "            #result = pd.concat([df1, df2, df3, df4, df5, df6 ], ignore_index=True)\n",
        "            result = pd.concat([df1, df2, df3, df4 ], ignore_index=True)\n",
        "            \n",
        "            print(result)\n",
        "            \n",
        "            print(\"Current result\",\"=\" * 50)\n",
        "            print(\"Sample count:\",len(test_result['SAM+WGAN']))\n",
        "            print(get_test_statistics(test_result))\n",
        "        \n",
        "    except KeyboardInterrupt as ki:\n",
        "        raise ki\n",
        "    except :\n",
        "        print(\"Unexpected error:\", sys.exc_info()[0])\n",
        "        #raise e\n",
        "        pass\n",
        " \n",
        "get_test_statistics(test_result)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "1 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "이 이야기에서는 내가 직접 입회한 사건이나 장면만을 이야기하는 전의 내 방법을 바꿔 보았다. 그래서 몇몇 장은 3인칭으로 씌어 있다. 이제부터의 각 장에서 이야기되는 사건들은 모두 내가 확증 할 수 있었던 것임을 밝혀둔다. 여러 인물들의 생각이나 감정을 서술하는 데 있어 얼마쯤 내가 시인의 특권을 행사했다 해도 그것은 아주 정확을 기해서 한 일이다. 또한 그것들은 모두 내 친구 에르큘 포아로의 검토를 받았음을 덧붙여 둔다. 끝으로, 나는 이 이상한 연쇄 범죄의 결과로서 일어나는 부차적인 인간관계에 대해 너무 많은 이야기를 했는지도 모른다. 하지만 인간적, 개인적 요소란 빠뜨려선 안 되는 것이다. 에르큘 포아로가 언젠가 과장된 몸짓으로 나에게 가르쳐 준 일이 있다. 로맨스란 범죄의 부산물일 경우가 있다고. ABC 수수께끼의 해결에 대해 말한다면, 에르큘 포아로는 이제까지 그가 다뤄 온 어느 사건과도 다른 방법으로 문제에 뛰어들어 그 진정한 천재성을 발휘했다고 말해도 좋으리라. < 편지 > 1935년 6월, 나는 남아메리카의 내 농장에서 떠나 여섯 달쯤 머무를 예정으로 귀국했다. 그때는 어려웠던 시대로, 다른 사람들과 마찬가지로 우리 역시 세계적인 불황에 어려움을 겪고 있었다. 영국에서 나 자신이 손대지 않으면 도저히 잘되어 나가지 않을 것 같은 볼일이 여러 가지 있었다. 농장 관리를 위해 아내가 뒤에 남았다. 영국에 와 닿아 내가 맨 먼저 한 일의 하나는 말할 나위도 없이 오랜 친구인 에르큘 포아로를 찾아간 것이었다. 그는 런던의 어떤 최신형 아파트에 살고 있었다. 내가 그것을 지적하며, 그가 이 특별한 건물을 고른 것은 완전히 그 기하학적이 겉모습과 넓이 때문일 거라고 말하자 그는 고개를 끄덕였다. “그러나 아주 기분 좋게 균형이 잡혀 있지. 그렇게 생각되지 않나?” 나는 좀 너무 모난 것같이 생각된다고 말했다. 그리고 오래된 농담이 생각나 이 아파트에서는 암탉에게 네모난 달걀을 낳게 할 수 있을 듯하다고 말했다.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.3519999980926514 Generator / grammar loss:-0.12269207835197449   similarity loss:-0.08712171018123627\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "직접 내 보았다. 몇몇 그것들은 내 둔다. 귀국했다. 같은 뒤에 내가 맨 한 오랜 친구인 그는 어떤 최신형 아파트에 겉모습과 넓이 그는 끄덕였다. “그러나 아주 기분 균형이 있지. 나는 모난 것같이 생각된다고 농담이 암탉에게 말했다. \n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "< 편지 > 1935년 6월, 나는 남아메리카의 내 농장에서 떠나 여섯 달쯤 머무를 예정으로 귀국했다.그때는 어려웠던 시대로, 다른 사람들과 마찬가지로 우리 역시 세계적인 불황에 어려움을 겪고 있었다.그는 런던의 어떤 최신형 아파트에 살고 있었다.그렇게 생각되지 않나?” 나는 좀 너무 모난 것같이 생각된다고 말했다.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "이 이야기에서는 내가 직접 입회한 사건이나 장면만을 이야기하는 전의 내 방법을 바꿔 보았다. 그때는 어려웠던 시대로, 다른 사람들과 마찬가지로 우리 역시 세계적인 불황에 어려움을 겪고 있었다. 영국에서 나 자신이 손대지 않으면 도저히 잘되어 나가지 않을 것 같은 볼일이 여러 가지 있었다.\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "이 이야기에서는 내가 직접 입회한 사건이나 장면만을 이야기하는 전의 내 방법을 바꿔 보았다. abc 수수께끼의 해결에 대해 말한다면, 에르큘 포아로는 이제까지 그가 다뤄 온 어느 사건과도 다른 방법으로 문제에 뛰어들어 그 진정한 천재성을 발휘했다고 말해도 좋으리라. < 편지 > 1935년 6월, 나는 남아메리카의 내 농장에서 떠나 여섯 달쯤 머무를 예정으로 귀국했다.\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.134497  0.483653  0.361809  0.617523  0.010906  0.462892   \n",
            "1  BERT+LexRank    0.181725  0.174120  0.265249  0.262216  0.001786  0.246113   \n",
            "2          BESM    0.165298  0.610233  0.504501  0.296348  0.017003  0.463201   \n",
            "3   BESM+kobert    0.211499  0.610233  0.486227  0.298381  0.016435  0.454675   \n",
            "\n",
            "    grammar  \n",
            "0  0.990238  \n",
            "1  0.999038  \n",
            "2  0.999022  \n",
            "3  0.999026  \n",
            "Current result ==================================================\n",
            "Sample count: 1\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.134497  0.483653  0.361809    0.617523  0.010906   \n",
            "1  BERT+LexRank   0.181725  0.174120  0.265249    0.262216  0.001786   \n",
            "2          BESM   0.165298  0.610233  0.504501    0.296348  0.017003   \n",
            "3   BESM+kobert   0.211499  0.610233  0.486227    0.298381  0.016435   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.462892    0.990238  \n",
            "1     0.246113    0.999038  \n",
            "2     0.463201    0.999022  \n",
            "3     0.454675    0.999026  \n",
            "==================================================\n",
            "2 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "포아로는 크게 웃었다. “아니, 자네는 아직도 그걸 기억하고 있나? 하지만 유감스럽게도 과학은 아직 암탉을 현대 취미에 알맞도록 하는 일에 성공하지 못하고 있네. 닭들이 지금도 여전히 크기와 빛깔이 서로 다른 달걀을 낳고 있지. ” 나는 애정어린 눈길로 오랜 친구를 관찰했다. 그는 굉장히 활기가 넘쳐 전에 만났을 때보다 조금도 더 나이먹은 것같이 보이지 않았다. “자네는 정말 건강해 보이는군, 포아로. 거의 나이를 안 먹었잖나. 전에 만났을 때보다 흰머리가 더 적어졌다고 해도 좋을 정도일세, 그런 일이 있을 수 있다면. ” 포아로는 나에게 빙그레 웃어 보였다. “어째서 그런 일이 있을 수 있겠나? 진짜 그 말대로인데. ” “자네 머리는 검은빛에서 잿빛이 되는 대신 잿빛에서 검은빛으로 된단 말인가?” “그렇다네. ” “그렇지만 그런 일은 과학적으로 불가능해!” “천만에. ” “하지만 있을 수 없는 일이잖나. 자연 법칙에 어긋나. ” “헤이스팅즈, 자네는 여전히 남을 의심하지 않는 아름다운 마음을 지니고 있군. 세월도 자네의 그 마음은 바꿔 놓지 못하는구먼! 자네는 한 가지 사실을 발견하면 곧바로 그 해결을 입에 담지. 자기 자신은 그것을 의식하지 못하지만!” 나는 무슨 소리인지 알 수가 없어 그를 쳐다보았다.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.215193510055542 Generator / grammar loss:-0.10970785468816757   similarity loss:-0.08810487389564514\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            " 포아로는 하지만 유감스럽게도 일에 성공하지 낳고 있지. 관찰했다. 그는 때보다 조금도 거의 먹었잖나. 더 웃어 보였다. ” 검은빛에서 검은빛으로 일은 불가능해!” 있을 일이잖나.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "하지만 유감스럽게도 과학은 아직 암탉을 현대 취미에 알맞도록 하는 일에 성공하지 못하고 있네.닭들이 지금도 여전히 크기와 빛깔이 서로 다른 달걀을 낳고 있지.” 나는 애정어린 눈길로 오랜 친구를 관찰했다.자연 법칙에 어긋나.자네는 한 가지 사실을 발견하면 곧바로 그 해결을 입에 담지.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "하지만 유감스럽게도 과학은 아직 암탉을 현대 취미에 알맞도록 하는 일에 성공하지 못하고 있네. 전에 만났을 때보다 흰머리가 더 적어졌다고 해도 좋을 정도일세, 그런 일이 있을 수 있다면. ” 헤이스팅즈, 자네는 여전히 남을 의심하지 않는 아름다운 마음을 지니고 있군.\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "하지만 유감스럽게도 과학은 아직 암탉을 현대 취미에 알맞도록 하는 일에 성공하지 못하고 있네. 그는 굉장히 활기가 넘쳐 전에 만났을 때보다 조금도 더 나이먹은 것같이 보이지 않았다. “ 헤이스팅즈, 자네는 여전히 남을 의심하지 않는 아름다운 마음을 지니고 있군.\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.159236  0.479789  0.511701  0.397760  0.002303  0.471137   \n",
            "1  BERT+LexRank    0.253185  0.394299  0.104906  0.211800  0.014276  0.194853   \n",
            "2          BESM    0.237261  0.489392  0.490315  0.436441  0.000634  0.473968   \n",
            "3   BESM+kobert    0.232484  0.542992  0.474982  0.462911  0.001243  0.484963   \n",
            "\n",
            "    grammar  \n",
            "0  0.923598  \n",
            "1  0.999028  \n",
            "2  0.999039  \n",
            "3  0.999038  \n",
            "Current result ==================================================\n",
            "Sample count: 2\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.146866  0.481721  0.436755    0.507641  0.006605   \n",
            "1  BERT+LexRank   0.217455  0.284209  0.185077    0.237008  0.008031   \n",
            "2          BESM   0.201279  0.549812  0.497408    0.366394  0.008819   \n",
            "3   BESM+kobert   0.221992  0.576613  0.480604    0.380646  0.008839   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.467014    0.956918  \n",
            "1     0.220483    0.999033  \n",
            "2     0.468585    0.999031  \n",
            "3     0.469819    0.999032  \n",
            "==================================================\n",
            "3 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "그는 잠자코 침실로 들어가더니 병을 하나 들고 돌아와 나에게 건네 주었다. 나는 까닭을 모르는 채 그 병을 보았다. 병에는 이렇게 씌어 있었다. 르비비 - 머리칼의 자연스러운 빛깔을 회색, 밤색, 빨강, 노랑, 갈색, 검은 색의 여섯 가지 색조로 되살린다. 르비비는 염료가 아니다. 나는 소리쳤다. “포아로, 머리를 염색하고 있구먼!” “아, 겨우 알아차린 모양이군!” “그래서 자네 머리가 전에 돌아왔을 때보다 훨씬 검어 보였단 말인가?” “그렇지. ” 놀라움이 가라앉자 나는 말했다. “그럼, 다음에 돌아왔을 때에는 가짜 수염이라도 달고 있을게 아닌가? 아니면 지금도 가짜 수염인가?” 포아로는 움찔했다. 수염은 늘 그가 세심하게 신경쓰는 부분이다. 그는 수염을 터무니없이 자랑했다. 그런데 내 말이 그의 아픈 데를 찌른 것이다. “아닐세, 당치도 않아. 그런 날은 되도록 오지 않기를 비네. 가짜 수염이라니? 끔찍한 소리를!” 그의 수염이 진짜인 것을 증명하기 위해 힘주어 잡아당겨 보였다. “과연 아직 숱이 꽤 많군. ” “그렇지? 온 런던을 다 찾아봐도 나에게 맞는 가짜 수염은 있을 리 없네.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:1.9521147012710571 Generator / grammar loss:-0.08671412616968155   similarity loss:-0.0915035828948021\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            " 잠자코 들어가더니 하나 건네 주었다. 머리칼의 “아, 겨우 모양이군!” 돌아왔을 때보다 훨씬 “그렇지. 말했다. 아닌가? 수염인가?” 신경쓰는 비네. 나에게 없네.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "” “그렇지?나는 소리쳤다.그는 잠자코 침실로 들어가더니 병을 하나 들고 돌아와 나에게 건네 주었다.병에는 이렇게 씌어 있었다.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "그는 잠자코 침실로 들어가더니 병을 하나 들고 돌아와 나에게 건네 주었다. 르비비 - 머리칼의 자연스러운 빛깔을 회색, 밤색, 빨강, 노랑, 갈색, 검은 색의 여섯 가지 색조로 되살린다.르비비는 염료가 아니다.\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "그는 잠자코 침실로 들어가더니 병을 하나 들고 돌아와 나에게 건네 주었다. 르비비 - 머리칼의 자연스러운 빛깔을 회색, 밤색, 빨강, 노랑, 갈색, 검은 색의 여섯 가지 색조로 되살린다.르비비는 염료가 아니다.\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.163993  0.447204  0.626516  0.662552  0.008870  0.601465   \n",
            "1  BERT+LexRank    0.126560  0.594878  0.207621  0.146262  0.039443  0.266665   \n",
            "2          BESM    0.208556  0.726457  0.286528  0.259698  0.045791  0.366465   \n",
            "3   BESM+kobert    0.208556  0.726457  0.286528  0.259698  0.045791  0.366465   \n",
            "\n",
            "    grammar  \n",
            "0  0.950295  \n",
            "1  0.998670  \n",
            "2  0.999012  \n",
            "3  0.999012  \n",
            "Current result ==================================================\n",
            "Sample count: 3\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.152575  0.470215  0.500009    0.559278  0.007360   \n",
            "1  BERT+LexRank   0.187156  0.387766  0.192592    0.206759  0.018502   \n",
            "2          BESM   0.203705  0.608694  0.427114    0.330829  0.021143   \n",
            "3   BESM+kobert   0.217513  0.626561  0.415912    0.340330  0.021156   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.511831    0.954710  \n",
            "1     0.235877    0.998912  \n",
            "2     0.434545    0.999025  \n",
            "3     0.435367    0.999025  \n",
            "==================================================\n",
            "4 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "” 꽤 우쭐대는군 하고 나는 마음속으로 생각했다. 그러나 나는 그런 소리를 해서 포아로의 기분을 상하게 할 생각은 전혀 없었다. 그 대신 그가 아직도 때로 일을 하는지 물어 보았다. “자네가 몇 해 전 은퇴한 것은 알고 있지만……” “그렇네. 대대적으로 호박을 가꾸기 위해서! 그런데 곧 살인 사건이 일어나 호박들에게 멸망으로의 행진을 시키고 만 셈일세. 그 뒤부터는, 자네가 뭐라고 할지 잘 알지만 나는 자진해서 고별 공연을 여는 프리마돈나가 됐네. 물론 그 고별 공연이 끝없이 되풀이되고 있지만 말일세. ” 나는 웃었다, “실로 그대로라네. 그때마다 나는 이것을 마지막이라고 하지. 그런데 안돼. 다른 사건이 일어나거든. 그래서 나는 인정하지 않을 수 없다네. 나는 은퇴를 바라지 않는다고. 이 조그만 회색 뇌세포는 쓰지 않으면 녹슬어 버리니까. ” “알았네. 적당히 운동을 시키고 있다는 거로군. ” “맞아, 요즘의 에르큘 포아로는 범죄의 진수밖에 다루지 않네. ” “그 진수는 충분히 있던가?” “꽤 있지. 바로 저번 사건 같은 경우는 위태로울 뻔했었어.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.3773484230041504 Generator / grammar loss:-0.12135651707649231   similarity loss:-0.08316410332918167\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "없었다. 그가 곧 행진을 잘 됐네. 물론 “실로 그대로라네. 그때마다 마지막이라고 안돼. 일어나거든. 그래서 뇌세포는 녹슬어 버리니까. “알았네.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "” “맞아, 요즘의 에르큘 포아로는 범죄의 진수밖에 다루지 않네.그 뒤부터는, 자네가 뭐라고 할지 잘 알지만 나는 자진해서 고별 공연을 여는 프리마돈나가 됐네.나는 은퇴를 바라지 않는다고.이 조그만 회색 뇌세포는 쓰지 않으면 녹슬어 버리니까.그런데 안돼.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "그러나 나는 그런 소리를 해서 포아로의 기분을 상하게 할 생각은 전혀 없었다. 자네가 몇 해 전 은퇴한 것은 알고 있지만……” “그렇네.대대적으로 호박을 가꾸기 위해서!\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "그러나 나는 그런 소리를 해서 포아로의 기분을 상하게 할 생각은 전혀 없었다. 자네가 몇 해 전 은퇴한 것은 알고 있지만……” “그렇네.대대적으로 호박을 가꾸기 위해서!\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.150558  0.466536  0.573695  0.565978  0.002381  0.549948   \n",
            "1  BERT+LexRank    0.263941  0.256045  0.274003  0.245911  0.000135  0.261984   \n",
            "2          BESM    0.174721  0.602274  0.345120  0.324094  0.015995  0.390243   \n",
            "3   BESM+kobert    0.174721  0.602274  0.345120  0.324094  0.015995  0.390243   \n",
            "\n",
            "    grammar  \n",
            "0  0.981966  \n",
            "1  0.999033  \n",
            "2  0.999023  \n",
            "3  0.999023  \n",
            "Current result ==================================================\n",
            "Sample count: 4\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.152071  0.469295  0.518430    0.560953  0.006115   \n",
            "1  BERT+LexRank   0.206352  0.354836  0.212945    0.216547  0.013910   \n",
            "2          BESM   0.196459  0.607089  0.406616    0.329145  0.019856   \n",
            "3   BESM+kobert   0.206815  0.620489  0.398214    0.336271  0.019866   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.521360    0.961524  \n",
            "1     0.242404    0.998942  \n",
            "2     0.423469    0.999024  \n",
            "3     0.424086    0.999025  \n",
            "==================================================\n",
            "5 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "” “실패했나?” 포아로는 놀라운 듯했다. “당치도 않네. 그렇지만 이 내가, 이 에르큘 포아로가 하마터면 살해될 뻔했었지. ” 나는 휘파람을 불었다. “대담한 범인이로군. ” “대담하다기보다 무모하지. 그래, 진짜 무모한 녀석이었어. 하지만 그 이야기는 그만두세. 그런데 헤이스팅즈, 알겠나? 나는 여러 가지 뜻에서 자네를 내 마스코트로 생각하고 있네. ” “정말인가? 어떤 뜻에서?” 포아로는 내 물음에는 직접 대답하지 않고 이야기를 계속했다. “자네가 온다는 이야기를 들으면 나는 곧 무언가 일어나겠군 하고 생각된다네. 예전처럼 둘이서 수사하지 않겠나. 하지만 만일 그렇게 한다면 평범한 사건은 안돼. 뭔가 이렇게……. ” 그는 흥분해서 손을 파도치듯 움직였다. “머리를 잔뜩 쓰게 하는, 미묘하고 피이누(섬세)한 것이 아니면 안 되지. ” 피아누라는 번역하기 어려운 말에 가득한 풍미를 곁들이는 듯한 말투였다. “포아로, 남이 들으면 마치 리츠에서 저녁 식사라도 주문하고 있는 줄로 생각하겠네.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.1899092197418213 Generator / grammar loss:-0.09586405754089355   similarity loss:-0.07681576162576675\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "” 휘파람을 범인이로군. 하지만 그만두세. 계속했다. “자네가 들으면 않겠나. 사건은 안돼. 이렇게……. 움직였다. 아니면 생각하겠네.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "“포아로, 남이 들으면 마치 리츠에서 저녁 식사라도 주문하고 있는 줄로 생각하겠네.나는 여러 가지 뜻에서 자네를 내 마스코트로 생각하고 있네.” 피아누라는 번역하기 어려운 말에 가득한 풍미를 곁들이는 듯한 말투였다.그런데 헤이스팅즈, 알겠나?\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "자네가 온다는 이야기를 들으면 나는 곧 무언가 일어나겠군 하고 생각된다네. 포아로, 남이 들으면 마치 리츠에서 저녁 식사라도 주문하고 있는 줄로 생각하겠네.\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "자네가 온다는 이야기를 들으면 나는 곧 무언가 일어나겠군 하고 생각된다네. 예전처럼 둘이서 수사하지 않겠나.하지만 만일 그렇게 한다면 평범한 사건은 안돼. 피아누라는 번역하기 어려운 말에 가득한 풍미를 곁들이는 듯한 말투였다. “\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.148810  0.575530  0.491906  0.506864  0.001326  0.513118   \n",
            "1  BERT+LexRank    0.267857  0.206440  0.245220  0.355161  0.003968  0.270446   \n",
            "2          BESM    0.172619  0.226682  0.318580  0.463892  0.009537  0.343794   \n",
            "3   BESM+kobert    0.253968  0.459526  0.498303  0.609212  0.004023  0.523820   \n",
            "\n",
            "    grammar  \n",
            "0  0.909339  \n",
            "1  0.999036  \n",
            "2  0.999009  \n",
            "3  0.993899  \n",
            "Current result ==================================================\n",
            "Sample count: 5\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.151419  0.490542  0.513125    0.550135  0.005157   \n",
            "1  BERT+LexRank   0.218653  0.325157  0.219400    0.244270  0.011922   \n",
            "2          BESM   0.191691  0.531007  0.389009    0.356094  0.017792   \n",
            "3   BESM+kobert   0.216246  0.588296  0.418232    0.390859  0.016697   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.519712    0.951087  \n",
            "1     0.248012    0.998961  \n",
            "2     0.407534    0.999021  \n",
            "3     0.444033    0.998000  \n",
            "==================================================\n",
            "6 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "” 그는 한숨을 쉬었다. “범죄란 주문할 수 있는 게 아닌데 말일세. 정말이야. 그렇지만 나는 운을 믿겠네. 운명이라 해도 좋아. 내 곁에 붙어 있으면서 내가 용서받을 수 없는 실책을 저지르는 걸 막아주는 게 자네 운명이야. ” “용서받을 수 없는 실책이란 뭔가?” “명백한 것을 놓치는 것이지. ” 나는 이 말을 가슴속에서 되풀이해 보았으나 핵심을 잡을 수 없었다. 나는 밝게 미소지으며 말했다. “그런데 그 진수라고 할 만한 범죄는 아직 일어나지 않았나?” “적어도 아직은. 왜냐하면……. ” 그는 말을 끊었다. 이마에 난처한 듯한 주름이 잡혔다. 그 손은 내가 생각없이 접어버린 물건을 무의식중에 펴고 있었다. 그는 천천히 말했다. “뚜렷이 알 수는 없지만……. ” 그 말투에 어떤 이상한 게 느껴져 나는 놀라며 그의 얼굴을 보았다. 가로진 주름은 아직 남아 있었다. 그는 갑자기 결심한 듯 고개를 끄덕이고 창 가까이의 책상 쪽으로 방을 가로질러 갔다. 책상 속의 것은 말할 나위도 없이 잘 분류되고 정리되어 손을 넣기만 하면 kq로 필요한 서류를 꺼낼 수 있었다.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.2624216079711914 Generator / grammar loss:-0.13140547275543213   similarity loss:-0.10501114279031754\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "그는 한숨을 믿겠네. 내가 용서받을 수 저지르는 걸 운명이야. 것이지. 없었다. 그 내가 있었다. 알 없지만……. 그 말투에 나는 놀라며 보았다.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "책상 속의 것은 말할 나위도 없이 잘 분류되고 정리되어 손을 넣기만 하면 kq로 필요한 서류를 꺼낼 수 있었다.그는 갑자기 결심한 듯 고개를 끄덕이고 창 가까이의 책상 쪽으로 방을 가로질러 갔다.“범죄란 주문할 수 있는 게 아닌데 말일세.” 그는 한숨을 쉬었다.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "내 곁에 붙어 있으면서 내가 용서받을 수 없는 실책을 저지르는 걸 막아주는 게 자네 운명이야. ” “ 그 손은 내가 생각없이 접어버린 물건을 무의식중에 펴고 있었다.그는 천천히 말했다. “ 그는 갑자기 결심한 듯 고개를 끄덕이고 창 가까이의 책상 쪽으로 방을 가로질러 갔다.\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "내 곁에 붙어 있으면서 내가 용서받을 수 없는 실책을 저지르는 걸 막아주는 게 자네 운명이야. ” “ 그 손은 내가 생각없이 접어버린 물건을 무의식중에 펴고 있었다.그는 천천히 말했다. “\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.149171  0.627850  0.574320  0.550683  0.001042  0.577935   \n",
            "1  BERT+LexRank    0.268877  0.210096  0.203510  0.350942  0.004624  0.249057   \n",
            "2          BESM    0.281768  0.350113  0.468322  0.519054  0.005010  0.459900   \n",
            "3   BESM+kobert    0.193370  0.446302  0.580410  0.371630  0.007461  0.490954   \n",
            "\n",
            "    grammar  \n",
            "0  0.973503  \n",
            "1  0.999019  \n",
            "2  0.999005  \n",
            "3  0.988661  \n",
            "Current result ==================================================\n",
            "Sample count: 6\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.151044  0.513427  0.523325    0.550227  0.004471   \n",
            "1  BERT+LexRank   0.227024  0.305980  0.216752    0.262049  0.010705   \n",
            "2          BESM   0.206704  0.500858  0.402227    0.383254  0.015662   \n",
            "3   BESM+kobert   0.212433  0.564631  0.445261    0.387654  0.015158   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.529416    0.954823  \n",
            "1     0.248186    0.998971  \n",
            "2     0.416262    0.999019  \n",
            "3     0.451853    0.996443  \n",
            "==================================================\n",
            "7 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "그는 한 통의 뜯어진 편지를 손에 들고 내 쪽으로 천천히 되돌아왔다. 그리고 그것에 눈길을 한 번 주더니 나에게 내밀며 말했다. “자네는 이걸 어떻게 생각하나?” 나는 어떤 흥미를 가지고 그것을 바았다. 그것은 좀 두꺼운 흰 편지지에 활자체로 씌어 있었다. 에르큘 포아로여, 너는 자만에 빠져 있는 게 아닐까. 갸엾은 우리 멍청이 영국 경찰이 감당하지 못하는 어려운 사건을 해결할 수 있는 건 자신이라고? 명민한 포아로여, 너의 명민함을 어디 한 번 보여 다오. 하지만 너에게는 이 호두가 너무 딱딱할걸. 이 달 21일, 앤도버(Andover)를 경계하라. 이만. ABC 나는 잠시 봉투에 눈길을 주었다. 역시 활자체로 씌어 있었다. 내가 소인에 주의를 돌리고 있는 것을 보자 그가 말했다. “소인은 서중앙 제1국일세. 그래, 어떻게 생각하나?” 나는 어깨를 으쓱해 보이며 편지를 돌려주었다. “아마도 미치광이 짓이겠지. ” “그뿐인가?” “자네한테는 미치광이로 여겨지지 않는다는 건가?” “아니, 그렇게 여겨지네. ” 그의 말투는 진지했다. 나는 호기심을 느끼며 그를 보았다. “자네는 이 편지를 진지하게 받아들이고 있는 모양이군, 포아로.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:1.9810421466827393 Generator / grammar loss:-0.09112455695867538   similarity loss:-0.09302041679620743\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            " 어떻게 나는 에르큘 포아로여, 해결할 포아로여, 호두가 딱딱할걸. 이만. 나는 그가 생각하나?” 보이며 편지를 짓이겠지. 이 진지하게 있는 모양이군, 포아로.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "이만.갸엾은 우리 멍청이 영국 경찰이 감당하지 못하는 어려운 사건을 해결할 수 있는 건 자신이라고?하지만 너에게는 이 호두가 너무 딱딱할걸.이 달 21일, 앤도버(Andover)를 경계하라.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "그것은 좀 두꺼운 흰 편지지에 활자체로 씌어 있었다.에르큘 포아로여, 너는 자만에 빠져 있는 게 아닐까. 갸엾은 우리 멍청이 영국 경찰이 감당하지 못하는 어려운 사건을 해결할 수 있는 건 자신이라고?\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "그것은 좀 두꺼운 흰 편지지에 활자체로 씌어 있었다.에르큘 포아로여, 너는 자만에 빠져 있는 게 아닐까. 갸엾은 우리 멍청이 영국 경찰이 감당하지 못하는 어려운 사건을 해결할 수 있는 건 자신이라고?\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.152659  0.519191  0.568588  0.567619  0.000532  0.558418   \n",
            "1  BERT+LexRank    0.181818  0.183497  0.283425  0.193003  0.002028  0.236313   \n",
            "2          BESM    0.190395  0.436673  0.395327  0.302113  0.003167  0.375632   \n",
            "3   BESM+kobert    0.190395  0.436673  0.395327  0.302113  0.003167  0.375632   \n",
            "\n",
            "    grammar  \n",
            "0  0.863206  \n",
            "1  0.999057  \n",
            "2  0.999039  \n",
            "3  0.999039  \n",
            "Current result ==================================================\n",
            "Sample count: 7\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.151275  0.514250  0.529791    0.552711  0.003909   \n",
            "1  BERT+LexRank   0.220566  0.288482  0.226276    0.252185  0.009466   \n",
            "2          BESM   0.204374  0.491689  0.401242    0.371663  0.013877   \n",
            "3   BESM+kobert   0.209285  0.546351  0.438128    0.375434  0.013445   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.533559    0.941735  \n",
            "1     0.246490    0.998983  \n",
            "2     0.410458    0.999022  \n",
            "3     0.440964    0.996814  \n",
            "==================================================\n",
            "8 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "” “미치광이란 진지하게 다루어야 하지. 미치광이는 아주 위험한 존재니까. ” “그렇지, 물론 그렇네. 나는 그 점을 생각지 못했어. 그러나 내 말은, 어쩐지 우스꽝스러운 장난같은 생각이 든다는 걸세. 누군가, 8이라는 숫자에 하나가 더 많은 것 같은 우쭐해진 주정꾼 바보 말이네. ” “뭐라고? 아홉이란 말인가? 그건 대체 무슨 뜻이지?” “아니, 그냥 말장난일세. 취한 녀석이라는 뜻이지. 아니, 그보다도 지나치게 마셔서 고주망태가 된 녀석이라는 뜻일세. ” “고맙네, 헤이스팅즈. 그 <취한다>는 말이라면 나도 알고 있네. 자네 말대로 그 이상의 뜻은 없는지도 모르지만. ‘ 나는 그의 불만스러운 말투에 자극되어 물어 보았다. “그럼, 자네는 무엇이 있다고 생각하나?” 포아로는 의심스러운 듯 머리를 흔들었지만 아무 말도 하지 않았다. 나는 물었다. “그래서 자네는 어떻게 했나?” “어떻게 할 수 있었겠나? 재프 경감에게 보였을 뿐이지. 그는 자네와 같은 의견이었어.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.035175085067749 Generator / grammar loss:-0.09290975332260132   similarity loss:-0.08939187973737717\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "나는 생각지 말은, 뜻이지?” “아니, 그냥 녀석이라는 고주망태가 나도 알고 있네. 자네 보았다. 생각하나?” 뿐이지.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "미치광이는 아주 위험한 존재니까.재프 경감에게 보였을 뿐이지.아홉이란 말인가?” “미치광이란 진지하게 다루어야 하지.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "나는 그 점을 생각지 못했어.그러나 내 말은, 어쩐지 우스꽝스러운 장난같은 생각이 든다는 걸세. 누군가, 8이라는 숫자에 하나가 더 많은 것 같은 우쭐해진 주정꾼 바보 말이네. ” “\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "나는 그 점을 생각지 못했어.그러나 내 말은, 어쩐지 우스꽝스러운 장난같은 생각이 든다는 걸세. 누군가, 8이라는 숫자에 하나가 더 많은 것 같은 우쭐해진 주정꾼 바보 말이네. ” “\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.134969  0.502203  0.536732  0.580140  0.001017  0.542848   \n",
            "1  BERT+LexRank    0.132924  0.357364  0.157886  0.121417  0.010755  0.186841   \n",
            "2          BESM    0.208589  0.470520  0.608479  0.418833  0.006408  0.523993   \n",
            "3   BESM+kobert    0.208589  0.470520  0.608479  0.418833  0.006408  0.523993   \n",
            "\n",
            "    grammar  \n",
            "0  0.820664  \n",
            "1  0.998984  \n",
            "2  0.995235  \n",
            "3  0.995235  \n",
            "Current result ==================================================\n",
            "Sample count: 8\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.149236  0.512744  0.530658    0.556140  0.003547   \n",
            "1  BERT+LexRank   0.209611  0.297093  0.217728    0.235839  0.009627   \n",
            "2          BESM   0.204901  0.489043  0.427146    0.377559  0.012943   \n",
            "3   BESM+kobert   0.209198  0.536872  0.459422    0.380859  0.012565   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.534720    0.926601  \n",
            "1     0.239034    0.998983  \n",
            "2     0.424649    0.998548  \n",
            "3     0.451343    0.996617  \n",
            "==================================================\n",
            "9 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "할 짓 없는 녀석의 장난이라고 말일세. 그것이 그의 표현이었는데, 런던 경찰국에서는 거의 날마다 이런 것을 받는다는군. 나도 그 바람에 휘말려 들었다는 거였어. ” “하지만 자네는 이 편지를 진지하게 생각하고 있잖은가?” 포아로는 천천히 대답했다. “아무래도 이 편지에는 내 마음에 들지 않는 게 있어, 헤이스팅즈. ” 그 말투가 묘하게 인상적이었다. “그래, 자네 의견은?” 그는 고개를 젓고 그 편지를 들어올려 다시 책상 속에 넣어 버렸다. “자네가 그토록 진지하게 생각한다면 왜 아무 일도 하지 않고 있는 건가?” “여전히 활동가로군, 자네는! 하지만 대체 어떻게 할 수 있겠나? 지방 경찰에도 편지를 보였지만 역시 진지하게 여겨 주지 않았어. 지문도 없고, 편지를 낸 사람에 대한 단서도 없으니. ” “그렇다면 자네 육감 말고는 아무것도 없단 말인가?” “육감이 아닐세, 헤이스팅즈. 육감이란 나쁜 말이야. 내 지식이며 경험일세. 그 편지에 뭔가 이상한 게 있다고 가르쳐 주는 것은. ” 말이 막히자 그는 손짓을 해보였다. 그리고 또 머리를 흔들었다. “개미집에서 산을 만들어 내려 하고 있는지도 모르지만 말일세. 어쨌든 기다려 보는 수밖에 없어. ” “옳지, 21일은 금요일이군.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.364590883255005 Generator / grammar loss:-0.13069266080856323   similarity loss:-0.0938214510679245\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "짓 없는 말일세. 런던 경찰국에서는 날마다 받는다는군. 거였어. 자네는 천천히 “아무래도 헤이스팅즈. 자네 의견은?” 고개를 편지를 들어올려 버렸다. 주지 말이야. 경험일세.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "” “옳지, 21일은 금요일이군.그것이 그의 표현이었는데, 런던 경찰국에서는 거의 날마다 이런 것을 받는다는군.“개미집에서 산을 만들어 내려 하고 있는지도 모르지만 말일세.나도 그 바람에 휘말려 들었다는 거였어.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "그것이 그의 표현이었는데, 런던 경찰국에서는 거의 날마다 이런 것을 받는다는군. 자네가 그토록 진지하게 생각한다면 왜 아무 일도 하지 않고 있는 건가?” “\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "그것이 그의 표현이었는데, 런던 경찰국에서는 거의 날마다 이런 것을 받는다는군. 자네가 그토록 진지하게 생각한다면 왜 아무 일도 하지 않고 있는 건가?” “\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.159278  0.718863  0.570060  0.430010  0.013910  0.557806   \n",
            "1  BERT+LexRank    0.193760  0.392384  0.129257  0.224416  0.011834  0.210430   \n",
            "2          BESM    0.142857  0.554238  0.393560  0.308699  0.010368  0.400237   \n",
            "3   BESM+kobert    0.142857  0.554238  0.393560  0.308699  0.010368  0.400237   \n",
            "\n",
            "    grammar  \n",
            "0  0.982520  \n",
            "1  0.999041  \n",
            "2  0.999015  \n",
            "3  0.999015  \n",
            "Current result ==================================================\n",
            "Sample count: 9\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.150352  0.535647  0.535036    0.542125  0.004699   \n",
            "1  BERT+LexRank   0.207850  0.307681  0.207897    0.234570  0.009872   \n",
            "2          BESM   0.198007  0.496287  0.423414    0.369908  0.012657   \n",
            "3   BESM+kobert   0.201827  0.538802  0.452104    0.372841  0.012321   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.537285    0.932814  \n",
            "1     0.235856    0.998989  \n",
            "2     0.421937    0.998600  \n",
            "3     0.445665    0.996883  \n",
            "==================================================\n",
            "10 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "앤도버에서 굉장한 강도 사건이라도 일어난다면 그야말로……. ” “아, 그렇다면 얼마나 기분전환이 되겠나. ” “기분전환이라고?” 나는 어이가 없었다. 그 자리에서 그 말은 너무나 이상스럽게 들렸다. 나는 항의했다. “강도는 스릴이 있을지 모르지만 기분전환이라고 할 수는 없어!” 포아로는 힘주어 고개를 저었다. “자네는 잘못 알고 있네. 자네는 내 말뜻을 모르고 있어. 내 마음을 차지하고 있는 더 큰 다른 염려에 비하면, 강도는 오히려 마음 놓을 수 있다는 걸세. ” “무슨 염려인가?” “살인이지. ‘ < 삽 화 > 앨릭잰더 보너퍼트 캐스트 씨는 의자에서 일어나 초라한 침실을 근시인 듯한 눈으로 둘러보았다. 답답스러운 자세로 앉아있었기 때문에 등이 완전히 뻣뻣해져 버렸다. 등을 쭉 펴고 기지개 켜는 그를 본 사람은, 그가 실제로는 키가 큰 사람임을 알았으리라. 그의 굽은 등과 근시처럼 기웃거리는 동작이 아주 다른 인상을 주고 있었다. 문 안쪽에 걸린 낡아빠진 외투로 다가가 주머니에서 싸구려 담뱃갑과 성냥을 꺼냈다. 담배에 불을 붙이고 지금까지 앉아있던 의자로 돌아왔다. 철도 안내서를 집어 들고 세밀히 보더니 이윽고 타이프된 이름 리스트를 훑어보기 시작했다. 그는 펜으로 그 리스트의 첫 번째 이름에 표시했다. 그것은 6월 20일 목요일의 일이었다. < 앤도버 살인 > 나는 그때 포아로가 받은 편지에 대한 그의 예감에 깊은 인상을 받은 건 사실이지만, 그 일은 내 머리에서 아주 사라져 버렸다고 해도 좋다.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.444972276687622 Generator / grammar loss:-0.14226622879505157   similarity loss:-0.09701221436262131\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "강도 “아, 얼마나 기분전환이 되겠나. 나는 그 이상스럽게 들렸다. 모르고 다른 염려에 근시처럼 세밀히 보더니 그것은 20일 < 앤도버 포아로가 받은 예감에 깊은 일은 아주 사라져 좋다.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "그것은 6월 20일 목요일의 일이었다.그는 펜으로 그 리스트의 첫 번째 이름에 표시했다.담배에 불을 붙이고 지금까지 앉아있던 의자로 돌아왔다.철도 안내서를 집어 들고 세밀히 보더니 이윽고 타이프된 이름 리스트를 훑어보기 시작했다.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "내 마음을 차지하고 있는 더 큰 다른 염려에 비하면, 강도는 오히려 마음 놓을 수 있다는 걸세. ” “ 등을 쭉 펴고 기지개 켜는 그를 본 사람은, 그가 실제로는 키가 큰 사람임을 알았으리라. 그의 굽은 등과 근시처럼 기웃거리는 동작이 아주 다른 인상을 주고 있었다.\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "내 마음을 차지하고 있는 더 큰 다른 염려에 비하면, 강도는 오히려 마음 놓을 수 있다는 걸세. ” “ 등을 쭉 펴고 기지개 켜는 그를 본 사람은, 그가 실제로는 키가 큰 사람임을 알았으리라. < 앤도버 살인 > 나는 그때 포아로가 받은 편지에 대한 그의 예감에 깊은 인상을 받은 건 사실이지만, 그 일은 내 머리에서 아주 사라져 버렸다고 해도 좋다.\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.140921  0.668936  0.411998  0.287683  0.025203  0.426091   \n",
            "1  BERT+LexRank    0.173442  0.002094  0.005932  0.486895  0.051819  0.149453   \n",
            "2          BESM    0.201897  0.445014  0.592064  0.178029  0.029370  0.438444   \n",
            "3   BESM+kobert    0.265583  0.427589  0.456546  0.344562  0.002252  0.417159   \n",
            "\n",
            "    grammar  \n",
            "0  0.996800  \n",
            "1  0.998982  \n",
            "2  0.998998  \n",
            "3  0.999031  \n",
            "Current result ==================================================\n",
            "Sample count: 10\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.149409  0.548976  0.522733    0.516681  0.006749   \n",
            "1  BERT+LexRank   0.204409  0.277122  0.187701    0.259802  0.014067   \n",
            "2          BESM   0.198396  0.491159  0.440279    0.350720  0.014328   \n",
            "3   BESM+kobert   0.208202  0.527680  0.452548    0.370013  0.011314   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.526166    0.939213  \n",
            "1     0.227216    0.998989  \n",
            "2     0.423588    0.998640  \n",
            "3     0.442814    0.997098  \n",
            "==================================================\n",
            "11 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "실제로 21일이 되어 런던 경찰국의 재프 경감이 포아로를 찾아왔을 때 나는 겨우 그 일을 생각해 냈다. 이 사법 경찰관과는 이미 오래 전부터 알고 있었기 때문에 그는 나를 보자 진심으로 환영해 주었다. 그는 큰소리로 말했다. “여, 내가 헤이스팅즈 대위를 몰라볼 리 있겠습니까. 드디어 당신의 야만 지대에서 돌아오셨군요! 포아로 씨와 함께 계신 당신을 뵈니 정말 예전 그대로입니다 그려. 게다가 건강하신 듯 하군요. 머리가 좀 벗겨졌는가요? 그렇습니다, 누구나 그렇게 되지요. 나도 그렇습니다. ” 나는 좀 놀랐다. 머리 꼭대기에 머리칼이 덮이도록 빗어 두었기 때문에 벗겨진 곳이 눈에 띄지 않으리라 여기고 있었던 것이다. 그러나 재프 경감은 그런 점에 그리 머리가 잘 도는 편이 아니었다. 그래서 나는 좋은 얼굴로 아무도 젊어지는 사람은 없다는 데 동의했다. 재프 경감은 말했다. “그러나 이 포아로 씨만은 다릅니다. 헤어토닉의 좋은 광고가 되지요. 얼굴 구석구석이 한층 더 싱싱해졌습니다. 늘그막에 이르러 점점 더 각광받게 되셨으니 말입니다. 요즘의 유명한 사건에는 모조리 관계되어 계시지요.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.2726197242736816 Generator / grammar loss:-0.12393184006214142   similarity loss:-0.09649909287691116\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "재프 나는 그는 나를 말했다. “여, 내가 리 정말 예전 듯 하군요. 나도 도는 그래서 다릅니다. 되지요. 얼굴 구석구석이 말입니다. 모조리\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "머리 꼭대기에 머리칼이 덮이도록 빗어 두었기 때문에 벗겨진 곳이 눈에 띄지 않으리라 여기고 있었던 것이다.그러나 재프 경감은 그런 점에 그리 머리가 잘 도는 편이 아니었다.실제로 21일이 되어 런던 경찰국의 재프 경감이 포아로를 찾아왔을 때 나는 겨우 그 일을 생각해 냈다.머리가 좀 벗겨졌는가요?\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "실제로 21일이 되어 런던 경찰국의 재프 경감이 포아로를 찾아왔을 때 나는 겨우 그 일을 생각해 냈다. 그래서 나는 좋은 얼굴로 아무도 젊어지는 사람은 없다는 데 동의했다.재프 경감은 말했다. “\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "실제로 21일이 되어 런던 경찰국의 재프 경감이 포아로를 찾아왔을 때 나는 겨우 그 일을 생각해 냈다. 이 사법 경찰관과는 이미 오래 전부터 알고 있었기 때문에 그는 나를 보자 진심으로 환영해 주었다. 여, 내가 헤이스팅즈 대위를 몰라볼 리 있겠습니까.드디어 당신의 야만 지대에서 돌아오셨군요!\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.140036  0.446721  0.496493  0.535682  0.001325  0.498296   \n",
            "1  BERT+LexRank    0.298025  0.286834  0.284170  0.217601  0.001026  0.264732   \n",
            "2          BESM    0.195691  0.572075  0.438657  0.388805  0.005986  0.450385   \n",
            "3   BESM+kobert    0.294434  0.738210  0.371531  0.349616  0.031771  0.438292   \n",
            "\n",
            "    grammar  \n",
            "0  0.921129  \n",
            "1  0.999038  \n",
            "2  0.988811  \n",
            "3  0.999000  \n",
            "Current result ==================================================\n",
            "Sample count: 11\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.148557  0.539680  0.520347    0.518409  0.006256   \n",
            "1  BERT+LexRank   0.212919  0.278005  0.196471    0.255966  0.012881   \n",
            "2          BESM   0.198150  0.498515  0.440132    0.354182  0.013570   \n",
            "3   BESM+kobert   0.216042  0.546819  0.445183    0.368159  0.013174   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.523632    0.937569  \n",
            "1     0.230626    0.998993  \n",
            "2     0.426024    0.997746  \n",
            "3     0.442403    0.997271  \n",
            "==================================================\n",
            "12 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "열차 사건, 공중에서의 사건, 사교계 살인 사건……. 정말이지 여기서기에 이분은 등장합니다. 은퇴하고 나서 훨씬 더 유명해지셨답니다. ” 포아로가 웃으며 말했다, “요전에도 헤이스팅즈에게 말했었지요. 나는 언제나 또다시 등장하는 프리마돈나 같다고“ “마지막에는 자신의 죽음을 탐정한다 해도 우스운 일이 아닐겁니다. 이건 기발한 생각인데, 정말. 책에 써둬야겠어. ” 재프 경감은 커다랗게 웃었다. 포아로는 내게 눈짓을 해보였다. “그것을 해야 할 사람은 우선 헤이스팅즈지요. ” 재프 경감은 웃었다. “하하하! 농담입니다, 농담입니다. ” 나는 그 생각이 어째서 악취미로 여겨졌다. 가엾게도 포아로는 점점 나이를 먹어 가고 있다. 죽음이 가까이 오는 것과 관계된 그 농담이 그에게 유쾌할 리 없을 것이다. 내 태도에 속마음이 나타나 있었던 모양이다. 재프 경감은 화재를 바꾸었다. “포아로 씨의 익명 편지에 대해 들으셨습니까?” 포아로가 말했다. “저번에 보여 줬지요.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.23047137260437 Generator / grammar loss:-0.12517908215522766   similarity loss:-0.10202910751104355\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            " 유명해지셨답니다. 헤이스팅즈에게 나는 탐정한다 써둬야겠어. 해보였다. 할 “하하하! 나는 그 생각이 여겨졌다. 유쾌할 것이다.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "책에 써둬야겠어.열차 사건, 공중에서의 사건, 사교계 살인 사건…….죽음이 가까이 오는 것과 관계된 그 농담이 그에게 유쾌할 리 없을 것이다.“포아로 씨의 익명 편지에 대해 들으셨습니까?” 포아로가 말했다.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "포아로가 웃으며 말했다, “요전에도 헤이스팅즈에게 말했었지요.나는 언제나 또다시 등장하는 프리마돈나 같다고“ “마지막에는 자신의 죽음을 탐정한다 해도 우스운 일이 아닐겁니다. 나는 그 생각이 어째서 악취미로 여겨졌다.가엾게도 포아로는 점점 나이를 먹어 가고 있다.\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "포아로가 웃으며 말했다, “요전에도 헤이스팅즈에게 말했었지요.나는 언제나 또다시 등장하는 프리마돈나 같다고“ “마지막에는 자신의 죽음을 탐정한다 해도 우스운 일이 아닐겁니다. 나는 그 생각이 어째서 악취미로 여겨졌다.가엾게도 포아로는 점점 나이를 먹어 가고 있다.\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.146091  0.515062  0.545035  0.418821  0.002899  0.501176   \n",
            "1  BERT+LexRank    0.236626  0.255164  0.172279  0.276255  0.002014  0.220049   \n",
            "2          BESM    0.302469  0.436276  0.381169  0.416057  0.000518  0.402657   \n",
            "3   BESM+kobert    0.302469  0.436276  0.381169  0.416057  0.000518  0.402657   \n",
            "\n",
            "    grammar  \n",
            "0  0.950365  \n",
            "1  0.999044  \n",
            "2  0.998967  \n",
            "3  0.998967  \n",
            "Current result ==================================================\n",
            "Sample count: 12\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.148351  0.537628  0.522404    0.510110  0.005976   \n",
            "1  BERT+LexRank   0.214895  0.276101  0.194455    0.257657  0.011976   \n",
            "2          BESM   0.206843  0.493329  0.435218    0.359339  0.012482   \n",
            "3   BESM+kobert   0.223244  0.537607  0.439848    0.372151  0.012119   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.521761    0.938635  \n",
            "1     0.229745    0.998997  \n",
            "2     0.424077    0.997848  \n",
            "3     0.439091    0.997412  \n",
            "==================================================\n",
            "13 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "” 나는 소리쳤다. “아, 그렇지. 완전히 잊고 있었어. 문제의 날짜가 언제였지?” 재프 경감이 말했다. “21일입니다. 그래서 내가 조사해 보았지요. 어제가 21일이었기 때문에, 어젯밤 혹시나 싶어 앤도버를 불러 보았습니다. 그랬더니 역시 장난이었지요. 아무 일도 없었으니까요. 어린아이가 돌을 던져 쇼윈도가 하나 깨진 일과 술주정꾼의 규칙 위반이 두 건. 그래서 우리 벨기에인 친구분(포아로)이 처음으로 헛짚으신 게 되었다는 이야기입니다. ” 포아로는 인정했다. “확실히 한시름 놓았습니다. ” 재프 경감이 동정하듯 말했다. “많이 염려하고 계신 것 같았습니다만? 가엾게도, 우리는 그런 것을 날마다 몇십 통씩 받는답니다. 달리 아무 하릴없는 머리가 좀 이상한 사람들이 그런 것을 쓰지요. 그리 악의가 있는 건 아닙니다. 뭐, 일종의 흥분에서지요. ” 포아로가 말했다.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.215564489364624 Generator / grammar loss:-0.12685763835906982   similarity loss:-0.10521713644266129\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "소리쳤다. “아, 경감이 “21일입니다. 그래서 보았지요. 어제가 그랬더니 없었으니까요. 하나 그래서 한시름 경감이 말했다. 흥분에서지요.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "어린아이가 돌을 던져 쇼윈도가 하나 깨진 일과 술주정꾼의 규칙 위반이 두 건.그래서 우리 벨기에인 친구분(포아로)이 처음으로 헛짚으신 게 되었다는 이야기입니다.” 재프 경감이 동정하듯 말했다.“많이 염려하고 계신 것 같았습니다만?문제의 날짜가 언제였지?” 재프 경감이 말했다.“21일입니다.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "어제가 21일이었기 때문에, 어젯밤 혹시나 싶어 앤도버를 불러 보았습니다. 그래서 우리 벨기에인 친구분(포아로)이 처음으로 헛짚으신 게 되었다는 이야기입니다. ”\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "어제가 21일이었기 때문에, 어젯밤 혹시나 싶어 앤도버를 불러 보았습니다. 그래서 우리 벨기에인 친구분(포아로)이 처음으로 헛짚으신 게 되었다는 이야기입니다. ”\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.178241  0.757084  0.556416  0.517003  0.011051  0.584726   \n",
            "1  BERT+LexRank    0.375000  0.084309  0.257293  0.176951  0.004996  0.198594   \n",
            "2          BESM    0.208333  0.552811  0.577939  0.409813  0.005483  0.522476   \n",
            "3   BESM+kobert    0.208333  0.552811  0.577939  0.409813  0.005483  0.522476   \n",
            "\n",
            "    grammar  \n",
            "0  0.959740  \n",
            "1  0.998997  \n",
            "2  0.995576  \n",
            "3  0.995576  \n",
            "Current result ==================================================\n",
            "Sample count: 13\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.150651  0.554509  0.525021    0.510640  0.006367   \n",
            "1  BERT+LexRank   0.227211  0.261348  0.199289    0.251448  0.011439   \n",
            "2          BESM   0.206958  0.497904  0.446197    0.363221  0.011944   \n",
            "3   BESM+kobert   0.222097  0.538777  0.450471    0.375048  0.011609   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.526604    0.940259  \n",
            "1     0.227348    0.998997  \n",
            "2     0.431646    0.997673  \n",
            "3     0.445505    0.997271  \n",
            "==================================================\n",
            "14 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "“그걸 그토록 진지하게 생각했던 건 정말 어리석은 짓이었습니다. 내가 코를 들이민 것은 새의 보금자리였던 셈이군요. ” 재프 경감이 말했다. “말과 벌을 혼동했던 겁니다. ” “뭐라고요?” “아니, 속담입니다. 자, 이제 가봐야겠군요. 이 가까이에 볼일이 있어서요. 도난품인 보석을 인수하러 왔지요. 그곳에 가는 길에 마음 놓으시도록 잠시 들렀던 겁니다. 회색 뇌세포를 뜻없이 써버리는 건 낭비니가요. ” 재프 경감은 기분좋게 웃으며 돌아갔다. 포아로가 말했다. “사람좋은 재프 경감은 그리 달라지지 않았지?” 나는 보복하듯 말했다. “아주 늙었군. 오소리같이 잿빛이 되었어. ” 포아로는 헛기침을 하고 나서 말했다. “헤이스팅즈, 아주 하찮은 장치가 있는데, 내 단골 이발사는 재간있는 사나이지. 머리에 그 장치를 붙이고 그 위에 자신의 머리칼을 벗어 놓는다네. 그건 가발이 아닐세, 잘 알겠지만. ” 나는 으르렁댔다.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.3092451095581055 Generator / grammar loss:-0.12003492563962936   similarity loss:-0.08886035531759262\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            " “그걸 건 정말 내가 “아니, 자, 가봐야겠군요. 가까이에 볼일이 있어서요. 말했다. “사람좋은 달라지지 않았지?”\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "도난품인 보석을 인수하러 왔지요.회색 뇌세포를 뜻없이 써버리는 건 낭비니가요.그곳에 가는 길에 마음 놓으시도록 잠시 들렀던 겁니다.내가 코를 들이민 것은 새의 보금자리였던 셈이군요.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "헤이스팅즈, 아주 하찮은 장치가 있는데, 내 단골 이발사는 재간있는 사나이지.\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "헤이스팅즈, 아주 하찮은 장치가 있는데, 내 단골 이발사는 재간있는 사나이지.\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.141612  0.320480  0.567967  0.468887  0.010343  0.488745   \n",
            "1  BERT+LexRank    0.220044  0.286580  0.251817  0.182143  0.001886  0.237868   \n",
            "2          BESM    0.093682  0.202661  0.228209  0.374317  0.005718  0.266932   \n",
            "3   BESM+kobert    0.093682  0.202661  0.228209  0.374317  0.005718  0.266932   \n",
            "\n",
            "    grammar  \n",
            "0  0.966156  \n",
            "1  0.999017  \n",
            "2  0.998940  \n",
            "3  0.998940  \n",
            "Current result ==================================================\n",
            "Sample count: 14\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.150005  0.537793  0.528088    0.507658  0.006651   \n",
            "1  BERT+LexRank   0.226699  0.263150  0.203041    0.246498  0.010756   \n",
            "2          BESM   0.198867  0.476816  0.430626    0.364014  0.011499   \n",
            "3   BESM+kobert   0.212924  0.514769  0.434595    0.374995  0.011188   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0      0.52390    0.942108  \n",
            "1      0.22810    0.998999  \n",
            "2      0.41988    0.997764  \n",
            "3      0.43275    0.997390  \n",
            "==================================================\n",
            "15 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "“포아로, 분통 치미는 자네 이발사의 더러운 발견 따윈 아무래도 좋네. 대체 내 머리가 어떻다고 그런 소리를 하는 건가?” “아니, 아무렇지도 않아, 아무렇지도. ” “내가 대머리가 되어가고 있다는 건 아니겠지?” “물론 그런 건 아닐세! 그런 건……. ” “그 나라의 뜨거운 여름은 절로 얼마쯤 머리를 벗겨지게 하지만 말이야. 그냥 질좋은 헤어토닉이나 가져가지. ” “그게 좋겠군. ” “그렇다 해도 재프 경감 따위가 관여할 일은 아니야. 녀석은 언제나 기분좋지 않았지. 게다가 유머 센스도 없어. 사람이 앉으려고 할 때 의자를 잡아당겨지면 웃는 그런 사나이거든. ” “그러면 사람들은 대개 웃지. ” “모름지기 센스가 없단 말일세. ” “앉으려던 사람의 입장에서 본다면 확실히 그렇지. ” “그렇네. ” 나는 얼마쯤 기분을 돌리며 다시 말했다―머리칼이 적어졌다는 말에 내가 아주 민감해 있다는 것을 인정하지 않으면 안 되겠다. “익명 편지가 아무 일 없었다니 유감이군. ‘ “그것은 완전히 내 잘못 생각이었네. 그 편지에 어쩐지 피비린내나는 것 같은 느낌이 있었는데, 그러나 단순한 장난이었어. 아, 나도 나이 먹어 아무것도 아닌 일에 짖어대는 눈먼 개처럼 의심이 많아져 버렸나 보네.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.332427501678467 Generator / grammar loss:-0.11993356049060822   similarity loss:-0.08637949824333191\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "“포아로, 좋네. “물론 건……. ” “그 나라의 절로 머리를 하지만 ” ” 재프 경감 유머 사나이거든. 대개 센스가 “그렇네. 일 나이\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "그냥 질좋은 헤어토닉이나 가져가지.” “그게 좋겠군.” “앉으려던 사람의 입장에서 본다면 확실히 그렇지.” “그렇네.사람이 앉으려고 할 때 의자를 잡아당겨지면 웃는 그런 사나이거든.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "나는 얼마쯤 기분을 돌리며 다시 말했다―머리칼이 적어졌다는 말에 내가 아주 민감해 있다는 것을 인정하지 않으면 안 되겠다. “ 그 편지에 어쩐지 피비린내나는 것 같은 느낌이 있었는데, 그러나 단순한 장난이었어.\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "나는 얼마쯤 기분을 돌리며 다시 말했다―머리칼이 적어졌다는 말에 내가 아주 민감해 있다는 것을 인정하지 않으면 안 되겠다. “ 아, 나도 나이 먹어 아무것도 아닌 일에 짖어대는 눈먼 개처럼 의심이 많아져 버렸나 보네.\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.124795  0.490123  0.459126  0.420288  0.000816  0.453674   \n",
            "1  BERT+LexRank    0.165846  0.231311  0.342144  0.240685  0.002518  0.289540   \n",
            "2          BESM    0.192118  0.401859  0.345628  0.543765  0.006951  0.416316   \n",
            "3   BESM+kobert    0.198686  0.378314  0.339265  0.511452  0.005433  0.398731   \n",
            "\n",
            "    grammar  \n",
            "0  0.935316  \n",
            "1  0.999003  \n",
            "2  0.999021  \n",
            "3  0.999033  \n",
            "Current result ==================================================\n",
            "Sample count: 15\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.148324  0.534615  0.523491    0.501833  0.006262   \n",
            "1  BERT+LexRank   0.222642  0.261028  0.212314    0.246111  0.010207   \n",
            "2          BESM   0.198417  0.471818  0.424960    0.375997  0.011196   \n",
            "3   BESM+kobert   0.211975  0.505672  0.428240    0.384093  0.010804   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.519218    0.941656  \n",
            "1     0.232196    0.998999  \n",
            "2     0.419643    0.997848  \n",
            "3     0.430482    0.997500  \n",
            "==================================================\n",
            "16 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "” 나는 웃으며 말했다. “내가 도우려면, 우리는 다른 데서 온갖 진수가 모아진 멋진 범죄를 찾아내야만 되겠군. ” “자네는 요전에 내가 했던 말을 기억하고 있나? 만일 요리를 주문하듯 범죄를 주문할 수 있다면 어떤 것을 고르겠나?” 나는 좋아진 그의 기분에 휩쓸려 말했다. “그렇지, 메뉴를 잘 봐야 하지 않겠나. 강도? 위조지폐? 아니, 이런 건 안 돼? 이건 식물성 요리 같지? 역시 살인이 좋겠군. 피비린내나는 살인사건, 물론 여러 가지가 딸린 것으로. ” “옳지, 오르되브르(식사 전 또는 술안주로 먹는 가벼운 요리)로군. ” “피해자는 남자로 할까, 여자로 할까? 역시 남자가 좋겠어. 누군가 유명한 사람, 미국의 백만장자나 국무장관이나 신문사 사장쯤 되는 인물. 범행 현장은……그렇지, 훌륭한 낡은 도서관 같은 데가 어떨까? 분위기로서 이 이상의 것은 없네. 흉기는 기묘한 형태로 구부러진 단도 아니면, 뭔가 둔기 같은 것, 예를 들면 조각된 돌상이라든지……. ” 포아로는 한숨을 쉬었다. “그렇잖으면 물론 독약.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.4535558223724365 Generator / grammar loss:-0.12763217091560364   similarity loss:-0.081474170088768\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "나는 우리는 “자네는 만일 주문하듯 고르겠나?” 나는 기분에 휩쓸려 안 좋겠군. “옳지, 역시 유명한 백만장자나 신문사 인물. 없네.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "역시 남자가 좋겠어.누군가 유명한 사람, 미국의 백만장자나 국무장관이나 신문사 사장쯤 되는 인물.” “피해자는 남자로 할까, 여자로 할까?아니, 이런 건 안 돼?” 포아로는 한숨을 쉬었다.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "내가 도우려면, 우리는 다른 데서 온갖 진수가 모아진 멋진 범죄를 찾아내야만 되겠군. ” “ 역시 남자가 좋겠어.누군가 유명한 사람, 미국의 백만장자나 국무장관이나 신문사 사장쯤 되는 인물.\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "내가 도우려면, 우리는 다른 데서 온갖 진수가 모아진 멋진 범죄를 찾아내야만 되겠군. ” “ 역시 남자가 좋겠어.누군가 유명한 사람, 미국의 백만장자나 국무장관이나 신문사 사장쯤 되는 인물. 흉기는 기묘한 형태로 구부러진 단도 아니면, 뭔가 둔기 같은 것, 예를 들면 조각된 돌상이라든지……. ”\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.142857  0.359273  0.392094  0.414743  0.000519  0.392325   \n",
            "1  BERT+LexRank    0.202703  0.137062  0.170831  0.228606  0.001429  0.181410   \n",
            "2          BESM    0.204633  0.437141  0.278188  0.327385  0.004415  0.324738   \n",
            "3   BESM+kobert    0.318533  0.558340  0.358646  0.598940  0.011030  0.470673   \n",
            "\n",
            "    grammar  \n",
            "0  0.975554  \n",
            "1  0.999038  \n",
            "2  0.999032  \n",
            "3  0.996944  \n",
            "Current result ==================================================\n",
            "Sample count: 16\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.147983  0.523656  0.515278    0.496390  0.005903   \n",
            "1  BERT+LexRank   0.221396  0.253280  0.209721    0.245016  0.009658   \n",
            "2          BESM   0.198805  0.469651  0.415787    0.372959  0.010772   \n",
            "3   BESM+kobert   0.218635  0.508963  0.423890    0.397521  0.010819   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.511287    0.943774  \n",
            "1     0.229022    0.999002  \n",
            "2     0.413711    0.997922  \n",
            "3     0.432994    0.997465  \n",
            "==================================================\n",
            "17 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "하지만 이것은 아무래도 너무 전문적인 것 같네. 그렇다면 깊은 밤에 메아리치는 권총 소리……이런 것으로 할까. 그리고 아름다운 여자 하나, 둘. ” 친구는 중얼거렸다. “그녀는 빨강머리겠지. ” “신통치 못한 농담이군. 물론 아름다운 여자 한 사람에게 잘못된 혐의가 씌워져야만 되겠지. 그리고 그녀와 젊은이 사이에 오해가 생기고. 물론 그 밖에도 몇 사람에게 혐의가 돌아가지 않으면 안 되네. 이를테면 피해자의 친구거나 경쟁 상대인 피부빛이 검고 위험한 타입의 중년 여자, 얌전한 비서. 이들이 유력한 혐의자인데, 거기에 행동거지가 무뚝뚝하고 성실한 사나이인 해고된 하인이라든지 사냥터 관리인 등이 두어 사람쯤 그리고 재프 경감 같은 얼치기 형사. 그래, 이쯤이면 되겠지. ” “그것이 자네가 말한 온갖 진수가 모아진 범죄인가?” “찬성하지 않는구먼?” 포아로는 한심스러운 듯 나를 보았다. “자네는 지금까지 씌어진 거의 모든 미스터리 소설의 아주 멋있는 줄거리를 만들어 주었네. ” “그럼, 자네라면 어떤 주문을 할 건가?” 포아로는 눈을 감고 의자에 기댔다. 그의 목소리는 입술 사이로 조용히 흘러나왔다. “아주 단순한 범죄, 복잡한 데가 조금도 없는 범죄. 조용한 가정 생활의 범죄……열광적이 아니고 아주 내밀스러운. ” “범죄에 내밀스러운 게 있을 수 있는가?” 포아로는 중얼거리듯 말했다. “네 사람이 앉아서 브리지를 하고 있네.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.4783127307891846 Generator / grammar loss:-0.1407381147146225   similarity loss:-0.0919622927904129\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "이것은 아무래도 그렇다면 하나, 둘. “그녀는 빨강머리겠지. 물론 그리고 그녀와 몇 친구거나 경쟁 비서. 거기에 해고된 자네가 범죄인가?” 아주 줄거리를 기댔다. 그의 복잡한 범죄.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "“네 사람이 앉아서 브리지를 하고 있네.그렇다면 깊은 밤에 메아리치는 권총 소리……이런 것으로 할까.그래, 이쯤이면 되겠지.하지만 이것은 아무래도 너무 전문적인 것 같네.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "이를테면 피해자의 친구거나 경쟁 상대인 피부빛이 검고 위험한 타입의 중년 여자, 얌전한 비서. 자네는 지금까지 씌어진 거의 모든 미스터리 소설의 아주 멋있는 줄거리를 만들어 주었네.” “\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "이를테면 피해자의 친구거나 경쟁 상대인 피부빛이 검고 위험한 타입의 중년 여자, 얌전한 비서. 이들이 유력한 혐의자인데, 거기에 행동거지가 무뚝뚝하고 성실한 사나이인 해고된 하인이라든지 사냥터 관리인 등이 두어 사람쯤 그리고 재프 경감 같은 얼치기 형사.\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.145743  0.433754  0.546696  0.327629  0.008001  0.458387   \n",
            "1  BERT+LexRank    0.137085  0.382522  0.186761  0.221257  0.007280  0.236262   \n",
            "2          BESM    0.150072  0.325465  0.419658  0.228706  0.006077  0.343533   \n",
            "3   BESM+kobert    0.204906  0.343544  0.404463  0.244562  0.004342  0.344309   \n",
            "\n",
            "    grammar  \n",
            "0  0.995966  \n",
            "1  0.999043  \n",
            "2  0.999022  \n",
            "3  0.992904  \n",
            "Current result ==================================================\n",
            "Sample count: 17\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.147851  0.518368  0.517127    0.486463  0.006026   \n",
            "1  BERT+LexRank   0.216436  0.260882  0.208371    0.243619  0.009519   \n",
            "2          BESM   0.195939  0.461170  0.416014    0.364474  0.010496   \n",
            "3   BESM+kobert   0.217827  0.499233  0.422747    0.388523  0.010438   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.508176    0.946844  \n",
            "1     0.229448    0.999004  \n",
            "2     0.409583    0.997986  \n",
            "3     0.427777    0.997197  \n",
            "==================================================\n",
            "18 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "그리고 한 사람이 그 게임에 끼지 않고 벽난로 옆 의자에 앉아 있지. 밤이 깊어졌을 즈음 난롯불 옆에 앉아 있던 사나이가 죽은 것을 알게 되네. 네 사람 가운데 누군가가 손이 비게 되었을 때 죽인 것인데, 모두들 게임에 정신이 팔려 모르고 있었지. 자, 이것이 사건이네. 범인은 네 사람 가운데 누구일까?” (나, 아시겠죠? 다들<테이블위의카드>네요. ) “도무지 자극적인 데가 조금도 없는걸. ” 포아로는 비난하듯 눈길로 나를 보았다. “없지. 이상한 모양으로 구부러진 단도도, 협박도, 신상의 눈에서 훔쳐 낸 에메랄드도, 흔적을 알 수 없는 동양의 독약 같은 것도 없네. 헤이스팅즈, 자네는 아무래도 멜러 드라마 애호가로군. 자네는 하나의 살인이 아니라 연쇄적인 살인 쪽이 좋은 거지?” “그렇네, 책 속의 두 번째 살인은 경기가 좋아 보이던걸. 제1장에서 살인이 일어나 마지막 페이지 바로 앞까지 모두들의 알리바이가 성립되어 있다는 건……그래, 좀 따분하지. ‘ 전화가 울려 포아로가 일어나 받으러 갔다. “여보세요, 에르큘 포아로입니다. ‘ 잠시 말없이 듣고 있던 그의 얼굴빛이 달라졌다. 그의 대답은 짧게 토막토막 끊어졌다, “그랬군요……물론, 그렇지요……아, 가겠습니다……당연합니다……그야 당신 말대로겠지요. 그렇지요, 갖고 가겠습니다. 그럼, 곧. ” 그는 수화기를 내려놓고 방을 가로질러 내 곁으로 돌아왔다.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.391206741333008 Generator / grammar loss:-0.12604853510856628   similarity loss:-0.08641715347766876\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "게임에 팔려 이상한 살인은 일어나 페이지 바로 앞까지 받으러 갔다. “여보세요, ‘ 그의 얼굴빛이 달라졌다. 짧게 토막토막 끊어졌다, “그랬군요……물론, 말대로겠지요. 곧. 곁으로 돌아왔다. \n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "이상한 모양으로 구부러진 단도도, 협박도, 신상의 눈에서 훔쳐 낸 에메랄드도, 흔적을 알 수 없는 동양의 독약 같은 것도 없네.밤이 깊어졌을 즈음 난롯불 옆에 앉아 있던 사나이가 죽은 것을 알게 되네.자네는 하나의 살인이 아니라 연쇄적인 살인 쪽이 좋은 거지?” “그렇네, 책 속의 두 번째 살인은 경기가 좋아 보이던걸.다들<테이블위의카드>네요. )\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "밤이 깊어졌을 즈음 난롯불 옆에 앉아 있던 사나이가 죽은 것을 알게 되네. 이상한 모양으로 구부러진 단도도, 협박도, 신상의 눈에서 훔쳐 낸 에메랄드도, 흔적을 알 수 없는 동양의 독약 같은 것도 없네. 제1장에서 살인이 일어나 마지막 페이지 바로 앞까지 모두들의 알리바이가 성립되어 있다는 건……그래, 좀 따분하지. ‘\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "밤이 깊어졌을 즈음 난롯불 옆에 앉아 있던 사나이가 죽은 것을 알게 되네. 네 사람 가운데 누군가가 손이 비게 되었을 때 죽인 것인데, 모두들 게임에 정신이 팔려 모르고 있었지. 그의 대답은 짧게 토막토막 끊어졌다, “그랬군요……물론, 그렇지요……아, 가겠습니다……당연합니다……그야 당신 말대로겠지요.\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.156433  0.418130  0.426163  0.699783  0.017140  0.506642   \n",
            "1  BERT+LexRank    0.285088  0.267143  0.297484  0.009302  0.016717  0.204961   \n",
            "2          BESM    0.261696  0.671324  0.479500  0.476772  0.008295  0.517046   \n",
            "3   BESM+kobert    0.245614  0.745944  0.369573  0.609932  0.024214  0.516955   \n",
            "\n",
            "    grammar  \n",
            "0  0.936830  \n",
            "1  0.999016  \n",
            "2  0.993604  \n",
            "3  0.995715  \n",
            "Current result ==================================================\n",
            "Sample count: 18\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.148328  0.512799  0.512073    0.498314  0.006644   \n",
            "1  BERT+LexRank   0.220250  0.261230  0.213322    0.230601  0.009918   \n",
            "2          BESM   0.199592  0.472845  0.419541    0.370712  0.010374   \n",
            "3   BESM+kobert   0.219371  0.512939  0.419793    0.400824  0.011203   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.508090    0.946288  \n",
            "1     0.228087    0.999005  \n",
            "2     0.415553    0.997743  \n",
            "3     0.432731    0.997114  \n",
            "==================================================\n",
            "19 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "“재프 경감에게서 온 걸세, 헤이스팅즈. ” “그래서?” “경찰국으로 돌아가자마자 마침 앤도버에서 연락이 있었다는 거야. ” 나는 흥분하여 소리쳤다. “앤도버?” 포아로가 천천히 말했다. “노파가 하나 살해되었다는군. 애셔(Ascher)라는 이름으로, 담배와 신문을 파는 조그만 가게의 노파일세. ” 나는 얼마쯤 맥이 풀렸다. 앤도버라는 이름으로 부채질되었던 내 흥미는 어리둥절해졌다. 나는 뭔가 환상적인, 아주 색다른 것을 기대하고 있었는데! 조그만 담배 가게 노파가 살해된 일 따위는 아무래도 그리 신통찮다. 포아로는 여전히 느릿느릿한 무게있는 목소리로 말을 이었다. “앤도버 경찰에서는 범인을 체표할 수 있다고 생각하는 모양이야. ” 나는 다시 한 번 맥이 풀렸다. “노파는 그 남편과 사이가 나빴던 것 같네. 남편은 술꾼이며 질나쁜 녀석으로 종종 노파를 죽이겠다고 협박했었다는군. 그러나 그곳 경찰에서는 다른 점도 고려하여 내가 받은 익명의 편지를 보고 싶다는 거야. 나는 곧 자네와 함께 앤도버로 가겟다고 말해 두었네. ” 나는 얼마쯤 기운을 되찾았다. 시시하게 보일지라도 아무튼 범죄임에 틀림없다. 내가 범죄니 범인이니 하는 것에 관계하고부터 벌써 많은 세월이 흘렀다.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.3472237586975098 Generator / grammar loss:-0.13552340865135193   similarity loss:-0.10044573247432709\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            " “재프 경감에게서 걸세, 헤이스팅즈. ” “그래서?” 앤도버에서 연락이 나는 흥분하여 소리쳤다. 말했다. 조그만 뭔가 환상적인, 노파가 있다고 풀렸다. 그러나 흘렀다.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "“노파는 그 남편과 사이가 나빴던 것 같네.남편은 술꾼이며 질나쁜 녀석으로 종종 노파를 죽이겠다고 협박했었다는군.애셔(Ascher)라는 이름으로, 담배와 신문을 파는 조그만 가게의 노파일세.조그만 담배 가게 노파가 살해된 일 따위는 아무래도 그리 신통찮다.그러나 그곳 경찰에서는 다른 점도 고려하여 내가 받은 익명의 편지를 보고 싶다는 거야.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "애셔(Ascher)라는 이름으로, 담배와 신문을 파는 조그만 가게의 노파일세. ” 그러나 그곳 경찰에서는 다른 점도 고려하여 내가 받은 익명의 편지를 보고 싶다는 거야.\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "애셔(ascher)라는 이름으로, 담배와 신문을 파는 조그만 가게의 노파일세. ” 그러나 그곳 경찰에서는 다른 점도 고려하여 내가 받은 익명의 편지를 보고 싶다는 거야.\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.155629  0.730064  0.444412  0.388091  0.022413  0.484646   \n",
            "1  BERT+LexRank    0.316225 -0.003079  0.255101  0.276348  0.016132  0.209839   \n",
            "2          BESM    0.155629  0.205396  0.288986  0.329535  0.002671  0.284432   \n",
            "3   BESM+kobert    0.155629  0.206035  0.289110  0.329535  0.002643  0.284622   \n",
            "\n",
            "    grammar  \n",
            "0  0.963879  \n",
            "1  0.999030  \n",
            "2  0.998953  \n",
            "3  0.998955  \n",
            "Current result ==================================================\n",
            "Sample count: 19\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.148712  0.524234  0.508512    0.492513  0.007474   \n",
            "1  BERT+LexRank   0.225302  0.247319  0.215520    0.233009  0.010245   \n",
            "2          BESM   0.197278  0.458769  0.412670    0.368545  0.009968   \n",
            "3   BESM+kobert   0.216016  0.496786  0.412915    0.397071  0.010752   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.506857    0.947214  \n",
            "1     0.227127    0.999006  \n",
            "2     0.408652    0.997807  \n",
            "3     0.424936    0.997211  \n",
            "==================================================\n",
            "20 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "나는 포아로의 다음 말을 거의 듣지 있지 않았다. 그러나 그것은 나중에 중요한 뜻을 지니고 내 기억 속에 되살아났다. 에르큘 포아로가 이렇게 말했다. “이것이 시작이다. ” < 철도 안내서 > 우리는 앤도버에서 글렌 형사의 마중을 받았다. 그는 키가 크고 머리칼이 아름다운 남자로 기분 좋은 미소를 떠올리고 있었다. 이야기를 간결이 하기 위해 사건의 사실만 간단히 밝혀 두는 게 좋으리라. 범죄는 22일 오전 1시에 그곳 순경에 의해 발견되었다. 순찰을 돌면서 가게 문을 밀어 보니 잠겨 있지 않았다. 안으로 들어가자 처음에는 아무도 없는 듯했으나, 계산대 쪽으로 회중전등을 돌리니 노파의 웅크린 시체가 눈에 들어왔다. 경찰의가 현장에 와 닿아 노파가 뒷머리를 강하게 얻어맞았음을 알아냈는데, 아마도 계산대 뒤의 선반에서 담배 봉지를 꺼내는 도중에 얻어맞은 듯했다. 범행은 일곱 시간 내지 아홉 시간 전에 행해진 것 같았다. 형사는 설명했다. “그러나 더 정확한 시간을 추정할 수 있습니다. 5시 30분에 담배를 사러 들어갔던 사나이가 있습니다. 그리고 6시 5분 좀 지나서 가게에 들어갔다가 아무도 없는 줄 알고 그냥 나온 다른 남자가 있습니다. 그러니까 범행 시간을 5시 30분에서 6시 5분 사이로 추정할 수 있지요. 이웃에서 애셔를 보았다고 말해 온 사람은 아직 없습니다. 그러나 물론 이제부터입니다. 그는 9시쯤 <스리크라운즈>에서 꽤 취해 있었습니다.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.502201557159424 Generator / grammar loss:-0.14643016457557678   similarity loss:-0.09511272609233856\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            " 나는 있지 않았다. 글렌 좋은 간결이 게 안으로 돌리니 웅크린 그리고 6시 지나서 아무도 없는 줄 알고 나온 다른 30분에서 애셔를 보았다고 말해 9시쯤 취해 있었습니다.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "이웃에서 애셔를 보았다고 말해 온 사람은 아직 없습니다.에르큘 포아로가 이렇게 말했다.그는 키가 크고 머리칼이 아름다운 남자로 기분 좋은 미소를 떠올리고 있었다.그는 9시쯤 <스리크라운즈>에서 꽤 취해 있었습니다.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "그는 키가 크고 머리칼이 아름다운 남자로 기분 좋은 미소를 떠올리고 있었다. 경찰의가 현장에 와 닿아 노파가 뒷머리를 강하게 얻어맞았음을 알아냈는데, 아마도 계산대 뒤의 선반에서 담배 봉지를 꺼내는 도중에 얻어맞은 듯했다. 그리고 6시 5분 좀 지나서 가게에 들어갔다가 아무도 없는 줄 알고 그냥 나온 다른 남자가 있습니다.\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "그는 키가 크고 머리칼이 아름다운 남자로 기분 좋은 미소를 떠올리고 있었다. 그러니까 범행 시간을 5시 30분에서 6시 5분 사이로 추정할 수 있지요.\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.135977  0.316636  0.279770  0.361639  0.001121  0.311704   \n",
            "1  BERT+LexRank    0.168555  0.139560  0.238680  0.256438  0.002645  0.224183   \n",
            "2          BESM    0.256374  0.208297  0.422653  0.411568  0.009710  0.376456   \n",
            "3   BESM+kobert    0.118980  0.182882  0.287266  0.366385  0.005648  0.290125   \n",
            "\n",
            "    grammar  \n",
            "0  0.984746  \n",
            "1  0.998971  \n",
            "2  0.999005  \n",
            "3  0.999020  \n",
            "Current result ==================================================\n",
            "Sample count: 20\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.148075  0.513854  0.497075    0.485969  0.007156   \n",
            "1  BERT+LexRank   0.222464  0.241931  0.216678    0.234180  0.009865   \n",
            "2          BESM   0.200233  0.446245  0.413169    0.370696  0.009955   \n",
            "3   BESM+kobert   0.211164  0.481091  0.406633    0.395537  0.010497   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.497099    0.949091  \n",
            "1     0.226980    0.999004  \n",
            "2     0.407042    0.997866  \n",
            "3     0.418196    0.997302  \n",
            "==================================================\n",
            "21 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "체포하는 대로 곧 용의자로 잡아 둘 겁니다. ” 포아로가 물었다. “그리 호감주는 타입의 사나이가 아닌 모양이군요?” “싫은 사람입니다. ” “그는 자기 아내와 함께 살고 있지 않았던가요?” “그렇습니다. 몇 해 전에 헤어졌지요. 애셔는 독일 사람으로 한때 급사일을 한 적도 있었습니다만, 술을 너무 마셔서 차츰 그를 고용하는 곳이 없게 되었습니다. 그래서 그 부인이 일을 나가게 되었지요. 마지막으로 한 일은 미스 로즈라는 노부인의 요리사 겸 가정부였습니다. 급료를 받아 남편에게 꽤 많은 돈을 주었던 듯한데, 그는 몽땅 마셔 버리고는 자기 마누라가 일하는 곳으로 가서 소동을 벌이곤 했답니다. 그래서 애셔 부인은 미스 로즈네 농장으로 가서 일하게 되었습니다. 거기는 앤도버에서 3마일 떨어진 완전한 시골이어서 그도 그리 자주 찾아가지 못했지요. 미스 로즈가 세상을 떠나자 애셔 부인은 유산을 조금 받았습니다. 그래서 그 돈으로 담배와 신문을 파는 이 조그만 가게를 시작했습니다. 싸구려 담배와 얼마 안 되는 신문뿐이어서 겨우 먹고 사는 정도였지요. 애셔가 자주 찾아와 그녀에게 욕을 하곤 했는데, 그녀 쪽에서는 귀찮고 하니까 잔돈푼이나 줘서 쫓아 버리곤 했지요. 1주일에 15실링은 줬던 것 같습니다. ” 포아로가 물었다. “아이들은 있었소?” “없습니다. 조카딸이 하나 오버튼 가까이에서 일하고 있습니다. 아주 고집이 센 똑똑한 아가씨지요.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.4041388034820557 Generator / grammar loss:-0.12794533371925354   similarity loss:-0.08696750551462173\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "둘 ” “그리 타입의 사나이가 모양이군요?” “싫은 자기 아내와 살고 몇 해 헤어졌지요. 있었습니다만, 부인이 되었지요. 거기는 자주 유산을 조금 파는 조그만 잔돈푼이나 했지요.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "체포하는 대로 곧 용의자로 잡아 둘 겁니다.그래서 그 돈으로 담배와 신문을 파는 이 조그만 가게를 시작했습니다.1주일에 15실링은 줬던 것 같습니다.“그리 호감주는 타입의 사나이가 아닌 모양이군요?” “싫은 사람입니다.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "애셔는 독일 사람으로 한때 급사일을 한 적도 있었습니다만, 술을 너무 마셔서 차츰 그를 고용하는 곳이 없게 되었습니다. 거기는 앤도버에서 3마일 떨어진 완전한 시골이어서 그도 그리 자주 찾아가지 못했지요. 싸구려 담배와 얼마 안 되는 신문뿐이어서 겨우 먹고 사는 정도였지요.애셔가 자주 찾아와 그녀에게 욕을 하곤 했는데, 그녀 쪽에서는 귀찮고 하니까 잔돈푼이나 줘서 쫓아 버리곤 했지요.\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "애셔는 독일 사람으로 한때 급사일을 한 적도 있었습니다만, 술을 너무 마셔서 차츰 그를 고용하는 곳이 없게 되었습니다. 급료를 받아 남편에게 꽤 많은 돈을 주었던 듯한데, 그는 몽땅 마셔 버리고는 자기 마누라가 일하는 곳으로 가서 소동을 벌이곤 했답니다. 거기는 앤도버에서 3마일 떨어진 완전한 시골이어서 그도 그리 자주 찾아가지 못했지요.\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.142037  0.387560  0.534521  0.379175  0.005089  0.458525   \n",
            "1  BERT+LexRank    0.175036  0.150935  0.217861  0.142296  0.001140  0.181806   \n",
            "2          BESM    0.309900  0.230511  0.542350  0.311870  0.017443  0.410838   \n",
            "3   BESM+kobert    0.272597  0.281762  0.538592  0.277069  0.014931  0.408769   \n",
            "\n",
            "    grammar  \n",
            "0  0.950800  \n",
            "1  0.998981  \n",
            "2  0.999026  \n",
            "3  0.999032  \n",
            "Current result ==================================================\n",
            "Sample count: 21\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.147788  0.507840  0.498858    0.480883  0.007057   \n",
            "1  BERT+LexRank   0.220206  0.237598  0.216735    0.229805  0.009450   \n",
            "2          BESM   0.205455  0.435972  0.419321    0.367895  0.010312   \n",
            "3   BESM+kobert   0.214090  0.471599  0.412916    0.389896  0.010708   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.495262    0.949172  \n",
            "1     0.224828    0.999003  \n",
            "2     0.407223    0.997922  \n",
            "3     0.417747    0.997384  \n",
            "==================================================\n",
            "22 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "” “그 애셔라는 사나이가 아내를 자주 협박했었다는 거지요?” “그렇습니다. 그는 술에 취하면 무섭게 변해서 아내의 머리를 박살내겠다는 둥 소리를 질러대곤 했답니다. 애셔 부인은 정말 끔찍한 일을 당한 거지요. ” “그녀는 몇 살이었소?” “60살이 다 되었지요. 아마. 훌륭하고 부지런한 사람이었습니다. ” 포아로는 신중하게 말했다. “그러면 그 애셔라는 사나이가 범인이라는 게 당신 의견이오?” 형사는 조심스럽게 헛기침을 했다. “그렇게 말하는 건 좀 성급한 판단입니다만, 프란츠 애셔가 지난밤에 어떻게 지냈는지 그 자신의 설명을 듣고 싶은 겁니다, 포아로 씨. 만일 만족할 만한 설명을 들을 수 있다면 좋겠지만, 그렇지 않으면……. ” 그는 꽤 의미심장하게 말을 끊었다. “가게에서는 아무것도 없어지지 않았소?” “네, 아무것도. 돈도 그대로 다 있고, 훔쳐 간 흔적이 전혀 없습니다. ” “그 애셔라는 사나이가 술에 취해 가게로 들어와 아내를 욕하다가 끝내 때려 죽였다는 거로군요?” “네, 그것이 가장 타당한 해석이겠지요. 그러나 당신이 받으셨다는 그 이상한 편지도 고려해 보고 싶습니다, 포아로 씨. 그것이 이 애셔라는 사나이로부터 보내진 것인지 어떤지 알 수 없으니까요. ” 포아로가 편지를 건네주자 형사는 이마를 찌푸리고 그것을 읽었다. 형사는 마침내 말했다. “아무래도 애셔가 쓴 것 같지는 않군요. 도대체 이 <우리> 영국 경찰이라는 말을 애셔가 쓸 턱이 없지요.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.448955774307251 Generator / grammar loss:-0.1347632259130478   similarity loss:-0.08908989280462265\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "” 애셔라는 아내를 자주 협박했었다는 거지요?” 무섭게 둥 질러대곤 거지요. “그녀는 살이었소?” 다 되었지요. 아마. “그러면 그 겁니다, 포아로 그는 “가게에서는 없어지지 않았소?” 아무것도.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "” “그녀는 몇 살이었소?” “60살이 다 되었지요.아마.돈도 그대로 다 있고, 훔쳐 간 흔적이 전혀 없습니다.“가게에서는 아무것도 없어지지 않았소?” “네, 아무것도.그는 술에 취하면 무섭게 변해서 아내의 머리를 박살내겠다는 둥 소리를 질러대곤 했답니다.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "그는 술에 취하면 무섭게 변해서 아내의 머리를 박살내겠다는 둥 소리를 질러대곤 했답니다. 만일 만족할 만한 설명을 들을 수 있다면 좋겠지만, 그렇지 않으면……. ” 네, 그것이 가장 타당한 해석이겠지요.그러나 당신이 받으셨다는 그 이상한 편지도 고려해 보고 싶습니다, 포아로 씨.\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "그는 술에 취하면 무섭게 변해서 아내의 머리를 박살내겠다는 둥 소리를 질러대곤 했답니다. 네, 그것이 가장 타당한 해석이겠지요.그러나 당신이 받으셨다는 그 이상한 편지도 고려해 보고 싶습니다, 포아로 씨. 그것이 이 애셔라는 사나이로부터 보내진 것인지 어떤지 알 수 없으니까요. ”\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.152022  0.694152  0.517158  0.376980  0.016842  0.510503   \n",
            "1  BERT+LexRank    0.199442  0.431548  0.219090  0.157998  0.013744  0.243254   \n",
            "2          BESM    0.217573  0.561748  0.438635  0.445418  0.003193  0.465293   \n",
            "3   BESM+kobert    0.218968  0.597146  0.450107  0.600513  0.004917  0.524636   \n",
            "\n",
            "    grammar  \n",
            "0  0.993703  \n",
            "1  0.999014  \n",
            "2  0.999031  \n",
            "3  0.996974  \n",
            "Current result ==================================================\n",
            "Sample count: 22\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.147980  0.516309  0.499690    0.476161  0.007502   \n",
            "1  BERT+LexRank   0.219262  0.246414  0.216842    0.226541  0.009645   \n",
            "2          BESM   0.206006  0.441689  0.420198    0.371419  0.009988   \n",
            "3   BESM+kobert   0.214312  0.477306  0.414607    0.399469  0.010445   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.495955    0.951196  \n",
            "1     0.225666    0.999004  \n",
            "2     0.409863    0.997972  \n",
            "3     0.422605    0.997365  \n",
            "==================================================\n",
            "23 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "그야말로 각별히 교묘하게 행동하려는 게 아니었다면 말입니다. 게다가 그에겐 그만한 머리가 없습니다. 그는 이제 산송장입니다. 다 망가져 버렸지요. 이런 글을 쓰기에는 그의 손이 너무 떨릴걸요. 편지지도 잉크도 고급품이고. 그러나 편지에는 21일이라고 한 것은 이상하군요. 물론 우연의 일치겠지만요. ” “그렇겠지요. ” “하지만 이런 일치는 좋지 않습니다, 포아로 씨. 너무 딱 들어맞으니 말입니다. ” 그는 잠시 입을 다물고 있었다. 그의 이마에 주름이 잡혔다. “ABC. 대체 ABC란 어떤 녀석일까요? 메리 드로워―조카딸입니다만―가 좀 도움이 될지도 모르겠군요. 뭐 수고하시는 김에 말입니다. 이 편지만 없다면 나느 프란츠 애셔에게 내기를 걸어도 좋은데요. ” “애셔 부인의 경력은 알고 있소?” “그녀는 햄프셔 태생으로 처녀 때 런던에 나가 직장 생활을 했지요. 거기서 애셔를 만나 결혼했습니다.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.278534173965454 Generator / grammar loss:-0.09130097925662994   similarity loss:-0.063265360891819\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "그야말로 행동하려는 게 아니었다면 없습니다. 편지지도 고급품이고. 이상하군요. 씨. 딱 “ABC. 프란츠 있소?” 때 \n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "“ABC.대체 ABC란 어떤 녀석일까요?” “애셔 부인의 경력은 알고 있소?” “그녀는 햄프셔 태생으로 처녀 때 런던에 나가 직장 생활을 했지요.메리 드로워―조카딸입니다만―가 좀 도움이 될지도 모르겠군요.거기서 애셔를 만나 결혼했습니다.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "Unexpected error: <class 'ValueError'>\n",
            "==================================================\n",
            "24 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "헤어진 것은 1922년으로, 그즈음 두 사람은 아직 런던에 있었지요. 그녀는 남자에게서 달아나 여기로 왔으나, 남자가 곧 알아차리고 따라와 귀찮게 굴었던 겁니다. ” 마침 거기에 순경이 들어왔다. “무슨 일인가, 브릭스?” “애셔를 연행해 왔습니다. ” “좋아. 이리로 데려오게. 어디 있던가?” “인입선의 화차 안에 숨어 있었습니다. ” “숨어 있었다고? 데려오게. ” 프란츠 애셔는 정말 보기 싫은, 초라한 인간의 표본이었다. 그는 엉엉 울고, 꾸벅꾸벅 절하고, 서슬이 시퍼래지기도 했다. 그 짓무른 눈을 이리저리 움직이며 모두들의 얼굴을 살폈다 “나를 어쩌자는 거야. 나는 아무 짓도 안 했어. 날 이런 데 데려오다니 너무하잖아. 네 놈들은 돼지야. 어쩌자는 거야?” 그의 태도가 갑자기 바뀌었다. “아니, 아니, 그게 아냐. 선생님들은 이 가엾은 늙은이에게 몹쓸 짓을 하고 있소. 심하게 대하고 있소. 누구나 이 가엾은 프란츠에게 심하게 군단 말야, 이 가엾은 프란츠에게.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.1842896938323975 Generator / grammar loss:-0.11370514333248138   similarity loss:-0.09522375464439392\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "왔으나, 굴었던 겁니다. 마침 연행해 왔습니다. ” 데려오게. 거야. 너무하잖아. 돼지야. 어쩌자는 “아니, 아냐. 이 몹쓸 있소. 있소.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "헤어진 것은 1922년으로, 그즈음 두 사람은 아직 런던에 있었지요.어디 있던가?” “인입선의 화차 안에 숨어 있었습니다.” “숨어 있었다고?나는 아무 짓도 안 했어.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "그녀는 남자에게서 달아나 여기로 왔으나, 남자가 곧 알아차리고 따라와 귀찮게 굴었던 겁니다. ” 그 짓무른 눈을 이리저리 움직이며 모두들의 얼굴을 살폈다 “나를 어쩌자는 거야.\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "그녀는 남자에게서 달아나 여기로 왔으나, 남자가 곧 알아차리고 따라와 귀찮게 굴었던 겁니다. ” 그 짓무른 눈을 이리저리 움직이며 모두들의 얼굴을 살폈다 “나를 어쩌자는 거야.\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.156504  0.484786  0.626210  0.631075  0.004603  0.599385   \n",
            "1  BERT+LexRank    0.189024  0.337859  0.152321  0.042066  0.014897  0.156352   \n",
            "2          BESM    0.199187  0.414627  0.323743  0.395028  0.001525  0.363305   \n",
            "3   BESM+kobert    0.199187  0.414627  0.323743  0.395028  0.001525  0.363305   \n",
            "\n",
            "    grammar  \n",
            "0  0.967222  \n",
            "1  0.999001  \n",
            "2  0.998978  \n",
            "3  0.998978  \n",
            "Current result ==================================================\n",
            "Sample count: 23\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.148351  0.514938  0.505191    0.482896  0.007376   \n",
            "1  BERT+LexRank   0.217947  0.250390  0.214037    0.218520  0.009874   \n",
            "2          BESM   0.205710  0.440512  0.416005    0.372445  0.009620   \n",
            "3   BESM+kobert   0.213654  0.474581  0.410656    0.399276  0.010057   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.500452    0.951893  \n",
            "1     0.222652    0.999004  \n",
            "2     0.407838    0.998016  \n",
            "3     0.420027    0.997436  \n",
            "==================================================\n",
            "25 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "” 애셔는 울기 시작했다. 형사가 말했다. “그만해 두오, 애셔. 정신차려요. 당신에게 무슨 죄를 뒤집어씌우려는 건 아니오. 지금으로서는. 당신이 싫으면 아무 말 않아도 좋소. 만일 당신이 당신 아내 살해에 관계가 없다면 말이오. ” 애셔는 그 말을 가로막았다. 그 목소리는 비명 같았다. “나는 죽이지 않았어! 죽이지 않았어! 모두 엉터리야! 네 놈들은 거지같은 영국 돼지야. 모두들 내게 죄를 덮어씌우고 있어. 나는 죽이지 않았어, 죽이지 않았어. ” “당신은 늘 아내를 협박하고 있었잖소, 애셔?” “아니, 아니, 네 놈들은 알 리 없어. 그건 농담이었어. 나와 앨리스만이 알고 있는 농담이야. 앨리스는 그걸 알고 있었어.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.3289575576782227 Generator / grammar loss:-0.12227246165275574   similarity loss:-0.08907515555620193\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "정신차려요. 무슨 건 지금으로서는. 만일 말을 가로막았다. ” “당신은 협박하고 그건 앨리스만이\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "네 놈들은 거지같은 영국 돼지야.모두들 내게 죄를 덮어씌우고 있어.” “당신은 늘 아내를 협박하고 있었잖소, 애셔?” “아니, 아니, 네 놈들은 알 리 없어.만일 당신이 당신 아내 살해에 관계가 없다면 말이오.그 목소리는 비명 같았다.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "Unexpected error: <class 'ValueError'>\n",
            "==================================================\n",
            "26 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "” “우스운 농담이로군! 어젯밤 어디 있었는지 말할 수 있소, 애셔?” “말할 수 있고말고, 있고말고. 모두 이야기하지. 난 앨리스한테 가지 않았어. 친구들하고 있었어. 멋있는 친구들하고. <세븐 스타즈>에 있다가……그리고 나서 <레드 독>에 갔어. ” 그는 기침이 나와 말이 막혔다. “딕 윌러즈, 그도 함께 있었지. 커디 녀석도 그리고 조지도……플랫도, 그 밖의 놈들도 많이 있었어. 나는 앨리스에게 가지 않았어. 하느님께 맹세코 나는 사실을 말하고 있어. ” 그 소리는 비명이었다. 형사는 부하에게 눈짓을 했다. “데려가. 용의자를 구금시켜. ” 떨며 욕지거리를 퍼부어대는 그 불쾌한 노인이 나가 버리자 형사는 말했다. “아무래도 알 수 없군요. 그 편지만 없다면 저 늙은이의 짓이 분명한데요. ” “저 사람이 말하는 다른 남자들은 어떻소?” “나쁜 놈들입니다.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.393932819366455 Generator / grammar loss:-0.1146562322974205   similarity loss:-0.07474131882190704\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "어젯밤 애셔?” 이야기하지. 스타즈>에 갔어. “딕 그도 조지도……플랫도, 나는 말하고 소리는 비명이었다. 구금시켜. 형사는 분명한데요.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "<세븐 스타즈>에 있다가……그리고 나서 <레드 독>에 갔어.나는 앨리스에게 가지 않았어.멋있는 친구들하고.용의자를 구금시켜.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "떨며 욕지거리를 퍼부어대는 그 불쾌한 노인이 나가 버리자 형사는 말했다. “\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "떨며 욕지거리를 퍼부어대는 그 불쾌한 노인이 나가 버리자 형사는 말했다. “\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.177156  0.455382  0.445545  0.408914  0.000400  0.436523   \n",
            "1  BERT+LexRank    0.160839  0.187674  0.175639  0.165993  0.000079  0.175152   \n",
            "2          BESM    0.097902  0.316464  0.304999  0.454074  0.004588  0.352014   \n",
            "3   BESM+kobert    0.097902  0.316464  0.304999  0.454074  0.004588  0.352014   \n",
            "\n",
            "    grammar  \n",
            "0  0.961064  \n",
            "1  0.998591  \n",
            "2  0.656096  \n",
            "3  0.656096  \n",
            "Current result ==================================================\n",
            "Sample count: 24\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.149551  0.512457  0.502706    0.479813  0.007085   \n",
            "1  BERT+LexRank   0.215568  0.247777  0.212437    0.216332  0.009465   \n",
            "2          BESM   0.201218  0.435344  0.411380    0.375847  0.009411   \n",
            "3   BESM+kobert   0.208831  0.467993  0.406254    0.401559  0.009829   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.497788    0.952275  \n",
            "1     0.220673    0.998986  \n",
            "2     0.405512    0.983769  \n",
            "3     0.417193    0.983213  \n",
            "==================================================\n",
            "27 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "모두 위증쯤은 손쉽게 할 녀석들이지요. 나도 저 늙은이가 그날 밤 어느 시간까지는 그들과 함께 있었다고 생각합니다. 그러니 6시 사이에 가게 언저리에서 저 늙은이를 본 사람이 있는지 없는지에 달렸다고 봐야겠지요. “ 포아로는 신중하게 머리를 저었다. “가게에서 아무것도 없어지지 않은 건 분명하지요?” 형사는 어깨를 으쓱했다. “그야 경우에 따라 다르겠지요. 담배 한두 갑이 없어졌는지도 모릅니다. 그러나 아무도 그런 일 때문에 사람을 죽이지는 않지요. ” “게다가 아무것도, 뭐라면 좋을까. 가지고 온 것이 없었다는, 그러니까 이상한, 그 장소에 어울리지 않는 그런 아무것도 거기에는 없었다는 거지요?” “철도 안내서가 있었습니다. ” “철도 안내서?” “그렇습니다. 계산대 위에 펼쳐진 채 뒤집혀 있었습니다. 꼭 누군가가 앤도버에서 떠나는 기차 편을 알아보고 있었던 것처럼. 그 할머니나 아니면 손님이 보고 있었다는 것이겠지요. ” “그런 것도 팔고 있었소?” 형사는 머리를 저었다. “1페니짜리 시간표를 팔고 있었습니다만, 그것은 큰 것이었으니까 스미스네 가게나 커다란 문방구점 같은 데서 다룰 겁니다. ” 포아로는 눈을 빛내며 몸을 앞으로 내밀었다. “철도 안내서라고 말했지요? <브레드쇼>던가요, <ABC>던가요?” 그러자 형사의 눈도 빛나기 시작했다. “정말, 그러고 보니 ABC였습니다.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.441568613052368 Generator / grammar loss:-0.1246090903878212   similarity loss:-0.07971301674842834\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "위증쯤은 나도 그날 시간까지는 있었다고 언저리에서 달렸다고 봐야겠지요. 포아로는 아무것도 없어지지 분명하지요?” 담배 모릅니다. 때문에 “게다가 안내서가 있었습니다. “철도 안내서?” “그렇습니다.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "“정말, 그러고 보니 ABC였습니다.담배 한두 갑이 없어졌는지도 모릅니다.” “게다가 아무것도, 뭐라면 좋을까.나도 저 늙은이가 그날 밤 어느 시간까지는 그들과 함께 있었다고 생각합니다.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "나도 저 늙은이가 그날 밤 어느 시간까지는 그들과 함께 있었다고 생각합니다. 가지고 온 것이 없었다는, 그러니까 이상한, 그 장소에 어울리지 않는 그런 아무것도 거기에는 없었다는 거지요?” “\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "나도 저 늙은이가 그날 밤 어느 시간까지는 그들과 함께 있었다고 생각합니다. 가지고 온 것이 없었다는, 그러니까 이상한, 그 장소에 어울리지 않는 그런 아무것도 거기에는 없었다는 거지요?” “\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.163934  0.426001  0.476618  0.411886  0.000772  0.447075   \n",
            "1  BERT+LexRank    0.154993  0.322690  0.215640  0.146855  0.005234  0.216414   \n",
            "2          BESM    0.159463  0.537685  0.373626  0.175993  0.021866  0.347148   \n",
            "3   BESM+kobert    0.159463  0.537685  0.373626  0.175993  0.021866  0.347148   \n",
            "\n",
            "    grammar  \n",
            "0  0.977608  \n",
            "1  0.999020  \n",
            "2  0.999013  \n",
            "3  0.999013  \n",
            "Current result ==================================================\n",
            "Sample count: 25\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.150126  0.508999  0.501662    0.477096  0.006833   \n",
            "1  BERT+LexRank   0.213145  0.250773  0.212565    0.213553  0.009296   \n",
            "2          BESM   0.199547  0.439437  0.409869    0.367852  0.009909   \n",
            "3   BESM+kobert   0.206856  0.470780  0.404949    0.392537  0.010311   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.495760    0.953288  \n",
            "1     0.220503    0.998988  \n",
            "2     0.403178    0.984379  \n",
            "3     0.414391    0.983845  \n",
            "==================================================\n",
            "28 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "” < 조카딸의 이야기 > 이 사건에 대한 내 관심은 ABC 철도 안내서가 나왔을 때 비로서 일기 시작했다고 생각된다. 그때까지 나는 이 사건에 그리 열중하고 있지 않았다. 뒷골목의 노파 살해 같은 시시한 사건은 날마다 신문에 보도되는 흔해빠진 범죄여서 거의 주의를 끌지 못했던 것이다. 나는 마음속으로 익명 편지가 21일이라는 날짜를 지정한 일 따위는 우연의 일치에 지나지 않는다고 생각하고 있었다. 당연히 애셔 부인은 그 남편이 술에 취한 나머지 폭력을 휘둘러 희생된 것으로 여겼다. 그런데 지금 철도 안내서(철도역을 알파벳 순서로 나열했기 때문에 ABC라는 준말로 알려져 있음)가 등장하자 내 온몸에는 흥분의 전율이 일었다. 확실히 이것은 우연의 일치 같은 것 일 리 없다. 시시한 범죄가 새로운 양상을 띠기 시작했다. 애셔 부인을 살해하고 ABC 철도 안내서를 남기고 사라진 신비의 인간은 대체 누구인가? 경찰서를 나와 우리는 먼저 살해된 여자의 시체를 보러 시체 안치소로 갔다. 얼마 안 되는 머리칼을 이마 위로 가지런히 빗어 넘긴 노파의 주름잡힌 얼굴을 보고 있는 동안, 나는 이상한 느낌이 들기 시작했다. 너무나 평화로워 폭력 같은 것과는 거리가 먼 느낌이었다. 경관이 말했다. “누가 무엇으로 자기를 때렸는지 조금도 모르는 얼굴입니다. 카 의사가 그렇게 말하더군요. 오히려 그게 잘된 일이라고 생각합니다. 가엾게도, 깔끔한 사람이었는데. ” 포아로가 말했다. “옛날엔 아름다웠을 것 같군. ” 나는 믿을 수 없는 마음이 들어 중얼거렸다.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.454463243484497 Generator / grammar loss:-0.13683800399303436   similarity loss:-0.09058431535959244\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            " > 이 관심은 시시한 것이다. 일치에 알려져 일었다. 확실히 띠기 시작했다. 노파의 느낌이 들기 때렸는지 카 그렇게 말하더군요. 그게 가엾게도, 사람이었는데. ” 포아로가 말했다. “옛날엔 아름다웠을\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "그런데 지금 철도 안내서(철도역을 알파벳 순서로 나열했기 때문에 ABC라는 준말로 알려져 있음)가 등장하자 내 온몸에는 흥분의 전율이 일었다.경관이 말했다.” 포아로가 말했다.“누가 무엇으로 자기를 때렸는지 조금도 모르는 얼굴입니다.카 의사가 그렇게 말하더군요.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "” < 조카딸의 이야기 > 이 사건에 대한 내 관심은 ABC 철도 안내서가 나왔을 때 비로서 일기 시작했다고 생각된다. 애셔 부인을 살해하고 ABC 철도 안내서를 남기고 사라진 신비의 인간은 대체 누구인가? 얼마 안 되는 머리칼을 이마 위로 가지런히 빗어 넘긴 노파의 주름잡힌 얼굴을 보고 있는 동안, 나는 이상한 느낌이 들기 시작했다.\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "” < 조카딸의 이야기 > 이 사건에 대한 내 관심은 abc 철도 안내서가 나왔을 때 비로서 일기 시작했다고 생각된다. 뒷골목의 노파 살해 같은 시시한 사건은 날마다 신문에 보도되는 흔해빠진 범죄여서 거의 주의를 끌지 못했던 것이다.\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.148148  0.413042  0.376877  0.705535  0.021653  0.482707   \n",
            "1  BERT+LexRank    0.193122  0.339848  0.261928  0.177518  0.004394  0.252189   \n",
            "2          BESM    0.248677  0.549769  0.472760  0.258645  0.015170  0.423927   \n",
            "3   BESM+kobert    0.171958  0.672427  0.357894  0.184702  0.040756  0.368843   \n",
            "\n",
            "    grammar  \n",
            "0  0.989961  \n",
            "1  0.999009  \n",
            "2  0.999019  \n",
            "3  0.999023  \n",
            "Current result ==================================================\n",
            "Sample count: 26\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.150050  0.505308  0.496863    0.485882  0.007403   \n",
            "1  BERT+LexRank   0.212375  0.254199  0.214463    0.212167  0.009108   \n",
            "2          BESM   0.201437  0.443681  0.412288    0.363652  0.010111   \n",
            "3   BESM+kobert   0.205514  0.478536  0.403139    0.384543  0.011482   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.495258    0.954699  \n",
            "1     0.221722    0.998989  \n",
            "2     0.403976    0.984942  \n",
            "3     0.412640    0.984429  \n",
            "==================================================\n",
            "29 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "“그럴까. ‘ “그렇네. 자, 턱의 선이며 뼈 모양이며 머리 생김새를 잘 보게. ” 그는 덮개를 본래대로 해두면서 한숨을 쉬었다. 그리고 나서 우리는 시체 안치소를 나왔다. 다음에는 경찰의와 간단히 면담했다. 카 의사는 유능해 보이는 중년 사나이였다. 그는 활발하게 단정적인 말투로 이야기했다. “흉기는 발견되지 않았습니다. 그것이 무엇이었는지는 알 수 없지요. 무거운 지팡이, 몽둥이, 모래주머니 같은 것……그런 거라면 어느 것이나 들어맞습니다. ” “그런 타격을 가하려면 억센 힘이 필요합니까?” 의사는 날카로운 눈으로 포아로를 보았다. “그 말뜻은 몸을 떨어대는 70살의 노인으로서도 할 수 있느냐는 거지요? 네, 물론 할 수 있습니다. 흉기의 머리 부분에 충분한 무게를 주면 체력이 약한 사람도 바라는 결과를 얻을 수 있습니다. ” “그렇다면 범인은 남자일 수 있는 것과 마찬가지로 여자일 수도 있군요?” 이 말은 얼마쯤 의사를 놀라게 한 모양이었다. “여자도? 네, 그렇습니다. 이런 종류의 범죄를 여자와 관련시켜 생각해 볼 마음은 없었습니다만, 물론 할 수 있습니다. 완전히 가능합니다.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.477334499359131 Generator / grammar loss:-0.12361573427915573   similarity loss:-0.0749436691403389\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            " “그렇네. 자, 턱의 모양이며 ” 그는 그리고 시체 다음에는 의사는 유능해 이야기했다. 무엇이었는지는 없지요. 필요합니까?” 노인으로서도 그렇습니다.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "다음에는 경찰의와 간단히 면담했다.“흉기는 발견되지 않았습니다.” 그는 덮개를 본래대로 해두면서 한숨을 쉬었다.그리고 나서 우리는 시체 안치소를 나왔다.” “그렇다면 범인은 남자일 수 있는 것과 마찬가지로 여자일 수도 있군요?” 이 말은 얼마쯤 의사를 놀라게 한 모양이었다.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "그는 덮개를 본래대로 해두면서 한숨을 쉬었다.그리고 나서 우리는 시체 안치소를 나왔다. 무거운 지팡이, 몽둥이, 모래주머니 같은 것……그런 거라면 어느 것이나 들어맞습니다. ” “\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "그는 덮개를 본래대로 해두면서 한숨을 쉬었다.그리고 나서 우리는 시체 안치소를 나왔다. 네, 물론 할 수 있습니다.흉기의 머리 부분에 충분한 무게를 주면 체력이 약한 사람도 바라는 결과를 얻을 수 있습니다. ” “ 이런 종류의 범죄를 여자와 관련시켜 생각해 볼 마음은 없었습니다만, 물론 할 수 있습니다.\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.150538  0.603150  0.579508  0.528719  0.000964  0.569000   \n",
            "1  BERT+LexRank    0.274194  0.097665  0.226777  0.189029  0.002938  0.189630   \n",
            "2          BESM    0.179211  0.629843  0.436748  0.373132  0.011915  0.456282   \n",
            "3   BESM+kobert    0.304659  0.445512  0.386905  0.591262  0.007382  0.459933   \n",
            "\n",
            "    grammar  \n",
            "0  0.989361  \n",
            "1  0.998999  \n",
            "2  0.995070  \n",
            "3  0.998991  \n",
            "Current result ==================================================\n",
            "Sample count: 27\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.150068  0.508932  0.499924    0.487469  0.007164   \n",
            "1  BERT+LexRank   0.214664  0.248402  0.214919    0.211310  0.008879   \n",
            "2          BESM   0.200614  0.450576  0.413194    0.364003  0.010178   \n",
            "3   BESM+kobert   0.209186  0.477313  0.402538    0.392199  0.011330   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.497989    0.955983  \n",
            "1     0.220533    0.998989  \n",
            "2     0.405913    0.985317  \n",
            "3     0.414391    0.984968  \n",
            "==================================================\n",
            "30 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "다만 심리적으로 말해서, 이건 여성의 범죄라고 할 수 없지요. ” 포아로도 그 말에 동의하여 열심히 고개를 끄덕였다. “그렇습니다. 그렇습니다. 확실히 있을 수 없는 일입니다. 그러나 모든 가능성을 염두해 두지 않으면 안 되니까요. 시체는 쓰러져 있었겠지요. 어떤 모습이었습니까?” 의사는 피해자의 위치를 세밀하게 우리에게 설명했다. 그의 말에 의하면, 타격이 주어졌을 때 그녀는 계산대 쪽으로 등을 돌리고―따라서 가해자에 대해서도―서 있었다고 한다. 머리를 얻어맞고 그녀는 계산대 뒤로 쭈그려 앉아 버려 가게에 들어온 사람 눈에 얼른 띄지 않았던 셈이다. 카 의사에게 인사하고 밖으로 나오자 포아로가 말했다. “이로써 애셔의 무죄 쪽으로 한 걸음 다가선 게 확실하네. 헤이스팅즈. 만일 그가 아내한테 덤벼들면서 협박한 거라면 그녀는 계산대를 사이에 두고 그와 마주서 있었을 걸세. 그런데 그녀는 가해자에게 등을 돌리고 있었지. 틀림없이 그녀는 손님에게 줄 파이프 담배나 궐련을 꺼내려 했던 걸 거야. ” 나는 조금 몸을 떨었다. “기분이 언짢군. ” 포아로는 무겁게 머리를 흔들었다. 그는 중얼거렸다.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.416602611541748 Generator / grammar loss:-0.12287955731153488   similarity loss:-0.08060057461261749\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            " 말해서, 이건 할 수 없지요. ” 열심히 고개를 “그렇습니다. 그렇습니다. 있었겠지요. 계산대 등을 한다. 말했다. 그런데 틀림없이\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "틀림없이 그녀는 손님에게 줄 파이프 담배나 궐련을 꺼내려 했던 걸 거야.어떤 모습이었습니까?” 의사는 피해자의 위치를 세밀하게 우리에게 설명했다.만일 그가 아내한테 덤벼들면서 협박한 거라면 그녀는 계산대를 사이에 두고 그와 마주서 있었을 걸세.그런데 그녀는 가해자에게 등을 돌리고 있었지.머리를 얻어맞고 그녀는 계산대 뒤로 쭈그려 앉아 버려 가게에 들어온 사람 눈에 얼른 띄지 않았던 셈이다.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "그의 말에 의하면, 타격이 주어졌을 때 그녀는 계산대 쪽으로 등을 돌리고―따라서 가해자에 대해서도―서 있었다고 한다. 머리를 얻어맞고 그녀는 계산대 뒤로 쭈그려 앉아 버려 가게에 들어온 사람 눈에 얼른 띄지 않았던 셈이다.\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "그의 말에 의하면, 타격이 주어졌을 때 그녀는 계산대 쪽으로 등을 돌리고―따라서 가해자에 대해서도―서 있었다고 한다. 만일 그가 아내한테 덤벼들면서 협박한 거라면 그녀는 계산대를 사이에 두고 그와 마주서 있었을 걸세.그런데 그녀는 가해자에게 등을 돌리고 있었지. 틀림없이 그녀는 손님에게 줄 파이프 담배나 궐련을 꺼내려 했던 걸 거야. ”\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.131907  0.796755  0.427816  0.500657  0.025455  0.523456   \n",
            "1  BERT+LexRank    0.390374 -0.059605  0.228442  0.124949  0.014194  0.139785   \n",
            "2          BESM    0.221034  0.028053  0.322740  0.202035  0.014631  0.227591   \n",
            "3   BESM+kobert    0.336898  0.516275  0.497997  0.633523  0.003605  0.542310   \n",
            "\n",
            "    grammar  \n",
            "0  0.983264  \n",
            "1  0.998994  \n",
            "2  0.998993  \n",
            "3  0.996906  \n",
            "Current result ==================================================\n",
            "Sample count: 28\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.149420  0.519211  0.497348    0.487940  0.007818   \n",
            "1  BERT+LexRank   0.220940  0.237401  0.215402    0.208225  0.009069   \n",
            "2          BESM   0.201343  0.435486  0.409964    0.358219  0.010337   \n",
            "3   BESM+kobert   0.213747  0.478704  0.405947    0.400818  0.011054   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.498898    0.956957  \n",
            "1     0.217649    0.998989  \n",
            "2     0.399545    0.985806  \n",
            "3     0.418960    0.985395  \n",
            "==================================================\n",
            "31 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "“가엾은 여자일세. ” 그리고 나서 그는 시계를 흘끗 보았다. “여기서 오버튼까지는 그리 멀지 않네. 거기 가서 노파의 조카딸을 만나 보는게 어떻겠나?” “범행 현장인 가게 쪽을 먼저 보는 게 좋지 않을까?” “그건 뒤로 미루고 싶네. 이유가 있어서. ” 그는 더 이상 설명하지 않았다. 잠시 뒤 우리는 자동차를 타고 오버튼 쪽으로 런던 행 도로를 달려갔다. 형사가 가르쳐 준 집은 마을에서 런던 쪽으로 1마일쯤 간 곳에 있었다. 훌륭한 집이었다. 벨을 누르자 아름다운 검은 머리의 아가씨가 나왔다. 지금까지 울고 있었던 듯 눈이 빨갰다. 포아로가 상냥하게 말했다. “아, 당신이 이 집 하녀인 메리 드로워 양이군요?” “그렇습니다. 제가 메리예요. ” “주인께서 허락해 주신다면 잠시 이야기를 좀 나누고 싶은데요. 이야기란 다름아닌 아가씨 아주머니인 애셔 부인에 대한 것입니다. ” “주인은 외출중이세요. 들어오셔도 그리 꾸중이 없으리라 생각됩니다. ” 그녀는 조그만 거실의 문을 열었다. 우리는 안으로 들어갔다.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.5204696655273438 Generator / grammar loss:-0.13607226312160492   similarity loss:-0.08280020952224731\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "“가엾은 ” 시계를 보는게 좋지 “그건 싶네. 그는 잠시 달려갔다. 마을에서 1마일쯤 있었다. 집이었다. 아가씨가 있었던 집 메리예요.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "지금까지 울고 있었던 듯 눈이 빨갰다.” 그리고 나서 그는 시계를 흘끗 보았다.” 그는 더 이상 설명하지 않았다.“가엾은 여자일세.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "Unexpected error: <class 'ValueError'>\n",
            "==================================================\n",
            "32 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "포아로는 창가 의자에 앉아 날카롭게 아가씨의 얼굴을 보았다. “아주머니가 돌아가신 이야기는 물론 들었겠지요?” 아가씨는 고개를 끄덕였는데 눈물이 다시 새삼스럽게 솟아났다. “오늘 아침 경찰에서 오셨었어요. 아, 무서운 일이에요!가엾은 아주머니! 그토록 괴로운 나날을 보내고서 또 이런 일을 당하시다니……너무해요. ” “경찰이 앤도버로 오라고 하지 않았습니까?” “월요일에 심문을 받기로 되어 있어요. 하지만 저는 그리로 가면 있을 데가 없어요. 이젠 그 가게로 갈 수도 없고. 게다가 저 말고는 하녀가 없는데 주인에게 폐 끼치고 싶지도 않아요. ” 포아로는 부드럽게 물었다. “당신은 아주머니를 아주 좋아했었군요, 메리 양?” “정말 좋아했어요. 아주머니는 언제나 제게 잘해 주셨지요. 어머니가 돌아가신 뒤 저는 11살 때 런던의 아주머니 집으로 갔어요. 16살 때부터 돈벌이를 하러 나와 있었지만, 쉬는 날이면 꼭 아주머니에게 가곤 했어요. 아주머니는 그 독일사람 때문에 아주 애를 먹고 계셨어요. 그 남자를 아주머니는 늘 <나의 악마>라고 부르곤 하셨지요. 그는 아주머니가 있는 데는 어디든 와서 가만히 두지 않았어요. 돈만 빼앗아 가는 거지같은 짐승이에요. ” 아가씨의 말투는 아주 격렬했다.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.209115743637085 Generator / grammar loss:-0.13056069612503052   similarity loss:-0.10957242548465729\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "포아로는 날카롭게 아침 무서운 싶지도 “당신은 돌아가신 때 돈벌이를 하러 나와 있었지만, 꼭 아주머니는 애를 먹고 하셨지요. 와서 짐승이에요. 격렬했다. \n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "“오늘 아침 경찰에서 오셨었어요.” 포아로는 부드럽게 물었다.” “경찰이 앤도버로 오라고 하지 않았습니까?” “월요일에 심문을 받기로 되어 있어요.포아로는 창가 의자에 앉아 날카롭게 아가씨의 얼굴을 보았다.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "그토록 괴로운 나날을 보내고서 또 이런 일을 당하시다니……너무해요. ” “ 어머니가 돌아가신 뒤 저는 11살 때 런던의 아주머니 집으로 갔어요.16살 때부터 돈벌이를 하러 나와 있었지만, 쉬는 날이면 꼭 아주머니에게 가곤 했어요.\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "그토록 괴로운 나날을 보내고서 또 이런 일을 당하시다니……너무해요. ” “ 어머니가 돌아가신 뒤 저는 11살 때 런던의 아주머니 집으로 갔어요.16살 때부터 돈벌이를 하러 나와 있었지만, 쉬는 날이면 꼭 아주머니에게 가곤 했어요.\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.139837  0.453998  0.391645  0.531165  0.003257  0.445971   \n",
            "1  BERT+LexRank    0.186992  0.324654  0.234238  0.139146  0.005737  0.223794   \n",
            "2          BESM    0.208130  0.372653  0.466699  0.527928  0.004078  0.466258   \n",
            "3   BESM+kobert    0.208130  0.372653  0.466699  0.527928  0.004078  0.466258   \n",
            "\n",
            "    grammar  \n",
            "0  0.965765  \n",
            "1  0.999029  \n",
            "2  0.999015  \n",
            "3  0.999015  \n",
            "Current result ==================================================\n",
            "Sample count: 29\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.149089  0.516962  0.493703    0.489430  0.007660   \n",
            "1  BERT+LexRank   0.219769  0.240410  0.216052    0.205843  0.008954   \n",
            "2          BESM   0.201577  0.433319  0.411920    0.364071  0.010121   \n",
            "3   BESM+kobert   0.213554  0.475047  0.408042    0.405201  0.010814   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.497073    0.957261  \n",
            "1     0.217861    0.998990  \n",
            "2     0.401845    0.986261  \n",
            "3     0.420591    0.985864  \n",
            "==================================================\n",
            "33 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "“아주머니는 법적 수단으로 그 남자의 압박에서 벗어나려고는 하지 않았습니까?” 아가씨는 단순하게, 그러나 딱 잘라 말했다. “아무래도 남편이었기 때문에 그럴 수가 없었지요. ” “메리 양, 그 남자는 아주머니를 협박했었지요?” “네, 아주 무서운 소리를 곧잘 했어요. 목을 부러뜨린다든가 하는 말들을. 저주스럽게 욕지거리를 해대면서. 독일 말과 영어 두 가지로요. 그렇지만 아주머니는 결혼했던 즈음에는 아주 멋있는 남자였다고 말씀하셨어요. 사람이 그렇게 된다는 것은 참으로 무서운 일이에요. ” “정말 그렇군요. 그런데 메리 양, 늘 그런 협박을 받고 있었다면 사건이 일어난 것을 알았을 때 그리 놀라지 않았겠군요?” “그래도 역시 놀랐어요. 아무튼 진짜로 하는 소리라고는 생각지 않았으니까요. 그저 말로만 해대는 것뿐 그 이상으로는 여기지 않았어요. 아주머니도 무서워하고 계셨던 것 같지 않아요. 아주머니가 대들면 개가 다리 사이로 꼬리를 감추듯 움츠러드는 것을 본 적도 있어요. 오히려 그쪽에서 아주머니를 무서워하고 있을 정도였지요. ” “그런데도 아주머니는 돈을 주고 있었습니까?” “남편인걸요. ” “그렇군요, 아까도 그렇게 말했었지요. ” 포아로는 잠시 말을 끊었다가 다시 계속했다. “그렇다면 결국 그 남자는 아주머니를 죽이지 않았다는 거로군요?” “죽이지 않았다고요?” 그녀는 눈을 크게 떠보였다. “그렇습니다.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.4727649688720703 Generator / grammar loss:-0.1288776695728302   similarity loss:-0.08068985491991043\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "아가씨는 협박했었지요?” “네, 저주스럽게 영어 두 가지로요. 그렇지만 즈음에는 말씀하셨어요. 된다는 일이에요. 그리 놀라지 않았겠군요?” “그래도 역시 놀랐어요. 아까도 말했었지요. “그렇습니다.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "독일 말과 영어 두 가지로요.“그렇습니다.” “정말 그렇군요.“아무래도 남편이었기 때문에 그럴 수가 없었지요.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "“아주머니는 법적 수단으로 그 남자의 압박에서 벗어나려고는 하지 않았습니까?” 그렇지만 아주머니는 결혼했던 즈음에는 아주 멋있는 남자였다고 말씀하셨어요. 아주머니가 대들면 개가 다리 사이로 꼬리를 감추듯 움츠러드는 것을 본 적도 있어요.오히려 그쪽에서 아주머니를 무서워하고 있을 정도였지요. ” “\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "“아주머니는 법적 수단으로 그 남자의 압박에서 벗어나려고는 하지 않았습니까?” 그렇지만 아주머니는 결혼했던 즈음에는 아주 멋있는 남자였다고 말씀하셨어요.\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.160584  0.320911  0.446809  0.583945  0.011538  0.462770   \n",
            "1  BERT+LexRank    0.089051  0.326343  0.159848  0.133054  0.007311  0.185109   \n",
            "2          BESM    0.242336  0.540753  0.495388  0.592881  0.001587  0.533709   \n",
            "3   BESM+kobert    0.124088  0.523365  0.259854  0.273133  0.014692  0.316540   \n",
            "\n",
            "    grammar  \n",
            "0  0.994110  \n",
            "1  0.998986  \n",
            "2  0.995258  \n",
            "3  0.999016  \n",
            "Current result ==================================================\n",
            "Sample count: 30\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.149472  0.510427  0.492140    0.492581  0.007790   \n",
            "1  BERT+LexRank   0.215412  0.243274  0.214178    0.203417  0.008899   \n",
            "2          BESM   0.202936  0.436900  0.414702    0.371698  0.009837   \n",
            "3   BESM+kobert   0.210571  0.476658  0.403102    0.400799  0.010943   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.495930    0.958489  \n",
            "1     0.216769    0.998990  \n",
            "2     0.406240    0.986561  \n",
            "3     0.417122    0.986303  \n",
            "==================================================\n",
            "34 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "누군가 다른 사람이 아가씨 아주머니를 죽였다는 말입니다. ……달리 짐작되는 사람 없습니까?” 그녀는 한층 더 놀란 듯 그의 얼굴을 보았다. “알 수 없어요. 하지만 그런 일이 있을 수 있을까요?” “당신 아주머니가 무서워한 다른 사람은 없었습니까?” 메리는 고개를 저었다. “아주머니는 남을 무서워하지 않으셨어요. 말솜씨가 좋아 누구에게나 맞설 수 있으셨어요. ” “아주머니에게 악의를 품고 있는 어떤 사람에 대한 이야기를 해준 일은 없습니까?” “네, 없어요. ” “익명의 편지를 받은 일도” “무슨 편지라고요?” “개인적인 서명이 없는 편지로, 예를 들어 그저 ABC라는 서명만 있는. ” 그는 아가씨의 얼굴을 찬찬히 들여다보고 있었는데, 그녀는 분명 난처해하는 모습이었다. 그녀는 묘한 표정으로 고개를 저었다. “아가씨 말고 또 다른 친척이 있습니까?” “지금은 없어요. 열 남매였는데 자란 사람은 셋뿐이었지요. 톰 아저씨는 전쟁터에서 돌아가시고, 해리 아저씨는 남아메리카로 가버리셔서 소식을 몰라요. 그리고 또 제 어머니는 돌아가셨기 때문에 저밖에 없어요. ” “아주머니는 저축을 했었습니까? 돈을 모으고 있었습니까?” “은행에 조금 있어요. 매장 비용만 된다면 하고 곧잘 말씀하곤 하셨지요. 그리고는 겨우 그럭저럭 살아 나가셨어요. 그 늙어빠진 악마가 있으니 안 그렇겠어요. ” 포아로는 생각 깊게 고개를 끄덕였다.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.36403226852417 Generator / grammar loss:-0.1369277834892273   similarity loss:-0.1001143604516983\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "누군가 사람이 짐작되는 “알 없어요. 수 “당신 말솜씨가 받은 일도” “무슨 없어요. 사람은 전쟁터에서 몰라요. 또 제 “아주머니는 있었습니까?” 있어요. 하셨지요. 그 늙어빠진 안\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "말솜씨가 좋아 누구에게나 맞설 수 있으셨어요.톰 아저씨는 전쟁터에서 돌아가시고, 해리 아저씨는 남아메리카로 가버리셔서 소식을 몰라요.돈을 모으고 있었습니까?” “은행에 조금 있어요.열 남매였는데 자란 사람은 셋뿐이었지요.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "아주머니에게 악의를 품고 있는 어떤 사람에 대한 이야기를 해준 일은 없습니까?” “ 그는 아가씨의 얼굴을 찬찬히 들여다보고 있었는데, 그녀는 분명 난처해하는 모습이었다.\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "아주머니에게 악의를 품고 있는 어떤 사람에 대한 이야기를 해준 일은 없습니까?” “ 개인적인 서명이 없는 편지로, 예를 들어 그저 abc라는 서명만 있는. ”\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.147445  0.504576  0.415939  0.475914  0.001364  0.451659   \n",
            "1  BERT+LexRank    0.179562  0.125089  0.276957  0.175443  0.003989  0.216129   \n",
            "2          BESM    0.137226  0.423818  0.358041  0.211395  0.007884  0.327202   \n",
            "3   BESM+kobert    0.128467  0.432954  0.424171  0.325686  0.002365  0.396382   \n",
            "\n",
            "    grammar  \n",
            "0  0.974543  \n",
            "1  0.998822  \n",
            "2  0.999014  \n",
            "3  0.990487  \n",
            "Current result ==================================================\n",
            "Sample count: 31\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.149407  0.510239  0.489682    0.492043  0.007582   \n",
            "1  BERT+LexRank   0.214255  0.239462  0.216203    0.202515  0.008741   \n",
            "2          BESM   0.200816  0.436478  0.412875    0.366527  0.009774   \n",
            "3   BESM+kobert   0.207923  0.475248  0.403782    0.398376  0.010666   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.494502    0.959007  \n",
            "1     0.216749    0.998985  \n",
            "2     0.403691    0.986963  \n",
            "3     0.416453    0.986438  \n",
            "==================================================\n",
            "35 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "그는 아가씨에게 말한다기 보다 혼잣말처럼 중얼거렸다. “지금으로선 어둠 속에 있는 것 같군. 방향도 잡을 수 없어. 만일 좀더 뚜렷해진다면……. ” 그는 일어섰다. “만일 아가씨한테 볼일이 생기면 여기로 편지하지요. 메리 양. ” “사실을 말씀드리면, 저는 여기를 나갈 생각으로 있어요. 시골을 그리 좋아하지 않거든요. 아주머니 곁에 있는 게 마음 든든히 여겨져 여기 있었던 거예요. 그러나 이젠……. ” 그 눈에 다시 눈물이 솟았다. “이제는 여기 있을 이유가 없어져 런던으로 되돌아가려고 해요. 그곳이 제게는 더 재미있는 걸요. ” “그럼, 그리고 가게 될 때에는 주소를 가르쳐 주십시오. 이것이 제 명함입니다. ” 그는 아가씨에게 명함을 건네주었다. 그녀는 곤혹스러운 듯 이마에 주름을 지으며 그것을 보았다. “그럼, 선생님은……경찰과는 관계가 없으신가요?” “나는 사립탐정입니다. ” 그녀는 선 채로 잠시 말없이 그를 쳐다보았다.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.4397149085998535 Generator / grammar loss:-0.12510675191879272   similarity loss:-0.08040550351142883\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "속에 있는 것 잡을 편지하지요. 말씀드리면, 저는 그리 든든히 여겨져 거예요. 눈에 가게 될 주십시오.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "“그럼, 선생님은……경찰과는 관계가 없으신가요?” “나는 사립탐정입니다.이것이 제 명함입니다.” 그는 아가씨에게 명함을 건네주었다.” 그 눈에 다시 눈물이 솟았다.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "Unexpected error: <class 'ValueError'>\n",
            "==================================================\n",
            "36 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "이윽고 그녀가 말했다. “뭔가 의심스러운 점이라고 있으신지요?” “그렇습니다, 아가씨. 좀 이상한 점이 있지요. 아마 앞으로 아가씨에게 도움 받을 일이 있을지도 모르겠습니다. ” “저는, 저는 무엇이든 하겠어요. 아주머니가 살해되시다니, 옳은 일이 아니니까요. ” 그것은 기묘한 표현이었다. 그러나 꽤 감동적이었다. 우리는 곧 자동차를 타고 앤도버로 돌아갔다. < 범행 현장 > 참극이 일어난 곳은 큰길에서 좁은 골목이었다. 애셔 부인의 가게는 그 중간쯤의 오른쪽에 있었다. 그 골목에 들어섰을 때, 포아로는 흘끗 시계를 보았다. 그래서 나는 그가 범행 현장으로 가는 시간을 지금까지 미룬 까닭을 알았다. 꼭 5시 30분이 되어 있었다. 그는 되도록 어젯밤의 상황을 재현하려 생각하고 있었던 것이다. 그러나 그것이 그의 목적이었다면 실패했다. 이 때 골목은 어젯밤의 그림자를 거의 전해주고 있지 않았다. 그곳에는 가난한 사람들 집에 섞여 조그만 가게가 몇 채 줄지어 있었다. 다른 때 같으면 이 언저리의 가난한 몇몇 사람들이 그곳을 오가고 또 찻길이나 보도 위에서는 몇 명의 아이들이 놀고 있을 뿐이었다. 그런데 이때는 많은 사람들이 쭉 둘러서서 집인지 가게를 보고 있었다.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.4267547130584717 Generator / grammar loss:-0.13088871538639069   similarity loss:-0.08754729479551315\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "이윽고 말했다. “뭔가 아가씨. 좀 하겠어요. 살해되시다니, 그것은 앤도버로 돌아갔다. 범행 일어난 곳은 오른쪽에 포아로는 꼭 30분이 있었다. 그런데 사람들이 쭉\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "아주머니가 살해되시다니, 옳은 일이 아니니까요.꼭 5시 30분이 되어 있었다.그는 되도록 어젯밤의 상황을 재현하려 생각하고 있었던 것이다.우리는 곧 자동차를 타고 앤도버로 돌아갔다.그래서 나는 그가 범행 현장으로 가는 시간을 지금까지 미룬 까닭을 알았다.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "그래서 나는 그가 범행 현장으로 가는 시간을 지금까지 미룬 까닭을 알았다. 다른 때 같으면 이 언저리의 가난한 몇몇 사람들이 그곳을 오가고 또 찻길이나 보도 위에서는 몇 명의 아이들이 놀고 있을 뿐이었다.\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "그래서 나는 그가 범행 현장으로 가는 시간을 지금까지 미룬 까닭을 알았다. 다른 때 같으면 이 언저리의 가난한 몇몇 사람들이 그곳을 오가고 또 찻길이나 보도 위에서는 몇 명의 아이들이 놀고 있을 뿐이었다.\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.151414  0.692585  0.506301  0.318734  0.023294  0.487288   \n",
            "1  BERT+LexRank    0.236273  0.122896  0.253095  0.232889  0.003273  0.220994   \n",
            "2          BESM    0.189684  0.178675  0.309311  0.512894  0.018913  0.344259   \n",
            "3   BESM+kobert    0.189684  0.178675  0.309311  0.512894  0.018913  0.344259   \n",
            "\n",
            "    grammar  \n",
            "0  0.973905  \n",
            "1  0.999029  \n",
            "2  0.999027  \n",
            "3  0.999027  \n",
            "Current result ==================================================\n",
            "Sample count: 32\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.149470  0.515937  0.490201    0.486627  0.008073   \n",
            "1  BERT+LexRank   0.214943  0.235819  0.217356    0.203464  0.008570   \n",
            "2          BESM   0.200468  0.428422  0.409638    0.371101  0.010059   \n",
            "3   BESM+kobert   0.207353  0.465980  0.400830    0.401955  0.010924   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.494276    0.959472  \n",
            "1     0.216881    0.998986  \n",
            "2     0.401834    0.987340  \n",
            "3     0.414197    0.986831  \n",
            "==================================================\n",
            "37 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "그것이 어느 집인지는 곧 알 수 있었다. 우리가 본 것은 한 사람이 살해된 곳을 아주 흥미롭게 보고 있는 여느 사람들의 무리였다. 가까이 다가감에 따라 확실히 그렇다는 것을 알 수 있었다. 블라인드를 내린 그을음 낀 듯한 구멍가게 앞에 젊은 순경이 애를 먹고 있는 듯한 얼굴로 서서 사람들에게 저리 가라고 딱딱하게 명령하고 있었다. 그는 동료의 도움을 받아 모여 있는 사라들을 해산시키기 시작했다. 꽤 많은 사람들이 불평스럽게 한숨을 쉬며 저마다 자기네 일로 돌아갔다. 그러나 곧 또 다른 사람들이 몰려와 살인 현장을 똑똑히 봐두려는 듯 그 자리를 다시 차지했다. 포아로는 사람들로부터 조금 떨어져 섰다. 그가 서 있는 곳에서는 문 위에 씌어진 글자를 똑똑히 볼 수 있었다. 포아로는 그것을 입속에서 되풀이했다. “A 애셔. 그렇지, 어쩌면……. ” 그는 말을 끊었다. “가세, 헤이스팅즈. 안으로 들어가 보세. ” 나는 기다리고 있던 바였다. 우리는 사람들을 헤치고 젊은 순경에게로 갔다. 포아로는 형사에게서 받아 둔 소개장을 내보였다. 순경은 머리를 끄덕이며 우리를 안으로 들여보내기 위해 문의 자물쇠를 열었다. 우리는 구경꾼들의 호기심에 찬 눈길을 받으며 안으로 들어갔다.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.3570408821105957 Generator / grammar loss:-0.13140229880809784   similarity loss:-0.09531152248382568\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            " 그것이 여느 일로 차지했다. 사람들로부터 조금 떨어져 있는 문 글자를 수 안으로 들어가 보세. 나는 젊은 포아로는 머리를 끄덕이며 들어갔다.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "블라인드를 내린 그을음 낀 듯한 구멍가게 앞에 젊은 순경이 애를 먹고 있는 듯한 얼굴로 서서 사람들에게 저리 가라고 딱딱하게 명령하고 있었다.그는 동료의 도움을 받아 모여 있는 사라들을 해산시키기 시작했다.” 그는 말을 끊었다.꽤 많은 사람들이 불평스럽게 한숨을 쉬며 저마다 자기네 일로 돌아갔다.포아로는 형사에게서 받아 둔 소개장을 내보였다.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "우리가 본 것은 한 사람이 살해된 곳을 아주 흥미롭게 보고 있는 여느 사람들의 무리였다. 그러나 곧 또 다른 사람들이 몰려와 살인 현장을 똑똑히 봐두려는 듯 그 자리를 다시 차지했다. 순경은 머리를 끄덕이며 우리를 안으로 들여보내기 위해 문의 자물쇠를 열었다.\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "우리가 본 것은 한 사람이 살해된 곳을 아주 흥미롭게 보고 있는 여느 사람들의 무리였다. 블라인드를 내린 그을음 낀 듯한 구멍가게 앞에 젊은 순경이 애를 먹고 있는 듯한 얼굴로 서서 사람들에게 저리 가라고 딱딱하게 명령하고 있었다.그는 동료의 도움을 받아 모여 있는 사라들을 해산시키기 시작했다.\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.131012  0.308833  0.451638  0.516093  0.007501  0.442413   \n",
            "1  BERT+LexRank    0.318408  0.270088  0.264005  0.238005  0.000194  0.257421   \n",
            "2          BESM    0.240464  0.501167  0.309861  0.568391  0.011995  0.425681   \n",
            "3   BESM+kobert    0.273632  0.651751  0.294804  0.273852  0.030073  0.359908   \n",
            "\n",
            "    grammar  \n",
            "0  0.996228  \n",
            "1  0.999029  \n",
            "2  0.998992  \n",
            "3  0.999024  \n",
            "Current result ==================================================\n",
            "Sample count: 33\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.148910  0.509661  0.489033    0.487520  0.008056   \n",
            "1  BERT+LexRank   0.218079  0.236858  0.218770    0.204511  0.008316   \n",
            "2          BESM   0.201680  0.430626  0.406615    0.377079  0.010118   \n",
            "3   BESM+kobert   0.209361  0.471610  0.397617    0.398073  0.011504   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.492705    0.960586  \n",
            "1     0.218110    0.998988  \n",
            "2     0.402556    0.987693  \n",
            "3     0.412552    0.987200  \n",
            "==================================================\n",
            "38 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "블라인드가 내려져 있어 안은 어두웠다. 순경이 전등 스위치를 찾아내어 당겼다. 그러나 전구의 촉수가 낮아 안은 여전히 어두웠다. 나는 가게 안을 빙 둘러보았다. 지저분하고 좁은 곳으로 몇 권의 싸구려 잡지가 흩어져 있고 어제 신문에는 하루치 먼지가 쌓여 있었다. 계산대 뒤에는 천장까지 선반이 매어져 파이프 담배며 궐련 봉지가 놓여 있었다. 박하가 든 과자와 사탕병도 있었다. 흔해빠진 구멍가게로 다른 데에도 몇천 군데나 있는 그런 곳이었다. 순경은 느릿한 햄프셔 사투리로 상황을 설명했다. “거기 계산대 뒤에 웅크린 채 쓰러져 있었지요. 할머니는 자신이 습격당하는 것을 모르고 있었다고 의사 선생님이 말씀하셨습니다. 아마 선반으로 막 손을 내민 순간이었는지도 모르지요. ” “손에는 아무것도 없었소?” “없었습니다. 다만 곁에 <플레이어즈>꾸러미가 하나 떨어져 있었지요. ” 포아로는 고개를 끄덕였다. 그의 눈은 그 좁은 가게를 탐색하듯 둘러보았다. 아무것도 없다. “그런데 철도 안내서는 어디에?” “여기입니다. ” 순경은 계산대 위를 가리켰다. “바로 앤도버 있는 데가 펼쳐진 채 뒤집혀져 있었습니다.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.4079718589782715 Generator / grammar loss:-0.12458596378564835   similarity loss:-0.0832083523273468\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "어두웠다. 당겼다. 싸구려 흩어져 있고 쌓여 있었다. 그런 손을 없었소?” 가게를 둘러보았다. “그런데 순경은 계산대 가리켰다. 데가 \n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "순경이 전등 스위치를 찾아내어 당겼다.아무것도 없다.할머니는 자신이 습격당하는 것을 모르고 있었다고 의사 선생님이 말씀하셨습니다.박하가 든 과자와 사탕병도 있었다.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "나는 가게 안을 빙 둘러보았다.지저분하고 좁은 곳으로 몇 권의 싸구려 잡지가 흩어져 있고 어제 신문에는 하루치 먼지가 쌓여 있었다. 계산대 뒤에는 천장까지 선반이 매어져 파이프 담배며 궐련 봉지가 놓여 있었다.\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "나는 가게 안을 빙 둘러보았다.지저분하고 좁은 곳으로 몇 권의 싸구려 잡지가 흩어져 있고 어제 신문에는 하루치 먼지가 쌓여 있었다. 할머니는 자신이 습격당하는 것을 모르고 있었다고 의사 선생님이 말씀하셨습니다.\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.133215  0.534286  0.432847  0.577605  0.003680  0.496562   \n",
            "1  BERT+LexRank    0.161634  0.202199  0.287148  0.060252  0.008761  0.202089   \n",
            "2          BESM    0.207815  0.307616  0.461808  0.222294  0.009825  0.359116   \n",
            "3   BESM+kobert    0.207815  0.307278  0.487159  0.195944  0.014395  0.363819   \n",
            "\n",
            "    grammar  \n",
            "0  0.963372  \n",
            "1  0.998985  \n",
            "2  0.999000  \n",
            "3  0.999002  \n",
            "Current result ==================================================\n",
            "Sample count: 34\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.148449  0.510385  0.487380    0.490170  0.007927   \n",
            "1  BERT+LexRank   0.216418  0.235838  0.220781    0.200268  0.008329   \n",
            "2          BESM   0.201861  0.427008  0.408238    0.372527  0.010109   \n",
            "3   BESM+kobert   0.209316  0.466776  0.400250    0.392128  0.011589   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.492818    0.960668  \n",
            "1     0.217639    0.998988  \n",
            "2     0.401279    0.988025  \n",
            "3     0.411119    0.987548  \n",
            "==================================================\n",
            "39 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "런던 행 기차를 보고 있었던 것 같습니다. 그렇다면 그는 앤도버 사람이 아닙니다. 그러나 물론 철도 안내서는 살인과 관계없는 다른 사람이 잃어버리고 간 거라고 생각할 수도 있습니다. ” 내가 물어 보았다. “지문은?” 순경은 머리를 저었다. “곧바로 모두 조사해 보았지만 없었지요. ” 포아로가 물었다. “계산대에도?” “굉장히 많았습니다. 모두 함께 뒤섞여 뒤죽박죽되어 있었지요. ” “그 속에 애셔의 지문은?” “아직 알 수 없습니다. ” 포아로는 고개를 끄덕이고 나서 죽은 사람이 가게 안에서 살고 있었느냐고 물었다. “그렇습니다. 안쪽 문을 지나면 그곳으로 들어가게 됩니다. 함께 가드렸으면 좋겠습니다만, 저는 여기 있지 않으면 안 돼서……. ” 포아로는 문을 열고 들어갔다. 나도 그 뒤를 따라갔다. 가게 안은 부엌 딸린 조그만 거실로 되어 있었다. 그곳은 깨끗하게 정리되어 있었지만 음침한 느낌이 들었으며 가구도 거의 없었다. 벽난로 위에 사진이 몇 장 있었다. 내가 다가가서 들여다보자 포아로도 옆으로 왔다.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.5124011039733887 Generator / grammar loss:-0.12660890817642212   similarity loss:-0.07420135289430618\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "런던 보고 있었던 안내서는 살인과 관계없는 수도 물어 보았다. 포아로가 모두 고개를 끄덕이고 안에서 살고 “그렇습니다. 들어가게 됩니다.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "런던 행 기차를 보고 있었던 것 같습니다.가게 안은 부엌 딸린 조그만 거실로 되어 있었다.” 내가 물어 보았다.“지문은?” 순경은 머리를 저었다.그러나 물론 철도 안내서는 살인과 관계없는 다른 사람이 잃어버리고 간 거라고 생각할 수도 있습니다.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "그러나 물론 철도 안내서는 살인과 관계없는 다른 사람이 잃어버리고 간 거라고 생각할 수도 있습니다. ” 포아로는 고개를 끄덕이고 나서 죽은 사람이 가게 안에서 살고 있었느냐고 물었다. “\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "그러나 물론 철도 안내서는 살인과 관계없는 다른 사람이 잃어버리고 간 거라고 생각할 수도 있습니다. ” 그곳은 깨끗하게 정리되어 있었지만 음침한 느낌이 들었으며 가구도 거의 없었다.\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.147573  0.433823  0.411196  0.467225  0.000530  0.432530   \n",
            "1  BERT+LexRank    0.264078  0.333706  0.208885  0.194017  0.003924  0.229389   \n",
            "2          BESM    0.201942  0.595372  0.466505  0.365759  0.008831  0.462054   \n",
            "3   BESM+kobert    0.196117  0.467602  0.265198  0.356111  0.006851  0.332953   \n",
            "\n",
            "    grammar  \n",
            "0  0.994861  \n",
            "1  0.998922  \n",
            "2  0.988796  \n",
            "3  0.999009  \n",
            "Current result ==================================================\n",
            "Sample count: 35\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.148424  0.508198  0.485204    0.489514  0.007716   \n",
            "1  BERT+LexRank   0.217780  0.238635  0.220441    0.200089  0.008203   \n",
            "2          BESM   0.201863  0.431819  0.409903    0.372333  0.010073   \n",
            "3   BESM+kobert   0.208939  0.466800  0.396392    0.391099  0.011454   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.491096    0.961645  \n",
            "1     0.217974    0.998986  \n",
            "2     0.403015    0.988047  \n",
            "3     0.408885    0.987875  \n",
            "==================================================\n",
            "40 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "사진은 모두 세 장이었다. 한 장은 오늘 오후에 만난 아가씨 메리 드로워의 싸구려 사진이었다. 그녀는 가장 좋은 옷을 입고 얼굴에 부자연스러운 미소를 떠올리고 있었다. 포즈를 취한 이런 사진은 표정을 엉망으로 만들기 때문에 스냅 사진 쪽이 훨씬 좋다. 두 번째 것은 더 고급스러운 것으로, 꽤 나이든 머리가 희끗희끗한 부인을 기교적으로 흐릿하게 찍은 사진이었다. 털목도리를 두르고 있었다. 나는 아마도 미스 로즈일 거라고 생각했다. 즉 애셔 부인에게 장사를 시작할 수 있도록 돈을 물려준 사람이다. 세 번째 사진은 아주 오래된 것으로 누렇게 빛이 바래 있었다. 얼마쯤 구식으로 보이는 것으로 팔짱낀 젊은 남녀가 찍혀있었다. 남자는 단춧구멍에 꽃을 꽂고 있으며, 전체적으로 고풍스러움이 느껴지는 딱딱한 사진이었다. 포아로가 말했다. “아마도 결혼 기념사진인 모양이군. 보게, 헤이스팅즈. 그녀는 아름다웠을 거라고 내가 말했잖나. ” 그 말대로였다. 시대에 뒤떨어진 머리 모양과 기묘한 옷 때문에 좀 이상해 보이긴 했지만 이목구비가 또렷하고 반듯한 아가씨의 아름다움은 의심할 바가 없었다. 나는 옆에 있는 다른 한 인물을 자세히 보았는데, 이 군인 같은 모습의 말쑥한 젊은이가 그 초라한 애셔였다고는 도저히 생각되지 않았다. 나는 그 곁눈질을 하는 주정꾼 노인과 피로에 지친 얼굴의 죽은 노파를 생각해 내고 세월의 무자비함에 몸을 떨었다. 그 거실로부터 2층의 두 방으로 층계가 이어져 있었다.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.43034291267395 Generator / grammar loss:-0.13516995310783386   similarity loss:-0.09145242720842361\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            " 사진은 세 오늘 오후에 만난 아가씨 메리 싸구려 사진이었다. 미소를 떠올리고 좋다. 더 나이든 희끗희끗한 사진이었다. 남자는 단춧구멍에 전체적으로 고풍스러움이 아름다웠을 내가 말대로였다. 기묘한 무자비함에 떨었다.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "그 거실로부터 2층의 두 방으로 층계가 이어져 있었다.사진은 모두 세 장이었다.” 그 말대로였다.포아로가 말했다.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "포즈를 취한 이런 사진은 표정을 엉망으로 만들기 때문에 스냅 사진 쪽이 훨씬 좋다. 시대에 뒤떨어진 머리 모양과 기묘한 옷 때문에 좀 이상해 보이긴 했지만 이목구비가 또렷하고 반듯한 아가씨의 아름다움은 의심할 바가 없었다.\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "포즈를 취한 이런 사진은 표정을 엉망으로 만들기 때문에 스냅 사진 쪽이 훨씬 좋다. 두 번째 것은 더 고급스러운 것으로, 꽤 나이든 머리가 희끗희끗한 부인을 기교적으로 흐릿하게 찍은 사진이었다.\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.166205  0.637122  0.418359  0.457801  0.009063  0.473944   \n",
            "1  BERT+LexRank    0.087258  0.262255  0.138299  0.214380  0.002605  0.185915   \n",
            "2          BESM    0.171745  0.518924  0.290211  0.420207  0.008773  0.374953   \n",
            "3   BESM+kobert    0.149584  0.467219  0.341312  0.382018  0.002752  0.378705   \n",
            "\n",
            "    grammar  \n",
            "0  0.979253  \n",
            "1  0.998698  \n",
            "2  0.998993  \n",
            "3  0.998996  \n",
            "Current result ==================================================\n",
            "Sample count: 36\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.148918  0.511779  0.483347    0.488633  0.007753   \n",
            "1  BERT+LexRank   0.214155  0.239291  0.218159    0.200486  0.008048   \n",
            "2          BESM   0.201026  0.434238  0.406578    0.373663  0.010037   \n",
            "3   BESM+kobert   0.207290  0.466812  0.394862    0.390846  0.011212   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.490619    0.962134  \n",
            "1     0.217084    0.998978  \n",
            "2     0.402236    0.988351  \n",
            "3     0.408047    0.988184  \n",
            "==================================================\n",
            "41 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "하나는 빈방으로 가구도 없고, 다른 하나는 죽은 노파의 침실이었다. 경찰이 조사한 뒤여서 그 흔적이 그대로 있었다. 침대에는 털이 빠진 낡은 담요가 두 장 있었다. 한 서랍에는 알뜰히 기워진 속옷 몇 벌, 또 한 서랍에는 요리책 종류, ≪녹색의 오아시스≫라는 제목의 표지가 달린 책, 번쩍거리는 싸구려 새 양말 한 컬레?그것은 번쩍거리는 싸구려였다?사기 그릇 장식 한 쌍?드레스덴 도자기로 된 깨어진 양치기며 파랑과 노랑점이 있는 개?나무못에 걸린 검은 레인코트와 털 자켓. 이러한 것들이 죽은 애셔 부인이 이 세상에 남긴 재산이었다. 무언가 개인적인 메모 같은 게 있었다 해도 경찰이 가져가 버렸을 것이다. 포아로가 중얼거렸다. “가엾게도. 자, 헤이스팅즈, 여기에는 이제 아무것도 없네. ” 다른 길로 나서자 그는 잠시 망설이더니 길을 건넜다. 바로 애셔 부인의 가게 맞은편에 야채 가게가 있었다. 안에 있는 물건보다 밖에 내놓은 물건이 더 많은 그런 종류의 가게였다. 포아로는 낮은 소리로 내게 몇 마디 일러두고 혼자 가게에 들어갔다. 나는 잠시 뒤 따라 들어갔다. 그는 막 상추를 사고 있는 중이었다. 나는 딸기를 1파운드 샀다. 포아로는 물건을 싸주는 뚱뚱한 아주머니와 큰소리로 이야기하고 있었다. “그 살인 사건이 일어난 곳이 바로 댁 맞은편이었군요. 이런 끔찍한 일이 있나. 얼마나 놀랐겠습니까!” 그 뚱뚱한 여자는 살인 사건 이야기에는 이제 질린 것 같았다.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.4976115226745605 Generator / grammar loss:-0.13872742652893066   similarity loss:-0.08789955824613571\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            " 하나는 가구도 흔적이 있었다. 속옷 한 서랍에는 오아시스≫라는 양말 컬레?그것은 장식 깨어진 양치기며 털 것들이 죽은 애셔 부인이 이 같은 게 중얼거렸다. 혼자 싸주는 뚱뚱한 여자는\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "나는 딸기를 1파운드 샀다.이런 끔찍한 일이 있나.안에 있는 물건보다 밖에 내놓은 물건이 더 많은 그런 종류의 가게였다.침대에는 털이 빠진 낡은 담요가 두 장 있었다.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "한 서랍에는 알뜰히 기워진 속옷 몇 벌, 또 한 서랍에는 요리책 종류, ≪녹색의 오아시스≫라는 제목의 표지가 달린 책, 번쩍거리는 싸구려 새 양말 한 컬레?그것은 번쩍거리는 싸구려였다?사기 그릇 장식 한 쌍?드레스덴 도자기로 된 깨어진 양치기며 파랑과 노랑점이 있는 개?나무못에 걸린 검은 레인코트와 털 자켓.이러한 것들이 죽은 애셔 부인이 이 세상에 남긴 재산이었다.\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "한 서랍에는 알뜰히 기워진 속옷 몇 벌, 또 한 서랍에는 요리책 종류, ≪녹색의 오아시스≫라는 제목의 표지가 달린 책, 번쩍거리는 싸구려 새 양말 한 컬레?그것은 번쩍거리는 싸구려였다?사기 그릇 장식 한 쌍?드레스덴 도자기로 된 깨어진 양치기며 파랑과 노랑점이 있는 개?나무못에 걸린 검은 레인코트와 털 자켓.이러한 것들이 죽은 애셔 부인이 이 세상에 남긴 재산이었다.\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.143258  0.516542  0.291644  0.334674  0.009501  0.349532   \n",
            "1  BERT+LexRank    0.130618  0.294117  0.137771  0.289008  0.005260  0.214411   \n",
            "2          BESM    0.289326  0.388312  0.167909  0.161514  0.011117  0.210071   \n",
            "3   BESM+kobert    0.289326  0.388312  0.167909  0.161514  0.011117  0.210071   \n",
            "\n",
            "    grammar  \n",
            "0  0.972124  \n",
            "1  0.999020  \n",
            "2  0.998947  \n",
            "3  0.998947  \n",
            "Current result ==================================================\n",
            "Sample count: 37\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.148765  0.511908  0.478166    0.484472  0.007801   \n",
            "1  BERT+LexRank   0.211897  0.240773  0.215987    0.202879  0.007973   \n",
            "2          BESM   0.203413  0.432997  0.400127    0.367929  0.010066   \n",
            "3   BESM+kobert   0.209507  0.464690  0.388728    0.384648  0.011209   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.486806    0.962404  \n",
            "1     0.217011    0.998979  \n",
            "2     0.397042    0.988638  \n",
            "3     0.402696    0.988475  \n",
            "==================================================\n",
            "42 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "그날은 그녀에게 있어 너무 길었던 모양이다. “이 법석거리는 구경꾼들을 어떻게 좀 할 수 없을까요? 대체 무엇을 그렇게 보는 것일까요?” 포아로가 말했다. “어젯밤에는 꽤 달랐을 테지요?아주머니는 범인이 가게로 들어가는 걸 보시지 못했습니까? 키가 큰 훌륭한 남자로 수염이 있었다지요? 러시아인이라든가 뭐 그렇다는 이야기던데요?” 여자가 날카롭게 돌아보았다. “뭐라고요? 러시아인이 했다고요?” “경찰이 체포했다던데요. ” “정말이에요?” 여자는 흥분해서 입이 가벼워졌다. “외국 사람인가요?” “그렇습니다. 나는 틀림없이 아주머니가 어젯밤 그 남자를 본 줄 알았지요. ” “아니, 그럴 기회가 없었어요. 그래요, 저녁 무렵의 한창 바쁜 때여서 일을 끝내고 돌아가는 사람들이 많이 비나가니까요. 키가 크고 수염이 난 훌륭한 남자라니……아니에요. 그런 사람이 이 언저리에 있었다고는 생각되지 않는데요. ” 그래서 내가 대사를 받았다. 나는 포아로에게 말했다. “실례지만, 당신이 잘못 들은 게 아닙니까? 키가 작고 얼굴빛이 검은 남자라고 나는 들었습니다만.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.204296350479126 Generator / grammar loss:-0.10772813856601715   similarity loss:-0.087227001786232\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            " 그날은 그녀에게 너무 법석거리는 말했다. 보시지 큰 수염이 러시아인이라든가 날카롭게 가벼워졌다. 나는 줄 “아니, 나는 아닙니까? \n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "러시아인이 했다고요?” “경찰이 체포했다던데요.그래요, 저녁 무렵의 한창 바쁜 때여서 일을 끝내고 돌아가는 사람들이 많이 비나가니까요.키가 크고 수염이 난 훌륭한 남자라니……아니에요.키가 큰 훌륭한 남자로 수염이 있었다지요?나는 틀림없이 아주머니가 어젯밤 그 남자를 본 줄 알았지요.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "어젯밤에는 꽤 달랐을 테지요?아주머니는 범인이 가게로 들어가는 걸 보시지 못했습니까? 그래요, 저녁 무렵의 한창 바쁜 때여서 일을 끝내고 돌아가는 사람들이 많이 비나가니까요.키가 크고 수염이 난 훌륭한 남자라니……아니에요.\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "어젯밤에는 꽤 달랐을 테지요?아주머니는 범인이 가게로 들어가는 걸 보시지 못했습니까? 그래요, 저녁 무렵의 한창 바쁜 때여서 일을 끝내고 돌아가는 사람들이 많이 비나가니까요.키가 크고 수염이 난 훌륭한 남자라니……아니에요.\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.138318  0.580874  0.411272  0.486619  0.004814  0.467796   \n",
            "1  BERT+LexRank    0.295327  0.138646  0.280979  0.106737  0.005737  0.200240   \n",
            "2          BESM    0.231776  0.328760  0.385354  0.227985  0.004236  0.326825   \n",
            "3   BESM+kobert    0.231776  0.328760  0.385354  0.227985  0.004236  0.326825   \n",
            "\n",
            "    grammar  \n",
            "0  0.955407  \n",
            "1  0.998629  \n",
            "2  0.998984  \n",
            "3  0.998984  \n",
            "Current result ==================================================\n",
            "Sample count: 38\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.148490  0.513723  0.476405    0.484529  0.007722   \n",
            "1  BERT+LexRank   0.214092  0.238085  0.217697    0.200348  0.007914   \n",
            "2          BESM   0.204159  0.430254  0.399739    0.364247  0.009913   \n",
            "3   BESM+kobert   0.210093  0.461113  0.388639    0.380526  0.011026   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.486306    0.962220  \n",
            "1     0.216570    0.998970  \n",
            "2     0.395194    0.988910  \n",
            "3     0.400700    0.988751  \n",
            "==================================================\n",
            "43 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "” 그리하여 이 뚱뚱한 여자에다 여윈 남편과 쇳소리 내는 심부름꾼 아이까지 합쳐 재미있는 토론이 시작되었다. 키 작은 검은 얼굴의 남자가 네 사람이나 목격된 이야기가 나오고, 쇳소리 내는 심부름꾼 아이는 키가 큰 훌륭한 남자를 보았지만 그에게는 수염이 없었다고 유감스러운 듯 덧붙였다. 겨우 쇼핑이 끝나 우리는 거짓말을 한 채 그대로 가게를 나왔다. 나는 얼마쯤 비난을 섞어 물었다. “대체 그건 무슨 연극이었나, 포아로?” “나는 다만 낯선 사람이 저쪽 가게로 들어갔는지 어떤지 듣고 싶었던 것뿐일세. ” “그럼, 그렇게 물어보면 되잖나, 그런 엉터리 같은 소리 하지 말고. ” “아니, 자네가 말하는 것처럼 그냥 물어 보아서는 아무 대답도 얻을 수 없다네. 자네는 자신도 영국 사람이면서, 그냥 물어보는 질문에 반발하는 게 영국 사람의 기질이라는 걸 모르고 있는 모양이군. 그것은 반드시 의심하는 마음을 불러일으켜 결과는 완강한 침묵으로 끝난다네. 이 사람들에게 뭘 물어보게나, 그들은 조가비처럼 입을 다물어 버리지. 그렇지만 이상하고 터무니없는 어떤 말을 한 가지 꺼내 거기서 자네가 반대되는 말이라도 해보이면, 금방 이야기가 풀려나온다네. 그런 방법으로 우리는 문제의 시각이 바쁜 때였다는 것, 그래서 누구나 자기 일 말고는 신경 쓸 수 없으며 많은 사람이 길을 지나가고 있었던 때임을 알게 된 거야. 우리의 살인범은 좋은 시간을 택했다는 말이 되네, 헤이스팅즈. ” 그는 말을 끊었다. 그리고는 엄격하게 나무라는 듯한 말투로 덧붙였다. “자네는 상식이라는 걸 갖고 있지 않는 것 같군, 헤이스팅즈. 무엇이든 사라고 했더니 하필이면 딸기를 고르다니! 보게, 벌써 포장지에서 물이 배어 나와 그 좋은 옷을 버리게 하고 있잖나. ” 정말 그의 말대로였으므로 나는 좀 당황했다. 나는 급히 한 아이에게 딸기를 줘버렸다.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:1.9103740453720093 Generator / grammar loss:-0.08959800750017166   similarity loss:-0.09856661409139633\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "그리하여 쇳소리 아이까지 재미있는 검은 얼굴의 쇳소리 아이는 키가 남자를 보았지만 그에게는 없었다고 덧붙였다. 겨우 그건 포아로?” “나는 낯선 들어갔는지 싶었던 엉터리 하지 아무 사람이면서, 반발하는 게 결과는 침묵으로 끝난다네. 버리지. 한 쓸\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "무엇이든 사라고 했더니 하필이면 딸기를 고르다니!우리의 살인범은 좋은 시간을 택했다는 말이 되네, 헤이스팅즈.” 그리하여 이 뚱뚱한 여자에다 여윈 남편과 쇳소리 내는 심부름꾼 아이까지 합쳐 재미있는 토론이 시작되었다.그런 방법으로 우리는 문제의 시각이 바쁜 때였다는 것, 그래서 누구나 자기 일 말고는 신경 쓸 수 없으며 많은 사람이 길을 지나가고 있었던 때임을 알게 된 거야.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "” 그리하여 이 뚱뚱한 여자에다 여윈 남편과 쇳소리 내는 심부름꾼 아이까지 합쳐 재미있는 토론이 시작되었다. 키 작은 검은 얼굴의 남자가 네 사람이나 목격된 이야기가 나오고, 쇳소리 내는 심부름꾼 아이는 키가 큰 훌륭한 남자를 보았지만 그에게는 수염이 없었다고 유감스러운 듯 덧붙였다. 그럼, 그렇게 물어보면 되잖나, 그런 엉터리 같은 소리 하지 말고. ” “\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "” 그리하여 이 뚱뚱한 여자에다 여윈 남편과 쇳소리 내는 심부름꾼 아이까지 합쳐 재미있는 토론이 시작되었다. 아니, 자네가 말하는 것처럼 그냥 물어 보아서는 아무 대답도 얻을 수 없다네. 그런 방법으로 우리는 문제의 시각이 바쁜 때였다는 것, 그래서 누구나 자기 일 말고는 신경 쓸 수 없으며 많은 사람이 길을 지나가고 있었던 때임을 알게 된 거야.\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.152150  0.494548  0.559310  0.362788  0.006686  0.487401   \n",
            "1  BERT+LexRank    0.232635  0.411492  0.238185  0.330130  0.005012  0.300430   \n",
            "2          BESM    0.221610  0.711161  0.450046  0.371778  0.021054  0.478789   \n",
            "3   BESM+kobert    0.214994  0.522187  0.535943  0.275813  0.014284  0.455153   \n",
            "\n",
            "    grammar  \n",
            "0  0.818964  \n",
            "1  0.999032  \n",
            "2  0.996828  \n",
            "3  0.999026  \n",
            "Current result ==================================================\n",
            "Sample count: 39\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.148584  0.513231  0.478531    0.481407  0.007695   \n",
            "1  BERT+LexRank   0.214568  0.242531  0.218222    0.203676  0.007839   \n",
            "2          BESM   0.204607  0.437457  0.401029    0.364440  0.010198   \n",
            "3   BESM+kobert   0.210219  0.462679  0.392416    0.377841  0.011110   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.486334    0.958547  \n",
            "1     0.218720    0.998971  \n",
            "2     0.397338    0.989113  \n",
            "3     0.402096    0.989015  \n",
            "==================================================\n",
            "44 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "그 아이는 깜짝 놀라 좀 경계하는 빛이 되었다. 포아로도 상추를 주자 아이는 완전히 당황한 모양이었다. 포아로는 설교를 계속했다. “허름한 야채 가게에서는 딸기 같은걸 사면 안돼. 딸기란 막 따온 게 아니면 물이 배어 나오지. 바나나, 사과, 양배추, 이런 것들이라면 그래도 좀 낫지만, 딸기는 안 되네. ” 나는 변명하듯 말했다. “막 들어서자 생각이 났으니 어쩌나. ” 포아로는 엄숙하게 대답했다. “그건 자네 상상력이 모자라기 때문일세,” 그는 보도에서 걸음을 멈췄다. 애셔 부인 가게 오른쪽에 있는 집 딸린 가게는 비어 있었다. 창에 <세놓음>이라고 씌어 있었다. 반대쪽 옆집에는 때낀 모슬린 커튼이 내려져 있었다. 포아로는 그 집 쪽으로 걸어갔는데 벨이 없어서 노커를 힘차게 몇 번이나 두드렸다. 한참 있다가 코를 훌쩍거리는 지저분한 아이가 문을 열었다. 포아로가 말했다. “안녕, 어머니 계시니?” “네?” 아이는 불쾌하고 의심스럽게 우리를 보았다. 포아로가 말했다. “네 어머니 말이야. ” 아이는 이 말을 알아듣는 데 5분의 1분쯤 걸렸다.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.423219919204712 Generator / grammar loss:-0.11804119497537613   similarity loss:-0.07506996393203735\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "그래도 ” 말했다. 들어서자 생각이 났으니 어쩌나. 대답했다. “그건 보도에서 걸음을 멈췄다. 오른쪽에 비어 씌어 있었다. 포아로가 걸렸다.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "바나나, 사과, 양배추, 이런 것들이라면 그래도 좀 낫지만, 딸기는 안 되네.“허름한 야채 가게에서는 딸기 같은걸 사면 안돼.딸기란 막 따온 게 아니면 물이 배어 나오지.반대쪽 옆집에는 때낀 모슬린 커튼이 내려져 있었다.포아로는 설교를 계속했다.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "허름한 야채 가게에서는 딸기 같은걸 사면 안돼.딸기란 막 따온 게 아니면 물이 배어 나오지. 바나나, 사과, 양배추, 이런 것들이라면 그래도 좀 낫지만, 딸기는 안 되네. ”\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "허름한 야채 가게에서는 딸기 같은걸 사면 안돼.딸기란 막 따온 게 아니면 물이 배어 나오지. 바나나, 사과, 양배추, 이런 것들이라면 그래도 좀 낫지만, 딸기는 안 되네. ”\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.146067  0.279582  0.449567  0.480700  0.007813  0.424910   \n",
            "1  BERT+LexRank    0.256554  0.350203  0.261164  0.065087  0.014185  0.220149   \n",
            "2          BESM    0.181648  0.471039  0.438160  0.461920  0.000192  0.451863   \n",
            "3   BESM+kobert    0.181648  0.471039  0.438160  0.461920  0.000192  0.451863   \n",
            "\n",
            "    grammar  \n",
            "0  0.968446  \n",
            "1  0.999036  \n",
            "2  0.995608  \n",
            "3  0.995608  \n",
            "Current result ==================================================\n",
            "Sample count: 40\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.148521  0.507390  0.477807    0.481390  0.007698   \n",
            "1  BERT+LexRank   0.215617  0.245223  0.219296    0.200211  0.007998   \n",
            "2          BESM   0.204033  0.438296  0.401957    0.366877  0.009948   \n",
            "3   BESM+kobert   0.209505  0.462888  0.393560    0.379943  0.010837   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.484798    0.958794  \n",
            "1     0.218756    0.998973  \n",
            "2     0.398701    0.989275  \n",
            "3     0.403340    0.989180  \n",
            "==================================================\n",
            "45 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "이윽고 아이는 층계 쪽을 향해 소리쳤다. “엄마, 손님. ” 그리고는 어두컴컴한 안쪽으로 들어가 버렸다. 딱딱한 얼굴을 한 여자가 난간 너머로 내려다보고 나서 층계를 내려왔다. “시간 낭비예요. ” 여자가 말을 시작했으나, 포아로가 가로막았다. 그는 모자를 벗고 정중하게 인사했다. “안녕하십니까, 아주머니. 저는 <이브닝 프리커>의 기자인데 살해된 이웃집의 애셔 부인에 대해 기사가 될 만한 것을 얻으러 왔습니다. 사례금으로 5파운드 드리지요. ” 화난 목소리를 억누르고 여자는 머리를 쓰다듬고 치마를 잡아당기며 층계를 내려왔다. “자, 안으로 들어오세요. 이쪽으로. 어서 앉으세요. ” 그 조그만 방은 커다란 모조 자코비언 식 가구로 어수선하여 우리는 가까스로 안으로 들어가 딱딱한 긴 의자에 앉았다. 여자는 이야기하기 시작했다. “죄송해요. 조금 전에 그런 실례되는 말을 드려서요. 그렇지만 우리가 얼마나 성가신 꼴을 당하고 있는지 도저히 모르실 거예요. 아무튼 여러 사람들이 진공청소기니 양말이니 향로 주머니니 뭐니 온갖 잡동사니들을 팔러 온답니다.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.4729421138763428 Generator / grammar loss:-0.13059203326702118   similarity loss:-0.08238545805215836\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "층계 그리고는 낭비예요. ” “안녕하십니까, 애셔 대해 얻으러 5파운드 ” 목소리를 억누르고 쓰다듬고 잡아당기며 내려왔다. 어서 조그만 앉았다.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "저는 <이브닝 프리커>의 기자인데 살해된 이웃집의 애셔 부인에 대해 기사가 될 만한 것을 얻으러 왔습니다.아무튼 여러 사람들이 진공청소기니 양말이니 향로 주머니니 뭐니 온갖 잡동사니들을 팔러 온답니다.사례금으로 5파운드 드리지요.이쪽으로.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "저는 <이브닝 프리커>의 기자인데 살해된 이웃집의 애셔 부인에 대해 기사가 될 만한 것을 얻으러 왔습니다. 화난 목소리를 억누르고 여자는 머리를 쓰다듬고 치마를 잡아당기며 층계를 내려왔다. “ 아무튼 여러 사람들이 진공청소기니 양말이니 향로 주머니니 뭐니 온갖 잡동사니들을 팔러 온답니다.\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "저는 <이브닝 프리커>의 기자인데 살해된 이웃집의 애셔 부인에 대해 기사가 될 만한 것을 얻으러 왔습니다. 그 조그만 방은 커다란 모조 자코비언 식 가구로 어수선하여 우리는 가까스로 안으로 들어가 딱딱한 긴 의자에 앉았다.\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.149533  0.354941  0.446191  0.393614  0.001399  0.412168   \n",
            "1  BERT+LexRank    0.248598  0.168332  0.156425  0.212848  0.000590  0.175733   \n",
            "2          BESM    0.300935  0.484552  0.366142  0.422842  0.002338  0.406834   \n",
            "3   BESM+kobert    0.231776  0.265177  0.345478  0.332145  0.001235  0.325418   \n",
            "\n",
            "    grammar  \n",
            "0  0.968176  \n",
            "1  0.999020  \n",
            "2  0.998984  \n",
            "3  0.999021  \n",
            "Current result ==================================================\n",
            "Sample count: 41\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.148545  0.503671  0.477036    0.479249  0.007545   \n",
            "1  BERT+LexRank   0.216422  0.243348  0.217763    0.200520  0.007817   \n",
            "2          BESM   0.206396  0.439424  0.401083    0.368242  0.009762   \n",
            "3   BESM+kobert   0.210048  0.458066  0.392387    0.378777  0.010602   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.483027    0.959023  \n",
            "1     0.217707    0.998974  \n",
            "2     0.398899    0.989512  \n",
            "3     0.401440    0.989420  \n",
            "==================================================\n",
            "46 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "그들은 정말 말솜씨가 좋고 점잖게 보이지요. 이름도 한 번 들으면 금방 외워서 이쪽은 파울러 부인이고, 저쪽은 누구라느니 하며 말예요. ” 재치있게 그 이름을 잡아서 포아로가 말했다. “갑작스러운 이야기입니다만, 파울러 부인, 우리가 부탁드린 일을 들어주시겠습니까?” “글쎄요. ” 그러나 이미 5파운드가 파울러 부인의 눈앞에 유혹하듯 어른거리고 있다. “애셔 부인은 알고 있지만, 글로 쓰는 일이고 보면. ” 포아로는 얼른 안심시키듯 그녀 쪽에서는 아무것도 하지 않아도 되며, 그녀로부터 사실 이야기를 들은 다음 기사는 자기 쪽에서 쓴다고 이야기해 주었다. 이에 용기를 얻어 파울러 부인은 자진해서 기억이며 억측이며 소문 따위를 이것저것 이야기해 주었다. 애셔 부인은 사람들을 멀리하며 살았다. 이웃과 어울리는 일이 거의 없었고, 그 가엾은 d자에게는 여러 가지 근심거리가 있었다. 그것은 누구나 모두 잘 알고 있는 일이었다. 프란츠 애셔는 벌써 형무소에 처넣어야 마땅할 그런 남자였다. 그러나 애셔 부인이 그를 무서워하고 있었던 건 아니다. 그녀가 화를 내면 굉장했다. 그리고 언제나 솜씨 있게 잘 응수해 왔다. 하지만 그런 일이 일어나다니……. 즉 일이 되어갈 데까지 가버린 것이다. 파울러 부인은 몇 번이고 되풀이 그녀에게 이야기했었다. “그 남자는 언젠가 당신에게 무서운 짓을 할 거예요. 내 말을 잘 기억해 둬요.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.1934783458709717 Generator / grammar loss:-0.13267643749713898   similarity loss:-0.11326789855957031\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "이쪽은 일이고 사실 이것저것 그 애셔는 부인이 아니다. 그녀가 내면 굉장했다. 그리고 잘 응수해 왔다. 즉 일이 파울러 부인은 몇 번이고 그녀에게 짓을 둬요.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "프란츠 애셔는 벌써 형무소에 처넣어야 마땅할 그런 남자였다.이웃과 어울리는 일이 거의 없었고, 그 가엾은 d자에게는 여러 가지 근심거리가 있었다.애셔 부인은 사람들을 멀리하며 살았다.“그 남자는 언젠가 당신에게 무서운 짓을 할 거예요.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "이름도 한 번 들으면 금방 외워서 이쪽은 파울러 부인이고, 저쪽은 누구라느니 하며 말예요. ” 이에 용기를 얻어 파울러 부인은 자진해서 기억이며 억측이며 소문 따위를 이것저것 이야기해 주었다.\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "이름도 한 번 들으면 금방 외워서 이쪽은 파울러 부인이고, 저쪽은 누구라느니 하며 말예요. ” 그러나 이미 5파운드가 파울러 부인의 눈앞에 유혹하듯 어른거리고 있다. “ 포아로는 얼른 안심시키듯 그녀 쪽에서는 아무것도 하지 않아도 되며, 그녀로부터 사실 이야기를 들은 다음 기사는 자기 쪽에서 쓴다고 이야기해 주었다.\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.128467  0.668793  0.479827  0.655767  0.007426  0.570402   \n",
            "1  BERT+LexRank    0.191241  0.078313  0.319538  0.141274  0.010437  0.217814   \n",
            "2          BESM    0.156204  0.636954  0.385599  0.387479  0.013936  0.436434   \n",
            "3   BESM+kobert    0.258394  0.636954  0.517610  0.408372  0.008714  0.508707   \n",
            "\n",
            "    grammar  \n",
            "0  0.989273  \n",
            "1  0.999017  \n",
            "2  0.998889  \n",
            "3  0.998936  \n",
            "Current result ==================================================\n",
            "Sample count: 42\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.148067  0.507603  0.477102    0.483451  0.007542   \n",
            "1  BERT+LexRank   0.215822  0.239418  0.220186    0.199109  0.007880   \n",
            "2          BESM   0.205201  0.444127  0.400715    0.368700  0.009862   \n",
            "3   BESM+kobert   0.211199  0.462325  0.395368    0.379481  0.010557   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.485107    0.959743  \n",
            "1     0.217709    0.998975  \n",
            "2     0.399793    0.989736  \n",
            "3     0.403994    0.989646  \n",
            "==================================================\n",
            "47 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "” 마침내 그 남자는 일을 저지르고 말았다. 그리고 그녀, 파울러 부인은 바로 이웃에 살면서 아무 소리도 못 들은 것이다. 잠시 사이를 두고 나서 포아로가 물었다. 애셔 부인은 어떤 이상한 편지?이를테면 개인적인 서명이 없는?예를 들어 ABC라는 서명이 든 편지를 받은 일이 없는가?파울러 부인은 유감스러운 듯 없다고 대답했다. “당신이 이야기하시는 그런 일은 저도 알고 있어요. 익명 편지라는 거지요. 큰소리로 말하기가 뭣할 정도로 창피스러운 게 가득 씌어 있는……네, 물론 프란츠 애셔가 그런 것을 썼는지 어떤지 저는 몰라요. 물론 썼다고 해도 애셔 부인이 제게 말했을 리 없고요. 뭐라고요? 철도 안내, ABC 철도 안내서라고요? 아니오, 그런 건 못 보았어요. 그리고 만일 애셔 부인에게 그런 게 보내져 왔다면 저한테 꼭 말해 줬을 거예요. 이번 사건을 들었을 때 전 하마터면 쓰러질 뻔했어요. 딸 에디가 알려 줬지요. ‘엄마, 옆 가게에 순경들이 많이 와 있어. ’라고 말예요. 정말 놀랐어요. 그 말을 듣고 전 말했지요. ‘저 아주머니는 그 집에 혼자 사는 게 아니었어.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.471155881881714 Generator / grammar loss:-0.14267800748348236   similarity loss:-0.09466061741113663\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "그 남자는 그리고 이웃에 잠시 사이를 두고 애셔 어떤 개인적인 서명이 받은 일이 없는가?파울러 대답했다. “당신이 창피스러운 것을 저는 몰라요. 보았어요.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "철도 안내, ABC 철도 안내서라고요?‘엄마, 옆 가게에 순경들이 많이 와 있어.“당신이 이야기하시는 그런 일은 저도 알고 있어요.큰소리로 말하기가 뭣할 정도로 창피스러운 게 가득 씌어 있는……네, 물론 프란츠 애셔가 그런 것을 썼는지 어떤지 저는 몰라요.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "그리고 그녀, 파울러 부인은 바로 이웃에 살면서 아무 소리도 못 들은 것이다. 애셔 부인은 어떤 이상한 편지?이를테면 개인적인 서명이 없는?예를 들어 ABC라는 서명이 든 편지를 받은 일이 없는가?파울러 부인은 유감스러운 듯 없다고 대답했다. “\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "그리고 그녀, 파울러 부인은 바로 이웃에 살면서 아무 소리도 못 들은 것이다. 애셔 부인은 어떤 이상한 편지?이를테면 개인적인 서명이 없는?예를 들어 abc라는 서명이 든 편지를 받은 일이 없는가?파울러 부인은 유감스러운 듯 없다고 대답했다. “ 그리고 만일 애셔 부인에게 그런 게 보내져 왔다면 저한테 꼭 말해 줬을 거예요.\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.156364  0.507657  0.412378  0.468511  0.001529  0.448274   \n",
            "1  BERT+LexRank    0.260000  0.183686  0.228546  0.217236  0.000363  0.216181   \n",
            "2          BESM    0.249091  0.666078  0.471434  0.517530  0.006897  0.524192   \n",
            "3   BESM+kobert    0.330909  0.645168  0.490032  0.383274  0.011561  0.489032   \n",
            "\n",
            "    grammar  \n",
            "0  0.992442  \n",
            "1  0.999007  \n",
            "2  0.988720  \n",
            "3  0.999015  \n",
            "Current result ==================================================\n",
            "Sample count: 43\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.148260  0.507604  0.475597    0.483104  0.007402   \n",
            "1  BERT+LexRank   0.216850  0.238122  0.220380    0.199531  0.007705   \n",
            "2          BESM   0.206222  0.449289  0.402359    0.372161  0.009793   \n",
            "3   BESM+kobert   0.213983  0.466577  0.397570    0.379570  0.010581   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.484251    0.960504  \n",
            "1     0.217674    0.998976  \n",
            "2     0.402686    0.989712  \n",
            "3     0.405971    0.989864  \n",
            "==================================================\n",
            "48 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "그 조카딸이라도 함께 있었더라면 좋았을걸. 주정꾼 남자란 정말 허기진 늑대나 다름없으니까. 그 아주머니 남편은 짐승과 다를 바 없어. 나는 그 아주머니한테 몇 번이나 말했었는데, 결국 내 말대로 되어 버렸구나!그 남자는 언젠가 심한 짓을 할 거라고 말했는데. ’ 그 남자는 진짜로 해치운 거예요. 남자란 술을 마시면 무슨 짓을 할지 모르니까요. 이 살인이 그걸 말해 주고 있잖아요. ” 그녀는 숨을 헐떡이며 이야기를 끝냈다. 포아로가 물었다. “그 애셔라는 남자가 가게로 들어가는 것은 아무도 못 본 셈이군요?” 파울러 부인은 경멸하는 듯 콧방귀를 뀌며 말했다. “그야 아무도 못 보도록 들어가는 게 당연하지요. “ 그러나 그녀는 애셔가 어떻게 남의 눈에 띄지 않고 들어갈 수 있었는지에 대해선 설명해 주지 못했다. 그 집에는 뒷문이 없다고 그녀는 말했다. 또 애셔가 이 가까이에 잘 알려져 있다는 데에도 동의했다. “그렇지만 그는 교수형에 처해지기 싫으니까 용케 숨어 들어간 거예요. ” 포아로는 얼마동안 이야기를 이끌어 나가다가 파울러 부인이 알고 있는 이야기를 몇 번이나 되풀이하는 것을 깨닫자 그 면담을 끝내고 약속한 돈을 주었다. 길을 나서자 나는 말했다. “5파운드는 너무 비싼데, 포아로. ” “그렇지, 그것만으로는.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.526634693145752 Generator / grammar loss:-0.14607320725917816   similarity loss:-0.09213928878307343\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "그 남편은 말했었는데, 말대로 남자는 심한 짓을 할 거예요. 또 가까이에 잘 동의했다. “그렇지만 싫으니까 부인이 있는 길을 나서자 너무 포아로. ” “그렇지, 그것만으로는.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "길을 나서자 나는 말했다.포아로가 물었다.그 조카딸이라도 함께 있었더라면 좋았을걸.이 살인이 그걸 말해 주고 있잖아요.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "나는 그 아주머니한테 몇 번이나 말했었는데, 결국 내 말대로 되어 버렸구나!그 남자는 언젠가 심한 짓을 할 거라고 말했는데. ’ 포아로는 얼마동안 이야기를 이끌어 나가다가 파울러 부인이 알고 있는 이야기를 몇 번이나 되풀이하는 것을 깨닫자 그 면담을 끝내고 약속한 돈을 주었다.\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "나는 그 아주머니한테 몇 번이나 말했었는데, 결국 내 말대로 되어 버렸구나!그 남자는 언젠가 심한 짓을 할 거라고 말했는데. ’ 그러나 그녀는 애셔가 어떻게 남의 눈에 띄지 않고 들어갈 수 있었는지에 대해선 설명해 주지 못했다.\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.153239  0.416916  0.406434  0.514848  0.002384  0.441055   \n",
            "1  BERT+LexRank    0.104265  0.385438  0.155831  0.155620  0.011726  0.201689   \n",
            "2          BESM    0.244866  0.422892  0.333033  0.381768  0.001349  0.365625   \n",
            "3   BESM+kobert    0.200632  0.422892  0.500702  0.331261  0.004796  0.434308   \n",
            "\n",
            "    grammar  \n",
            "0  0.991657  \n",
            "1  0.998800  \n",
            "2  0.999031  \n",
            "3  0.999021  \n",
            "Current result ==================================================\n",
            "Sample count: 44\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.148373  0.505543  0.474025    0.483825  0.007288   \n",
            "1  BERT+LexRank   0.214291  0.241470  0.218913    0.198533  0.007796   \n",
            "2          BESM   0.207100  0.448689  0.400784    0.372379  0.009601   \n",
            "3   BESM+kobert   0.213679  0.465584  0.399914    0.378472  0.010449   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.483269    0.961212  \n",
            "1     0.217310    0.998972  \n",
            "2     0.401843    0.989924  \n",
            "3     0.406615    0.990072  \n",
            "==================================================\n",
            "49 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "‘ “자네는 그녀가 이야기한 이상의 것을 알고 있다고 생각하나?” “우리는 지금 무엇을 물어야 좋을지 모르는 기묘한 위치에 놓여 있네. 우리는 어둠 속에서 숨바꼭질하고 있는 어린이들과도 같은 걸세. 우리는 손을 내밀어 찾고 있지. 파울러 부인은 자기가 알고 있다고 여기는 일들을 우리에게 말해줬네. 더욱이 꽤 억측을 해가면서. 그러나 언젠가 그 진술이 쓸모 있게 될 걸세. 5파운드를 투자한 건 결국 그 언젠가를 위해서라네. ” 나는 요점을 잘 잡을 수가 없었다. 그리고 마침 그때 우리는 글렌 형사와 마주쳤다. < 두 증인 > 글렌 형사는 좀 핼쑥해져 있는 듯 했다. 그는 오후 내내 담배 가게에 들어간 사람들 리스트를 만들고 있었던 모양이다. 포아로가 물었다. “결국은 눈에 띈 사람이 아무도 없다는 거로군요?” “아니, 보기는 본 모양입니다. 흘끔거리는 것 같은 느낌의 키가 큰 남자 셋, 시커먼 수염의 키 작은 남자 넷, 턱수염이 있는 사람 둘, 뚱뚱한 사람 셋, 모두 낯선 사람들로, 증언을 믿는다면 다 어딘지 수상쩍은 데가 있는 이들뿐입니다. 권총을 든 복면한 갱 한 무리가 범행을 저지르는 걸 보았다는 사람이 없는 게 이상할 정도입니다. ” 포아로는 동정적인 미소를 지었다. “애셔라는 사나이를 본 사람은 없던가요?” “없습니다. 이것도 그에게 유리한 점입니다. 저는 지금 막 서장님에게, 이것을 런던 경찰국에서 맡아야 할 일이라고 이야기하고 오는 참입니다. 이건 지방적인 범죄가 아닙니다.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.531792402267456 Generator / grammar loss:-0.13938488066196442   similarity loss:-0.08489635586738586\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "그녀가 “우리는 좋을지 모르는 어둠 속에서 내밀어 찾고 파울러 부인은 일들을 말해줬네. 더욱이 들어간 리스트를 물었다. 없다는 모양입니다. 흘끔거리는 남자 사람 사람 걸 지었다. 런던 이건 아닙니다.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "그는 오후 내내 담배 가게에 들어간 사람들 리스트를 만들고 있었던 모양이다.저는 지금 막 서장님에게, 이것을 런던 경찰국에서 맡아야 할 일이라고 이야기하고 오는 참입니다.이건 지방적인 범죄가 아닙니다.5파운드를 투자한 건 결국 그 언젠가를 위해서라네.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "파울러 부인은 자기가 알고 있다고 여기는 일들을 우리에게 말해줬네.더욱이 꽤 억측을 해가면서. 흘끔거리는 것 같은 느낌의 키가 큰 남자 셋, 시커먼 수염의 키 작은 남자 넷, 턱수염이 있는 사람 둘, 뚱뚱한 사람 셋, 모두 낯선 사람들로, 증언을 믿는다면 다 어딘지 수상쩍은 데가 있는 이들뿐입니다.권총을 든 복면한 갱 한 무리가 범행을 저지르는 걸 보았다는 사람이 없는 게 이상할 정도입니다. ” 저는 지금 막 서장님에게, 이것을 런던 경찰국에서 맡아야 할 일이라고 이야기하고 오는 참입니다.\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "파울러 부인은 자기가 알고 있다고 여기는 일들을 우리에게 말해줬네.더욱이 꽤 억측을 해가면서. 그는 오후 내내 담배 가게에 들어간 사람들 리스트를 만들고 있었던 모양이다. 흘끔거리는 것 같은 느낌의 키가 큰 남자 셋, 시커먼 수염의 키 작은 남자 넷, 턱수염이 있는 사람 둘, 뚱뚱한 사람 셋, 모두 낯선 사람들로, 증언을 믿는다면 다 어딘지 수상쩍은 데가 있는 이들뿐입니다.권총을 든 복면한 갱 한 무리가 범행을 저지르는 걸 보았다는 사람이 없는 게 이상할 정도입니다. ”\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.152263  0.552478  0.463807  0.347859  0.007020  0.446757   \n",
            "1  BERT+LexRank    0.192044  0.099539  0.144613  0.161658  0.000687  0.140712   \n",
            "2          BESM    0.378601  0.542296  0.349804  0.441173  0.006181  0.415713   \n",
            "3   BESM+kobert    0.363512  0.587391  0.510371  0.429172  0.004173  0.501415   \n",
            "\n",
            "    grammar  \n",
            "0  0.992406  \n",
            "1  0.999036  \n",
            "2  0.999032  \n",
            "3  0.996973  \n",
            "Current result ==================================================\n",
            "Sample count: 45\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.148460  0.506586  0.473798    0.480804  0.007282   \n",
            "1  BERT+LexRank   0.213797  0.238316  0.217262    0.197713  0.007638   \n",
            "2          BESM   0.210911  0.450769  0.399651    0.373908  0.009525   \n",
            "3   BESM+kobert   0.217009  0.468291  0.402368    0.379598  0.010310   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.482457    0.961905  \n",
            "1     0.215608    0.998973  \n",
            "2     0.402152    0.990126  \n",
            "3     0.408722    0.990226  \n",
            "==================================================\n",
            "50 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "” 포아로는 신중하게 말했다. “나도 그렇게 생각하오. ” “포아로 씨, 싫은 사건입니다. 참으로 싫은 사건입니다. 저는 아무래도 마음에 들지 않습니다. ” 우리는 런던으로 돌아가기 전에 두 사람을 더 만났다. 하나는 제임즈 패트리지라는 인물이었다. 패트리지 씨는 애셔 부인이 살아 있는 동안 맨 마지막으로 만난 사람이었다. 그는 5시 30분에 그녀 가게에서 물건을 샀던 것이다. 페트리지 씨는 몸집 작은 빈약한 남자로 은행원이었다. 코안경은 걸친 무뚝뚝하고 빼빼 마른 느낌의 사나이였으나 말씨는 또박또박했다. 그는 자기에게 잘 어울리는 깨끗한 작은 집에 살고 있었다. 내 친구가 내민 명함을 보며 그는 말했다. “네, 포아로 씨. 글렌 형사에게서 들으셨습니까? 무슨 도움이 될까요, 포아로 씨?” “패트리지 씨, 당신은 살아있는 애셔 부인을 맨 마지막으로 만난 분이시니까요. ” 패트리지 씨는 두 손을 마주대고 미심쩍은 수표라도 들여다보듯 포아로를 보았다. “그것이 토론의 여지가 있는 점입니다, 포아로 씨. 제 다음에도 더 많은 손님이 애셔 부인한테서 물건을 샀을지 모르니 말입니다. ” “그렇더라도 지금으로선 아직 신고해 온 사람이 없습니다.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.453137159347534 Generator / grammar loss:-0.1388217955827713   similarity loss:-0.09270790964365005\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "” 포아로는 생각하오. “포아로 싫은 사건입니다. 참으로 우리는 돌아가기 더 만났다. 하나는 제임즈 씨는 맨 빈약한 느낌의 사나이였으나 또박또박했다. “그것이 \n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "그는 5시 30분에 그녀 가게에서 물건을 샀던 것이다.“나도 그렇게 생각하오.그는 자기에게 잘 어울리는 깨끗한 작은 집에 살고 있었다.참으로 싫은 사건입니다.저는 아무래도 마음에 들지 않습니다.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "패트리지 씨는 애셔 부인이 살아 있는 동안 맨 마지막으로 만난 사람이었다. 코안경은 걸친 무뚝뚝하고 빼빼 마른 느낌의 사나이였으나 말씨는 또박또박했다.\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "패트리지 씨는 애셔 부인이 살아 있는 동안 맨 마지막으로 만난 사람이었다. 패트리지 씨는 두 손을 마주대고 미심쩍은 수표라도 들여다보듯 포아로를 보았다. “\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.151877  0.812305  0.501326  0.415500  0.029059  0.537774   \n",
            "1  BERT+LexRank    0.184300  0.123815  0.227500  0.103749  0.002941  0.169638   \n",
            "2          BESM    0.143345  0.186226  0.394107  0.374558  0.008785  0.346666   \n",
            "3   BESM+kobert    0.148464  0.436992  0.390959  0.556019  0.004837  0.449684   \n",
            "\n",
            "    grammar  \n",
            "0  0.980826  \n",
            "1  0.999031  \n",
            "2  0.997315  \n",
            "3  0.988742  \n",
            "Current result ==================================================\n",
            "Sample count: 46\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.148534  0.513232  0.474397    0.479384  0.007755   \n",
            "1  BERT+LexRank   0.213155  0.235827  0.217485    0.195671  0.007536   \n",
            "2          BESM   0.209442  0.445018  0.399530    0.373922  0.009509   \n",
            "3   BESM+kobert   0.215519  0.467611  0.402120    0.383434  0.010191   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.483660    0.962316  \n",
            "1     0.214609    0.998974  \n",
            "2     0.400945    0.990282  \n",
            "3     0.409612    0.990193  \n",
            "==================================================\n",
            "51 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "” 패트리지 씨는 헛기침을 했다. “그들은 시민의 의무에 대한 관념을 갖고 있지 않으니까요, 포아로 씨. ” 그는 부엉이처럼 코안경 너머로 우리를 보았다. 포아로가 중얼거렸다. “정말 그렇습니다. 당신은 자진해서 경찰에 신고하셨습니까?” “그럼요, 저는 이 무서운 사건 이야기를 듣자 곧 제 진술이 도움이 될지도 모른다고 생각했습니다. 그래서 바로 신고했지요. ” 포아로는 엄숙하게 말했다. “정말 훌륭한 마음씨입니다. 저에게도 그 이야기를 되풀이 들려주실 수 있으시겠지요?” “알겠습니다. 저는 집으로 돌아오는 길이었는데, 정각 5시 30분에……. ” “실례입니다만, 어째서 그토록 정확하게 시간을 알고 계십니까?” 패트리지 씨는 방해를 받아 기분이 좀 상한 모양이었다. “교회 시계가 울렸습니다. 저는 제 시계를 보고 1분 늦는 것을 알았지요. 그때가 바로 애셔 부인 가게로 들어가기 직전이었습니다. ” “거기서 자주 물건을 사셨습니까?” “네, 자주 샀습니다. 집으로 돌아오는 길목이니까요. 1주일에 한두 번씩 저는 <존 코튼>을 순한 것으로 2온스씩 사고 있습니다. ” “애셔 부인을 알고 계셨습니까? 그녀의 가정에 대해서라든지 과거에 대해?” “전혀 모릅니다.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.3087401390075684 Generator / grammar loss:-0.11273819953203201   similarity loss:-0.0816153734922409\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "” 의무에 포아로 우리를 보았다. 중얼거렸다. “정말 그렇습니다. “그럼요, 제 될지도 ” 저에게도 “알겠습니다. 정각 상한 시계가 울렸습니다.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "“그들은 시민의 의무에 대한 관념을 갖고 있지 않으니까요, 포아로 씨.1주일에 한두 번씩 저는 <존 코튼>을 순한 것으로 2온스씩 사고 있습니다.” 그는 부엉이처럼 코안경 너머로 우리를 보았다.” 포아로는 엄숙하게 말했다.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "그럼요, 저는 이 무서운 사건 이야기를 듣자 곧 제 진술이 도움이 될지도 모른다고 생각했습니다. 저는 제 시계를 보고 1분 늦는 것을 알았지요.그때가 바로 애셔 부인 가게로 들어가기 직전이었습니다. ” “\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "그럼요, 저는 이 무서운 사건 이야기를 듣자 곧 제 진술이 도움이 될지도 모른다고 생각했습니다. 저는 제 시계를 보고 1분 늦는 것을 알았지요.그때가 바로 애셔 부인 가게로 들어가기 직전이었습니다. ” “\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.133779  0.457725  0.628511  0.334909  0.014495  0.506273   \n",
            "1  BERT+LexRank    0.207358  0.276641  0.108462  0.269782  0.006040  0.190494   \n",
            "2          BESM    0.190635  0.331757  0.592236  0.359949  0.013622  0.470454   \n",
            "3   BESM+kobert    0.190635  0.331757  0.592236  0.359949  0.013622  0.470454   \n",
            "\n",
            "    grammar  \n",
            "0  0.984299  \n",
            "1  0.999037  \n",
            "2  0.995295  \n",
            "3  0.995295  \n",
            "Current result ==================================================\n",
            "Sample count: 47\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.148220  0.512051  0.477676    0.476310  0.007899   \n",
            "1  BERT+LexRank   0.213032  0.236696  0.215165    0.197247  0.007504   \n",
            "2          BESM   0.209042  0.442609  0.403630    0.373625  0.009596   \n",
            "3   BESM+kobert   0.214989  0.464720  0.406165    0.382934  0.010264   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.484141    0.962784  \n",
            "1     0.214096    0.998976  \n",
            "2     0.402424    0.990389  \n",
            "3     0.410907    0.990302  \n",
            "==================================================\n",
            "52 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "사는 물건이나 또는 날씨에 대해 몇 마디 나눈 것 말고는 이야기한 일이 없습니다. ” “그녀의 생명을 위협하는 말을 자주 했었던 주정꾼 남편에 대해 알고 계셨습니까?” “아니오, 그 사람에 대해서는 아무것도 모릅니다. ” “그러나 당신도 얼굴은 알고 계셨겠지요. 어제 여느 때와 다른 어떤 기색은 없었습니까?흥분해 있었다던가, 화를 내고 있었다던가?” 패트리지 씨는 생각해 보더니 대답했다. “내가 아느 한에서는 여느 때와 다른 점이 전혀 없었습니다. ” 포아로는 일어섰다. “패트리지 씨, 질문에 대답해 주셔서 고맙습니다. 그런데 댁은 혹시 ABC가 없는지요? 런던으로 가는 시간표를 좀 보고 싶어서요. ” 그가 말한 선반 위에는 ABC와 함께 브래드쇼, 주식연감, 케리의 인명록 그리고 현대 인명록 및 지방 신사록 등이 있었다. 포아로는 ABC를 들고 기차시간을 살펴보는 시늉을 한 다음 패트리지 씨에게 고맙다는 인사를 하고 나왔다. 또 다른 한 사람은 앨버트 리딜로, 이 또한 색다른 사람이었다. 앨버트 리딜 씨는 철도 인부였다. 그의 신경질적인 아내가 접시 씻는 소리며, 그 집 개가 으르렁대는 소리며, 리딜 씨 자신의 노골적인 적의 등과 더불어 이야기가 진행되었다. 그는 넓적한 얼굴에 의심 많은 눈을 한 크고 우둥퉁한 거인으로, 고기든 파이를 아주 진한 차와 함께 집어삼키고 있었다. 그는 찻잔 가장자리께로부터 화난 듯한 얼굴로 우리를 노려보고 있었다. 그는 으르렁거렸다. “필요한 말은 다 한 줄로 아는데, 대체 나와 무슨 관계가 있다는거요? 나는 경찰에 다 말해줬소.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.4778292179107666 Generator / grammar loss:-0.11056773364543915   similarity loss:-0.061843179166316986\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            " 생명을 자주 모릅니다. ” 다른 어떤 패트리지 대답했다. 한에서는 점이 전혀 없었습니다. ” 포아로는 일어섰다. 주셔서 고맙습니다. 위에는 ABC를 패트리지 한 사람은 리딜 철도 인부였다. 으르렁거렸다.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "그런데 댁은 혹시 ABC가 없는지요?그는 넓적한 얼굴에 의심 많은 눈을 한 크고 우둥퉁한 거인으로, 고기든 파이를 아주 진한 차와 함께 집어삼키고 있었다.런던으로 가는 시간표를 좀 보고 싶어서요.그는 찻잔 가장자리께로부터 화난 듯한 얼굴로 우리를 노려보고 있었다.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "사는 물건이나 또는 날씨에 대해 몇 마디 나눈 것 말고는 이야기한 일이 없습니다. ” “ 그녀의 생명을 위협하는 말을 자주 했었던 주정꾼 남편에 대해 알고 계셨습니까?” “ 그가 말한 선반 위에는 ABC와 함께 브래드쇼, 주식연감, 케리의 인명록 그리고 현대 인명록 및 지방 신사록 등이 있었다.\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "사는 물건이나 또는 날씨에 대해 몇 마디 나눈 것 말고는 이야기한 일이 없습니다. ” “ 포아로는 abc를 들고 기차시간을 살펴보는 시늉을 한 다음 패트리지 씨에게 고맙다는 인사를 하고 나왔다. 그는 넓적한 얼굴에 의심 많은 눈을 한 크고 우둥퉁한 거인으로, 고기든 파이를 아주 진한 차와 함께 집어삼키고 있었다.\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.147287  0.340862  0.536146  0.444992  0.006365  0.469743   \n",
            "1  BERT+LexRank    0.189922  0.108351  0.153961  0.383368  0.014482  0.213661   \n",
            "2          BESM    0.213178  0.546926  0.294954  0.306087  0.013513  0.348688   \n",
            "3   BESM+kobert    0.226098  0.454240  0.383536  0.449088  0.001036  0.417342   \n",
            "\n",
            "    grammar  \n",
            "0  0.965805  \n",
            "1  0.998963  \n",
            "2  0.999001  \n",
            "3  0.998979  \n",
            "Current result ==================================================\n",
            "Sample count: 48\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.148201  0.508485  0.478894    0.475658  0.007867   \n",
            "1  BERT+LexRank   0.212551  0.234022  0.213890    0.201125  0.007650   \n",
            "2          BESM   0.209128  0.444782  0.401366    0.372218  0.009678   \n",
            "3   BESM+kobert   0.215221  0.464502  0.405694    0.384312  0.010072   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.483841    0.962847  \n",
            "1     0.214087    0.998975  \n",
            "2     0.401305    0.990568  \n",
            "3     0.411041    0.990483  \n",
            "==================================================\n",
            "53 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "그런데 이번엔 또 외국놈 따위에게 다시 한 번 말하지 않으면 안 된다는 거요?” 포아로는 재빨리 내 쪽으로 흥미가 끌리는 듯한 눈짓을 보내고 나서 입을 열었다. “정말이지 안 되셨습니다. 그렇지만 할 수 없잖습니까? 어쨌든 살인 사건이니까요. 아주 신중하게 하지 않으면 안 되지요. ” 앨버트의 아내가 신경질적으로 말했다. “이분들이 듣고 싶어하시는 것을 모조리 이야기하는 게 좋아요. ” 거인이 소리쳤다. “잠자코 있어. ” 포아로가 솜씨 좋게 끼어들었다. “당신은 자진해서 경찰에 가신 게 아니잖습니까. ” “어째서 그런 짓을 해야 되는 거요? 그런 건 내 일이 아니잖소. ” 포아로는 아무렇지도 않게 말했다. “생각할 나름이지요. 살인이 일어났고 경찰은 가게에 왔던 사람을 알고 싶어했습니다. 저는 당신이 자진해서 신고하시는 편이 뭐랄까, 자연스럽다고 생각되는데요. ” “나한테는 일이 있소. 스스로 자진해서 가지 않았다니, 그렇게 말하면 곤란한데. ” “하지만 당신이 애셔 부인 가게로 들어가는 것을 본 사람이 있습니다.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.433051586151123 Generator / grammar loss:-0.1379595249891281   similarity loss:-0.09395790100097656\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "내 신경질적으로 “잠자코 있어. ” 포아로가 끼어들었다. 가신 건 아니잖소. “생각할 살인이 일어났고 알고 자연스럽다고 생각되는데요. 그렇게 곤란한데.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "살인이 일어났고 경찰은 가게에 왔던 사람을 알고 싶어했습니다.“잠자코 있어.” 거인이 소리쳤다.” 앨버트의 아내가 신경질적으로 말했다.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "그런데 이번엔 또 외국놈 따위에게 다시 한 번 말하지 않으면 안 된다는 거요?” 저는 당신이 자진해서 신고하시는 편이 뭐랄까, 자연스럽다고 생각되는데요. ” “\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "그런데 이번엔 또 외국놈 따위에게 다시 한 번 말하지 않으면 안 된다는 거요?” 포아로는 재빨리 내 쪽으로 흥미가 끌리는 듯한 눈짓을 보내고 나서 입을 열었다. “\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.161850  0.609372  0.506393  0.449439  0.004381  0.509902   \n",
            "1  BERT+LexRank    0.144509  0.162671  0.144190  0.222621  0.001121  0.171416   \n",
            "2          BESM    0.171484  0.442424  0.430036  0.496790  0.000841  0.452540   \n",
            "3   BESM+kobert    0.175337  0.518600  0.413361  0.372980  0.003768  0.422295   \n",
            "\n",
            "    grammar  \n",
            "0  0.996105  \n",
            "1  0.997696  \n",
            "2  0.989614  \n",
            "3  0.963113  \n",
            "Current result ==================================================\n",
            "Sample count: 49\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.148479  0.510544  0.479455    0.475123  0.007796   \n",
            "1  BERT+LexRank   0.211162  0.232566  0.212467    0.201564  0.007516   \n",
            "2          BESM   0.208360  0.444734  0.401951    0.374760  0.009498   \n",
            "3   BESM+kobert   0.214407  0.465606  0.405850    0.384081  0.009943   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.484373    0.963526  \n",
            "1     0.213216    0.998949  \n",
            "2     0.402350    0.990549  \n",
            "3     0.411271    0.989924  \n",
            "==================================================\n",
            "54 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "그래서 경찰은 당신을 만나러 오지 않으면 안 되었던 것입니다. 그런데 경찰은 당신 이야기에 만족했습니까?” 앨버트는 사납게 되물었다. “어째서 만족하지 않겠소?” 포아로는 다만 어깨를 으쓱했을 뿐이었다. “대체 뭘 냄새 맡으려는 거요. 당신은. 나한테서 뭘 끄집어낼 수 있을 리 없잖소. 그 노파를 죽인 게 누군지 모두들 알고 있어. 그 남편이잖소?” “그렇지만 그날 밤 그는 그곳에 있지 않았고, 당신은 있었지요. ‘ “나한테 죄를 뒤집어씌우려는 거요, 당신? 잘됐어, 이거 잘해 봐야겠군. 대체 내가 그런 짓을 해야 될 이유가 어디 있소? 그 늙은이의 피로 얼룩진 담배를 한 갑 훔치려고? 내가 남들이 말하는 피에 굶주린 살인광이란 말이오? 이 내가?” 그는 위협하듯 의자에서 일어섰다. 그의 아내가 양 같은 소리를 질렀다. “버트, 버트, 그런 말을 해선 안 돼요. 버트, 그런 말을 하면 모두들……. ” 포아로가 말했다. “좀 침착하십시오. 저는 그저 당신이 그 가게에 가셨었다는 이야기를 듣고 싶었던 것뿐입니다.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.1932191848754883 Generator / grammar loss:-0.12371939420700073   similarity loss:-0.10433702915906906\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "그래서 당신을 오지 사납게 “어째서 포아로는 다만 뭘 맡으려는 거요. 당신은. 수 게 뒤집어씌우려는 “버트, 버트, 그런 것뿐입니다.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "그 늙은이의 피로 얼룩진 담배를 한 갑 훔치려고?“좀 침착하십시오.그래서 경찰은 당신을 만나러 오지 않으면 안 되었던 것입니다.그 노파를 죽인 게 누군지 모두들 알고 있어.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "Unexpected error: <class 'ValueError'>\n",
            "==================================================\n",
            "55 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "그런데 그걸 거부하면 저에게는 어쩐지, 뭐라고 하나, 좀 이상한 기분이 드는군요. ” “내가 거부한다고 누가 말했소?” 리딜은 다시 의자에 앉았다. “이야기해 주겠소. ‘ “가게에 들어가셨던 게 6시였지요?” “그렇소. 사실은 1. 2분 지나 있었소. 골든 프레이크를 한 갑 사려고. 내가 문을 밀자……. ” “그러니까, 즉 가게 문이 닫혀 있었다는 거로군요?” “그렇소. 나는 벌써 가게를 닫았나 하고 생각했소. 그런데 그게 아니었소. 안으로 들어가니 아무도 없었소. 그래서 계산대를 쾅 두드리고는 잠시 기다려 보았소. 그래도 아무도 나오지 않길래 나는 밖으로 나왔소. ” “계산대 뒤에 쓰러져 있는 시체는 못 보셨군요?” “못 보았소. 다른 사람도 못 봤을 거요, 찾기라도 하지 않았다면. ” “철도 안내서는 있었습니까?” “있었소, 책장이 펼쳐져서 말이오. 그래서 나는 생각했소. 이 할머니, 너무 급하게 나가느라 문잠그는 걸 잊었나 보다고. ” “그래서 당신은 철도 안내서를 건드리거나 움직여 보셨군요?” “당치도 않소, 누가 그런 짓을 하겠소.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.379554510116577 Generator / grammar loss:-0.12764503061771393   similarity loss:-0.08922381699085236\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "좀 말했소?” “가게에 있었소. 한 문이 있었다는 나는 아무도 없었소. 밖으로 나왔소. “못 찾기라도 “철도 있었습니까?” 말이오. 생각했소.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "골든 프레이크를 한 갑 사려고.2분 지나 있었소.그런데 그게 아니었소.“이야기해 주겠소.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "그런데 그걸 거부하면 저에게는 어쩐지, 뭐라고 하나, 좀 이상한 기분이 드는군요. ” “\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "그런데 그걸 거부하면 저에게는 어쩐지, 뭐라고 하나, 좀 이상한 기분이 드는군요. ” “\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.148218  0.436992  0.451300  0.579282  0.004092  0.486833   \n",
            "1  BERT+LexRank    0.091932  0.215170  0.245545  0.225747  0.000158  0.233531   \n",
            "2          BESM    0.091932  0.543959  0.426088  0.367169  0.005402  0.431986   \n",
            "3   BESM+kobert    0.091932  0.543959  0.426088  0.367169  0.005402  0.431986   \n",
            "\n",
            "    grammar  \n",
            "0  0.962518  \n",
            "1  0.998253  \n",
            "2  0.989553  \n",
            "3  0.989553  \n",
            "Current result ==================================================\n",
            "Sample count: 50\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.148474  0.509073  0.478892    0.477206  0.007722   \n",
            "1  BERT+LexRank   0.208777  0.232218  0.213129    0.202047  0.007369   \n",
            "2          BESM   0.206032  0.446718  0.402434    0.374608  0.009416   \n",
            "3   BESM+kobert   0.211957  0.467173  0.406255    0.383743  0.009852   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.484422    0.963506  \n",
            "1     0.213622    0.998935  \n",
            "2     0.402943    0.990529  \n",
            "3     0.411685    0.989917  \n",
            "==================================================\n",
            "56 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "지금 말한 일밖에 하지 않았소. ” “당신이 거기 들어가기 전에 누가 나오는 건 못 보셨습니까?” “못 보았소. 내가 말하고 싶은 건, 어째서 나에게 누명을 씌우려고……. ” 포아로는 일어섰다. “아무도 그러지 않습니다. 아직 지금 단계에서는. 그럼, 안녕히 계십시오. ” 그는 멍하니 입을 벌린 채 있는 사나이를 뒤에 남겨 두고 나왔다. 나는 그 뒤를 따랐다. 길로 나오자 그는 시계를 보았다. “빨리 가면 7시 2분 기차를 탈 수 있겠군. 자, 서둘러 가세. ‘ < 두 번째 편지 > 나는 열심히 물었다. “그래서?” 우리는 우리 말고는 아무도 없는 1등 차칸에 앉아 있었다. 기차는 급행으로 막 앤도버를 떠난 참이었다. 포아로가 말했다. “범죄는 빨강 머리에 왼쪽 눈이 사팔뜨기인 중키의 사나이에 의해 저질러졌네. 그 사나이는 오른쪽 다리를 조금 절고, 왼쪽 어때 밑에 점이 있지. ” 나는 소리쳤다. “포아로!” 한순간 나는 정말로 믿어 버렸던 것이다.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.263277292251587 Generator / grammar loss:-0.12474145740270615   similarity loss:-0.09826003760099411\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "지금 말한 일밖에 하지 않았소. 거기 “못 보았소. 나에게 씌우려고……. 보았다. 탈 자, 서둘러 < 나는 열심히 말했다.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "그 사나이는 오른쪽 다리를 조금 절고, 왼쪽 어때 밑에 점이 있지.“범죄는 빨강 머리에 왼쪽 눈이 사팔뜨기인 중키의 사나이에 의해 저질러졌네.“그래서?” 우리는 우리 말고는 아무도 없는 1등 차칸에 앉아 있었다.기차는 급행으로 막 앤도버를 떠난 참이었다.길로 나오자 그는 시계를 보았다.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "우리는 우리 말고는 아무도 없는 1등 차칸에 앉아 있었다.기차는 급행으로 막 앤도버를 떠난 참이었다. 범죄는 빨강 머리에 왼쪽 눈이 사팔뜨기인 중키의 사나이에 의해 저질러졌네.\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "우리는 우리 말고는 아무도 없는 1등 차칸에 앉아 있었다.기차는 급행으로 막 앤도버를 떠난 참이었다. 범죄는 빨강 머리에 왼쪽 눈이 사팔뜨기인 중키의 사나이에 의해 저질러졌네.\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.141372  0.702510  0.539977  0.394059  0.015872  0.528708   \n",
            "1  BERT+LexRank    0.332640  0.161837  0.161680  0.298036  0.004127  0.202618   \n",
            "2          BESM    0.203742  0.272268  0.253242  0.417116  0.005355  0.306209   \n",
            "3   BESM+kobert    0.203742  0.272268  0.253242  0.417116  0.005355  0.306209   \n",
            "\n",
            "    grammar  \n",
            "0  0.984165  \n",
            "1  0.998924  \n",
            "2  0.997970  \n",
            "3  0.997970  \n",
            "Current result ==================================================\n",
            "Sample count: 51\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.148335  0.512866  0.480090    0.475576  0.007882   \n",
            "1  BERT+LexRank   0.211206  0.230838  0.212120    0.203929  0.007306   \n",
            "2          BESM   0.205987  0.443298  0.399509    0.375442  0.009336   \n",
            "3   BESM+kobert   0.211796  0.463351  0.403255    0.384397  0.009764   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.485291    0.963911  \n",
            "1     0.213406    0.998935  \n",
            "2     0.401046    0.990675  \n",
            "3     0.409617    0.990075  \n",
            "==================================================\n",
            "57 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "그러나 곧 이 친구의 장난기 어린 눈빛이 사실을 가르쳐 주었다. 나는 되풀이했다. 이번에는 나무라듯이 “포아로!” “자네는 어떻게 하자는 건가? 자네는 나에게 충성스러운 개같이 헌신적인 눈길을 보내면서 셜록 홈즈 같은 해결을 바라고 있네. 그런데 진상은 말일세. 살인범이 어떤 사니이며, 어디에 살고, 어떻게 하면 잡을 수 있는지 나는 도무지 알 수 없네. ” 나는 중얼거렸다. “녀석이 무슨 단서라도 남겨 줬더라면. ‘ 그렇지, 단서. 언제나 자네 마음을 끄는 건 그 단서라는 걸세. 유감스럽게도 그 사나이는 담배를 피워 담뱃재를 남겨 둬 주지도 않았고, 야릇한 모양의 징을 박은 신 자국도 남겨 주지 않았네. 그렇지, 그는 그리 친절하지 않았어. 그러나 적어도 철도 안내서가 있잖나. 그 ABC야말로 자네의 단서가 아니겠나!“ “그가 실수해서 그것을 남겨 뒀다고 생각하나?” “물론 그렇지는 않네. 일부러 두고 간 걸세. 지문 상태를 보면 알 수 있지. ” “지문이 없었잖나?” “바로 그걸세. 어제는 어떤 밤이었나? 더운 6월의 밤이었지. 이런 밤에 장갑을 끼고 다니는 사나이가 있을까?\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.4254846572875977 Generator / grammar loss:-0.14632628858089447   similarity loss:-0.10311789065599442\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "그러나 사실을 하자는 자네는 나에게 해결을 바라고 있네. 말일세. 박은 자국도 일부러 간 걸세. 상태를 보면 “지문이 없었잖나?” 그걸세. 어떤 사나이가\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "더운 6월의 밤이었지.살인범이 어떤 사니이며, 어디에 살고, 어떻게 하면 잡을 수 있는지 나는 도무지 알 수 없네.그러나 적어도 철도 안내서가 있잖나.유감스럽게도 그 사나이는 담배를 피워 담뱃재를 남겨 둬 주지도 않았고, 야릇한 모양의 징을 박은 신 자국도 남겨 주지 않았네.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "자네는 나에게 충성스러운 개같이 헌신적인 눈길을 보내면서 셜록 홈즈 같은 해결을 바라고 있네.그런데 진상은 말일세. 살인범이 어떤 사니이며, 어디에 살고, 어떻게 하면 잡을 수 있는지 나는 도무지 알 수 없네. ”\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "자네는 나에게 충성스러운 개같이 헌신적인 눈길을 보내면서 셜록 홈즈 같은 해결을 바라고 있네.그런데 진상은 말일세. 살인범이 어떤 사니이며, 어디에 살고, 어떻게 하면 잡을 수 있는지 나는 도무지 알 수 없네. ”\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.153153  0.499548  0.452040  0.502059  0.000529  0.476547   \n",
            "1  BERT+LexRank    0.277477  0.095438  0.251706  0.215154  0.004454  0.209487   \n",
            "2          BESM    0.214414  0.622718  0.472904  0.297320  0.017684  0.450191   \n",
            "3   BESM+kobert    0.214414  0.622718  0.472904  0.297320  0.017684  0.450191   \n",
            "\n",
            "    grammar  \n",
            "0  0.993840  \n",
            "1  0.999029  \n",
            "2  0.995588  \n",
            "3  0.995588  \n",
            "Current result ==================================================\n",
            "Sample count: 52\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.148428  0.512609  0.479550    0.476085  0.007740   \n",
            "1  BERT+LexRank   0.212480  0.228234  0.212882    0.204145  0.007251   \n",
            "2          BESM   0.206149  0.446748  0.400920    0.373939  0.009497   \n",
            "3   BESM+kobert   0.211847  0.466416  0.404594    0.382722  0.009916   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.485122    0.964486  \n",
            "1     0.213331    0.998937  \n",
            "2     0.401992    0.990769  \n",
            "3     0.410397    0.990181  \n",
            "==================================================\n",
            "58 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "그런 사나이는 곧 눈에 띄지. 그러니까 ABC에 지문이 없었다면 주의깊게 닦아 낸 게 틀림없네. 죄없는 남자라면 지문을 남겨 두겠지만, 죄가 있는 자는 남기지 않네. 그러므로 우리의 살인범은 그것을 일부러 남겨두고 간 걸세. 그러나 그 때문에 이것이 또 한 단서가 되지. ABC는 누군가가 사서 갖다 두었다……이런 가능성이 있는 셈일세. ” “그 방법으로 무엇을 알 수 있나?” “사실을 말하면, 헤이스팅즈. 나는 그리 희망을 가지고 있지 않네. 이 사나이, 이 미지의 X라는 사나이는 확실히 자기 능력에 자부심을 갖고 있네. 그는 뒤를 밟힐 그런 따위의 표시를 남겨 두지 않아. ” “그렇다면 ABC는 전혀 희망이 없는가?” “자네가 말하는 뜻에서는. ” “다른 뜻에서라면 있다는 건가?” 포아로는 곧바로 대답하지 않았다. 이윽고 그는 천천히 말했다. “그 답은 <있다>일세. 우리는 지금 미지의 인물과 마주하고 있네. 상대는 어둠 속에 있고, 언제까지나 어둠 속에 있으려 하지. 그러나 일의 성질로 보아 그는 자기에게 빛을 비추지 않고는 견디지 못할 걸세. 어떤 뜻에서는 이미 많은 것을 알고 있지. 나에게는 그의 모습이 흐릿하게 형태를 갖추어 오는 게 보인다네. 올바른 활자체를 달필로 쓸 수 있는 사나이?그것을 부당하게 느끼며 싸워 온 사나이가 내 눈에 보이네.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.411684274673462 Generator / grammar loss:-0.1311788707971573   similarity loss:-0.08941374719142914\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "그런 사나이는 눈에 띄지. 지문이 없었다면 닦아 낸 게 죄없는 자는 않네. 살인범은 그것을 걸세. 때문에 또 사서 X라는 능력에 “그렇다면 말하는 뜻에서는. ”\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "ABC는 누군가가 사서 갖다 두었다……이런 가능성이 있는 셈일세.올바른 활자체를 달필로 쓸 수 있는 사나이?그것을 부당하게 느끼며 싸워 온 사나이가 내 눈에 보이네.” “그렇다면 ABC는 전혀 희망이 없는가?” “자네가 말하는 뜻에서는.어떤 뜻에서는 이미 많은 것을 알고 있지.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "그러므로 우리의 살인범은 그것을 일부러 남겨두고 간 걸세.그러나 그 때문에 이것이 또 한 단서가 되지. 올바른 활자체를 달필로 쓸 수 있는 사나이?그것을 부당하게 느끼며 싸워 온 사나이가 내 눈에 보이네.\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "그러므로 우리의 살인범은 그것을 일부러 남겨두고 간 걸세.그러나 그 때문에 이것이 또 한 단서가 되지. 이 사나이, 이 미지의 x라는 사나이는 확실히 자기 능력에 자부심을 갖고 있네.\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.136086  0.782654  0.417893  0.428541  0.028729  0.494040   \n",
            "1  BERT+LexRank    0.237003  0.324332  0.281168  0.263336  0.000656  0.284451   \n",
            "2          BESM    0.174312  0.533744  0.287474  0.407462  0.010110  0.372724   \n",
            "3   BESM+kobert    0.155963  0.595724  0.342680  0.337920  0.014502  0.391861   \n",
            "\n",
            "    grammar  \n",
            "0  0.971818  \n",
            "1  0.999027  \n",
            "2  0.999019  \n",
            "3  0.999011  \n",
            "Current result ==================================================\n",
            "Sample count: 53\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.148195  0.517705  0.478387    0.475188  0.008136   \n",
            "1  BERT+LexRank   0.212943  0.230047  0.214170    0.205262  0.007126   \n",
            "2          BESM   0.205548  0.448389  0.398780    0.374572  0.009508   \n",
            "3   BESM+kobert   0.210792  0.468856  0.403426    0.381877  0.010003   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.485291    0.964625  \n",
            "1     0.214673    0.998939  \n",
            "2     0.401439    0.990925  \n",
            "3     0.410047    0.990347  \n",
            "==================================================\n",
            "59 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "자신을 주장하고 남의 관심을 끌고 싶은 마음속 충동이 점점 강해지고, 사건이며 사물이 그것을 부숴 버려 한층 더 비굴한 감정을 쌓아 올려 간 것이 내 눈에는 보이네. 그리하여 내부의 성냥이 이 화약을 실은 열차에 불을 붙이게 된 걸세. ‘ 나는 반대했다. 그런 것은 모두 억측에 지나지 않잖나. 실제로는 아무 쓸모도 없어. “ “자네는 성냥 끄트러기라든가 담뱃재라든가 징 박은 구두 쪽이 마음에 드는가 보군. 그러나 적어도 우리는 스스로 실제적인 질문을 해보지 않으면 안 되네. 어째서 ABC인가? 어째서 애셔 부인인가? 어째서 앤도버인가?” “그 여자의 과거 생활은 아주 단순해 보이네. ” 나는 생각에 잠겼다. “그 두 남자와의 면담은 실망이었어. 그들은 우리가 이미 알고 있는 것 이상의 일은 아무것도 말하지 못했잖아. ” “사실을 말하면 나는 그들에게 큰 기대를 걸고 있지 않았네. 그러나 살인 후보자로서의 두 사람의 가능성을 무시할 수도 없었지. ‘ “자네는 정말로……. ” “적어도 범인이 앤도버 또는 그 언저리에 살고 있을 가능성은 있는 걸세. 그것이 어째서 앤도버인가 하는 우리들의 질문에 대한 가능한 대답이 되네. 게다가 그날 그 시간 가게에 있었던 것으로 알려진 사나이가 둘이나 있는 걸세. 그 어느 쪽이 범인일지도 모르잖나.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.526705265045166 Generator / grammar loss:-0.13651283085346222   similarity loss:-0.08257132768630981\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "관심을 점점 버려 감정을 올려 간 것이 보이네. 이 실은 불을 붙이게 아무 담뱃재라든가 보군. 그러나 우리는 실제적인 “그 여자의 두 실망이었어. 우리가 정말로…….\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "그리하여 내부의 성냥이 이 화약을 실은 열차에 불을 붙이게 된 걸세.게다가 그날 그 시간 가게에 있었던 것으로 알려진 사나이가 둘이나 있는 걸세.어째서 ABC인가?“그 두 남자와의 면담은 실망이었어.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "자신을 주장하고 남의 관심을 끌고 싶은 마음속 충동이 점점 강해지고, 사건이며 사물이 그것을 부숴 버려 한층 더 비굴한 감정을 쌓아 올려 간 것이 내 눈에는 보이네. 그들은 우리가 이미 알고 있는 것 이상의 일은 아무것도 말하지 못했잖아. ” “\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "자신을 주장하고 남의 관심을 끌고 싶은 마음속 충동이 점점 강해지고, 사건이며 사물이 그것을 부숴 버려 한층 더 비굴한 감정을 쌓아 올려 간 것이 내 눈에는 보이네. 게다가 그날 그 시간 가게에 있었던 것으로 알려진 사나이가 둘이나 있는 걸세.\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.143750  0.540220  0.394053  0.309739  0.009066  0.397992   \n",
            "1  BERT+LexRank    0.173437  0.273625  0.140452  0.161990  0.003407  0.173548   \n",
            "2          BESM    0.214062  0.577444  0.431163  0.382758  0.006849  0.445897   \n",
            "3   BESM+kobert    0.212500  0.476028  0.184535  0.396194  0.015127  0.306331   \n",
            "\n",
            "    grammar  \n",
            "0  0.991290  \n",
            "1  0.999033  \n",
            "2  0.995231  \n",
            "3  0.999019  \n",
            "Current result ==================================================\n",
            "Sample count: 54\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.148112  0.518122  0.476825    0.472124  0.008153   \n",
            "1  BERT+LexRank   0.212212  0.230854  0.212805    0.204461  0.007058   \n",
            "2          BESM   0.205706  0.450779  0.399379    0.374723  0.009459   \n",
            "3   BESM+kobert   0.210824  0.468989  0.399373    0.382142  0.010098   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.483674    0.965118  \n",
            "1     0.213911    0.998940  \n",
            "2     0.402263    0.991005  \n",
            "3     0.408127    0.990508  \n",
            "==================================================\n",
            "60 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "그리고 그 가운데 어느 쪽인가가 범인이 아님을 알리는 표시는 아직 아무것도 나타나지 않았으니 말일세. ‘ 나는 인정했다. “그 꼴사나운 짐승 같은 리딜일지도 모르지. ” “그런데 나는 리딜은 풀어줘도 좋다고 생각하고 있네. 그는 신경질적이고 큰소리를 치며 분명 초조해 있었어. ” “그러나 그것은 확실히……. ” “그 ABC 편지를 쓰는 그런 자와는 정반대의 성격일세. 자신감과 자부심이 우리가 찾고 있는 특징이지. ” “누군가 자신의 중대성을 알리고 싶어하는 사람이란 말인가?” “아마도 그럴 걸세. 그러나 어떤 종류의 사람들은 신경질적이고 겸손한 속에 오히려 크나큰 허영심과 자기만족을 숨기고 있기도 하지. ” “그 몸집 작은 패트리지 씨는 어떤가?” “그 사나이 쪽이 그런 타입에 가까워. 그 이상은 말할 수 없지만. 그는 마치 그 편지를 쓴 자가 할 것 같은 행동을 취하고 있었지. 바로 경찰에 갔고, 자기를 돋보이려 했으며, 자기 위치를 즐기고 있었거든. ” “정말로 그렇게 생각하나?” “아니, 헤이스팅즈. 내 개인적으로는 범인이 앤도버 밖에서 왔다고 생각하지만 어떤 수사도 소홀히 할 수 없네. 그리고 나는 언제나 <그>라고 말하고 있지만, 여성이 관계해 있을 가능성도 빼놓을 수 없네. ” “설마!” “물론 공격 수법으로 보아선 남자일세. 그러나 익명 편지란 남자보다 오히려 여자에 의해 잘 씌어지는 법이지. 이 사실을 머리에 넣어두지 않으면 안 되네.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.437723159790039 Generator / grammar loss:-0.146023228764534   similarity loss:-0.10153119266033173\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "그리고 그 범인이 아직 나타나지 ‘ 나는 인정했다. “그 꼴사나운 짐승 같은 ” “그런데 생각하고 있네. 그는 ” “그러나 그것은 ABC 자와는 성격일세. 걸세. 하지. 수 없네.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "” “그 ABC 편지를 쓰는 그런 자와는 정반대의 성격일세.그러나 익명 편지란 남자보다 오히려 여자에 의해 잘 씌어지는 법이지.‘ 나는 인정했다.이 사실을 머리에 넣어두지 않으면 안 되네.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "그리고 그 가운데 어느 쪽인가가 범인이 아님을 알리는 표시는 아직 아무것도 나타나지 않았으니 말일세. ‘ 그리고 나는 언제나 <그>라고 말하고 있지만, 여성이 관계해 있을 가능성도 빼놓을 수 없네. ” “\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "그리고 그 가운데 어느 쪽인가가 범인이 아님을 알리는 표시는 아직 아무것도 나타나지 않았으니 말일세. ‘ 그런데 나는 리딜은 풀어줘도 좋다고 생각하고 있네.그는 신경질적이고 큰소리를 치며 분명 초조해 있었어. ” “ 바로 경찰에 갔고, 자기를 돋보이려 했으며, 자기 위치를 즐기고 있었거든.” “\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.141044  0.642430  0.491857  0.436208  0.007588  0.505277   \n",
            "1  BERT+LexRank    0.148096  0.170011  0.243813  0.344757  0.005130  0.259336   \n",
            "2          BESM    0.160790  0.592258  0.364934  0.494975  0.008672  0.449411   \n",
            "3   BESM+kobert    0.232722  0.588496  0.471930  0.369877  0.007977  0.464627   \n",
            "\n",
            "    grammar  \n",
            "0  0.988344  \n",
            "1  0.999039  \n",
            "2  0.995240  \n",
            "3  0.999011  \n",
            "Current result ==================================================\n",
            "Sample count: 55\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.147984  0.520382  0.477098    0.471471  0.008143   \n",
            "1  BERT+LexRank   0.211046  0.229748  0.213369    0.207012  0.007022   \n",
            "2          BESM   0.204889  0.453352  0.398753    0.376910  0.009445   \n",
            "3   BESM+kobert   0.211222  0.471161  0.400692    0.381919  0.010059   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.484067    0.965541  \n",
            "1     0.214737    0.998942  \n",
            "2     0.403120    0.991082  \n",
            "3     0.409154    0.990662  \n",
            "==================================================\n",
            "61 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "” 나는 잠시 입을 다물고 있다가 말했다. “이번엔 뭘 하면 되나?” “오, 정력가 헤이스팅즈여. ” 포아로는 나에게 미소를 보냈다. “아니, 정말 뭘 하지?” “아무것도. ” “아무것도?” 내 목소리에는 뚜렷한 실망이 나타나 있었다. “내가 마술사인가, 마법사인가? 내게 뭘 시키고 싶은가?” 마음속으로 사태를 잘 생각해 보고 나서 나는 대답하기 힘들다는 것을 알았다. 그러나 나는 뭔가 하지 않으면 안 된다. 발밑에 풀이 나게 해선 안 된다는 확신을 가지고 있었다. 나는 말했다. “ABC도 있고, 편지지도 있고, 봉투도 있고……. ” “그 점에서는 물론 여러 가지 수배가 되어 있네. 경찰은 그런 종류의 수사를 하는 데 자유스러운 여러 가지 수단을 갖고 있지. 그런 점에서 무엇이 발견된다면 그들이 찾아내 줄 테니 걱정할 것 없네. ” 그래서 나 또한 만족하는 수밖에 없었다. 그뒤 며칠동안 이상하게도 포아로는 사건에 대한 토론을 피하는 듯했다. 내가 그 문제를 꺼내려 하면 못 참겠는지 손을 흔들며 말머리를 돌려 버렸다. 나는 그 까닭을 깊이 생각해 보기를 은근히 두려워하고 있었다. 애셔 부인 살해에 대해서는 포아로도 자신의 패배를 인정하고 있었다. ABC가 그에게 도전했고, 그리고 이겼다.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.291031837463379 Generator / grammar loss:-0.11968646198511124   similarity loss:-0.09037520736455917\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            " ” 말했다. “이번엔 하면 되나?” “오, 헤이스팅즈여. 포아로는 정말 뭘 “아무것도. 뚜렷한 마법사인가? 내게 시키고 알았다. 뭔가 말했다. 수배가 버렸다.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "“내가 마술사인가, 마법사인가?경찰은 그런 종류의 수사를 하는 데 자유스러운 여러 가지 수단을 갖고 있지.“ABC도 있고, 편지지도 있고, 봉투도 있고…….ABC가 그에게 도전했고, 그리고 이겼다.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "마음속으로 사태를 잘 생각해 보고 나서 나는 대답하기 힘들다는 것을 알았다. 경찰은 그런 종류의 수사를 하는 데 자유스러운 여러 가지 수단을 갖고 있지.\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "마음속으로 사태를 잘 생각해 보고 나서 나는 대답하기 힘들다는 것을 알았다. 내가 그 문제를 꺼내려 하면 못 참겠는지 손을 흔들며 말머리를 돌려 버렸다.\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.144481  0.702590  0.506746  0.437148  0.012629  0.525035   \n",
            "1  BERT+LexRank    0.178571  0.083921  0.238218  0.130865  0.004171  0.175153   \n",
            "2          BESM    0.137987  0.341557  0.471788  0.424944  0.002901  0.431689   \n",
            "3   BESM+kobert    0.137987  0.318436  0.322052  0.506024  0.007672  0.376520   \n",
            "\n",
            "    grammar  \n",
            "0  0.972454  \n",
            "1  0.999016  \n",
            "2  0.999015  \n",
            "3  0.999007  \n",
            "Current result ==================================================\n",
            "Sample count: 56\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.147921  0.523635  0.477628    0.470858  0.008223   \n",
            "1  BERT+LexRank   0.210466  0.227144  0.213812    0.205652  0.006972   \n",
            "2          BESM   0.203694  0.451355  0.400057    0.377768  0.009328   \n",
            "3   BESM+kobert   0.209914  0.468434  0.399288    0.384135  0.010017   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.484798    0.965664  \n",
            "1     0.214030    0.998944  \n",
            "2     0.403630    0.991223  \n",
            "3     0.408571    0.990811  \n",
            "==================================================\n",
            "62 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "잇따른 성공에 익숙해 있던 내 친구는 자기 실패에 민감했다. 그런 만큼 그는 이 문제에 대한 토론을 참을 수 없었던 것이다. 그것은 분명 위대한 남자의 왜소함을 나타내는 일임에 틀림없지만, 아무리 냉정한 사람일지라도 성공했을 때 흥분되는 것은 흔히 있는 일이다. 포아로의 경우는 그 흥분이 몇 해나 계속 되고 있었다. 포아로의 경우는 그 흥분이 몇 해나 계속 되고 있었다. 그 결과 지금에야 겨우 눈을 뜨게 되었다 해도 그리 이상할 것은 없으리라. 나는 친구의 약점을 존중하여 사건에 대해 그 이상 이야기하는 것을 삼갔다. 심문 보고는 신문에서 읽었다. 그것은 무척 간단한 것으로, ABC 편지에 대해서도 아무 언급이 없었다. 배심원 평결은 한 사람 또는 몇 사람의 알 수 없는 인물에 의한 살인으로 되어 있었다. 이 사건은 신문의 여러 기사들 속에서 그리 주의를 끌지 못했다. 이야깃거리가 될 듯한 데도, 구경거리가 될 듯한 데도 없었다. 뒷골목의 노파 살해 따윈 더 스릴 있는 화제 때문에 곧 흐지부지되어 버리는 것이다. 사실을 말하면, 사건은 내 머리 속에서도 사라져 가고 있었다. 그것은 포아로가 실패했다고 생각하는 게 싫었기 때문이라고도 할 수 있다. 그런데 7월 25일이 되어 사건은 갑자기 되살아났다. 나는 주말에 요크셔에 가 있었기 때문에 이틀쯤 포아로를 만나지 못했다. 월요일 오후에 돌아왔는데, 그 편지는 6시 우편으로 배달되었다. 나는 문제의 봉투를 뜯어 펼쳐 보았을 때 포아로가 갑자기 날카롭게 숨을 들이 쉬었던 것을 기억하고 있다. “왔네.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.3846516609191895 Generator / grammar loss:-0.14043587446212769   similarity loss:-0.10148563981056213\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            " 내 민감했다. 그는 이 참을 수 없었던 것이다. 그것은 사람일지라도 일이다. 포아로의 이상할 심문 한 주의를 못했다. 이야깃거리가 내 속에서도 사라져 되어 갑자기 나는 포아로를 만나지 못했다. 오후에 “왔네.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "월요일 오후에 돌아왔는데, 그 편지는 6시 우편으로 배달되었다.“왔네.그런데 7월 25일이 되어 사건은 갑자기 되살아났다.그것은 무척 간단한 것으로, ABC 편지에 대해서도 아무 언급이 없었다.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "그것은 분명 위대한 남자의 왜소함을 나타내는 일임에 틀림없지만, 아무리 냉정한 사람일지라도 성공했을 때 흥분되는 것은 흔히 있는 일이다. 배심원 평결은 한 사람 또는 몇 사람의 알 수 없는 인물에 의한 살인으로 되어 있었다. 나는 문제의 봉투를 뜯어 펼쳐 보았을 때 포아로가 갑자기 날카롭게 숨을 들이 쉬었던 것을 기억하고 있다. “\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "그것은 분명 위대한 남자의 왜소함을 나타내는 일임에 틀림없지만, 아무리 냉정한 사람일지라도 성공했을 때 흥분되는 것은 흔히 있는 일이다. 나는 문제의 봉투를 뜯어 펼쳐 보았을 때 포아로가 갑자기 날카롭게 숨을 들이 쉬었던 것을 기억하고 있다. “\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.153342  0.512182  0.489242  0.458696  0.000480  0.484666   \n",
            "1  BERT+LexRank    0.141547  0.069567  0.249306  0.386371  0.016829  0.254478   \n",
            "2          BESM    0.243775  0.622193  0.414700  0.543467  0.007315  0.494829   \n",
            "3   BESM+kobert    0.179554  0.622193  0.294700  0.516802  0.018632  0.426829   \n",
            "\n",
            "    grammar  \n",
            "0  0.983406  \n",
            "1  0.999014  \n",
            "2  0.993886  \n",
            "3  0.988902  \n",
            "Current result ==================================================\n",
            "Sample count: 57\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.148016  0.523434  0.477832    0.470645  0.008087   \n",
            "1  BERT+LexRank   0.209257  0.224379  0.214435    0.208822  0.007144   \n",
            "2          BESM   0.204398  0.454353  0.400314    0.380675  0.009293   \n",
            "3   BESM+kobert   0.209382  0.471132  0.397453    0.386463  0.010168   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.484796    0.965975  \n",
            "1     0.214740    0.998945  \n",
            "2     0.405230    0.991270  \n",
            "3     0.408892    0.990778  \n",
            "==================================================\n",
            "63 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "” 그가 말했다. 나는 그를 쳐다보았으나 잘 알 수가 없었다. “뭐가 왔다는 건가?” “ABC 사건의 제2장일세. ” 나는 잠시 멍해진 채 그를 보고 있엇다. 사건은 완전히 내 기억에서 떨어져 나가 있었던 것이다. “읽어보게. ” 포아로는 나에게 편지를 건네주었다. 전과 마찬가지로 좋은 편지지에 활자체로 씌어 있었다. 친애하는 포아로여, 대체 어떻게 된 건가? 첫 번째 게임은 내 승리다. 앤도버 사건은 실로 잘 되었잖은가? 그러나 재미는 이제 시작이다. 이번에는 벡스힐 바닷가로 주의를 돌리도록. 날짜는 오는 25일. 이 얼마나 유쾌한 일인가! 이만. ABC 나는 소리쳤다. “이런, 포아로! 이 미치광이는 또 다른 범죄를 저지르려는 게 아닌가?” “물론이지, 헤이스팅즈. 자네는 어떻게 생각하고 있었나?\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.3237075805664062 Generator / grammar loss:-0.10751805454492569   similarity loss:-0.0748601034283638\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            " 건가?” 사건의 제2장일세. 그를 내 떨어져 나가 “읽어보게. 포아로여, 실로 바닷가로\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "날짜는 오는 25일.이 얼마나 유쾌한 일인가!전과 마찬가지로 좋은 편지지에 활자체로 씌어 있었다.이만.나는 그를 쳐다보았으나 잘 알 수가 없었다.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "나는 잠시 멍해진 채 그를 보고 있엇다.사건은 완전히 내 기억에서 떨어져 나가 있었던 것이다. “\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "나는 잠시 멍해진 채 그를 보고 있엇다.사건은 완전히 내 기억에서 떨어져 나가 있었던 것이다. “\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.123737  0.402240  0.394627  0.412583  0.000054  0.401536   \n",
            "1  BERT+LexRank    0.204545  0.232221  0.318607  0.073131  0.010337  0.227687   \n",
            "2          BESM    0.136364  0.662386  0.399030  0.496337  0.011822  0.480893   \n",
            "3   BESM+kobert    0.136364  0.662386  0.399030  0.496337  0.011822  0.480893   \n",
            "\n",
            "    grammar  \n",
            "0  0.969740  \n",
            "1  0.998862  \n",
            "2  0.961957  \n",
            "3  0.961957  \n",
            "Current result ==================================================\n",
            "Sample count: 58\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.147598  0.521345  0.476397    0.469644  0.007949   \n",
            "1  BERT+LexRank   0.209176  0.224514  0.216231    0.206483  0.007200   \n",
            "2          BESM   0.203225  0.457939  0.400292    0.382669  0.009336   \n",
            "3   BESM+kobert   0.208123  0.474429  0.397480    0.388357  0.010196   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.483361    0.966040  \n",
            "1     0.214963    0.998943  \n",
            "2     0.406535    0.990765  \n",
            "3     0.410133    0.990281  \n",
            "==================================================\n",
            "64 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "앤도버 사건 하나로 끝날 줄 여겼나? 자네는 내가 말한 걸 기억하고 있겠지. <무서운 시작이다>라고. ” “무서운 일이군. ” “그렇네, 무서운 일일세. ” “우리는 살인광을 상대하고 있어. ” “바로 그렇네. ” 그의 냉정함은 어떤 극적인 태도보다도 인상적이었다. 나는 편지를 돌려주며 몸을 떨었다. 다음날 아침, 담당자들의 회의가 열렸다. 서섹스 주 경찰서장, 범죄 수사과장, 앤도버의 글렌 형사, 서섹스 경찰의 카터 경감, 재프 경감과 크롬이라는 이름의 젊은 형사, 그리고 저명한 정신과의 솜프슨 박사가 한 자리에 모였다. 편지의 소인은 햄스티드로 되어 있었지만, 포아로의 의견으로 이것은 그리 중요시되지 않았다. 사건은 충분히 검토되었다. 솜프슨 박사는 인상 좋은 중년 신사로, 그 풍부한 학식에도 불구하고 직업상의 전문용어를 피해 아주 평범한 말을 쓰도록 마음 쓰고 있었다. 범죄 수사과장이 말했다. “이 두 편지가 같은 필적임은 틀림없습니다. 둘 다 한 인물에 의해 씌어진 겁니다. 그리고 그 인물이 앤도버 살인 사건에 관련해 있는 것도 확실합니다. ” “과연 그렇습니다. 우리는 지금 또다른 명백한 두 번째의 계획적 살인 예고를 받고 있습니다.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.339205026626587 Generator / grammar loss:-0.1329026073217392   similarity loss:-0.09865112602710724\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            " 앤도버 하나로 끝날 여겼나? ” “그렇네, 일일세. 상대하고 ” ” 냉정함은 어떤 극적인 태도보다도 인상적이었다. 다음날 열렸다. 검토되었다. “이 그렇습니다.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "솜프슨 박사는 인상 좋은 중년 신사로, 그 풍부한 학식에도 불구하고 직업상의 전문용어를 피해 아주 평범한 말을 쓰도록 마음 쓰고 있었다.범죄 수사과장이 말했다.편지의 소인은 햄스티드로 되어 있었지만, 포아로의 의견으로 이것은 그리 중요시되지 않았다.” 그의 냉정함은 어떤 극적인 태도보다도 인상적이었다.다음날 아침, 담당자들의 회의가 열렸다.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "서섹스 주 경찰서장, 범죄 수사과장, 앤도버의 글렌 형사, 서섹스 경찰의 카터 경감, 재프 경감과 크롬이라는 이름의 젊은 형사, 그리고 저명한 정신과의 솜프슨 박사가 한 자리에 모였다. 그리고 그 인물이 앤도버 살인 사건에 관련해 있는 것도 확실합니다. ” “\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "서섹스 주 경찰서장, 범죄 수사과장, 앤도버의 글렌 형사, 서섹스 경찰의 카터 경감, 재프 경감과 크롬이라는 이름의 젊은 형사, 그리고 저명한 정신과의 솜프슨 박사가 한 자리에 모였다. 편지의 소인은 햄스티드로 되어 있었지만, 포아로의 의견으로 이것은 그리 중요시되지 않았다. 솜프슨 박사는 인상 좋은 중년 신사로, 그 풍부한 학식에도 불구하고 직업상의 전문용어를 피해 아주 평범한 말을 쓰도록 마음 쓰고 있었다.범죄 수사과장이 말했다. “\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.152284  0.598070  0.543099  0.465810  0.002943  0.530906   \n",
            "1  BERT+LexRank    0.323181  0.117389  0.329498  0.123643  0.009712  0.225320   \n",
            "2          BESM    0.245347  0.434844  0.423609  0.651441  0.010994  0.494206   \n",
            "3   BESM+kobert    0.416244  0.474295  0.609693  0.446849  0.005067  0.533760   \n",
            "\n",
            "    grammar  \n",
            "0  0.989618  \n",
            "1  0.999012  \n",
            "2  0.995308  \n",
            "3  0.993889  \n",
            "Current result ==================================================\n",
            "Sample count: 59\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.147677  0.522645  0.477528    0.469579  0.007864   \n",
            "1  BERT+LexRank   0.211108  0.222699  0.218151    0.205079  0.007242   \n",
            "2          BESM   0.203939  0.457548  0.400687    0.387224  0.009364   \n",
            "3   BESM+kobert   0.211650  0.474427  0.401077    0.389349  0.010109   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.484166    0.966440  \n",
            "1     0.215139    0.998945  \n",
            "2     0.408021    0.990842  \n",
            "3     0.412228    0.990342  \n",
            "==================================================\n",
            "65 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "그 살인은 오는 25일 벡스힐로 예정되어 있습니다. 어떤 수단을 취하면 좋겠습니까?” 서섹스 주 경찰서장이 자기 경감 쪽을 보았다. “카터, 어떻게 해야 할까?” 카터 경감은 무겁게 고개를 저었다. “어렵군요, 예정된 피해자에 대한 최소한의 단서도 없습니다. 정직하고 공정하게 말해서 과연 우리가 어떤 수단을 취할 수 있겠습니까?” 포아로가 나섰다. “힌트가 있습니다. ” 모두의 얼굴이 그에게로 돌려졌다. “예정된 피해자의 이름은 B로 시작된다고 여겨집니다. ” 수사과장이 의심스러운 듯 말했다. “그것은 다만 생각일 따름이지요. ” 솜프슨 박사가 생각에 잠겨 말했다. “알파벡 콤플렉스군요. ” “나는 단지 가능성으로 말하고 있는 겁니다. 그 이상은 아니지요. 이 생각은 지난달에 살해된 그 불운한 여자의 가게 문에 애셔라는 글자가 씌어져 있는 것을 보았을 때 떠오른 겁니다. 벡스힐이라고 장소를 정한 편지를 보고 피해자도 알파벳순으로 정해지리라는 게 하나의 가능성으로 떠올랐지요. ” “그것은 확실히 가능성이 있습니다. 그러나 동시에 애셔라는 이름은 우연의 일치였는지도 모릅니다. 이번 피해자의 이름이 무엇이든 또 가게를 가진 노파일지도 모르지요. 알겠습니까?\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.3641538619995117 Generator / grammar loss:-0.12937508523464203   similarity loss:-0.09254907816648483\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "예정되어 좋겠습니까?” 쪽을 보았다. 돌려졌다. 피해자의 “그것은 ” 박사가 “나는 겁니다. 그 이상은 이 생각은 여자의 문에 떠오른 겁니다. 이름이 알겠습니까?\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "” 모두의 얼굴이 그에게로 돌려졌다.그 이상은 아니지요.이 생각은 지난달에 살해된 그 불운한 여자의 가게 문에 애셔라는 글자가 씌어져 있는 것을 보았을 때 떠오른 겁니다.그 살인은 오는 25일 벡스힐로 예정되어 있습니다.“알파벡 콤플렉스군요.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "어렵군요, 예정된 피해자에 대한 최소한의 단서도 없습니다.정직하고 공정하게 말해서 과연 우리가 어떤 수단을 취할 수 있겠습니까?” 이 생각은 지난달에 살해된 그 불운한 여자의 가게 문에 애셔라는 글자가 씌어져 있는 것을 보았을 때 떠오른 겁니다.\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "어렵군요, 예정된 피해자에 대한 최소한의 단서도 없습니다.정직하고 공정하게 말해서 과연 우리가 어떤 수단을 취할 수 있겠습니까?” 이 생각은 지난달에 살해된 그 불운한 여자의 가게 문에 애셔라는 글자가 씌어져 있는 것을 보았을 때 떠오른 겁니다.\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.151007  0.307478  0.441422  0.455165  0.004438  0.418756   \n",
            "1  BERT+LexRank    0.226510  0.336779  0.064977  0.316430  0.015280  0.194773   \n",
            "2          BESM    0.229866  0.359552  0.087027  0.322685  0.014574  0.212229   \n",
            "3   BESM+kobert    0.229866  0.359552  0.087027  0.322685  0.014574  0.212229   \n",
            "\n",
            "    grammar  \n",
            "0  0.927695  \n",
            "1  0.999034  \n",
            "2  0.999018  \n",
            "3  0.999018  \n",
            "Current result ==================================================\n",
            "Sample count: 60\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.147733  0.519059  0.476926    0.469338  0.007807   \n",
            "1  BERT+LexRank   0.211365  0.224600  0.215598    0.206935  0.007376   \n",
            "2          BESM   0.204371  0.455915  0.395460    0.386149  0.009451   \n",
            "3   BESM+kobert   0.211954  0.472512  0.395843    0.388238  0.010184   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.483076    0.965794  \n",
            "1     0.214799    0.998946  \n",
            "2     0.404757    0.990978  \n",
            "3     0.408895    0.990487  \n",
            "==================================================\n",
            "66 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "우리는 한 미치광이를 상대하고 있습니다. 상대는 동기에 대한 실마리를 아무것도 나타내 보이고 있지 않습니다. ” 카터 경감이 의심스러운 듯 물었다. “미치광이에게도 동기가 있을까요?” “물론 있습니다. 편집광의 특징 가운데 하나는 아주 논리적이라는 겁니다. 그는 자신이 목사, 의사 도는 담배 가게 노파라도 좋은데, 이들을 죽이게끔 신에 의해 정해져 있다고 믿지요. 그 배후에는 하나같이 어떤 완전하고도 타당한 이유가 있는 법입니다. 그러니 우리는 알파벳 같은 것에 정신을 팔면 안 됩니다. 앤도버 다음이 벡스힐인 것은 아마 우연의 일치에 지나지 않을 겁니다. ” “우리는 적어도 그 경계만은 할 수 있는 셈이네, 카터. 특히 조그만 가게의 B로 시작되는 이름에 주의하여, 혼자서 경영하는 조그만 담배 가게라든가 신문 가게를 지키게. 그 밖에는 달리 우리가 할 수 있는 일이 없어. 낯선 사람을 특히 주의해야 할 것은 물론이지만. ” 카터 경감은 신음 소리를 냈다. “학교의 방학이 시작되었는데 말입니까? 이번 주일에는 그곳으로 굉장한 인파가 몰릴 겁니다. ” 경찰서장이 날카롭게 말했다. “우리는 할 수 있는 건 다 해야만 되네. ” 이번에는 글렌 형사가 말했다. “애셔 사건에 관계있는 사람은 제가 지키지요.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.573218584060669 Generator / grammar loss:-0.12909477949142456   similarity loss:-0.0701211541891098\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "미치광이를 실마리를 나타내 물었다. “물론 특징 가운데 논리적이라는 그는 의사 좋은데, 정해져 그 타당한 이유가 있는 알파벳 것에 팔면 됩니다. 담배\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "그는 자신이 목사, 의사 도는 담배 가게 노파라도 좋은데, 이들을 죽이게끔 신에 의해 정해져 있다고 믿지요.이번 주일에는 그곳으로 굉장한 인파가 몰릴 겁니다.특히 조그만 가게의 B로 시작되는 이름에 주의하여, 혼자서 경영하는 조그만 담배 가게라든가 신문 가게를 지키게.상대는 동기에 대한 실마리를 아무것도 나타내 보이고 있지 않습니다.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "그는 자신이 목사, 의사 도는 담배 가게 노파라도 좋은데, 이들을 죽이게끔 신에 의해 정해져 있다고 믿지요. 앤도버 다음이 벡스힐인 것은 아마 우연의 일치에 지나지 않을 겁니다. ” “\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "그는 자신이 목사, 의사 도는 담배 가게 노파라도 좋은데, 이들을 죽이게끔 신에 의해 정해져 있다고 믿지요. 앤도버 다음이 벡스힐인 것은 아마 우연의 일치에 지나지 않을 겁니다. ” “\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.133441  0.494021  0.343181  0.200014  0.014410  0.330399   \n",
            "1  BERT+LexRank    0.300643  0.168889  0.224390  0.057232  0.004832  0.163142   \n",
            "2          BESM    0.165595  0.335005  0.437246  0.338979  0.002236  0.387318   \n",
            "3   BESM+kobert    0.165595  0.335005  0.437246  0.338979  0.002236  0.387318   \n",
            "\n",
            "    grammar  \n",
            "0  0.980495  \n",
            "1  0.999034  \n",
            "2  0.995275  \n",
            "3  0.995275  \n",
            "Current result ==================================================\n",
            "Sample count: 61\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.147498  0.518649  0.474733    0.464923  0.007915   \n",
            "1  BERT+LexRank   0.212828  0.223687  0.215742    0.204480  0.007334   \n",
            "2          BESM   0.203735  0.453932  0.396145    0.385375  0.009333   \n",
            "3   BESM+kobert   0.211194  0.470258  0.396521    0.387430  0.010053   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.480573    0.966035  \n",
            "1     0.213953    0.998947  \n",
            "2     0.404471    0.991049  \n",
            "3     0.408541    0.990565  \n",
            "==================================================\n",
            "67 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "패트리지와 리딜 두 증인과 물론 애셔도. 그들이 앤도버를 떠나면 미행시키겠습니다. ” 그리고 나서 두세 가지 제안과 얼마쯤 산만한 대화가 오간 뒤 회의는 끝났다. 나는 강을 따라 걸으며 말했다. “포아로, 이 범죄를 예방할 수 있을까?” 그는 야윈 얼굴을 내 쪽으로 돌렸다. “한 사람의 광기에 대해, 이렇게 사람 들끊는 정상적인 거리에서? 나는 걱정일세, 헤이스팅즈. 아주 걱정이네. 살인광 잭의 그 오래 계속된 성공을 기억하고 있겠지!” “무서운 일이군. ” “헤이스팅즈, 광기란 무서운 거라네. 나는 걱정일세. 아주 걱정이야. ” < 벡스힐 바닷가 살인 > 나는 지금도 7월 25일 아침, 잠에서 깨어난 무렵의 일을 기억하고 있다. 그것은 아마 7시 30분쯤이었다고 생각된다. 포아로가 침대 옆에 서서 가만히 내 어깨를 흔들고 있었다. 그의 얼굴을 한 번 보자 나는 곧 반쯤 잠든 상태에서 눈을 떴다. 나는 얼른 일어나면서 물었다. “무슨 일인가?” 그는 아주 간단하게 대답했지만, 그의 짧은 말 속에는 풍부한 감동이 담겨 있었다. “일어났네. ” 나는 소리쳤다.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.325648784637451 Generator / grammar loss:-0.13799966871738434   similarity loss:-0.10514233261346817\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "물론 떠나면 미행시키겠습니다. 두세 헤이스팅즈. 잭의 광기란 살인 나는 기억하고 서서 어깨를 곧 얼른 물었다. “무슨 아주 대답했지만, 속에는 있었다. “일어났네.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "나는 강을 따라 걸으며 말했다.“포아로, 이 범죄를 예방할 수 있을까?” 그는 야윈 얼굴을 내 쪽으로 돌렸다.패트리지와 리딜 두 증인과 물론 애셔도.” < 벡스힐 바닷가 살인 > 나는 지금도 7월 25일 아침, 잠에서 깨어난 무렵의 일을 기억하고 있다.” 그리고 나서 두세 가지 제안과 얼마쯤 산만한 대화가 오간 뒤 회의는 끝났다.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "그리고 나서 두세 가지 제안과 얼마쯤 산만한 대화가 오간 뒤 회의는 끝났다. < 벡스힐 바닷가 살인 > 나는 지금도 7월 25일 아침, 잠에서 깨어난 무렵의 일을 기억하고 있다.\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "그리고 나서 두세 가지 제안과 얼마쯤 산만한 대화가 오간 뒤 회의는 끝났다. 그것은 아마 7시 30분쯤이었다고 생각된다.포아로가 침대 옆에 서서 가만히 내 어깨를 흔들고 있었다. 그는 아주 간단하게 대답했지만, 그의 짧은 말 속에는 풍부한 감동이 담겨 있었다. “\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.167897  0.490329  0.421934  0.602155  0.005518  0.489679   \n",
            "1  BERT+LexRank    0.341328  0.434652  0.249999  0.287152  0.006359  0.298075   \n",
            "2          BESM    0.182657  0.424039  0.265169  0.291006  0.004845  0.304694   \n",
            "3   BESM+kobert    0.271218  0.467402  0.323989  0.698940  0.023863  0.465157   \n",
            "\n",
            "    grammar  \n",
            "0  0.973921  \n",
            "1  0.994359  \n",
            "2  0.999023  \n",
            "3  0.993918  \n",
            "Current result ==================================================\n",
            "Sample count: 62\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.147827  0.518192  0.473882    0.467137  0.007876   \n",
            "1  BERT+LexRank   0.214901  0.227089  0.216295    0.205814  0.007319   \n",
            "2          BESM   0.203395  0.453450  0.394032    0.383853  0.009260   \n",
            "3   BESM+kobert   0.212162  0.470212  0.395351    0.392454  0.010276   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.480720    0.966162  \n",
            "1     0.215309    0.998873  \n",
            "2     0.402862    0.991177  \n",
            "3     0.409454    0.990619  \n",
            "==================================================\n",
            "68 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "“뭐라고? 하지만 오늘은 25일이잖나. ” “어젯밤. 아니, 오늘 아침 일찍 일어났네. ” 내가 침대에서 튀어 일어나 재빨리 옷을 입자 그는 지금 막 전화로 들은 이야기를 간간히 들려주었다. “젊은 여자의 시체가 벡스힐 바닷가에서 발견됐네. 그녀는 일리저버스 버너드(Barnard)라는 이름의 카페 여급사로 밝혀졌네. 이 아가씨는 최근 갓 지은 조그만 방갈로에서 부모와 함께 살고 있었지. 검시 결과에 의하면 12시에서 새벽 1시 사이에 살해된 모양일세. ” 나는 면도를 하며 물었다. “그러나 이것이 그 범죄라는 건 확실한가?” “벡스힐 행 기차 시간표가 있는 데가 펼쳐진 ABC 철도 안내서가 시체 밑에서 나왔네. ” 나는 손이 떨렸다. “무서운 이야기로군. ” “조심하게, 헤이스팅즈. 내 방에서 또 하나의 사건이 일어나면 참을 수 없으니까. ” 나는 얼마쯤 맥이 풀려 턱의 피를 닦았다. 그리고 물었다. “우리들의 전투 계획은?” “이제 곧 경찰차가 우리를 데리러 오게 되어 있네. 자네 커피는 이리로 가져오도록 시켰네. 출발을 늦출 수 없으니까.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.539182662963867 Generator / grammar loss:-0.12767226994037628   similarity loss:-0.07238766551017761\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "하지만 오늘은 25일이잖나. “어젯밤. 아침 일어났네. ” 발견됐네. 여급사로 밝혀졌네. 아가씨는 모양일세. 면도를 물었다. 기차 이야기로군. 닦았다. 자네\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "이 아가씨는 최근 갓 지은 조그만 방갈로에서 부모와 함께 살고 있었지.“우리들의 전투 계획은?” “이제 곧 경찰차가 우리를 데리러 오게 되어 있네.하지만 오늘은 25일이잖나.“젊은 여자의 시체가 벡스힐 바닷가에서 발견됐네.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "내가 침대에서 튀어 일어나 재빨리 옷을 입자 그는 지금 막 전화로 들은 이야기를 간간히 들려주었다. “ 그녀는 일리저버스 버너드(Barnard)라는 이름의 카페 여급사로 밝혀졌네.\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "내가 침대에서 튀어 일어나 재빨리 옷을 입자 그는 지금 막 전화로 들은 이야기를 간간히 들려주었다. “ 그녀는 일리저버스 버너드(barnard)라는 이름의 카페 여급사로 밝혀졌네.\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.162921  0.892325  0.512515  0.389698  0.045775  0.551632   \n",
            "1  BERT+LexRank    0.232210  0.105685  0.189211  0.168461  0.001261  0.166281   \n",
            "2          BESM    0.187266  0.290399  0.369269  0.319357  0.001061  0.338522   \n",
            "3   BESM+kobert    0.187266  0.283376  0.367192  0.313272  0.001203  0.334253   \n",
            "\n",
            "    grammar  \n",
            "0  0.977920  \n",
            "1  0.999040  \n",
            "2  0.998889  \n",
            "3  0.998884  \n",
            "Current result ==================================================\n",
            "Sample count: 63\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.148067  0.524131  0.474495    0.465908  0.008478   \n",
            "1  BERT+LexRank   0.215176  0.225162  0.215865    0.205221  0.007222   \n",
            "2          BESM   0.203139  0.450862  0.393639    0.382830  0.009130   \n",
            "3   BESM+kobert   0.211767  0.467246  0.394904    0.391198  0.010132   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.481846    0.966349  \n",
            "1     0.214531    0.998876  \n",
            "2     0.401841    0.991300  \n",
            "3     0.408261    0.990751  \n",
            "==================================================\n",
            "69 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "” 20분 뒤 우리는 속력 빠른 경찰차를 타고 템즈 강을 건너 런던을 벗어났다. 경찰차에는 크롬 형사가 함께 타고 있었다. 지난번 회의에 참석했었던 이 사건 담당 형사였다. 크롬 형사는 재프 경감과는 다른 타입의 경관이었다. 훨씬 젊고 말수가 적은 남자였다. 교양과 지식을 지녔으나, 내 기호로 말하면 얼마쯤 자기만족에 빠져있는 듯했다. 그는 최근에 있었던 일련의 어린이 살해 사건으로, 지금 브로드무어에 들어가 있는 범인을 끈질기게 추적해 이름을 떨친 참이었다. 그는 분명 이번 사건을 맡는 데 알맞은 인물이었으나, 그것을 그 자신이 지나치게 의식하고 있는 듯 생각되었다. 포아로를 대하는 그의 태도에는 좀 잘난 체하는 데가 있고, 얼마쯤 자의식적인 공립학교 식 방법으로 젊은 사람이 연장자를 대하는 것같이 그에게 복종하고 있었다. 그가 말했다. “저는 솜프슨 박사와 많은 이야기를 했습니다. 그분은 연속 살인 사건에 아주 흥미를 갖고 계시지요. 그것은 특수하게 비틀린 심성의 산물입니다. 물론 비전문가로서는 그가 의학적 견지에 대해 보이는 세부적인 점은 이해할 수 없지요. ” 그는 헛기침을 했다. “사실 저의 지난번 사건으로 말하면, 그 기사를 읽으셨는지 모르겠습니다만, 머스윌 힐 여학교 학생 메이벌 호머 사건 말입니다. 그 캐퍼라는 사나이는 무서운 녀석이었습니다. 그 범죄를 그의 짓이라고 밝혀내기까지 참으로 힘이 들었습니다. 아무튼 세 사람째였으니 말입니다. 아주 멀쩡해 보였지요.\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.4818503856658936 Generator / grammar loss:-0.12554554641246796   similarity loss:-0.07639432698488235\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            " 뒤 우리는 빠른 경찰차를 타고 템즈 런던을 함께 타고 회의에 참석했었던 담당 크롬 형사는 재프 타입의 경관이었다. 훨씬 젊고 말수가 적은 남자였다. 학생\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "” 20분 뒤 우리는 속력 빠른 경찰차를 타고 템즈 강을 건너 런던을 벗어났다.“사실 저의 지난번 사건으로 말하면, 그 기사를 읽으셨는지 모르겠습니다만, 머스윌 힐 여학교 학생 메이벌 호머 사건 말입니다.경찰차에는 크롬 형사가 함께 타고 있었다.물론 비전문가로서는 그가 의학적 견지에 대해 보이는 세부적인 점은 이해할 수 없지요.” 그는 헛기침을 했다.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "” 20분 뒤 우리는 속력 빠른 경찰차를 타고 템즈 강을 건너 런던을 벗어났다. 그는 분명 이번 사건을 맡는 데 알맞은 인물이었으나, 그것을 그 자신이 지나치게 의식하고 있는 듯 생각되었다.\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "” 20분 뒤 우리는 속력 빠른 경찰차를 타고 템즈 강을 건너 런던을 벗어났다. 교양과 지식을 지녔으나, 내 기호로 말하면 얼마쯤 자기만족에 빠져있는 듯했다. 그는 최근에 있었던 일련의 어린이 살해 사건으로, 지금 브로드무어에 들어가 있는 범인을 끈질기게 추적해 이름을 떨친 참이었다.\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.118949  0.522936  0.369678  0.268704  0.010924  0.370037   \n",
            "1  BERT+LexRank    0.272476  0.444615  0.171666  0.162187  0.017151  0.223412   \n",
            "2          BESM    0.146611  0.425698  0.472762  0.302276  0.005168  0.412204   \n",
            "3   BESM+kobert    0.219917  0.485778  0.566188  0.343387  0.008487  0.483266   \n",
            "\n",
            "    grammar  \n",
            "0  0.968796  \n",
            "1  0.999047  \n",
            "2  0.999052  \n",
            "3  0.999034  \n",
            "Current result ==================================================\n",
            "Sample count: 64\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.147612  0.524112  0.472857    0.462826  0.008516   \n",
            "1  BERT+LexRank   0.216071  0.228591  0.215174    0.204549  0.007378   \n",
            "2          BESM   0.202256  0.450469  0.394875    0.381571  0.009068   \n",
            "3   BESM+kobert   0.211894  0.467536  0.397581    0.390451  0.010106   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.480099    0.966387  \n",
            "1     0.214670    0.998879  \n",
            "2     0.402003    0.991421  \n",
            "3     0.409433    0.990880  \n",
            "==================================================\n",
            "70 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "하지만 여러 가지 테스트라는 게 있잖습니까. 아시겠지요, 유도심문이라는 것을. 물론 아주 새로운 것이어서 전에는 별로 문제되지 않았던 겁니다. 한 번 잡아들이면 이미 문제없습니다!이쪽이 알고 있다는 걸 알게 되면 상대방은 그만 손을 들지요. 상대는 완전히 이쪽이 하는 대로 따라옵니다. ” 포아로가 말했다. “우리 때에도 그런 일은 흔히 있었지요. ” 크롬 형사는 그를 돌아보고 대화를 계속하는 것처럼 입속으로 말했다. “아, 그렇습니까?” 잠시 침묵이 이어졌다. 뉴크로스 역을 지났을 즈음 크롬이 말했다. “이 사건에 대해 무언가 들어주고 싶은 게 있으시면 말씀하십시오. ” “그럼, 죽은 아가씨에 대해 이야기해 주겠소?” “그녀는 23살로 카페 <진저 캣>의 여급사로 일하고 있었습니다. ” “아니, 그런 점이 아니라, 이를테면 그녀는 아름다웠다든가……. ” 크롬 형사는 당할 수 없다는 투로 말했다. “그런 일에 대해서는 전혀 들은 게 없습니다. ” 그의 몸짓은 이렇게 말하고 있었다. 이거 정말, 외국인이란 모두 이렇다니까! 포아로의 눈에 재미있어 하는 빛이 슬며시 떠올랐다. “그런 사실이 당신에게는 중요하게 보이지 않소?\n",
            "------------------------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.313047170639038 Generator / grammar loss:-0.13558095693588257   similarity loss:-0.1040167510509491\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "문제없습니다!이쪽이 있었지요. “아, 크롬이 사건에 싶은 게 아가씨에 ” 당할 수 전혀 들은 게 없습니다. ” 정말, 포아로의 눈에 재미있어 떠올랐다.\n",
            "--------------------------------------------------\n",
            "bert_lexrank summary:\n",
            "” “그럼, 죽은 아가씨에 대해 이야기해 주겠소?” “그녀는 23살로 카페 <진저 캣>의 여급사로 일하고 있었습니다.“그런 일에 대해서는 전혀 들은 게 없습니다.” “아니, 그런 점이 아니라, 이를테면 그녀는 아름다웠다든가…….” 크롬 형사는 당할 수 없다는 투로 말했다.포아로의 눈에 재미있어 하는 빛이 슬며시 떠올랐다.\n",
            "--------------------------------------------------\n",
            "besm summary:\n",
            "한 번 잡아들이면 이미 문제없습니다!이쪽이 알고 있다는 걸 알게 되면 상대방은 그만 손을 들지요.\n",
            "--------------------------------------------------\n",
            "besm_bert summary:\n",
            "한 번 잡아들이면 이미 문제없습니다!이쪽이 알고 있다는 걸 알게 되면 상대방은 그만 손을 들지요.\n",
            "--------------------------------------------------\n",
            "         method  comp ratio     intro      body    ending       var     total  \\\n",
            "0      SAM+WGAN    0.145329  0.506491  0.427722  0.577719  0.003753  0.488475   \n",
            "1  BERT+LexRank    0.311419  0.110574  0.218690  0.268787  0.004359  0.212096   \n",
            "2          BESM    0.093426  0.386339  0.292677  0.294018  0.001922  0.311812   \n",
            "3   BESM+kobert    0.093426  0.386339  0.292677  0.294018  0.001922  0.311812   \n",
            "\n",
            "    grammar  \n",
            "0  0.929732  \n",
            "1  0.999029  \n",
            "2  0.999026  \n",
            "3  0.999026  \n",
            "Current result ==================================================\n",
            "Sample count: 65\n",
            "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
            "0      SAM+WGAN   0.147577  0.523841  0.472163    0.464594  0.008443   \n",
            "1  BERT+LexRank   0.217538  0.226776  0.215228    0.205537  0.007331   \n",
            "2          BESM   0.200581  0.449482  0.393303    0.380224  0.008958   \n",
            "3   BESM+kobert   0.210072  0.466287  0.395967    0.388967  0.009980   \n",
            "\n",
            "   simlirality  grammarity  \n",
            "0     0.480228    0.965823  \n",
            "1     0.214630    0.998881  \n",
            "2     0.400615    0.991538  \n",
            "3     0.407931    0.991005  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>method</th>\n",
              "      <th>comp rate</th>\n",
              "      <th>intro</th>\n",
              "      <th>body</th>\n",
              "      <th>conclusion</th>\n",
              "      <th>isthmus</th>\n",
              "      <th>simlirality</th>\n",
              "      <th>grammarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SAM+WGAN</td>\n",
              "      <td>0.147577</td>\n",
              "      <td>0.523841</td>\n",
              "      <td>0.472163</td>\n",
              "      <td>0.464594</td>\n",
              "      <td>0.008443</td>\n",
              "      <td>0.480228</td>\n",
              "      <td>0.965823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BERT+LexRank</td>\n",
              "      <td>0.217538</td>\n",
              "      <td>0.226776</td>\n",
              "      <td>0.215228</td>\n",
              "      <td>0.205537</td>\n",
              "      <td>0.007331</td>\n",
              "      <td>0.214630</td>\n",
              "      <td>0.998881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>BESM</td>\n",
              "      <td>0.200581</td>\n",
              "      <td>0.449482</td>\n",
              "      <td>0.393303</td>\n",
              "      <td>0.380224</td>\n",
              "      <td>0.008958</td>\n",
              "      <td>0.400615</td>\n",
              "      <td>0.991538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>BESM+kobert</td>\n",
              "      <td>0.210072</td>\n",
              "      <td>0.466287</td>\n",
              "      <td>0.395967</td>\n",
              "      <td>0.388967</td>\n",
              "      <td>0.009980</td>\n",
              "      <td>0.407931</td>\n",
              "      <td>0.991005</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         method  comp rate     intro      body  conclusion   isthmus  \\\n",
              "0      SAM+WGAN   0.147577  0.523841  0.472163    0.464594  0.008443   \n",
              "1  BERT+LexRank   0.217538  0.226776  0.215228    0.205537  0.007331   \n",
              "2          BESM   0.200581  0.449482  0.393303    0.380224  0.008958   \n",
              "3   BESM+kobert   0.210072  0.466287  0.395967    0.388967  0.009980   \n",
              "\n",
              "   simlirality  grammarity  \n",
              "0     0.480228    0.965823  \n",
              "1     0.214630    0.998881  \n",
              "2     0.400615    0.991538  \n",
              "3     0.407931    0.991005  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Oj2diLc-mAa"
      },
      "source": [
        "## 한국어 Sample Test (with frame token)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUb6xK7Cigry",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8194ad00-d129-452f-e0ca-ef443ad71fb1"
      },
      "source": [
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "\n",
        "test_result = {}\n",
        "test_result['SAM+WGAN']=[]\n",
        "\n",
        "step = 0\n",
        "for intro,body,end in ko_docs:\n",
        "    step += 1\n",
        "    print(\"=\" * 50)\n",
        "    print(str(step),\"/\",len(ko_docs))\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    org_text_1 = intro\n",
        "    org_text_2 = body\n",
        "    org_text_3 = end\n",
        "\n",
        "    try:\n",
        "        df1,dct1 = sam_wgan('',[org_text_1,org_text_2,org_text_3],init_bias=1.0,display= False)\n",
        "        if dct1['grammar'][0] > 0.0:\n",
        "\n",
        "            test_result['SAM+WGAN'].append(get_features(dct1))\n",
        "            #result = pd.concat([df1, df2, df3, df4, df5, df6 ], ignore_index=True)\n",
        "            #result = pd.concat([df1, df2, df3, df5, df6 ], ignore_index=True)\n",
        "            \n",
        "            print(df1)\n",
        "            \n",
        "            print(\"Current result\",\"=\" * 50)\n",
        "            print(\"Sample count:\",len(test_result['SAM+WGAN']))\n",
        "            print(get_test_statistics(test_result))\n",
        "        \n",
        "    except KeyboardInterrupt as ki:\n",
        "        raise ki\n",
        "    except :\n",
        "        print(\"Unexpected error:\", sys.exc_info()[0])\n",
        "        #raise e\n",
        "        pass\n",
        "\n",
        "get_test_statistics(test_result)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "1 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "이 이야기에서는 내가 직접 입회한 사건이나 장면만을 이야기하는 전의 내 방법을 바꿔 보았다. 그래서 몇몇 장은 3인칭으로 씌어 있다. 이제부터의 각 장에서 이야기되는 사건들은 모두 내가 확증 할 수 있었던 것임을 밝혀둔다. 여러 인물들의 생각이나 감정을 서술하는 데 있어 얼마쯤 내가 시인의 특권을 행사했다 해도 그것은 아주 정확을 기해서 한 일이다. 또한 그것들은 모두 내 친구 에르큘 포아로의 검토를 받았음을 덧붙여 둔다. 끝으로, 나는 이 이상한 연쇄 범죄의 결과로서 일어나는 부차적인 인간관계에 대해 너무 많은 이야기를 했는지도 모른다. 하지만 인간적, 개인적 요소란 빠뜨려선 안 되는 것이다. 에르큘 포아로가 언젠가 과장된 몸짓으로 나에게 가르쳐 준 일이 있다. 로맨스란 범죄의 부산물일 경우가 있다고. ABC 수수께끼의 해결에 대해 말한다면, 에르큘 포아로는 이제까지 그가 다뤄 온 어느 사건과도 다른 방법으로 문제에 뛰어들어 그 진정한 천재성을 발휘했다고 말해도 좋으리라. < 편지 > 1935년 6월, 나는 남아메리카의 내 농장에서 떠나 여섯 달쯤 머무를 예정으로 귀국했다. 그때는 어려웠던 시대로, 다른 사람들과 마찬가지로 우리 역시 세계적인 불황에 어려움을 겪고 있었다. 영국에서 나 자신이 손대지 않으면 도저히 잘되어 나가지 않을 것 같은 볼일이 여러 가지 있었다. 농장 관리를 위해 아내가 뒤에 남았다. 영국에 와 닿아 내가 맨 먼저 한 일의 하나는 말할 나위도 없이 오랜 친구인 에르큘 포아로를 찾아간 것이었다. 그는 런던의 어떤 최신형 아파트에 살고 있었다. 내가 그것을 지적하며, 그가 이 특별한 건물을 고른 것은 완전히 그 기하학적이 겉모습과 넓이 때문일 거라고 말하자 그는 고개를 끄덕였다. “그러나 아주 기분 좋게 균형이 잡혀 있지. 그렇게 생각되지 않나?” 나는 좀 너무 모난 것같이 생각된다고 말했다. 그리고 오래된 농담이 생각나 이 아파트에서는 암탉에게 네모난 달걀을 낳게 할 수 있을 듯하다고 말했다.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/251       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/250       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/249       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/248       \n",
            "Negative tokens: ['전의' '몇몇' '할' '있었던' '행사했다' '나는' '했는지도' '언젠가' '뛰어들어' '사람들과' '볼일이' '일의'\n",
            " '그가' '고른' '거라고']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 249/250       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 248/249       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 247/248       \n",
            "Peak count: 13\n",
            "Frame tokens: 직접 이제부터의 사건들은 한 있다. 있다고. 그가 말해도 있었다. 있었다. 내가 것이었다. 말했다. \n",
            "\n",
            "Similarity : 0.36380188284323656\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.3022871017456055 Generator / grammar loss:-0.1700814813375473   similarity loss:-0.139619380235672\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "직접 이제부터의 각 사건들은 내가 특권을 기해서 한 친구 나는 일이 있다. 도저히 잘되어 나가지 볼일이 있었다. 내가 먼저 한 것이었다. 내가 고른 기하학적이 거라고 말하자 끄덕였다. “그러나 아주 좋게 잡혀 나는 좀 너무 것같이 생각나 있을 듯하다고 말했다.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.148871  0.491361  0.409355  0.538013  0.002828  0.464353   \n",
            "\n",
            "    grammar  \n",
            "0  0.961231  \n",
            "Current result ==================================================\n",
            "Sample count: 1\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.148871  0.491361  0.409355    0.538013  0.002828     0.464353   \n",
            "\n",
            "   grammarity  \n",
            "0    0.961231  \n",
            "==================================================\n",
            "2 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "포아로는 크게 웃었다. “아니, 자네는 아직도 그걸 기억하고 있나? 하지만 유감스럽게도 과학은 아직 암탉을 현대 취미에 알맞도록 하는 일에 성공하지 못하고 있네. 닭들이 지금도 여전히 크기와 빛깔이 서로 다른 달걀을 낳고 있지. ” 나는 애정어린 눈길로 오랜 친구를 관찰했다. 그는 굉장히 활기가 넘쳐 전에 만났을 때보다 조금도 더 나이먹은 것같이 보이지 않았다. “자네는 정말 건강해 보이는군, 포아로. 거의 나이를 안 먹었잖나. 전에 만났을 때보다 흰머리가 더 적어졌다고 해도 좋을 정도일세, 그런 일이 있을 수 있다면. ” 포아로는 나에게 빙그레 웃어 보였다. “어째서 그런 일이 있을 수 있겠나? 진짜 그 말대로인데. ” “자네 머리는 검은빛에서 잿빛이 되는 대신 잿빛에서 검은빛으로 된단 말인가?” “그렇다네. ” “그렇지만 그런 일은 과학적으로 불가능해!” “천만에. ” “하지만 있을 수 없는 일이잖나. 자연 법칙에 어긋나. ” “헤이스팅즈, 자네는 여전히 남을 의심하지 않는 아름다운 마음을 지니고 있군. 세월도 자네의 그 마음은 바꿔 놓지 못하는구먼! 자네는 한 가지 사실을 발견하면 곧바로 그 해결을 입에 담지. 자기 자신은 그것을 의식하지 못하지만!” 나는 무슨 소리인지 알 수가 없어 그를 쳐다보았다.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/160       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/159       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/158       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/157       \n",
            "Negative tokens: ['아직' '”' '그는' '”' '여전히' '자네는' '의식하지']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 158/159       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 157/158       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 156/157       \n",
            "Peak count: 9\n",
            "Frame tokens: “아니, 않았다. 나이를 “어째서 진짜 ” ” 있군. 쳐다보았다. \n",
            "\n",
            "Similarity : 0.4230735324477164\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.412827968597412 Generator / grammar loss:-0.1959313601255417   similarity loss:-0.15404678881168365\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "포아로는 “아니, 기억하고 성공하지 있지. 관찰했다. 포아로. 거의 나이를 안 더 웃어 보였다. “어째서 진짜 검은빛에서 검은빛으로 ” ” 있을 일이잖나. 법칙에 있군. 자기 그것을 나는 쳐다보았다.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.176752  0.431242  0.593779  0.480171  0.004635  0.527189   \n",
            "\n",
            "    grammar  \n",
            "0  0.950727  \n",
            "Current result ==================================================\n",
            "Sample count: 2\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.162811  0.461301  0.501567    0.509092  0.003732     0.495771   \n",
            "\n",
            "   grammarity  \n",
            "0    0.955979  \n",
            "==================================================\n",
            "3 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "그는 잠자코 침실로 들어가더니 병을 하나 들고 돌아와 나에게 건네 주었다. 나는 까닭을 모르는 채 그 병을 보았다. 병에는 이렇게 씌어 있었다. 르비비 - 머리칼의 자연스러운 빛깔을 회색, 밤색, 빨강, 노랑, 갈색, 검은 색의 여섯 가지 색조로 되살린다. 르비비는 염료가 아니다. 나는 소리쳤다. “포아로, 머리를 염색하고 있구먼!” “아, 겨우 알아차린 모양이군!” “그래서 자네 머리가 전에 돌아왔을 때보다 훨씬 검어 보였단 말인가?” “그렇지. ” 놀라움이 가라앉자 나는 말했다. “그럼, 다음에 돌아왔을 때에는 가짜 수염이라도 달고 있을게 아닌가? 아니면 지금도 가짜 수염인가?” 포아로는 움찔했다. 수염은 늘 그가 세심하게 신경쓰는 부분이다. 그는 수염을 터무니없이 자랑했다. 그런데 내 말이 그의 아픈 데를 찌른 것이다. “아닐세, 당치도 않아. 그런 날은 되도록 오지 않기를 비네. 가짜 수염이라니? 끔찍한 소리를!” 그의 수염이 진짜인 것을 증명하기 위해 힘주어 잡아당겨 보였다. “과연 아직 숱이 꽤 많군. ” “그렇지? 온 런던을 다 찾아봐도 나에게 맞는 가짜 수염은 있을 리 없네.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/141       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/140       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/139       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/138       \n",
            "Negative tokens: ['병을' '병에는' '르비비' '르비비는' '수염이라도' '수염인가?”' '수염을' '수염이라니?' '수염이' '숱이' '수염은']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 139/140       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 138/139       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 137/138       \n",
            "Peak count: 8\n",
            "Frame tokens: 머리가 가짜 수염은 터무니없이 가짜 “과연 가짜 없네. \n",
            "\n",
            "Similarity : 0.4292594483236668\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.2233152389526367 Generator / grammar loss:-0.20469599962234497   similarity loss:-0.18227098882198334\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            " 채 머리칼의 “아, 겨우 머리가 돌아왔을 때보다 놀라움이 말했다. 지금도 가짜 수염인가?” 수염은 터무니없이 되도록 비네. 가짜 증명하기 잡아당겨 “과연 찾아봐도 가짜 없네.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.174688  0.323768  0.561792  0.636586  0.017789  0.536625   \n",
            "\n",
            "    grammar  \n",
            "0  0.926488  \n",
            "Current result ==================================================\n",
            "Sample count: 3\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN    0.16677  0.415457  0.521642     0.55159  0.008418     0.509389   \n",
            "\n",
            "   grammarity  \n",
            "0    0.946149  \n",
            "==================================================\n",
            "4 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "” 꽤 우쭐대는군 하고 나는 마음속으로 생각했다. 그러나 나는 그런 소리를 해서 포아로의 기분을 상하게 할 생각은 전혀 없었다. 그 대신 그가 아직도 때로 일을 하는지 물어 보았다. “자네가 몇 해 전 은퇴한 것은 알고 있지만……” “그렇네. 대대적으로 호박을 가꾸기 위해서! 그런데 곧 살인 사건이 일어나 호박들에게 멸망으로의 행진을 시키고 만 셈일세. 그 뒤부터는, 자네가 뭐라고 할지 잘 알지만 나는 자진해서 고별 공연을 여는 프리마돈나가 됐네. 물론 그 고별 공연이 끝없이 되풀이되고 있지만 말일세. ” 나는 웃었다, “실로 그대로라네. 그때마다 나는 이것을 마지막이라고 하지. 그런데 안돼. 다른 사건이 일어나거든. 그래서 나는 인정하지 않을 수 없다네. 나는 은퇴를 바라지 않는다고. 이 조그만 회색 뇌세포는 쓰지 않으면 녹슬어 버리니까. ” “알았네. 적당히 운동을 시키고 있다는 거로군. ” “맞아, 요즘의 에르큘 포아로는 범죄의 진수밖에 다루지 않네. ” “그 진수는 충분히 있던가?” “꽤 있지. 바로 저번 사건 같은 경우는 위태로울 뻔했었어.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/138       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/137       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/136       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/135       \n",
            "Negative tokens: ['그러나' '그' '“자네가' '있지만……”' '그런데' '멸망으로의' '잘' '고별' '그때마다' '그래서' '”']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 136/137       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 135/136       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 134/135       \n",
            "Peak count: 7\n",
            "Frame tokens: 우쭐대는군 셈일세. 말일세. ” ” 있지. 뻔했었어. \n",
            "\n",
            "Similarity : 0.445318707063046\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.270016670227051 Generator / grammar loss:-0.18682748079299927   similarity loss:-0.1596599519252777\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "우쭐대는군 마음속으로 때로 곧 시키고 만 셈일세. 자네가 뭐라고 물론 말일세. 그때마다 마지막이라고 인정하지 녹슬어 ” “알았네. ” 진수는 있지. 뻔했었어.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.163569  0.518055  0.504338  0.547731  0.000328  0.520099   \n",
            "\n",
            "    grammar  \n",
            "0  0.968985  \n",
            "Current result ==================================================\n",
            "Sample count: 4\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN    0.16597  0.441107  0.517316    0.550625  0.006395     0.512067   \n",
            "\n",
            "   grammarity  \n",
            "0    0.951858  \n",
            "==================================================\n",
            "5 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "” “실패했나?” 포아로는 놀라운 듯했다. “당치도 않네. 그렇지만 이 내가, 이 에르큘 포아로가 하마터면 살해될 뻔했었지. ” 나는 휘파람을 불었다. “대담한 범인이로군. ” “대담하다기보다 무모하지. 그래, 진짜 무모한 녀석이었어. 하지만 그 이야기는 그만두세. 그런데 헤이스팅즈, 알겠나? 나는 여러 가지 뜻에서 자네를 내 마스코트로 생각하고 있네. ” “정말인가? 어떤 뜻에서?” 포아로는 내 물음에는 직접 대답하지 않고 이야기를 계속했다. “자네가 온다는 이야기를 들으면 나는 곧 무언가 일어나겠군 하고 생각된다네. 예전처럼 둘이서 수사하지 않겠나. 하지만 만일 그렇게 한다면 평범한 사건은 안돼. 뭔가 이렇게……. ” 그는 흥분해서 손을 파도치듯 움직였다. “머리를 잔뜩 쓰게 하는, 미묘하고 피이누(섬세)한 것이 아니면 안 되지. ” 피아누라는 번역하기 어려운 말에 가득한 풍미를 곁들이는 듯한 말투였다. “포아로, 남이 들으면 마치 리츠에서 저녁 식사라도 주문하고 있는 줄로 생각하겠네.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/118       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/117       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/116       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/115       \n",
            "Negative tokens: ['“당치도' '하지만' '그런데' '“정말인가?' '하지만' '어려운']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 116/117       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 115/116       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 114/115       \n",
            "Peak count: 6\n",
            "Frame tokens: 듯했다. 그렇지만 ” 뭔가 말투였다. 생각하겠네. \n",
            "\n",
            "Similarity : 0.4317683975812677\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.221489429473877 Generator / grammar loss:-0.15761414170265198   similarity loss:-0.13537397980690002\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "듯했다. 그렇지만 휘파람을 범인이로군. ” 하지만 그만두세. “자네가 들으면 않겠나. 사건은 안돼. 뭔가 이렇게……. 움직였다. 듯한 말투였다. 생각하겠네.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.172619  0.580648  0.491729  0.559733  0.001441  0.529914   \n",
            "\n",
            "    grammar  \n",
            "0  0.914614  \n",
            "Current result ==================================================\n",
            "Sample count: 5\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN     0.1673  0.469015  0.512199    0.552447  0.005404     0.515636   \n",
            "\n",
            "   grammarity  \n",
            "0    0.944409  \n",
            "==================================================\n",
            "6 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "” 그는 한숨을 쉬었다. “범죄란 주문할 수 있는 게 아닌데 말일세. 정말이야. 그렇지만 나는 운을 믿겠네. 운명이라 해도 좋아. 내 곁에 붙어 있으면서 내가 용서받을 수 없는 실책을 저지르는 걸 막아주는 게 자네 운명이야. ” “용서받을 수 없는 실책이란 뭔가?” “명백한 것을 놓치는 것이지. ” 나는 이 말을 가슴속에서 되풀이해 보았으나 핵심을 잡을 수 없었다. 나는 밝게 미소지으며 말했다. “그런데 그 진수라고 할 만한 범죄는 아직 일어나지 않았나?” “적어도 아직은. 왜냐하면……. ” 그는 말을 끊었다. 이마에 난처한 듯한 주름이 잡혔다. 그 손은 내가 생각없이 접어버린 물건을 무의식중에 펴고 있었다. 그는 천천히 말했다. “뚜렷이 알 수는 없지만……. ” 그 말투에 어떤 이상한 게 느껴져 나는 놀라며 그의 얼굴을 보았다. 가로진 주름은 아직 남아 있었다. 그는 갑자기 결심한 듯 고개를 끄덕이고 창 가까이의 책상 쪽으로 방을 가로질러 갔다. 책상 속의 것은 말할 나위도 없이 잘 분류되고 정리되어 손을 넣기만 하면 kq로 필요한 서류를 꺼낼 수 있었다.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/145       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/144       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/143       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/142       \n",
            "Negative tokens: ['실책이란' '보았으나' '“그런데' '이마에' '생각없이' '느껴져' '듯']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 143/144       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 142/143       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 141/142       \n",
            "Peak count: 8\n",
            "Frame tokens: 정말이야. 운명이라 것이지. 그 왜냐하면……. 천천히 ” 있었다. \n",
            "\n",
            "Similarity : 0.5221758014466584\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.388674259185791 Generator / grammar loss:-0.19979779422283173   similarity loss:-0.1604296863079071\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "그는 정말이야. 믿겠네. 운명이라 내가 용서받을 수 운명이야. 것이지. 끊었다. 이마에 그 손은 내가 있었다. 천천히 알 없지만……. ” 말투에 보았다. 아직 끄덕이고 있었다.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro     body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.180479  0.584447  0.61287  0.536127  0.001004  0.584163   \n",
            "\n",
            "    grammar  \n",
            "0  0.984584  \n",
            "Current result ==================================================\n",
            "Sample count: 6\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.169496  0.488254  0.528977    0.549727  0.004671     0.527057   \n",
            "\n",
            "   grammarity  \n",
            "0    0.951105  \n",
            "==================================================\n",
            "7 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "그는 한 통의 뜯어진 편지를 손에 들고 내 쪽으로 천천히 되돌아왔다. 그리고 그것에 눈길을 한 번 주더니 나에게 내밀며 말했다. “자네는 이걸 어떻게 생각하나?” 나는 어떤 흥미를 가지고 그것을 바았다. 그것은 좀 두꺼운 흰 편지지에 활자체로 씌어 있었다. 에르큘 포아로여, 너는 자만에 빠져 있는 게 아닐까. 갸엾은 우리 멍청이 영국 경찰이 감당하지 못하는 어려운 사건을 해결할 수 있는 건 자신이라고? 명민한 포아로여, 너의 명민함을 어디 한 번 보여 다오. 하지만 너에게는 이 호두가 너무 딱딱할걸. 이 달 21일, 앤도버(Andover)를 경계하라. 이만. ABC 나는 잠시 봉투에 눈길을 주었다. 역시 활자체로 씌어 있었다. 내가 소인에 주의를 돌리고 있는 것을 보자 그가 말했다. “소인은 서중앙 제1국일세. 그래, 어떻게 생각하나?” 나는 어깨를 으쓱해 보이며 편지를 돌려주었다. “아마도 미치광이 짓이겠지. ” “그뿐인가?” “자네한테는 미치광이로 여겨지지 않는다는 건가?” “아니, 그렇게 여겨지네. ” 그의 말투는 진지했다. 나는 호기심을 느끼며 그를 보았다. “자네는 이 편지를 진지하게 받아들이고 있는 모양이군, 포아로.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/143       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/142       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/141       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/140       \n",
            "Negative tokens: ['편지를' '번' '명민함을' '눈길을' '생각하나?”' '“아마도' '호기심을' '“자네는']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 141/142       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 140/141       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 139/140       \n",
            "Peak count: 8\n",
            "Frame tokens: 에르큘 아닐까. 자신이라고? 내가 말했다. 돌려주었다. ” 포아로. \n",
            "\n",
            "Similarity : 0.4016976032262092\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.175910472869873 Generator / grammar loss:-0.17727985978126526   similarity loss:-0.1596432328224182\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "두꺼운 에르큘 빠져 아닐까. 멍청이 자신이라고? 나는 역시 내가 돌리고 말했다. 그래, 어떻게 생각하나?” 어깨를 으쓱해 편지를 돌려주었다. ” “자네한테는 건가?” “아니, 말투는 포아로.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.181818  0.472454  0.448379  0.536249  0.001375  0.479555   \n",
            "\n",
            "    grammar  \n",
            "0  0.983851  \n",
            "Current result ==================================================\n",
            "Sample count: 7\n",
            "     method  comp rate     intro      body  conclusion  isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.171256  0.485996  0.517463    0.547801   0.0042     0.520271   \n",
            "\n",
            "   grammarity  \n",
            "0    0.955783  \n",
            "==================================================\n",
            "8 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "” “미치광이란 진지하게 다루어야 하지. 미치광이는 아주 위험한 존재니까. ” “그렇지, 물론 그렇네. 나는 그 점을 생각지 못했어. 그러나 내 말은, 어쩐지 우스꽝스러운 장난같은 생각이 든다는 걸세. 누군가, 8이라는 숫자에 하나가 더 많은 것 같은 우쭐해진 주정꾼 바보 말이네. ” “뭐라고? 아홉이란 말인가? 그건 대체 무슨 뜻이지?” “아니, 그냥 말장난일세. 취한 녀석이라는 뜻이지. 아니, 그보다도 지나치게 마셔서 고주망태가 된 녀석이라는 뜻일세. ” “고맙네, 헤이스팅즈. 그 <취한다>는 말이라면 나도 알고 있네. 자네 말대로 그 이상의 뜻은 없는지도 모르지만. ‘ 나는 그의 불만스러운 말투에 자극되어 물어 보았다. “그럼, 자네는 무엇이 있다고 생각하나?” 포아로는 의심스러운 듯 머리를 흔들었지만 아무 말도 하지 않았다. 나는 물었다. “그래서 자네는 어떻게 했나?” “어떻게 할 수 있었겠나? 재프 경감에게 보였을 뿐이지. 그는 자네와 같은 의견이었어.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/118       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/117       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/116       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/115       \n",
            "Negative tokens: ['”' '그러나' '어쩐지' '<취한다>는' '자네' '했나?”' '있었겠나?' '그는']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 116/117       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 115/116       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 114/115       \n",
            "Peak count: 6\n",
            "Frame tokens: ” 그건 말장난일세. 모르지만. 물었다. 의견이었어. \n",
            "\n",
            "Similarity : 0.5336671253147756\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.2845089435577393 Generator / grammar loss:-0.18830718100070953   similarity loss:-0.15966200828552246\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "물론 나는 그 생각지 말은, ” 그건 “아니, 그냥 말장난일세. 녀석이라는 나도 알고 모르지만. 보았다. 물었다. 뿐이지. 의견이었어.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio    intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.153374  0.50164  0.559133  0.592858  0.001418  0.557752   \n",
            "\n",
            "    grammar  \n",
            "0  0.943235  \n",
            "Current result ==================================================\n",
            "Sample count: 8\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.169021  0.487952  0.522672    0.553433  0.003852     0.524956   \n",
            "\n",
            "   grammarity  \n",
            "0    0.954214  \n",
            "==================================================\n",
            "9 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "할 짓 없는 녀석의 장난이라고 말일세. 그것이 그의 표현이었는데, 런던 경찰국에서는 거의 날마다 이런 것을 받는다는군. 나도 그 바람에 휘말려 들었다는 거였어. ” “하지만 자네는 이 편지를 진지하게 생각하고 있잖은가?” 포아로는 천천히 대답했다. “아무래도 이 편지에는 내 마음에 들지 않는 게 있어, 헤이스팅즈. ” 그 말투가 묘하게 인상적이었다. “그래, 자네 의견은?” 그는 고개를 젓고 그 편지를 들어올려 다시 책상 속에 넣어 버렸다. “자네가 그토록 진지하게 생각한다면 왜 아무 일도 하지 않고 있는 건가?” “여전히 활동가로군, 자네는! 하지만 대체 어떻게 할 수 있겠나? 지방 경찰에도 편지를 보였지만 역시 진지하게 여겨 주지 않았어. 지문도 없고, 편지를 낸 사람에 대한 단서도 없으니. ” “그렇다면 자네 육감 말고는 아무것도 없단 말인가?” “육감이 아닐세, 헤이스팅즈. 육감이란 나쁜 말이야. 내 지식이며 경험일세. 그 편지에 뭔가 이상한 게 있다고 가르쳐 주는 것은. ” 말이 막히자 그는 손짓을 해보였다. 그리고 또 머리를 흔들었다. “개미집에서 산을 만들어 내려 하고 있는지도 모르지만 말일세. 어쨌든 기다려 보는 수밖에 없어. ” “옳지, 21일은 금요일이군.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/153       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/152       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/151       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/150       \n",
            "Negative tokens: ['“하지만' '“아무래도' '그토록' '하지만' '있겠나?' '말고는' '육감이란' '편지에' '막히자']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 151/152       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 150/151       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 149/150       \n",
            "Peak count: 8\n",
            "Frame tokens: 녀석의 천천히 생각한다면 주지 없으니. 말인가?” 말일세. 금요일이군. \n",
            "\n",
            "Similarity : 0.4246868821117\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.4069156646728516 Generator / grammar loss:-0.1872703731060028   similarity loss:-0.14600294828414917\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "녀석의 이런 거였어. 생각하고 천천히 자네 버렸다. 생각한다면 하지만 진지하게 여겨 주지 않았어. 사람에 없으니. 아무것도 말인가?” “개미집에서 내려 모르지만 말일세. 없어. ” 금요일이군.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro     body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.175698  0.474876  0.49882  0.574567  0.001805  0.516755   \n",
            "\n",
            "    grammar  \n",
            "0  0.968161  \n",
            "Current result ==================================================\n",
            "Sample count: 9\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.169763  0.486499  0.520022    0.555782  0.003625     0.524045   \n",
            "\n",
            "   grammarity  \n",
            "0    0.955764  \n",
            "==================================================\n",
            "10 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "앤도버에서 굉장한 강도 사건이라도 일어난다면 그야말로……. ” “아, 그렇다면 얼마나 기분전환이 되겠나. ” “기분전환이라고?” 나는 어이가 없었다. 그 자리에서 그 말은 너무나 이상스럽게 들렸다. 나는 항의했다. “강도는 스릴이 있을지 모르지만 기분전환이라고 할 수는 없어!” 포아로는 힘주어 고개를 저었다. “자네는 잘못 알고 있네. 자네는 내 말뜻을 모르고 있어. 내 마음을 차지하고 있는 더 큰 다른 염려에 비하면, 강도는 오히려 마음 놓을 수 있다는 걸세. ” “무슨 염려인가?” “살인이지. ‘ < 삽 화 > 앨릭잰더 보너퍼트 캐스트 씨는 의자에서 일어나 초라한 침실을 근시인 듯한 눈으로 둘러보았다. 답답스러운 자세로 앉아있었기 때문에 등이 완전히 뻣뻣해져 버렸다. 등을 쭉 펴고 기지개 켜는 그를 본 사람은, 그가 실제로는 키가 큰 사람임을 알았으리라. 그의 굽은 등과 근시처럼 기웃거리는 동작이 아주 다른 인상을 주고 있었다. 문 안쪽에 걸린 낡아빠진 외투로 다가가 주머니에서 싸구려 담뱃갑과 성냥을 꺼냈다. 담배에 불을 붙이고 지금까지 앉아있던 의자로 돌아왔다. 철도 안내서를 집어 들고 세밀히 보더니 이윽고 타이프된 이름 리스트를 훑어보기 시작했다. 그는 펜으로 그 리스트의 첫 번째 이름에 표시했다. 그것은 6월 20일 목요일의 일이었다. < 앤도버 살인 > 나는 그때 포아로가 받은 편지에 대한 그의 예감에 깊은 인상을 받은 건 사실이지만, 그 일은 내 머리에서 아주 사라져 버렸다고 해도 좋다.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/187       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/186       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/185       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/184       \n",
            "Negative tokens: ['어이가' '기분전환이라고' '힘주어' '잘못' '모르고' '보너퍼트' '초라한' '답답스러운' '실제로는' '굽은' '인상을'\n",
            " '낡아빠진' '예감에' '사실이지만,' '아주']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 185/186       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 184/185       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 183/184       \n",
            "Peak count: 10\n",
            "Frame tokens: 일어난다면 되겠나. 이상스럽게 항의했다. 저었다. 강도는 걸세. “살인이지. 버렸다. 좋다. \n",
            "\n",
            "Similarity : 0.4532648989976308\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.5664846897125244 Generator / grammar loss:-0.18684269487857819   similarity loss:-0.12860193848609924\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "강도 일어난다면 되겠나. 나는 그 이상스럽게 항의했다. 수는 저었다. “자네는 모르고 다른 염려에 강도는 마음 놓을 걸세. “살인이지. 화 > 눈으로 버렸다. 펴고 그의 의자로 돌아왔다. 20일 예감에 사실이지만, 좋다.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body   ending       var     total  \\\n",
            "0  SAM+WGAN    0.166667  0.532266  0.561995  0.41671  0.003927  0.512464   \n",
            "\n",
            "    grammar  \n",
            "0  0.956622  \n",
            "Current result ==================================================\n",
            "Sample count: 10\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.169453  0.491076  0.524219    0.541874  0.003655     0.522887   \n",
            "\n",
            "   grammarity  \n",
            "0     0.95585  \n",
            "==================================================\n",
            "11 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "실제로 21일이 되어 런던 경찰국의 재프 경감이 포아로를 찾아왔을 때 나는 겨우 그 일을 생각해 냈다. 이 사법 경찰관과는 이미 오래 전부터 알고 있었기 때문에 그는 나를 보자 진심으로 환영해 주었다. 그는 큰소리로 말했다. “여, 내가 헤이스팅즈 대위를 몰라볼 리 있겠습니까. 드디어 당신의 야만 지대에서 돌아오셨군요! 포아로 씨와 함께 계신 당신을 뵈니 정말 예전 그대로입니다 그려. 게다가 건강하신 듯 하군요. 머리가 좀 벗겨졌는가요? 그렇습니다, 누구나 그렇게 되지요. 나도 그렇습니다. ” 나는 좀 놀랐다. 머리 꼭대기에 머리칼이 덮이도록 빗어 두었기 때문에 벗겨진 곳이 눈에 띄지 않으리라 여기고 있었던 것이다. 그러나 재프 경감은 그런 점에 그리 머리가 잘 도는 편이 아니었다. 그래서 나는 좋은 얼굴로 아무도 젊어지는 사람은 없다는 데 동의했다. 재프 경감은 말했다. “그러나 이 포아로 씨만은 다릅니다. 헤어토닉의 좋은 광고가 되지요. 얼굴 구석구석이 한층 더 싱싱해졌습니다. 늘그막에 이르러 점점 더 각광받게 되셨으니 말입니다. 요즘의 유명한 사건에는 모조리 관계되어 계시지요.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/140       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/139       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/138       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/137       \n",
            "Negative tokens: ['되어' '그는' '곳이' '좋은' '되셨으니']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 138/139       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 137/138       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 136/137       \n",
            "Peak count: 8\n",
            "Frame tokens: 주었다. 드디어 하군요. 그렇습니다, 그렇습니다. 것이다. 싱싱해졌습니다. 계시지요. \n",
            "\n",
            "Similarity : 0.4905595347079681\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.347499370574951 Generator / grammar loss:-0.2026877999305725   similarity loss:-0.1675817221403122\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "실제로 경찰국의 재프 나를 주었다. 말했다. 리 드디어 하군요. 벗겨졌는가요? 그렇습니다, 나도 그렇습니다. 그래서 얼굴로 말했다. “그러나 되지요. 얼굴 구석구석이 싱싱해졌습니다. 말입니다. 모조리 계시지요.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.210054  0.453106  0.596627  0.613381  0.005174  0.572949   \n",
            "\n",
            "    grammar  \n",
            "0  0.984023  \n",
            "Current result ==================================================\n",
            "Sample count: 11\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.173144  0.487624  0.530802    0.548375  0.003793     0.527438   \n",
            "\n",
            "   grammarity  \n",
            "0    0.958411  \n",
            "==================================================\n",
            "12 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "열차 사건, 공중에서의 사건, 사교계 살인 사건……. 정말이지 여기서기에 이분은 등장합니다. 은퇴하고 나서 훨씬 더 유명해지셨답니다. ” 포아로가 웃으며 말했다, “요전에도 헤이스팅즈에게 말했었지요. 나는 언제나 또다시 등장하는 프리마돈나 같다고“ “마지막에는 자신의 죽음을 탐정한다 해도 우스운 일이 아닐겁니다. 이건 기발한 생각인데, 정말. 책에 써둬야겠어. ” 재프 경감은 커다랗게 웃었다. 포아로는 내게 눈짓을 해보였다. “그것을 해야 할 사람은 우선 헤이스팅즈지요. ” 재프 경감은 웃었다. “하하하! 농담입니다, 농담입니다. ” 나는 그 생각이 어째서 악취미로 여겨졌다. 가엾게도 포아로는 점점 나이를 먹어 가고 있다. 죽음이 가까이 오는 것과 관계된 그 농담이 그에게 유쾌할 리 없을 것이다. 내 태도에 속마음이 나타나 있었던 모양이다. 재프 경감은 화재를 바꾸었다. “포아로 씨의 익명 편지에 대해 들으셨습니까?” 포아로가 말했다. “저번에 보여 줬지요.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/113       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/112       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/111       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/110       \n",
            "Negative tokens: ['자신의' '우스운' '”' '있었던' '“포아로']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 111/112       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 110/111       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 109/110       \n",
            "Peak count: 6\n",
            "Frame tokens: 정말이지 ” 포아로는 “하하하! 속마음이 줬지요. \n",
            "\n",
            "Similarity : 0.4519609675750982\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.131354808807373 Generator / grammar loss:-0.16617387533187866   similarity loss:-0.15301945805549622\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "정말이지 훨씬 유명해지셨답니다. ” 헤이스팅즈에게 나는 탐정한다 포아로는 해보였다. 할 “하하하! 그 생각이 여겨졌다. 유쾌할 것이다. 속마음이 말했다.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.174897  0.562476  0.553003  0.430698  0.003601  0.518206   \n",
            "\n",
            "    grammar  \n",
            "0  0.919874  \n",
            "Current result ==================================================\n",
            "Sample count: 12\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN    0.17329  0.493862  0.532652    0.538569  0.003777     0.526669   \n",
            "\n",
            "   grammarity  \n",
            "0      0.9552  \n",
            "==================================================\n",
            "13 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "” 나는 소리쳤다. “아, 그렇지. 완전히 잊고 있었어. 문제의 날짜가 언제였지?” 재프 경감이 말했다. “21일입니다. 그래서 내가 조사해 보았지요. 어제가 21일이었기 때문에, 어젯밤 혹시나 싶어 앤도버를 불러 보았습니다. 그랬더니 역시 장난이었지요. 아무 일도 없었으니까요. 어린아이가 돌을 던져 쇼윈도가 하나 깨진 일과 술주정꾼의 규칙 위반이 두 건. 그래서 우리 벨기에인 친구분(포아로)이 처음으로 헛짚으신 게 되었다는 이야기입니다. ” 포아로는 인정했다. “확실히 한시름 놓았습니다. ” 재프 경감이 동정하듯 말했다. “많이 염려하고 계신 것 같았습니다만? 가엾게도, 우리는 그런 것을 날마다 몇십 통씩 받는답니다. 달리 아무 하릴없는 머리가 좀 이상한 사람들이 그런 것을 쓰지요. 그리 악의가 있는 건 아닙니다. 뭐, 일종의 흥분에서지요. ” 포아로가 말했다.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/101       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/100       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/99       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/98       \n",
            "Negative tokens: ['“아,' '그랬더니' '하나' '그래서' '되었다는' '”' '한시름' '달리' '뭐,']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 99/100       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 98/99       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 97/98       \n",
            "Peak count: 6\n",
            "Frame tokens: “21일입니다. ” 인정했다. 뭐, 흥분에서지요. 말했다. \n",
            "\n",
            "Similarity : 0.5350194938909592\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.2940261363983154 Generator / grammar loss:-0.20451724529266357   similarity loss:-0.17490001022815704\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "경감이 “21일입니다. 그래서 어제가 불러 없었으니까요. ” 인정했다. 한시름 재프 경감이 말했다. 계신 날마다 뭐, 흥분에서지요. ” 말했다.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.185185  0.698265  0.579837  0.520843  0.005443  0.585824   \n",
            "\n",
            "    grammar  \n",
            "0  0.954548  \n",
            "Current result ==================================================\n",
            "Sample count: 13\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.174205  0.509585  0.536281    0.537205  0.003905     0.531219   \n",
            "\n",
            "   grammarity  \n",
            "0    0.955149  \n",
            "==================================================\n",
            "14 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "“그걸 그토록 진지하게 생각했던 건 정말 어리석은 짓이었습니다. 내가 코를 들이민 것은 새의 보금자리였던 셈이군요. ” 재프 경감이 말했다. “말과 벌을 혼동했던 겁니다. ” “뭐라고요?” “아니, 속담입니다. 자, 이제 가봐야겠군요. 이 가까이에 볼일이 있어서요. 도난품인 보석을 인수하러 왔지요. 그곳에 가는 길에 마음 놓으시도록 잠시 들렀던 겁니다. 회색 뇌세포를 뜻없이 써버리는 건 낭비니가요. ” 재프 경감은 기분좋게 웃으며 돌아갔다. 포아로가 말했다. “사람좋은 재프 경감은 그리 달라지지 않았지?” 나는 보복하듯 말했다. “아주 늙었군. 오소리같이 잿빛이 되었어. ” 포아로는 헛기침을 하고 나서 말했다. “헤이스팅즈, 아주 하찮은 장치가 있는데, 내 단골 이발사는 재간있는 사나이지. 머리에 그 장치를 붙이고 그 위에 자신의 머리칼을 벗어 놓는다네. 그건 가발이 아닐세, 잘 알겠지만. ” 나는 으르렁댔다.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/109       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/108       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/107       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/106       \n",
            "Negative tokens: ['놓으시도록' '않았지?”' '”']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 107/108       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 106/107       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 105/106       \n",
            "Peak count: 6\n",
            "Frame tokens: ” 속담입니다. 말했다. ” 말했다. 으르렁댔다. \n",
            "\n",
            "Similarity : 0.4276316964680702\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.3286077976226807 Generator / grammar loss:-0.20295606553554535   similarity loss:-0.1697947084903717\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            " “그걸 정말 내가 ” “아니, 속담입니다. 가봐야겠군요. 말했다. 달라지지 않았지?” ” 하고 말했다. 사나이지. 그 나는 으르렁댔다.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.165577  0.345769  0.528169  0.568948  0.009416  0.503923   \n",
            "\n",
            "   grammar  \n",
            "0   0.9824  \n",
            "Current result ==================================================\n",
            "Sample count: 14\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.173589  0.497884  0.535702    0.539473  0.004299     0.529269   \n",
            "\n",
            "   grammarity  \n",
            "0    0.957096  \n",
            "==================================================\n",
            "15 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "“포아로, 분통 치미는 자네 이발사의 더러운 발견 따윈 아무래도 좋네. 대체 내 머리가 어떻다고 그런 소리를 하는 건가?” “아니, 아무렇지도 않아, 아무렇지도. ” “내가 대머리가 되어가고 있다는 건 아니겠지?” “물론 그런 건 아닐세! 그런 건……. ” “그 나라의 뜨거운 여름은 절로 얼마쯤 머리를 벗겨지게 하지만 말이야. 그냥 질좋은 헤어토닉이나 가져가지. ” “그게 좋겠군. ” “그렇다 해도 재프 경감 따위가 관여할 일은 아니야. 녀석은 언제나 기분좋지 않았지. 게다가 유머 센스도 없어. 사람이 앉으려고 할 때 의자를 잡아당겨지면 웃는 그런 사나이거든. ” “그러면 사람들은 대개 웃지. ” “모름지기 센스가 없단 말일세. ” “앉으려던 사람의 입장에서 본다면 확실히 그렇지. ” “그렇네. ” 나는 얼마쯤 기분을 돌리며 다시 말했다―머리칼이 적어졌다는 말에 내가 아주 민감해 있다는 것을 인정하지 않으면 안 되겠다. “익명 편지가 아무 일 없었다니 유감이군. ‘ “그것은 완전히 내 잘못 생각이었네. 그 편지에 어쩐지 피비린내나는 것 같은 느낌이 있었는데, 그러나 단순한 장난이었어. 아, 나도 나이 먹어 아무것도 아닌 일에 짖어대는 눈먼 개처럼 의심이 많아져 버렸나 보네.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/154       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/153       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/152       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/151       \n",
            "Negative tokens: ['그런' '아니겠지?”' '그런' '”' '기분을' '적어졌다는' '아무' '아닌']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 152/153       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 151/152       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 150/151       \n",
            "Peak count: 8\n",
            "Frame tokens: 그런 말이야. 않았지. 없어. 없단 생각이었네. 장난이었어. 보네. \n",
            "\n",
            "Similarity : 0.49486034362745346\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.4103124141693115 Generator / grammar loss:-0.18893688917160034   similarity loss:-0.14731501042842865\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "대머리가 그런 건……. ” “그 뜨거운 머리를 하지만 말이야. ” “그렇다 재프 않았지. 유머 없어. 사나이거든. 사람들은 대개 없단 “그렇네. “익명 생각이었네. 장난이었어. 보네.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.167488  0.631209  0.585819  0.542621  0.001308  0.581937   \n",
            "\n",
            "    grammar  \n",
            "0  0.981625  \n",
            "Current result ==================================================\n",
            "Sample count: 15\n",
            "     method  comp rate     intro      body  conclusion  isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.173182  0.506772  0.539043    0.539682   0.0041     0.532781   \n",
            "\n",
            "   grammarity  \n",
            "0    0.958731  \n",
            "==================================================\n",
            "16 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "” 나는 웃으며 말했다. “내가 도우려면, 우리는 다른 데서 온갖 진수가 모아진 멋진 범죄를 찾아내야만 되겠군. ” “자네는 요전에 내가 했던 말을 기억하고 있나? 만일 요리를 주문하듯 범죄를 주문할 수 있다면 어떤 것을 고르겠나?” 나는 좋아진 그의 기분에 휩쓸려 말했다. “그렇지, 메뉴를 잘 봐야 하지 않겠나. 강도? 위조지폐? 아니, 이런 건 안 돼? 이건 식물성 요리 같지? 역시 살인이 좋겠군. 피비린내나는 살인사건, 물론 여러 가지가 딸린 것으로. ” “옳지, 오르되브르(식사 전 또는 술안주로 먹는 가벼운 요리)로군. ” “피해자는 남자로 할까, 여자로 할까? 역시 남자가 좋겠어. 누군가 유명한 사람, 미국의 백만장자나 국무장관이나 신문사 사장쯤 되는 인물. 범행 현장은……그렇지, 훌륭한 낡은 도서관 같은 데가 어떨까? 분위기로서 이 이상의 것은 없네. 흉기는 기묘한 형태로 구부러진 단도 아니면, 뭔가 둔기 같은 것, 예를 들면 조각된 돌상이라든지……. ” 포아로는 한숨을 쉬었다. “그렇잖으면 물론 독약.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/130       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/129       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/128       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/127       \n",
            "Negative tokens: ['만일' '수' '것을' '강도?' '살인이' '또는' '범행' '분위기로서' '흉기는' '“그렇잖으면']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 128/129       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 127/128       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 126/127       \n",
            "Peak count: 7\n",
            "Frame tokens: 되겠군. 말했다. 피비린내나는 것으로. 요리)로군. 역시 독약. \n",
            "\n",
            "Similarity : 0.4395191262867242\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.7068848609924316 Generator / grammar loss:-0.19552917778491974   similarity loss:-0.12165472656488419\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "” 웃으며 도우려면, 우리는 되겠군. 고르겠나?” 말했다. 아니, 안 이건 식물성 역시 좋겠군. 피비린내나는 것으로. “옳지, 전 요리)로군. 역시 독약. \n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.167954  0.495579  0.460848  0.361386  0.003234  0.437956   \n",
            "\n",
            "    grammar  \n",
            "0  0.976477  \n",
            "Current result ==================================================\n",
            "Sample count: 16\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.172856  0.506073  0.534156    0.528539  0.004045     0.526854   \n",
            "\n",
            "   grammarity  \n",
            "0     0.95984  \n",
            "==================================================\n",
            "17 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "하지만 이것은 아무래도 너무 전문적인 것 같네. 그렇다면 깊은 밤에 메아리치는 권총 소리……이런 것으로 할까. 그리고 아름다운 여자 하나, 둘. ” 친구는 중얼거렸다. “그녀는 빨강머리겠지. ” “신통치 못한 농담이군. 물론 아름다운 여자 한 사람에게 잘못된 혐의가 씌워져야만 되겠지. 그리고 그녀와 젊은이 사이에 오해가 생기고. 물론 그 밖에도 몇 사람에게 혐의가 돌아가지 않으면 안 되네. 이를테면 피해자의 친구거나 경쟁 상대인 피부빛이 검고 위험한 타입의 중년 여자, 얌전한 비서. 이들이 유력한 혐의자인데, 거기에 행동거지가 무뚝뚝하고 성실한 사나이인 해고된 하인이라든지 사냥터 관리인 등이 두어 사람쯤 그리고 재프 경감 같은 얼치기 형사. 그래, 이쯤이면 되겠지. ” “그것이 자네가 말한 온갖 진수가 모아진 범죄인가?” “찬성하지 않는구먼?” 포아로는 한심스러운 듯 나를 보았다. “자네는 지금까지 씌어진 거의 모든 미스터리 소설의 아주 멋있는 줄거리를 만들어 주었네. ” “그럼, 자네라면 어떤 주문을 할 건가?” 포아로는 눈을 감고 의자에 기댔다. 그의 목소리는 입술 사이로 조용히 흘러나왔다. “아주 단순한 범죄, 복잡한 데가 조금도 없는 범죄. 조용한 가정 생활의 범죄……열광적이 아니고 아주 내밀스러운. ” “범죄에 내밀스러운 게 있을 수 있는가?” 포아로는 중얼거리듯 말했다. “네 사람이 앉아서 브리지를 하고 있네.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/168       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/167       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/166       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/165       \n",
            "Negative tokens: ['메아리치는' '아름다운' '오해가' '혐의가' '친구거나' '사람쯤' '범죄인가?”' '“자네는' '미스터리' '멋있는' '그의'\n",
            " '단순한' '있을' '중얼거리듯']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 166/167       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 165/166       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 164/165       \n",
            "Peak count: 9\n",
            "Frame tokens: 중얼거렸다. 못한 잘못된 이들이 이쯤이면 흘러나왔다. 범죄. 내밀스러운. 있네. \n",
            "\n",
            "Similarity : 0.42960332851699223\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.5492403507232666 Generator / grammar loss:-0.19223575294017792   similarity loss:-0.13586494326591492\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "이것은 아무래도 밤에 아름다운 하나, 둘. 중얼거렸다. “그녀는 빨강머리겠지. 못한 이들이 그래, 이쯤이면 되겠지. 아주 감고 흘러나왔다. 데가 없는 범죄. 내밀스러운 게 있는가?” 포아로는 앉아서 하고 있네.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio    intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.168831  0.53143  0.475164  0.542326  0.000866  0.506566   \n",
            "\n",
            "    grammar  \n",
            "0  0.991531  \n",
            "Current result ==================================================\n",
            "Sample count: 17\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.172619  0.507564  0.530686     0.52935  0.003858     0.525661   \n",
            "\n",
            "   grammarity  \n",
            "0    0.961705  \n",
            "==================================================\n",
            "18 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "그리고 한 사람이 그 게임에 끼지 않고 벽난로 옆 의자에 앉아 있지. 밤이 깊어졌을 즈음 난롯불 옆에 앉아 있던 사나이가 죽은 것을 알게 되네. 네 사람 가운데 누군가가 손이 비게 되었을 때 죽인 것인데, 모두들 게임에 정신이 팔려 모르고 있었지. 자, 이것이 사건이네. 범인은 네 사람 가운데 누구일까?” (나, 아시겠죠? 다들<테이블위의카드>네요. ) “도무지 자극적인 데가 조금도 없는걸. ” 포아로는 비난하듯 눈길로 나를 보았다. “없지. 이상한 모양으로 구부러진 단도도, 협박도, 신상의 눈에서 훔쳐 낸 에메랄드도, 흔적을 알 수 없는 동양의 독약 같은 것도 없네. 헤이스팅즈, 자네는 아무래도 멜러 드라마 애호가로군. 자네는 하나의 살인이 아니라 연쇄적인 살인 쪽이 좋은 거지?” “그렇네, 책 속의 두 번째 살인은 경기가 좋아 보이던걸. 제1장에서 살인이 일어나 마지막 페이지 바로 앞까지 모두들의 알리바이가 성립되어 있다는 건……그래, 좀 따분하지. ‘ 전화가 울려 포아로가 일어나 받으러 갔다. “여보세요, 에르큘 포아로입니다. ‘ 잠시 말없이 듣고 있던 그의 얼굴빛이 달라졌다. 그의 대답은 짧게 토막토막 끊어졌다, “그랬군요……물론, 그렇지요……아, 가겠습니다……당연합니다……그야 당신 말대로겠지요. 그렇지요, 갖고 가겠습니다. 그럼, 곧. ” 그는 수화기를 내려놓고 방을 가로질러 내 곁으로 돌아왔다.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/164       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/163       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/162       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/161       \n",
            "Negative tokens: ['그' '끼지' '자,' '자네는' '거지?”' '그의' '내']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 162/163       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 161/162       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 160/161       \n",
            "Peak count: 9\n",
            "Frame tokens: 자, 사건이네. 다들<테이블위의카드>네요. 없는걸. “없지. 따분하지. “그랬군요……물론, 곧. 돌아왔다. \n",
            "\n",
            "Similarity : 0.5319976462788151\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.4381494522094727 Generator / grammar loss:-0.1808730512857437   similarity loss:-0.13633625209331512\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "죽인 자, 이것이 사건이네. 범인은 사람 (나, 다들<테이블위의카드>네요. ) 데가 없는걸. “없지. 이상한 협박도, 에메랄드도, 알 없네. 따분하지. 받으러 그의 얼굴빛이 “그랬군요……물론, 말대로겠지요. 곧. 곁으로 돌아왔다. \n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.188596  0.472639  0.604457  0.632636  0.004863  0.586547   \n",
            "\n",
            "    grammar  \n",
            "0  0.949451  \n",
            "Current result ==================================================\n",
            "Sample count: 18\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.173506  0.505624  0.534784    0.535088  0.003914     0.529043   \n",
            "\n",
            "   grammarity  \n",
            "0    0.961024  \n",
            "==================================================\n",
            "19 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "“재프 경감에게서 온 걸세, 헤이스팅즈. ” “그래서?” “경찰국으로 돌아가자마자 마침 앤도버에서 연락이 있었다는 거야. ” 나는 흥분하여 소리쳤다. “앤도버?” 포아로가 천천히 말했다. “노파가 하나 살해되었다는군. 애셔(Ascher)라는 이름으로, 담배와 신문을 파는 조그만 가게의 노파일세. ” 나는 얼마쯤 맥이 풀렸다. 앤도버라는 이름으로 부채질되었던 내 흥미는 어리둥절해졌다. 나는 뭔가 환상적인, 아주 색다른 것을 기대하고 있었는데! 조그만 담배 가게 노파가 살해된 일 따위는 아무래도 그리 신통찮다. 포아로는 여전히 느릿느릿한 무게있는 목소리로 말을 이었다. “앤도버 경찰에서는 범인을 체표할 수 있다고 생각하는 모양이야. ” 나는 다시 한 번 맥이 풀렸다. “노파는 그 남편과 사이가 나빴던 것 같네. 남편은 술꾼이며 질나쁜 녀석으로 종종 노파를 죽이겠다고 협박했었다는군. 그러나 그곳 경찰에서는 다른 점도 고려하여 내가 받은 익명의 편지를 보고 싶다는 거야. 나는 곧 자네와 함께 앤도버로 가겟다고 말해 두었네. ” 나는 얼마쯤 기운을 되찾았다. 시시하게 보일지라도 아무튼 범죄임에 틀림없다. 내가 범죄니 범인이니 하는 것에 관계하고부터 벌써 많은 세월이 흘렀다.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/141       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/140       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/139       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/138       \n",
            "Negative tokens: ['걸세,' '앤도버에서' '“앤도버?”' '“노파가' '앤도버라는' '있었는데!' '“앤도버' '남편은' '곧' '앤도버로'\n",
            " '기운을' '관계하고부터']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 139/140       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 138/139       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 137/138       \n",
            "Peak count: 8\n",
            "Frame tokens: 살해되었다는군. 풀렸다. 어리둥절해졌다. “노파는 협박했었다는군. 되찾았다. 틀림없다. 흘렀다. \n",
            "\n",
            "Similarity : 0.463683113662672\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.2536778450012207 Generator / grammar loss:-0.15865226089954376   similarity loss:-0.13314710557460785\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "“재프 “그래서?” 앤도버에서 흥분하여 소리쳤다. 천천히 살해되었다는군. 풀렸다. 내 어리둥절해졌다. 환상적인, “노파는 협박했었다는군. 경찰에서는 고려하여 가겟다고 되찾았다. 시시하게 범죄임에 틀림없다. 내가 범인이니 흘렀다. \n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.211921  0.587932  0.535212  0.570748  0.000482  0.556417   \n",
            "\n",
            "    grammar  \n",
            "0  0.977785  \n",
            "Current result ==================================================\n",
            "Sample count: 19\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.175528  0.509956  0.534807    0.536965  0.003734     0.530484   \n",
            "\n",
            "   grammarity  \n",
            "0    0.961906  \n",
            "==================================================\n",
            "20 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "나는 포아로의 다음 말을 거의 듣지 있지 않았다. 그러나 그것은 나중에 중요한 뜻을 지니고 내 기억 속에 되살아났다. 에르큘 포아로가 이렇게 말했다. “이것이 시작이다. ” < 철도 안내서 > 우리는 앤도버에서 글렌 형사의 마중을 받았다. 그는 키가 크고 머리칼이 아름다운 남자로 기분 좋은 미소를 떠올리고 있었다. 이야기를 간결이 하기 위해 사건의 사실만 간단히 밝혀 두는 게 좋으리라. 범죄는 22일 오전 1시에 그곳 순경에 의해 발견되었다. 순찰을 돌면서 가게 문을 밀어 보니 잠겨 있지 않았다. 안으로 들어가자 처음에는 아무도 없는 듯했으나, 계산대 쪽으로 회중전등을 돌리니 노파의 웅크린 시체가 눈에 들어왔다. 경찰의가 현장에 와 닿아 노파가 뒷머리를 강하게 얻어맞았음을 알아냈는데, 아마도 계산대 뒤의 선반에서 담배 봉지를 꺼내는 도중에 얻어맞은 듯했다. 범행은 일곱 시간 내지 아홉 시간 전에 행해진 것 같았다. 형사는 설명했다. “그러나 더 정확한 시간을 추정할 수 있습니다. 5시 30분에 담배를 사러 들어갔던 사나이가 있습니다. 그리고 6시 5분 좀 지나서 가게에 들어갔다가 아무도 없는 줄 알고 그냥 나온 다른 남자가 있습니다. 그러니까 범행 시간을 5시 30분에서 6시 5분 사이로 추정할 수 있지요. 이웃에서 애셔를 보았다고 말해 온 사람은 아직 없습니다. 그러나 물론 이제부터입니다. 그는 9시쯤 <스리크라운즈>에서 꽤 취해 있었습니다.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/179       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/178       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/177       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/176       \n",
            "Negative tokens: ['나중에' '사건의' '듯했으나,' '시간' '시간을']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 177/178       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 176/177       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 175/176       \n",
            "Peak count: 10\n",
            "Frame tokens: 시작이다. 받았다. 이야기를 그곳 발견되었다. 들어가자 형사는 그냥 이제부터입니다. 있었습니다. \n",
            "\n",
            "Similarity : 0.4123344420876902\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.693338632583618 Generator / grammar loss:-0.17744579911231995   similarity loss:-0.10511518269777298\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            " 있지 않았다. 그러나 그것은 나중에 되살아났다. 이렇게 말했다. 시작이다. ” 우리는 앤도버에서 마중을 받았다. 좋은 두는 가게 눈에 행해진 시간을 지나서 남자가 없습니다. 그러나 이제부터입니다. 9시쯤 취해 있었습니다. \n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro     body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.177054  0.734224  0.43074  0.586248  0.015354  0.538089   \n",
            "\n",
            "    grammar  \n",
            "0  0.989356  \n",
            "Current result ==================================================\n",
            "Sample count: 20\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.175605  0.521169  0.529603    0.539429  0.004315     0.530864   \n",
            "\n",
            "   grammarity  \n",
            "0    0.963278  \n",
            "==================================================\n",
            "21 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "체포하는 대로 곧 용의자로 잡아 둘 겁니다. ” 포아로가 물었다. “그리 호감주는 타입의 사나이가 아닌 모양이군요?” “싫은 사람입니다. ” “그는 자기 아내와 함께 살고 있지 않았던가요?” “그렇습니다. 몇 해 전에 헤어졌지요. 애셔는 독일 사람으로 한때 급사일을 한 적도 있었습니다만, 술을 너무 마셔서 차츰 그를 고용하는 곳이 없게 되었습니다. 그래서 그 부인이 일을 나가게 되었지요. 마지막으로 한 일은 미스 로즈라는 노부인의 요리사 겸 가정부였습니다. 급료를 받아 남편에게 꽤 많은 돈을 주었던 듯한데, 그는 몽땅 마셔 버리고는 자기 마누라가 일하는 곳으로 가서 소동을 벌이곤 했답니다. 그래서 애셔 부인은 미스 로즈네 농장으로 가서 일하게 되었습니다. 거기는 앤도버에서 3마일 떨어진 완전한 시골이어서 그도 그리 자주 찾아가지 못했지요. 미스 로즈가 세상을 떠나자 애셔 부인은 유산을 조금 받았습니다. 그래서 그 돈으로 담배와 신문을 파는 이 조그만 가게를 시작했습니다. 싸구려 담배와 얼마 안 되는 신문뿐이어서 겨우 먹고 사는 정도였지요. 애셔가 자주 찾아와 그녀에게 욕을 하곤 했는데, 그녀 쪽에서는 귀찮고 하니까 잔돈푼이나 줘서 쫓아 버리곤 했지요. 1주일에 15실링은 줬던 것 같습니다. ” 포아로가 물었다. “아이들은 있었소?” “없습니다. 조카딸이 하나 오버튼 가까이에서 일하고 있습니다. 아주 고집이 센 똑똑한 아가씨지요.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/171       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/170       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/169       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/168       \n",
            "Negative tokens: ['”' '아내와' '없게' '나가게' '미스' '듯한데,' '몽땅' '마누라가' '시골이어서' '애셔가' '하곤' '잔돈푼이나'\n",
            " '버리곤' '조카딸이' '고집이']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 169/170       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 168/169       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 167/168       \n",
            "Peak count: 9\n",
            "Frame tokens: 헤어졌지요. 급사일을 로즈라는 가정부였습니다. 부인은 못했지요. 부인은 “없습니다. 아가씨지요. \n",
            "\n",
            "Similarity : 0.44149803609523275\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.590379238128662 Generator / grammar loss:-0.2030167430639267   similarity loss:-0.14216840267181396\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "“그리 타입의 사나이가 자기 살고 몇 해 헤어졌지요. 급사일을 한 미스 로즈라는 겸 가정부였습니다. 부인은 거기는 못했지요. 부인은 조금 조그만 사는 15실링은 “없습니다. 오버튼 아주 똑똑한 아가씨지요.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.163558  0.307232  0.542587  0.535806  0.011965  0.493482   \n",
            "\n",
            "    grammar  \n",
            "0  0.984619  \n",
            "Current result ==================================================\n",
            "Sample count: 21\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.175031  0.510982  0.530222    0.539257  0.004679     0.529084   \n",
            "\n",
            "   grammarity  \n",
            "0    0.964295  \n",
            "==================================================\n",
            "22 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "” “그 애셔라는 사나이가 아내를 자주 협박했었다는 거지요?” “그렇습니다. 그는 술에 취하면 무섭게 변해서 아내의 머리를 박살내겠다는 둥 소리를 질러대곤 했답니다. 애셔 부인은 정말 끔찍한 일을 당한 거지요. ” “그녀는 몇 살이었소?” “60살이 다 되었지요. 아마. 훌륭하고 부지런한 사람이었습니다. ” 포아로는 신중하게 말했다. “그러면 그 애셔라는 사나이가 범인이라는 게 당신 의견이오?” 형사는 조심스럽게 헛기침을 했다. “그렇게 말하는 건 좀 성급한 판단입니다만, 프란츠 애셔가 지난밤에 어떻게 지냈는지 그 자신의 설명을 듣고 싶은 겁니다, 포아로 씨. 만일 만족할 만한 설명을 들을 수 있다면 좋겠지만, 그렇지 않으면……. ” 그는 꽤 의미심장하게 말을 끊었다. “가게에서는 아무것도 없어지지 않았소?” “네, 아무것도. 돈도 그대로 다 있고, 훔쳐 간 흔적이 전혀 없습니다. ” “그 애셔라는 사나이가 술에 취해 가게로 들어와 아내를 욕하다가 끝내 때려 죽였다는 거로군요?” “네, 그것이 가장 타당한 해석이겠지요. 그러나 당신이 받으셨다는 그 이상한 편지도 고려해 보고 싶습니다, 포아로 씨. 그것이 이 애셔라는 사나이로부터 보내진 것인지 어떤지 알 수 없으니까요. ” 포아로가 편지를 건네주자 형사는 이마를 찌푸리고 그것을 읽었다. 형사는 마침내 말했다. “아무래도 애셔가 쓴 것 같지는 않군요. 도대체 이 <우리> 영국 경찰이라는 말을 애셔가 쓸 턱이 없지요.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/174       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/173       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/172       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/171       \n",
            "Negative tokens: ['무섭게' '부지런한' '신중하게' '애셔라는' '헛기침을' '판단입니다만,' '애셔가' '지냈는지' '만일' '의미심장하게'\n",
            " '않았소?”' '애셔라는' '그러나' '이상한' '애셔라는' '것인지' '찌푸리고' '“아무래도']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 172/173       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 171/172       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 170/171       \n",
            "Peak count: 9\n",
            "Frame tokens: 애셔라는 했답니다. 아마. 씨. ” 없으니까요. 말했다. 애셔가 없지요. \n",
            "\n",
            "Similarity : 0.3820919707496613\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.5504372119903564 Generator / grammar loss:-0.19431906938552856   similarity loss:-0.13781878352165222\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "” 애셔라는 자주 둥 했답니다. 되었지요. 아마. 부지런한 당신 의견이오?” 씨. ” 그는 “가게에서는 아무것도. ” 취해 타당한 그 이 없으니까요. 찌푸리고 말했다. 애셔가 쓸 턱이 없지요.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.149233  0.366047  0.520143  0.508884  0.004919  0.485946   \n",
            "\n",
            "    grammar  \n",
            "0  0.977998  \n",
            "Current result ==================================================\n",
            "Sample count: 22\n",
            "     method  comp rate     intro      body  conclusion  isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.173858  0.504394  0.529763    0.537876  0.00469     0.527123   \n",
            "\n",
            "   grammarity  \n",
            "0    0.964918  \n",
            "==================================================\n",
            "23 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "그야말로 각별히 교묘하게 행동하려는 게 아니었다면 말입니다. 게다가 그에겐 그만한 머리가 없습니다. 그는 이제 산송장입니다. 다 망가져 버렸지요. 이런 글을 쓰기에는 그의 손이 너무 떨릴걸요. 편지지도 잉크도 고급품이고. 그러나 편지에는 21일이라고 한 것은 이상하군요. 물론 우연의 일치겠지만요. ” “그렇겠지요. ” “하지만 이런 일치는 좋지 않습니다, 포아로 씨. 너무 딱 들어맞으니 말입니다. ” 그는 잠시 입을 다물고 있었다. 그의 이마에 주름이 잡혔다. “ABC. 대체 ABC란 어떤 녀석일까요? 메리 드로워―조카딸입니다만―가 좀 도움이 될지도 모르겠군요. 뭐 수고하시는 김에 말입니다. 이 편지만 없다면 나느 프란츠 애셔에게 내기를 걸어도 좋은데요. ” “애셔 부인의 경력은 알고 있소?” “그녀는 햄프셔 태생으로 처녀 때 런던에 나가 직장 생활을 했지요. 거기서 애셔를 만나 결혼했습니다.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/106       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/105       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/104       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/103       \n",
            "Negative tokens: ['그는' '너무' '그의' '수고하시는' '이' '없다면' '“그녀는']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 104/105       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 103/104       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 102/103       \n",
            "Peak count: 6\n",
            "Frame tokens: 말입니다. 그는 버렸지요. 일치겠지만요. 잡혔다. 결혼했습니다. \n",
            "\n",
            "Similarity : 0.438711912758314\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.581496238708496 Generator / grammar loss:-0.20983782410621643   similarity loss:-0.1499611884355545\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "그야말로 행동하려는 게 아니었다면 말입니다. 그는 버렸지요. 일치겠지만요. 들어맞으니 잠시 주름이 잡혔다. 대체 도움이 뭐 수고하시는 말입니다. 결혼했습니다.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio    intro      body    ending      var    total  \\\n",
            "0  SAM+WGAN    0.195991  0.59957  0.450424  0.490647  0.00397  0.49232   \n",
            "\n",
            "    grammar  \n",
            "0  0.983096  \n",
            "Current result ==================================================\n",
            "Sample count: 23\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.174821  0.508532  0.526314    0.535823  0.004659      0.52561   \n",
            "\n",
            "   grammarity  \n",
            "0    0.965708  \n",
            "==================================================\n",
            "24 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "헤어진 것은 1922년으로, 그즈음 두 사람은 아직 런던에 있었지요. 그녀는 남자에게서 달아나 여기로 왔으나, 남자가 곧 알아차리고 따라와 귀찮게 굴었던 겁니다. ” 마침 거기에 순경이 들어왔다. “무슨 일인가, 브릭스?” “애셔를 연행해 왔습니다. ” “좋아. 이리로 데려오게. 어디 있던가?” “인입선의 화차 안에 숨어 있었습니다. ” “숨어 있었다고? 데려오게. ” 프란츠 애셔는 정말 보기 싫은, 초라한 인간의 표본이었다. 그는 엉엉 울고, 꾸벅꾸벅 절하고, 서슬이 시퍼래지기도 했다. 그 짓무른 눈을 이리저리 움직이며 모두들의 얼굴을 살폈다 “나를 어쩌자는 거야. 나는 아무 짓도 안 했어. 날 이런 데 데려오다니 너무하잖아. 네 놈들은 돼지야. 어쩌자는 거야?” 그의 태도가 갑자기 바뀌었다. “아니, 아니, 그게 아냐. 선생님들은 이 가엾은 늙은이에게 몹쓸 짓을 하고 있소. 심하게 대하고 있소. 누구나 이 가엾은 프란츠에게 심하게 군단 말야, 이 가엾은 프란츠에게.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/120       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/119       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/118       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/117       \n",
            "Negative tokens: ['그즈음' '왔으나,' '”' '꾸벅꾸벅' '모두들의' '이런' '심하게' '심하게' '말야,']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 118/119       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 117/118       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 116/117       \n",
            "Peak count: 7\n",
            "Frame tokens: 굴었던 데려오게. 그는 그 너무하잖아. 돼지야. 프란츠에게. \n",
            "\n",
            "Similarity : 0.49776683362693164\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.413719892501831 Generator / grammar loss:-0.1954096555709839   similarity loss:-0.1534319370985031\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "왔으나, 굴었던 겁니다. 마침 연행해 있던가?” 데려오게. 그는 그 거야. 너무하잖아. 돼지야. “아니, 아냐. 이 몹쓸 있소. 있소. 가엾은 가엾은 프란츠에게.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body   ending       var     total  \\\n",
            "0  SAM+WGAN    0.182927  0.488805  0.612953  0.68741  0.006711  0.610461   \n",
            "\n",
            "    grammar  \n",
            "0  0.976715  \n",
            "Current result ==================================================\n",
            "Sample count: 24\n",
            "     method  comp rate    intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.175158  0.50771  0.529924    0.542139  0.004744     0.529146   \n",
            "\n",
            "   grammarity  \n",
            "0    0.966167  \n",
            "==================================================\n",
            "25 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "” 애셔는 울기 시작했다. 형사가 말했다. “그만해 두오, 애셔. 정신차려요. 당신에게 무슨 죄를 뒤집어씌우려는 건 아니오. 지금으로서는. 당신이 싫으면 아무 말 않아도 좋소. 만일 당신이 당신 아내 살해에 관계가 없다면 말이오. ” 애셔는 그 말을 가로막았다. 그 목소리는 비명 같았다. “나는 죽이지 않았어! 죽이지 않았어! 모두 엉터리야! 네 놈들은 거지같은 영국 돼지야. 모두들 내게 죄를 덮어씌우고 있어. 나는 죽이지 않았어, 죽이지 않았어. ” “당신은 늘 아내를 협박하고 있었잖소, 애셔?” “아니, 아니, 네 놈들은 알 리 없어. 그건 농담이었어. 나와 앨리스만이 알고 있는 농담이야. 앨리스는 그걸 알고 있었어.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/88       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/87       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/86       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/85       \n",
            "Negative tokens: ['“나는']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 86/87       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 85/86       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 84/85       \n",
            "Peak count: 5\n",
            "Frame tokens: 정신차려요. 지금으로서는. ” 모두 있었어. \n",
            "\n",
            "Similarity : 0.4111031955373643\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.489774227142334 Generator / grammar loss:-0.20413881540298462   similarity loss:-0.15414553880691528\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "정신차려요. 무슨 건 지금으로서는. ” 목소리는 모두 돼지야. ” “당신은 협박하고 “아니, 그건 앨리스는 있었어.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.182336  0.548421  0.536417  0.484819  0.000761  0.523338   \n",
            "\n",
            "   grammar  \n",
            "0  0.99311  \n",
            "Current result ==================================================\n",
            "Sample count: 25\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.175445  0.509338  0.530184    0.539846  0.004585     0.528913   \n",
            "\n",
            "   grammarity  \n",
            "0    0.967244  \n",
            "==================================================\n",
            "26 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "” “우스운 농담이로군! 어젯밤 어디 있었는지 말할 수 있소, 애셔?” “말할 수 있고말고, 있고말고. 모두 이야기하지. 난 앨리스한테 가지 않았어. 친구들하고 있었어. 멋있는 친구들하고. <세븐 스타즈>에 있다가……그리고 나서 <레드 독>에 갔어. ” 그는 기침이 나와 말이 막혔다. “딕 윌러즈, 그도 함께 있었지. 커디 녀석도 그리고 조지도……플랫도, 그 밖의 놈들도 많이 있었어. 나는 앨리스에게 가지 않았어. 하느님께 맹세코 나는 사실을 말하고 있어. ” 그 소리는 비명이었다. 형사는 부하에게 눈짓을 했다. “데려가. 용의자를 구금시켜. ” 떨며 욕지거리를 퍼부어대는 그 불쾌한 노인이 나가 버리자 형사는 말했다. “아무래도 알 수 없군요. 그 편지만 없다면 저 늙은이의 짓이 분명한데요. ” “저 사람이 말하는 다른 남자들은 어떻소?” “나쁜 놈들입니다.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/104       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/103       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/102       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/101       \n",
            "Negative tokens: ['“아무래도']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 102/103       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 101/102       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 100/101       \n",
            "Peak count: 6\n",
            "Frame tokens: 모두 친구들하고 있었지. “데려가. 없군요. 놈들입니다. \n",
            "\n",
            "Similarity : 0.5168125375589397\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.445662021636963 Generator / grammar loss:-0.1838645339012146   similarity loss:-0.1385379284620285\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "농담이로군! 어젯밤 애셔?” 모두 이야기하지. 친구들하고 친구들하고. “딕 있었지. 조지도……플랫도, 나는 말하고 비명이었다. “데려가. 없군요. 분명한데요. “나쁜 놈들입니다.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.230769  0.684963  0.614082  0.526735  0.004188  0.602054   \n",
            "\n",
            "    grammar  \n",
            "0  0.972748  \n",
            "Current result ==================================================\n",
            "Sample count: 26\n",
            "     method  comp rate     intro     body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.177573  0.516093  0.53341    0.539342  0.004569     0.531726   \n",
            "\n",
            "   grammarity  \n",
            "0    0.967456  \n",
            "==================================================\n",
            "27 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "모두 위증쯤은 손쉽게 할 녀석들이지요. 나도 저 늙은이가 그날 밤 어느 시간까지는 그들과 함께 있었다고 생각합니다. 그러니 6시 사이에 가게 언저리에서 저 늙은이를 본 사람이 있는지 없는지에 달렸다고 봐야겠지요. “ 포아로는 신중하게 머리를 저었다. “가게에서 아무것도 없어지지 않은 건 분명하지요?” 형사는 어깨를 으쓱했다. “그야 경우에 따라 다르겠지요. 담배 한두 갑이 없어졌는지도 모릅니다. 그러나 아무도 그런 일 때문에 사람을 죽이지는 않지요. ” “게다가 아무것도, 뭐라면 좋을까. 가지고 온 것이 없었다는, 그러니까 이상한, 그 장소에 어울리지 않는 그런 아무것도 거기에는 없었다는 거지요?” “철도 안내서가 있었습니다. ” “철도 안내서?” “그렇습니다. 계산대 위에 펼쳐진 채 뒤집혀 있었습니다. 꼭 누군가가 앤도버에서 떠나는 기차 편을 알아보고 있었던 것처럼. 그 할머니나 아니면 손님이 보고 있었다는 것이겠지요. ” “그런 것도 팔고 있었소?” 형사는 머리를 저었다. “1페니짜리 시간표를 팔고 있었습니다만, 그것은 큰 것이었으니까 스미스네 가게나 커다란 문방구점 같은 데서 다룰 겁니다. ” 포아로는 눈을 빛내며 몸을 앞으로 내밀었다. “철도 안내서라고 말했지요? <브레드쇼>던가요, <ABC>던가요?” 그러자 형사의 눈도 빛나기 시작했다. “정말, 그러고 보니 ABC였습니다.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/154       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/153       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/152       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/151       \n",
            "Negative tokens: ['할' '“가게에서' '그런' '어울리지' '안내서?”' '시간표를' '가게나' '문방구점']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 152/153       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 151/152       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 150/151       \n",
            "Peak count: 8\n",
            "Frame tokens: 봐야겠지요. “그야 모릅니다. 좋을까. 있었습니다. 계산대 “철도 ABC였습니다. \n",
            "\n",
            "Similarity : 0.4040819737029029\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.4576776027679443 Generator / grammar loss:-0.15830597281455994   similarity loss:-0.11171320825815201\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "위증쯤은 나도 그날 있었다고 언저리에서 달렸다고 봐야겠지요. 아무것도 분명하지요?” “그야 담배 한두 모릅니다. 아무도 “게다가 좋을까. 안내서가 있었습니다. “철도 “그렇습니다. 계산대 누군가가 “철도 ABC였습니다.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio    intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.181818  0.43342  0.536456  0.417277  0.002787  0.480095   \n",
            "\n",
            "    grammar  \n",
            "0  0.957059  \n",
            "Current result ==================================================\n",
            "Sample count: 27\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN    0.17773  0.513031  0.533523    0.534821  0.004503     0.529814   \n",
            "\n",
            "   grammarity  \n",
            "0    0.967071  \n",
            "==================================================\n",
            "28 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "” < 조카딸의 이야기 > 이 사건에 대한 내 관심은 ABC 철도 안내서가 나왔을 때 비로서 일기 시작했다고 생각된다. 그때까지 나는 이 사건에 그리 열중하고 있지 않았다. 뒷골목의 노파 살해 같은 시시한 사건은 날마다 신문에 보도되는 흔해빠진 범죄여서 거의 주의를 끌지 못했던 것이다. 나는 마음속으로 익명 편지가 21일이라는 날짜를 지정한 일 따위는 우연의 일치에 지나지 않는다고 생각하고 있었다. 당연히 애셔 부인은 그 남편이 술에 취한 나머지 폭력을 휘둘러 희생된 것으로 여겼다. 그런데 지금 철도 안내서(철도역을 알파벳 순서로 나열했기 때문에 ABC라는 준말로 알려져 있음)가 등장하자 내 온몸에는 흥분의 전율이 일었다. 확실히 이것은 우연의 일치 같은 것 일 리 없다. 시시한 범죄가 새로운 양상을 띠기 시작했다. 애셔 부인을 살해하고 ABC 철도 안내서를 남기고 사라진 신비의 인간은 대체 누구인가? 경찰서를 나와 우리는 먼저 살해된 여자의 시체를 보러 시체 안치소로 갔다. 얼마 안 되는 머리칼을 이마 위로 가지런히 빗어 넘긴 노파의 주름잡힌 얼굴을 보고 있는 동안, 나는 이상한 느낌이 들기 시작했다. 너무나 평화로워 폭력 같은 것과는 거리가 먼 느낌이었다. 경관이 말했다. “누가 무엇으로 자기를 때렸는지 조금도 모르는 얼굴입니다. 카 의사가 그렇게 말하더군요. 오히려 그게 잘된 일이라고 생각합니다. 가엾게도, 깔끔한 사람이었는데. ” 포아로가 말했다. “옛날엔 아름다웠을 것 같군. ” 나는 믿을 수 없는 마음이 들어 중얼거렸다.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/192       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/191       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/190       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/189       \n",
            "Negative tokens: ['그리' '살해' '시시한' '우연의' '나머지' '때문에' '우연의' '시시한' '신비의' '느낌이' '모르는' '믿을'\n",
            " '마음이']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 190/191       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 189/190       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 188/189       \n",
            "Peak count: 10\n",
            "Frame tokens: 생각된다. 범죄여서 지나지 당연히 시작했다. 경관이 오히려 사람이었는데. “옛날엔 중얼거렸다. \n",
            "\n",
            "Similarity : 0.4402707510619227\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.49914288520813 Generator / grammar loss:-0.1710434854030609   similarity loss:-0.12005233764648438\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "> 이 사건에 비로서 일기 시작했다고 생각된다. 일었다. 애셔 인간은 우리는 안치소로 얼마 머리칼을 보고 시작했다. 거리가 먼 경관이 얼굴입니다. 그렇게 말하더군요. 오히려 그게 생각합니다. 사람이었는데. ” “옛날엔 없는 중얼거렸다.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN     0.17328  0.423467  0.394591  0.586685  0.007153  0.457995   \n",
            "\n",
            "    grammar  \n",
            "0  0.982156  \n",
            "Current result ==================================================\n",
            "Sample count: 28\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.177572  0.509833  0.528561    0.536673  0.004598     0.527249   \n",
            "\n",
            "   grammarity  \n",
            "0     0.96761  \n",
            "==================================================\n",
            "29 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "“그럴까. ‘ “그렇네. 자, 턱의 선이며 뼈 모양이며 머리 생김새를 잘 보게. ” 그는 덮개를 본래대로 해두면서 한숨을 쉬었다. 그리고 나서 우리는 시체 안치소를 나왔다. 다음에는 경찰의와 간단히 면담했다. 카 의사는 유능해 보이는 중년 사나이였다. 그는 활발하게 단정적인 말투로 이야기했다. “흉기는 발견되지 않았습니다. 그것이 무엇이었는지는 알 수 없지요. 무거운 지팡이, 몽둥이, 모래주머니 같은 것……그런 거라면 어느 것이나 들어맞습니다. ” “그런 타격을 가하려면 억센 힘이 필요합니까?” 의사는 날카로운 눈으로 포아로를 보았다. “그 말뜻은 몸을 떨어대는 70살의 노인으로서도 할 수 있느냐는 거지요? 네, 물론 할 수 있습니다. 흉기의 머리 부분에 충분한 무게를 주면 체력이 약한 사람도 바라는 결과를 얻을 수 있습니다. ” “그렇다면 범인은 남자일 수 있는 것과 마찬가지로 여자일 수도 있군요?” 이 말은 얼마쯤 의사를 놀라게 한 모양이었다. “여자도? 네, 그렇습니다. 이런 종류의 범죄를 여자와 관련시켜 생각해 볼 마음은 없었습니다만, 물론 할 수 있습니다. 완전히 가능합니다.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/136       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/135       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/134       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/133       \n",
            "Negative tokens: ['그리고' '유능해' '그는' '“흉기는' '그것이' '무거운' '네,']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 134/135       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 133/134       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 132/133       \n",
            "Peak count: 7\n",
            "Frame tokens: “그렇네. 들어맞습니다. 물론 있습니다. “여자도? 수 가능합니다. \n",
            "\n",
            "Similarity : 0.46815005986550795\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.1196718215942383 Generator / grammar loss:-0.12390576303005219   similarity loss:-0.11192426830530167\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            " “그렇네. ” 그는 나왔다. 경찰의와 의사는 유능해 이야기했다. 무엇이었는지는 없지요. 모래주머니 같은 것……그런 것이나 들어맞습니다. 물론 있습니다. “여자도? 네, 그렇습니다.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.181004  0.557482  0.604389  0.668609  0.002075  0.614274   \n",
            "\n",
            "    grammar  \n",
            "0  0.983259  \n",
            "Current result ==================================================\n",
            "Sample count: 29\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN    0.17769  0.511476  0.531176    0.541223  0.004511      0.53025   \n",
            "\n",
            "   grammarity  \n",
            "0    0.968149  \n",
            "==================================================\n",
            "30 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "다만 심리적으로 말해서, 이건 여성의 범죄라고 할 수 없지요. ” 포아로도 그 말에 동의하여 열심히 고개를 끄덕였다. “그렇습니다. 그렇습니다. 확실히 있을 수 없는 일입니다. 그러나 모든 가능성을 염두해 두지 않으면 안 되니까요. 시체는 쓰러져 있었겠지요. 어떤 모습이었습니까?” 의사는 피해자의 위치를 세밀하게 우리에게 설명했다. 그의 말에 의하면, 타격이 주어졌을 때 그녀는 계산대 쪽으로 등을 돌리고―따라서 가해자에 대해서도―서 있었다고 한다. 머리를 얻어맞고 그녀는 계산대 뒤로 쭈그려 앉아 버려 가게에 들어온 사람 눈에 얼른 띄지 않았던 셈이다. 카 의사에게 인사하고 밖으로 나오자 포아로가 말했다. “이로써 애셔의 무죄 쪽으로 한 걸음 다가선 게 확실하네. 헤이스팅즈. 만일 그가 아내한테 덤벼들면서 협박한 거라면 그녀는 계산대를 사이에 두고 그와 마주서 있었을 걸세. 그런데 그녀는 가해자에게 등을 돌리고 있었지. 틀림없이 그녀는 손님에게 줄 파이프 담배나 궐련을 꺼내려 했던 걸 거야. ” 나는 조금 몸을 떨었다. “기분이 언짢군. ” 포아로는 무겁게 머리를 흔들었다. 그는 중얼거렸다.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/137       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/136       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/135       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/134       \n",
            "Negative tokens: ['”' '돌리고―따라서' '쭈그려' '틀림없이']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 135/136       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 134/135       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 133/134       \n",
            "Peak count: 7\n",
            "Frame tokens: ” 셈이다. “이로써 헤이스팅즈. 걸세. “기분이 중얼거렸다. \n",
            "\n",
            "Similarity : 0.4277358067870861\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.551668643951416 Generator / grammar loss:-0.18061436712741852   similarity loss:-0.12398082762956619\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "이건 수 없지요. ” 그렇습니다. 그녀는 나오자 말했다. “이로써 걸음 헤이스팅즈. 협박한 걸세. 돌리고 있었지. 그녀는 손님에게 했던 “기분이 중얼거렸다. \n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.156863  0.778482  0.445023  0.495067  0.021558  0.526728   \n",
            "\n",
            "   grammar  \n",
            "0  0.98074  \n",
            "Current result ==================================================\n",
            "Sample count: 30\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.176996  0.520376  0.528304    0.539684  0.005079     0.530133   \n",
            "\n",
            "   grammarity  \n",
            "0    0.968569  \n",
            "==================================================\n",
            "31 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "“가엾은 여자일세. ” 그리고 나서 그는 시계를 흘끗 보았다. “여기서 오버튼까지는 그리 멀지 않네. 거기 가서 노파의 조카딸을 만나 보는게 어떻겠나?” “범행 현장인 가게 쪽을 먼저 보는 게 좋지 않을까?” “그건 뒤로 미루고 싶네. 이유가 있어서. ” 그는 더 이상 설명하지 않았다. 잠시 뒤 우리는 자동차를 타고 오버튼 쪽으로 런던 행 도로를 달려갔다. 형사가 가르쳐 준 집은 마을에서 런던 쪽으로 1마일쯤 간 곳에 있었다. 훌륭한 집이었다. 벨을 누르자 아름다운 검은 머리의 아가씨가 나왔다. 지금까지 울고 있었던 듯 눈이 빨갰다. 포아로가 상냥하게 말했다. “아, 당신이 이 집 하녀인 메리 드로워 양이군요?” “그렇습니다. 제가 메리예요. ” “주인께서 허락해 주신다면 잠시 이야기를 좀 나누고 싶은데요. 이야기란 다름아닌 아가씨 아주머니인 애셔 부인에 대한 것입니다. ” “주인은 외출중이세요. 들어오셔도 그리 꾸중이 없으리라 생각됩니다. ” 그녀는 조그만 거실의 문을 열었다. 우리는 안으로 들어갔다.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/128       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/127       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/126       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/125       \n",
            "Negative tokens: ['오버튼까지는' '거기' '먼저' '잠시' '오버튼' '상냥하게' '하녀인' '부인에']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 126/127       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 125/126       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 124/125       \n",
            "Peak count: 7\n",
            "Frame tokens: ” 집이었다. 드로워 ” 아주머니인 외출중이세요. 들어갔다. \n",
            "\n",
            "Similarity : 0.4683068083281646\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.582400321960449 Generator / grammar loss:-0.20313237607479095   similarity loss:-0.14315694570541382\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "” 시계를 보는게 좋지 “그건 싶네. ” 그는 집은 마을에서 1마일쯤 있었다. 집이었다. 드로워 제가 메리예요. ” “주인께서 아주머니인 “주인은 외출중이세요.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.173828  0.561834  0.503197  0.526027  0.000582  0.521773   \n",
            "\n",
            "    grammar  \n",
            "0  0.990423  \n",
            "Current result ==================================================\n",
            "Sample count: 31\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.176893  0.521713  0.527494    0.539243  0.004934     0.529863   \n",
            "\n",
            "   grammarity  \n",
            "0    0.969274  \n",
            "==================================================\n",
            "32 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "포아로는 창가 의자에 앉아 날카롭게 아가씨의 얼굴을 보았다. “아주머니가 돌아가신 이야기는 물론 들었겠지요?” 아가씨는 고개를 끄덕였는데 눈물이 다시 새삼스럽게 솟아났다. “오늘 아침 경찰에서 오셨었어요. 아, 무서운 일이에요!가엾은 아주머니! 그토록 괴로운 나날을 보내고서 또 이런 일을 당하시다니……너무해요. ” “경찰이 앤도버로 오라고 하지 않았습니까?” “월요일에 심문을 받기로 되어 있어요. 하지만 저는 그리로 가면 있을 데가 없어요. 이젠 그 가게로 갈 수도 없고. 게다가 저 말고는 하녀가 없는데 주인에게 폐 끼치고 싶지도 않아요. ” 포아로는 부드럽게 물었다. “당신은 아주머니를 아주 좋아했었군요, 메리 양?” “정말 좋아했어요. 아주머니는 언제나 제게 잘해 주셨지요. 어머니가 돌아가신 뒤 저는 11살 때 런던의 아주머니 집으로 갔어요. 16살 때부터 돈벌이를 하러 나와 있었지만, 쉬는 날이면 꼭 아주머니에게 가곤 했어요. 아주머니는 그 독일사람 때문에 아주 애를 먹고 계셨어요. 그 남자를 아주머니는 늘 <나의 악마>라고 부르곤 하셨지요. 그는 아주머니가 있는 데는 어디든 와서 가만히 두지 않았어요. 돈만 빼앗아 가는 거지같은 짐승이에요. ” 아가씨의 말투는 아주 격렬했다.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/145       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/144       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/143       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/142       \n",
            "Negative tokens: ['아가씨의' '“아주머니가' '아가씨는' '눈물이' '아주머니!' '하녀가' '아주머니를' '아주머니는' '어머니가' '아주머니'\n",
            " '아주머니에게' '아주머니는' '아주머니가' '두지']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 143/144       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 142/143       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 141/142       \n",
            "Peak count: 8\n",
            "Frame tokens: 일이에요!가엾은 괴로운 당하시다니……너무해요. 주인에게 아주머니는 애를 ” 격렬했다. \n",
            "\n",
            "Similarity : 0.44384050066230163\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.351470470428467 Generator / grammar loss:-0.1929520070552826   similarity loss:-0.1574362963438034\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "의자에 날카롭게 아가씨의 “아주머니가 돌아가신 아가씨는 아침 일이에요!가엾은 괴로운 나날을 당하시다니……너무해요. 이젠 수도 없고. 주인에게 물었다. 하러 아주머니는 애를 먹고 짐승이에요. ” 격렬했다. \n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.185366  0.505792  0.499096  0.552723  0.000569  0.516523   \n",
            "\n",
            "    grammar  \n",
            "0  0.982917  \n",
            "Current result ==================================================\n",
            "Sample count: 32\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.177158  0.521216  0.526607    0.539665  0.004798     0.529446   \n",
            "\n",
            "   grammarity  \n",
            "0      0.9697  \n",
            "==================================================\n",
            "33 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "“아주머니는 법적 수단으로 그 남자의 압박에서 벗어나려고는 하지 않았습니까?” 아가씨는 단순하게, 그러나 딱 잘라 말했다. “아무래도 남편이었기 때문에 그럴 수가 없었지요. ” “메리 양, 그 남자는 아주머니를 협박했었지요?” “네, 아주 무서운 소리를 곧잘 했어요. 목을 부러뜨린다든가 하는 말들을. 저주스럽게 욕지거리를 해대면서. 독일 말과 영어 두 가지로요. 그렇지만 아주머니는 결혼했던 즈음에는 아주 멋있는 남자였다고 말씀하셨어요. 사람이 그렇게 된다는 것은 참으로 무서운 일이에요. ” “정말 그렇군요. 그런데 메리 양, 늘 그런 협박을 받고 있었다면 사건이 일어난 것을 알았을 때 그리 놀라지 않았겠군요?” “그래도 역시 놀랐어요. 아무튼 진짜로 하는 소리라고는 생각지 않았으니까요. 그저 말로만 해대는 것뿐 그 이상으로는 여기지 않았어요. 아주머니도 무서워하고 계셨던 것 같지 않아요. 아주머니가 대들면 개가 다리 사이로 꼬리를 감추듯 움츠러드는 것을 본 적도 있어요. 오히려 그쪽에서 아주머니를 무서워하고 있을 정도였지요. ” “그런데도 아주머니는 돈을 주고 있었습니까?” “남편인걸요. ” “그렇군요, 아까도 그렇게 말했었지요. ” 포아로는 잠시 말을 끊었다가 다시 계속했다. “그렇다면 결국 그 남자는 아주머니를 죽이지 않았다는 거로군요?” “죽이지 않았다고요?” 그녀는 눈을 크게 떠보였다. “그렇습니다.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/156       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/155       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/154       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/153       \n",
            "Negative tokens: ['“아무래도' '아주머니를' '아주머니는' '무서운' '그런데' '그런' '있었다면' '놀라지' '소리라고는' '아주머니도' '”'\n",
            " '잠시' '아주머니를']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 154/155       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 153/154       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 152/153       \n",
            "Peak count: 8\n",
            "Frame tokens: 했어요. 저주스럽게 아무튼 무서워하고 아주머니가 아주머니를 ” “그렇습니다. \n",
            "\n",
            "Similarity : 0.4784850659778326\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.4035513401031494 Generator / grammar loss:-0.1737399697303772   similarity loss:-0.1328233927488327\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "그 아가씨는 ” 아주머니를 협박했었지요?” 했어요. 저주스럽게 영어 두 가지로요. 그렇지만 즈음에는 일이에요. 놀라지 “그래도 역시 놀랐어요. 아무튼 아주머니도 무서워하고 아주머니가 아주머니를 ” “그렇습니다.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro     body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.170803  0.448726  0.48266  0.506591  0.000564  0.483053   \n",
            "\n",
            "    grammar  \n",
            "0  0.951992  \n",
            "Current result ==================================================\n",
            "Sample count: 33\n",
            "     method  comp rate     intro      body  conclusion  isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.176966  0.519019  0.525275    0.538662  0.00467      0.52804   \n",
            "\n",
            "   grammarity  \n",
            "0    0.969164  \n",
            "==================================================\n",
            "34 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "누군가 다른 사람이 아가씨 아주머니를 죽였다는 말입니다. ……달리 짐작되는 사람 없습니까?” 그녀는 한층 더 놀란 듯 그의 얼굴을 보았다. “알 수 없어요. 하지만 그런 일이 있을 수 있을까요?” “당신 아주머니가 무서워한 다른 사람은 없었습니까?” 메리는 고개를 저었다. “아주머니는 남을 무서워하지 않으셨어요. 말솜씨가 좋아 누구에게나 맞설 수 있으셨어요. ” “아주머니에게 악의를 품고 있는 어떤 사람에 대한 이야기를 해준 일은 없습니까?” “네, 없어요. ” “익명의 편지를 받은 일도” “무슨 편지라고요?” “개인적인 서명이 없는 편지로, 예를 들어 그저 ABC라는 서명만 있는. ” 그는 아가씨의 얼굴을 찬찬히 들여다보고 있었는데, 그녀는 분명 난처해하는 모습이었다. 그녀는 묘한 표정으로 고개를 저었다. “아가씨 말고 또 다른 친척이 있습니까?” “지금은 없어요. 열 남매였는데 자란 사람은 셋뿐이었지요. 톰 아저씨는 전쟁터에서 돌아가시고, 해리 아저씨는 남아메리카로 가버리셔서 소식을 몰라요. 그리고 또 제 어머니는 돌아가셨기 때문에 저밖에 없어요. ” “아주머니는 저축을 했었습니까? 돈을 모으고 있었습니까?” “은행에 조금 있어요. 매장 비용만 된다면 하고 곧잘 말씀하곤 하셨지요. 그리고는 겨우 그럭저럭 살아 나가셨어요. 그 늙어빠진 악마가 있으니 안 그렇겠어요. ” 포아로는 생각 깊게 고개를 끄덕였다.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/160       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/159       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/158       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/157       \n",
            "Negative tokens: ['아주머니를' '없습니까?”' '놀란' '아주머니가' '없었습니까?”' '“아주머니는' '“아주머니에게' '어떤' '없습니까?”'\n",
            " '없는' '아가씨의' '그녀는' '그녀는' '“아가씨' '돌아가시고,' '어머니는' '“아주머니는' '했었습니까?' '조금'\n",
            " '겨우' '늙어빠진' '안' '생각']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 158/159       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 157/158       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 156/157       \n",
            "Peak count: 9\n",
            "Frame tokens: 없어요. 무서워한 저었다. 저었다. 말고 없어요. 몰라요. 없어요. 끄덕였다. \n",
            "\n",
            "Similarity : 0.4032389159120163\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.404448986053467 Generator / grammar loss:-0.17271101474761963   similarity loss:-0.13170084357261658\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "누군가 사람이 없어요. “당신 무서워한 저었다. “네, ” 저었다. 말고 친척이 없어요. 사람은 전쟁터에서 남아메리카로 몰라요. 또 저밖에 없어요. “아주머니는 있었습니까?” 조금 있어요. 늙어빠진 안 ” 끄덕였다.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body   ending       var     total  \\\n",
            "0  SAM+WGAN    0.175182  0.517364  0.484054  0.55897  0.000939  0.513191   \n",
            "\n",
            "    grammar  \n",
            "0  0.967144  \n",
            "Current result ==================================================\n",
            "Sample count: 34\n",
            "     method  comp rate    intro      body  conclusion  isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.176913  0.51897  0.524063     0.53926  0.00456     0.527603   \n",
            "\n",
            "   grammarity  \n",
            "0    0.969104  \n",
            "==================================================\n",
            "35 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "그는 아가씨에게 말한다기 보다 혼잣말처럼 중얼거렸다. “지금으로선 어둠 속에 있는 것 같군. 방향도 잡을 수 없어. 만일 좀더 뚜렷해진다면……. ” 그는 일어섰다. “만일 아가씨한테 볼일이 생기면 여기로 편지하지요. 메리 양. ” “사실을 말씀드리면, 저는 여기를 나갈 생각으로 있어요. 시골을 그리 좋아하지 않거든요. 아주머니 곁에 있는 게 마음 든든히 여겨져 여기 있었던 거예요. 그러나 이젠……. ” 그 눈에 다시 눈물이 솟았다. “이제는 여기 있을 이유가 없어져 런던으로 되돌아가려고 해요. 그곳이 제게는 더 재미있는 걸요. ” “그럼, 그리고 가게 될 때에는 주소를 가르쳐 주십시오. 이것이 제 명함입니다. ” 그는 아가씨에게 명함을 건네주었다. 그녀는 곤혹스러운 듯 이마에 주름을 지으며 그것을 보았다. “그럼, 선생님은……경찰과는 관계가 없으신가요?” “나는 사립탐정입니다. ” 그녀는 선 채로 잠시 말없이 그를 쳐다보았다.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/113       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/112       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/111       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/110       \n",
            "Negative tokens: ['만일' '말씀드리면,' '“이제는' '될' '”' '그녀는' '주름을']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 111/112       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 110/111       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 109/110       \n",
            "Peak count: 6\n",
            "Frame tokens: 중얼거렸다. 만일 “만일 이젠……. 그녀는 쳐다보았다. \n",
            "\n",
            "Similarity : 0.37830775411397843\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.497178554534912 Generator / grammar loss:-0.18152521550655365   similarity loss:-0.13074351847171783\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            " 중얼거렸다. 만일 “만일 생기면 편지하지요. 양. 말씀드리면, 저는 거예요. 이젠……. 눈에 제 그녀는 곤혹스러운 듯 주름을 쳐다보았다.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var    total  \\\n",
            "0  SAM+WGAN     0.16453  0.435842  0.495635  0.424313  0.000977  0.46228   \n",
            "\n",
            "    grammar  \n",
            "0  0.971305  \n",
            "Current result ==================================================\n",
            "Sample count: 35\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.176559  0.516595  0.523251    0.535976  0.004457     0.525737   \n",
            "\n",
            "   grammarity  \n",
            "0    0.969167  \n",
            "==================================================\n",
            "36 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "이윽고 그녀가 말했다. “뭔가 의심스러운 점이라고 있으신지요?” “그렇습니다, 아가씨. 좀 이상한 점이 있지요. 아마 앞으로 아가씨에게 도움 받을 일이 있을지도 모르겠습니다. ” “저는, 저는 무엇이든 하겠어요. 아주머니가 살해되시다니, 옳은 일이 아니니까요. ” 그것은 기묘한 표현이었다. 그러나 꽤 감동적이었다. 우리는 곧 자동차를 타고 앤도버로 돌아갔다. < 범행 현장 > 참극이 일어난 곳은 큰길에서 좁은 골목이었다. 애셔 부인의 가게는 그 중간쯤의 오른쪽에 있었다. 그 골목에 들어섰을 때, 포아로는 흘끗 시계를 보았다. 그래서 나는 그가 범행 현장으로 가는 시간을 지금까지 미룬 까닭을 알았다. 꼭 5시 30분이 되어 있었다. 그는 되도록 어젯밤의 상황을 재현하려 생각하고 있었던 것이다. 그러나 그것이 그의 목적이었다면 실패했다. 이 때 골목은 어젯밤의 그림자를 거의 전해주고 있지 않았다. 그곳에는 가난한 사람들 집에 섞여 조그만 가게가 몇 채 줄지어 있었다. 다른 때 같으면 이 언저리의 가난한 몇몇 사람들이 그곳을 오가고 또 찻길이나 보도 위에서는 몇 명의 아이들이 놀고 있을 뿐이었다. 그런데 이때는 많은 사람들이 쭉 둘러서서 집인지 가게를 보고 있었다.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/149       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/148       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/147       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/146       \n",
            "Negative tokens: ['“뭔가' '좀' '아마' '그러나' '<' '일어난' '흘끗' '그래서' '미룬' '어젯밤의' '거의' '때' '몇']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 147/148       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 146/147       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 145/146       \n",
            "Peak count: 8\n",
            "Frame tokens: 이상한 있지요. 모르겠습니다. 표현이었다. 감동적이었다. 골목이었다. 있었다. 있었다. \n",
            "\n",
            "Similarity : 0.4673684537888263\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.518789768218994 Generator / grammar loss:-0.18725760281085968   similarity loss:-0.1341657042503357\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "말했다. 아가씨. 앞으로 모르겠습니다. 하겠어요. 아주머니가 표현이었다. 감동적이었다. 타고 앤도버로 일어난 곳은 좁은 골목이었다. 오른쪽에 가난한 이 또 몇 있을 뿐이었다. 그런데 사람들이 있었다.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.184692  0.621264  0.565471  0.492331  0.002787  0.554687   \n",
            "\n",
            "    grammar  \n",
            "0  0.987242  \n",
            "Current result ==================================================\n",
            "Sample count: 36\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.176785  0.519503  0.524423    0.534763  0.004411     0.526541   \n",
            "\n",
            "   grammarity  \n",
            "0    0.969669  \n",
            "==================================================\n",
            "37 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "그것이 어느 집인지는 곧 알 수 있었다. 우리가 본 것은 한 사람이 살해된 곳을 아주 흥미롭게 보고 있는 여느 사람들의 무리였다. 가까이 다가감에 따라 확실히 그렇다는 것을 알 수 있었다. 블라인드를 내린 그을음 낀 듯한 구멍가게 앞에 젊은 순경이 애를 먹고 있는 듯한 얼굴로 서서 사람들에게 저리 가라고 딱딱하게 명령하고 있었다. 그는 동료의 도움을 받아 모여 있는 사라들을 해산시키기 시작했다. 꽤 많은 사람들이 불평스럽게 한숨을 쉬며 저마다 자기네 일로 돌아갔다. 그러나 곧 또 다른 사람들이 몰려와 살인 현장을 똑똑히 봐두려는 듯 그 자리를 다시 차지했다. 포아로는 사람들로부터 조금 떨어져 섰다. 그가 서 있는 곳에서는 문 위에 씌어진 글자를 똑똑히 볼 수 있었다. 포아로는 그것을 입속에서 되풀이했다. “A 애셔. 그렇지, 어쩌면……. ” 그는 말을 끊었다. “가세, 헤이스팅즈. 안으로 들어가 보세. ” 나는 기다리고 있던 바였다. 우리는 사람들을 헤치고 젊은 순경에게로 갔다. 포아로는 형사에게서 받아 둔 소개장을 내보였다. 순경은 머리를 끄덕이며 우리를 안으로 들여보내기 위해 문의 자물쇠를 열었다. 우리는 구경꾼들의 호기심에 찬 눈길을 받으며 안으로 들어갔다.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/155       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/154       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/153       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/152       \n",
            "Negative tokens: ['다가감에' '듯한' '앞에' '몰려와' '그가' '들어가' '받으며']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 153/154       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 152/153       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 151/152       \n",
            "Peak count: 8\n",
            "Frame tokens: 꽤 돌아갔다. 차지했다. 애셔. 안으로 바였다. 우리는 들어갔다. \n",
            "\n",
            "Similarity : 0.46150342566536984\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.3017630577087402 Generator / grammar loss:-0.16436952352523804   similarity loss:-0.1339610517024994\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "그것이 알 있는 사람들의 구멍가게 사람들에게 꽤 돌아갔다. 수 애셔. 끊었다. 안으로 들어가 보세. 나는 있던 바였다. 젊은 포아로는 머리를 끄덕이며 우리는 들어갔다. \n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.155887  0.468315  0.554569  0.680279  0.007575  0.575031   \n",
            "\n",
            "   grammar  \n",
            "0  0.95975  \n",
            "Current result ==================================================\n",
            "Sample count: 37\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.176221  0.518119  0.525238    0.538696  0.004497     0.527852   \n",
            "\n",
            "   grammarity  \n",
            "0    0.969401  \n",
            "==================================================\n",
            "38 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "블라인드가 내려져 있어 안은 어두웠다. 순경이 전등 스위치를 찾아내어 당겼다. 그러나 전구의 촉수가 낮아 안은 여전히 어두웠다. 나는 가게 안을 빙 둘러보았다. 지저분하고 좁은 곳으로 몇 권의 싸구려 잡지가 흩어져 있고 어제 신문에는 하루치 먼지가 쌓여 있었다. 계산대 뒤에는 천장까지 선반이 매어져 파이프 담배며 궐련 봉지가 놓여 있었다. 박하가 든 과자와 사탕병도 있었다. 흔해빠진 구멍가게로 다른 데에도 몇천 군데나 있는 그런 곳이었다. 순경은 느릿한 햄프셔 사투리로 상황을 설명했다. “거기 계산대 뒤에 웅크린 채 쓰러져 있었지요. 할머니는 자신이 습격당하는 것을 모르고 있었다고 의사 선생님이 말씀하셨습니다. 아마 선반으로 막 손을 내민 순간이었는지도 모르지요. ” “손에는 아무것도 없었소?” “없었습니다. 다만 곁에 <플레이어즈>꾸러미가 하나 떨어져 있었지요. ” 포아로는 고개를 끄덕였다. 그의 눈은 그 좁은 가게를 탐색하듯 둘러보았다. 아무것도 없다. “그런데 철도 안내서는 어디에?” “여기입니다. ” 순경은 계산대 위를 가리켰다. “바로 앤도버 있는 데가 펼쳐진 채 뒤집혀져 있었습니다.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/134       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/133       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/132       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/131       \n",
            "Negative tokens: ['안은' '가게' '계산대' '흔해빠진' '계산대' '모르고' '선반으로' '다만' '가게를' '계산대']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 132/133       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 131/132       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 130/131       \n",
            "Peak count: 7\n",
            "Frame tokens: 안을 둘러보았다. 흔해빠진 순경은 ” 둘러보았다. 있었습니다. \n",
            "\n",
            "Similarity : 0.3651038417381125\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.4270899295806885 Generator / grammar loss:-0.18229222297668457   similarity loss:-0.13891567289829254\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "어두웠다. 찾아내어 그러나 안을 빙 둘러보았다. 싸구려 있고 있었다. 흔해빠진 순경은 햄프셔 선반으로 ” 다만 가게를 둘러보았다. “그런데 순경은 계산대 가리켰다.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio   intro     body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.161634  0.5552  0.41831  0.486468  0.003123  0.466136   \n",
            "\n",
            "    grammar  \n",
            "0  0.971172  \n",
            "Current result ==================================================\n",
            "Sample count: 38\n",
            "     method  comp rate     intro      body  conclusion  isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.175837  0.519095  0.522424    0.537322  0.00446     0.526228   \n",
            "\n",
            "   grammarity  \n",
            "0    0.969448  \n",
            "==================================================\n",
            "39 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "런던 행 기차를 보고 있었던 것 같습니다. 그렇다면 그는 앤도버 사람이 아닙니다. 그러나 물론 철도 안내서는 살인과 관계없는 다른 사람이 잃어버리고 간 거라고 생각할 수도 있습니다. ” 내가 물어 보았다. “지문은?” 순경은 머리를 저었다. “곧바로 모두 조사해 보았지만 없었지요. ” 포아로가 물었다. “계산대에도?” “굉장히 많았습니다. 모두 함께 뒤섞여 뒤죽박죽되어 있었지요. ” “그 속에 애셔의 지문은?” “아직 알 수 없습니다. ” 포아로는 고개를 끄덕이고 나서 죽은 사람이 가게 안에서 살고 있었느냐고 물었다. “그렇습니다. 안쪽 문을 지나면 그곳으로 들어가게 됩니다. 함께 가드렸으면 좋겠습니다만, 저는 여기 있지 않으면 안 돼서……. ” 포아로는 문을 열고 들어갔다. 나도 그 뒤를 따라갔다. 가게 안은 부엌 딸린 조그만 거실로 되어 있었다. 그곳은 깨끗하게 정리되어 있었지만 음침한 느낌이 들었으며 가구도 거의 없었다. 벽난로 위에 사진이 몇 장 있었다. 내가 다가가서 들여다보자 포아로도 옆으로 왔다.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/127       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/126       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/125       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/124       \n",
            "Negative tokens: ['그러나' '다른' '”' '”' '내가']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 125/126       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 124/125       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 123/124       \n",
            "Peak count: 7\n",
            "Frame tokens: ” 보았다. ” 물었다. ” “그렇습니다. 왔다. \n",
            "\n",
            "Similarity : 0.4435005445789578\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.6929128170013428 Generator / grammar loss:-0.20594274997711182   similarity loss:-0.13366052508354187\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "런던 기차를 보고 있었던 살인과 관계없는 거라고 ” 보았다. “곧바로 ” 포아로가 물었다. ” 속에 안에서 살고 “그렇습니다. 들어가게 됩니다. 왔다.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.163107  0.592689  0.470296  0.533044  0.002497  0.513599   \n",
            "\n",
            "    grammar  \n",
            "0  0.978414  \n",
            "Current result ==================================================\n",
            "Sample count: 39\n",
            "     method  comp rate     intro      body  conclusion  isthmus  simlirality  \\\n",
            "0  SAM+WGAN    0.17551  0.520982  0.521088    0.537212  0.00441     0.525904   \n",
            "\n",
            "   grammarity  \n",
            "0    0.969678  \n",
            "==================================================\n",
            "40 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "사진은 모두 세 장이었다. 한 장은 오늘 오후에 만난 아가씨 메리 드로워의 싸구려 사진이었다. 그녀는 가장 좋은 옷을 입고 얼굴에 부자연스러운 미소를 떠올리고 있었다. 포즈를 취한 이런 사진은 표정을 엉망으로 만들기 때문에 스냅 사진 쪽이 훨씬 좋다. 두 번째 것은 더 고급스러운 것으로, 꽤 나이든 머리가 희끗희끗한 부인을 기교적으로 흐릿하게 찍은 사진이었다. 털목도리를 두르고 있었다. 나는 아마도 미스 로즈일 거라고 생각했다. 즉 애셔 부인에게 장사를 시작할 수 있도록 돈을 물려준 사람이다. 세 번째 사진은 아주 오래된 것으로 누렇게 빛이 바래 있었다. 얼마쯤 구식으로 보이는 것으로 팔짱낀 젊은 남녀가 찍혀있었다. 남자는 단춧구멍에 꽃을 꽂고 있으며, 전체적으로 고풍스러움이 느껴지는 딱딱한 사진이었다. 포아로가 말했다. “아마도 결혼 기념사진인 모양이군. 보게, 헤이스팅즈. 그녀는 아름다웠을 거라고 내가 말했잖나. ” 그 말대로였다. 시대에 뒤떨어진 머리 모양과 기묘한 옷 때문에 좀 이상해 보이긴 했지만 이목구비가 또렷하고 반듯한 아가씨의 아름다움은 의심할 바가 없었다. 나는 옆에 있는 다른 한 인물을 자세히 보았는데, 이 군인 같은 모습의 말쑥한 젊은이가 그 초라한 애셔였다고는 도저히 생각되지 않았다. 나는 그 곁눈질을 하는 주정꾼 노인과 피로에 지친 얼굴의 죽은 노파를 생각해 내고 세월의 무자비함에 몸을 떨었다. 그 거실로부터 2층의 두 방으로 층계가 이어져 있었다.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/180       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/179       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/178       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/177       \n",
            "Negative tokens: ['미소를' '나이든' '기교적으로' '로즈일' '오래된' '구식으로' '고풍스러움이' '기념사진인' '시대에' '애셔였다고는'\n",
            " '노파를']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 178/179       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 177/178       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 176/177       \n",
            "Peak count: 10\n",
            "Frame tokens: 그녀는 것으로, 털목도리를 미스 포아로가 그녀는 머리 했지만 아가씨의 있었다. \n",
            "\n",
            "Similarity : 0.28803418625150023\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.450047016143799 Generator / grammar loss:-0.15789952874183655   similarity loss:-0.11211127042770386\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "사진은 모두 세 장이었다. 장은 오후에 만난 메리 사진이었다. 그녀는 쪽이 좋다. 고급스러운 나이든 기교적으로 얼마쯤 단춧구멍에 내가 그 옷 아름다움은 보았는데, 나는 그 얼굴의 내고 무자비함에 이어져 있었다. \n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro     body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.163435  0.703184  0.35389  0.480912  0.020838  0.461855   \n",
            "\n",
            "   grammar  \n",
            "0   0.9854  \n",
            "Current result ==================================================\n",
            "Sample count: 40\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.175208  0.525537  0.516908    0.535804  0.004821     0.524303   \n",
            "\n",
            "   grammarity  \n",
            "0    0.970071  \n",
            "==================================================\n",
            "41 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "하나는 빈방으로 가구도 없고, 다른 하나는 죽은 노파의 침실이었다. 경찰이 조사한 뒤여서 그 흔적이 그대로 있었다. 침대에는 털이 빠진 낡은 담요가 두 장 있었다. 한 서랍에는 알뜰히 기워진 속옷 몇 벌, 또 한 서랍에는 요리책 종류, ≪녹색의 오아시스≫라는 제목의 표지가 달린 책, 번쩍거리는 싸구려 새 양말 한 컬레?그것은 번쩍거리는 싸구려였다?사기 그릇 장식 한 쌍?드레스덴 도자기로 된 깨어진 양치기며 파랑과 노랑점이 있는 개?나무못에 걸린 검은 레인코트와 털 자켓. 이러한 것들이 죽은 애셔 부인이 이 세상에 남긴 재산이었다. 무언가 개인적인 메모 같은 게 있었다 해도 경찰이 가져가 버렸을 것이다. 포아로가 중얼거렸다. “가엾게도. 자, 헤이스팅즈, 여기에는 이제 아무것도 없네. ” 다른 길로 나서자 그는 잠시 망설이더니 길을 건넜다. 바로 애셔 부인의 가게 맞은편에 야채 가게가 있었다. 안에 있는 물건보다 밖에 내놓은 물건이 더 많은 그런 종류의 가게였다. 포아로는 낮은 소리로 내게 몇 마디 일러두고 혼자 가게에 들어갔다. 나는 잠시 뒤 따라 들어갔다. 그는 막 상추를 사고 있는 중이었다. 나는 딸기를 1파운드 샀다. 포아로는 물건을 싸주는 뚱뚱한 아주머니와 큰소리로 이야기하고 있었다. “그 살인 사건이 일어난 곳이 바로 댁 맞은편이었군요. 이런 끔찍한 일이 있나. 얼마나 놀랐겠습니까!” 그 뚱뚱한 여자는 살인 사건 이야기에는 이제 질린 것 같았다.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/182       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/181       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/180       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/179       \n",
            "Negative tokens: ['없고,' '노파의' '뒤여서' '빠진' '한' '몇' '싸구려' '컬레?그것은' '싸구려였다?사기' '된' '애셔' '남긴'\n",
            " '망설이더니' '애셔' '내놓은' '“그' '질린']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 180/181       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 179/180       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 178/179       \n",
            "Peak count: 10\n",
            "Frame tokens: 빈방으로 “가엾게도. 없네. 잠시 포아로는 혼자 잠시 포아로는 맞은편이었군요. 같았다. \n",
            "\n",
            "Similarity : 0.35117239819280077\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.6093180179595947 Generator / grammar loss:-0.20423713326454163   similarity loss:-0.14130766689777374\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "빈방으로 번쩍거리는 양말 털 “가엾게도. 헤이스팅즈, 아무것도 없네. ” 그는 잠시 그런 포아로는 일러두고 혼자 나는 잠시 막 포아로는 뚱뚱한 살인 곳이 바로 맞은편이었군요. 얼마나 여자는 살인 사건 이야기에는 같았다.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var   total  \\\n",
            "0  SAM+WGAN    0.171348  0.459792  0.417501  0.322304  0.003306  0.3974   \n",
            "\n",
            "    grammar  \n",
            "0  0.980317  \n",
            "Current result ==================================================\n",
            "Sample count: 41\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.175114  0.523934  0.514483    0.530597  0.004784     0.521207   \n",
            "\n",
            "   grammarity  \n",
            "0    0.970321  \n",
            "==================================================\n",
            "42 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "그날은 그녀에게 있어 너무 길었던 모양이다. “이 법석거리는 구경꾼들을 어떻게 좀 할 수 없을까요? 대체 무엇을 그렇게 보는 것일까요?” 포아로가 말했다. “어젯밤에는 꽤 달랐을 테지요?아주머니는 범인이 가게로 들어가는 걸 보시지 못했습니까? 키가 큰 훌륭한 남자로 수염이 있었다지요? 러시아인이라든가 뭐 그렇다는 이야기던데요?” 여자가 날카롭게 돌아보았다. “뭐라고요? 러시아인이 했다고요?” “경찰이 체포했다던데요. ” “정말이에요?” 여자는 흥분해서 입이 가벼워졌다. “외국 사람인가요?” “그렇습니다. 나는 틀림없이 아주머니가 어젯밤 그 남자를 본 줄 알았지요. ” “아니, 그럴 기회가 없었어요. 그래요, 저녁 무렵의 한창 바쁜 때여서 일을 끝내고 돌아가는 사람들이 많이 비나가니까요. 키가 크고 수염이 난 훌륭한 남자라니……아니에요. 그런 사람이 이 언저리에 있었다고는 생각되지 않는데요. ” 그래서 내가 대사를 받았다. 나는 포아로에게 말했다. “실례지만, 당신이 잘못 들은 게 아닙니까? 키가 작고 얼굴빛이 검은 남자라고 나는 들었습니다만.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/119       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/118       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/117       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/116       \n",
            "Negative tokens: ['무엇을' '“어젯밤에는' '있었다지요?' '“뭐라고요?' '어젯밤' '”' '들은' '아닙니까?']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 117/118       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 116/117       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 115/116       \n",
            "Peak count: 7\n",
            "Frame tokens: 모양이다. “어젯밤에는 못했습니까? ” 비나가니까요. 않는데요. 들었습니다만. \n",
            "\n",
            "Similarity : 0.43839375681584547\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.445701837539673 Generator / grammar loss:-0.2131340652704239   similarity loss:-0.16780325770378113\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            " 그날은 길었던 모양이다. 말했다. “어젯밤에는 못했습니까? ” 나는 한창 비나가니까요. 난 이 언저리에 있었다고는 않는데요. 그래서 게 키가 들었습니다만. \n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro     body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.164486  0.631802  0.41652  0.600917  0.009034  0.514896   \n",
            "\n",
            "    grammar  \n",
            "0  0.991067  \n",
            "Current result ==================================================\n",
            "Sample count: 42\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.174861  0.526502  0.512151    0.532271  0.004885     0.521057   \n",
            "\n",
            "   grammarity  \n",
            "0    0.970815  \n",
            "==================================================\n",
            "43 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "” 그리하여 이 뚱뚱한 여자에다 여윈 남편과 쇳소리 내는 심부름꾼 아이까지 합쳐 재미있는 토론이 시작되었다. 키 작은 검은 얼굴의 남자가 네 사람이나 목격된 이야기가 나오고, 쇳소리 내는 심부름꾼 아이는 키가 큰 훌륭한 남자를 보았지만 그에게는 수염이 없었다고 유감스러운 듯 덧붙였다. 겨우 쇼핑이 끝나 우리는 거짓말을 한 채 그대로 가게를 나왔다. 나는 얼마쯤 비난을 섞어 물었다. “대체 그건 무슨 연극이었나, 포아로?” “나는 다만 낯선 사람이 저쪽 가게로 들어갔는지 어떤지 듣고 싶었던 것뿐일세. ” “그럼, 그렇게 물어보면 되잖나, 그런 엉터리 같은 소리 하지 말고. ” “아니, 자네가 말하는 것처럼 그냥 물어 보아서는 아무 대답도 얻을 수 없다네. 자네는 자신도 영국 사람이면서, 그냥 물어보는 질문에 반발하는 게 영국 사람의 기질이라는 걸 모르고 있는 모양이군. 그것은 반드시 의심하는 마음을 불러일으켜 결과는 완강한 침묵으로 끝난다네. 이 사람들에게 뭘 물어보게나, 그들은 조가비처럼 입을 다물어 버리지. 그렇지만 이상하고 터무니없는 어떤 말을 한 가지 꺼내 거기서 자네가 반대되는 말이라도 해보이면, 금방 이야기가 풀려나온다네. 그런 방법으로 우리는 문제의 시각이 바쁜 때였다는 것, 그래서 누구나 자기 일 말고는 신경 쓸 수 없으며 많은 사람이 길을 지나가고 있었던 때임을 알게 된 거야. 우리의 살인범은 좋은 시간을 택했다는 말이 되네, 헤이스팅즈. ” 그는 말을 끊었다. 그리고는 엄격하게 나무라는 듯한 말투로 덧붙였다. “자네는 상식이라는 걸 갖고 있지 않는 것 같군, 헤이스팅즈. 무엇이든 사라고 했더니 하필이면 딸기를 고르다니! 보게, 벌써 포장지에서 물이 배어 나와 그 좋은 옷을 버리게 하고 있잖나. ” 정말 그의 말대로였으므로 나는 좀 당황했다. 나는 급히 한 아이에게 딸기를 줘버렸다.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/230       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/229       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/228       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/227       \n",
            "Negative tokens: ['심부름꾼' '남자가' '쇳소리' '겨우' '거짓말을' '비난을' '낯선' '엉터리' '모르고' '의심하는' '터무니없는'\n",
            " '문제의' '바쁜' '자기' '없으며' '시간을' '않는' '배어' '버리게']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 228/229       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 227/228       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 226/227       \n",
            "Peak count: 12\n",
            "Frame tokens: 것뿐일세. 물어 없다네. 질문에 버리지. 말이라도 신경 끊었다. 말투로 보게, 당황했다. 줘버렸다. \n",
            "\n",
            "Similarity : 0.4313699919429057\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.369115114212036 Generator / grammar loss:-0.1922210156917572   similarity loss:-0.15488165616989136\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "그리하여 얼굴의 남자가 남자를 보았지만 그에게는 겨우 끝나 그대로 그건 사람이 싶었던 것뿐일세. 엉터리 물어 아무 없다네. 질문에 결과는 완강한 침묵으로 끝난다네. 버리지. 자네가 말이라도 그래서 신경 끊었다. 말투로 덧붙였다. 것 보게, ” 말대로였으므로 당황했다. 줘버렸다.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio    intro      body    ending       var    total  \\\n",
            "0  SAM+WGAN    0.170893  0.46432  0.574313  0.478096  0.002394  0.52345   \n",
            "\n",
            "   grammar  \n",
            "0  0.95642  \n",
            "Current result ==================================================\n",
            "Sample count: 43\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.174769  0.525056  0.513596    0.531012  0.004827     0.521113   \n",
            "\n",
            "   grammarity  \n",
            "0     0.97048  \n",
            "==================================================\n",
            "44 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "그 아이는 깜짝 놀라 좀 경계하는 빛이 되었다. 포아로도 상추를 주자 아이는 완전히 당황한 모양이었다. 포아로는 설교를 계속했다. “허름한 야채 가게에서는 딸기 같은걸 사면 안돼. 딸기란 막 따온 게 아니면 물이 배어 나오지. 바나나, 사과, 양배추, 이런 것들이라면 그래도 좀 낫지만, 딸기는 안 되네. ” 나는 변명하듯 말했다. “막 들어서자 생각이 났으니 어쩌나. ” 포아로는 엄숙하게 대답했다. “그건 자네 상상력이 모자라기 때문일세,” 그는 보도에서 걸음을 멈췄다. 애셔 부인 가게 오른쪽에 있는 집 딸린 가게는 비어 있었다. 창에 <세놓음>이라고 씌어 있었다. 반대쪽 옆집에는 때낀 모슬린 커튼이 내려져 있었다. 포아로는 그 집 쪽으로 걸어갔는데 벨이 없어서 노커를 힘차게 몇 번이나 두드렸다. 한참 있다가 코를 훌쩍거리는 지저분한 아이가 문을 열었다. 포아로가 말했다. “안녕, 어머니 계시니?” “네?” 아이는 불쾌하고 의심스럽게 우리를 보았다. 포아로가 말했다. “네 어머니 말이야. ” 아이는 이 말을 알아듣는 데 5분의 1분쯤 걸렸다.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/133       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/132       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/131       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/130       \n",
            "Negative tokens: ['놀라' '좀' '”' '때문일세,”' '애셔' '포아로는' '벨이' '어머니' '데']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 131/132       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 130/131       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 129/130       \n",
            "Peak count: 7\n",
            "Frame tokens: 깜짝 모양이었다. 말했다. ” 말했다. 포아로가 걸렸다. \n",
            "\n",
            "Similarity : 0.3769874857083003\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.6381523609161377 Generator / grammar loss:-0.2027723491191864   similarity loss:-0.13664871454238892\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "깜짝 좀 아이는 모양이었다. 되네. ” 말했다. 났으니 ” 대답했다. “그건 보도에서 씌어 있었다. 옆집에는 지저분한 포아로가 말했다. 포아로가 1분쯤 걸렸다.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.166667  0.365385  0.413243  0.639963  0.014343  0.471688   \n",
            "\n",
            "    grammar  \n",
            "0  0.989467  \n",
            "Current result ==================================================\n",
            "Sample count: 44\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.174585  0.521427  0.511316    0.533488  0.005043      0.51999   \n",
            "\n",
            "   grammarity  \n",
            "0    0.970911  \n",
            "==================================================\n",
            "45 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "이윽고 아이는 층계 쪽을 향해 소리쳤다. “엄마, 손님. ” 그리고는 어두컴컴한 안쪽으로 들어가 버렸다. 딱딱한 얼굴을 한 여자가 난간 너머로 내려다보고 나서 층계를 내려왔다. “시간 낭비예요. ” 여자가 말을 시작했으나, 포아로가 가로막았다. 그는 모자를 벗고 정중하게 인사했다. “안녕하십니까, 아주머니. 저는 <이브닝 프리커>의 기자인데 살해된 이웃집의 애셔 부인에 대해 기사가 될 만한 것을 얻으러 왔습니다. 사례금으로 5파운드 드리지요. ” 화난 목소리를 억누르고 여자는 머리를 쓰다듬고 치마를 잡아당기며 층계를 내려왔다. “자, 안으로 들어오세요. 이쪽으로. 어서 앉으세요. ” 그 조그만 방은 커다란 모조 자코비언 식 가구로 어수선하여 우리는 가까스로 안으로 들어가 딱딱한 긴 의자에 앉았다. 여자는 이야기하기 시작했다. “죄송해요. 조금 전에 그런 실례되는 말을 드려서요. 그렇지만 우리가 얼마나 성가신 꼴을 당하고 있는지 도저히 모르실 거예요. 아무튼 여러 사람들이 진공청소기니 양말이니 향로 주머니니 뭐니 온갖 잡동사니들을 팔러 온답니다.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/125       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/124       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/123       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/122       \n",
            "Negative tokens: ['“엄마,' '들어가' '”' '“자,' '자코비언' '여자는' '실례되는' '아무튼']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 123/124       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 122/123       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 121/122       \n",
            "Peak count: 7\n",
            "Frame tokens: “엄마, 낭비예요. 아주머니. “자, 들어오세요. “죄송해요. 온답니다. \n",
            "\n",
            "Similarity : 0.5045193179926102\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.4838762283325195 Generator / grammar loss:-0.20933447778224945   similarity loss:-0.15996812283992767\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "층계 “엄마, 그리고는 얼굴을 내려다보고 층계를 낭비예요. ” 아주머니. 애셔 5파운드 쓰다듬고 내려왔다. “자, 들어오세요. 어서 조그만 방은 “죄송해요. 전에 온답니다.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.179439  0.529841  0.562182  0.491128  0.000844  0.534397   \n",
            "\n",
            "    grammar  \n",
            "0  0.955781  \n",
            "Current result ==================================================\n",
            "Sample count: 45\n",
            "     method  comp rate     intro      body  conclusion  isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.174693  0.521614  0.512446    0.532546  0.00495      0.52031   \n",
            "\n",
            "   grammarity  \n",
            "0    0.970575  \n",
            "==================================================\n",
            "46 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "그들은 정말 말솜씨가 좋고 점잖게 보이지요. 이름도 한 번 들으면 금방 외워서 이쪽은 파울러 부인이고, 저쪽은 누구라느니 하며 말예요. ” 재치있게 그 이름을 잡아서 포아로가 말했다. “갑작스러운 이야기입니다만, 파울러 부인, 우리가 부탁드린 일을 들어주시겠습니까?” “글쎄요. ” 그러나 이미 5파운드가 파울러 부인의 눈앞에 유혹하듯 어른거리고 있다. “애셔 부인은 알고 있지만, 글로 쓰는 일이고 보면. ” 포아로는 얼른 안심시키듯 그녀 쪽에서는 아무것도 하지 않아도 되며, 그녀로부터 사실 이야기를 들은 다음 기사는 자기 쪽에서 쓴다고 이야기해 주었다. 이에 용기를 얻어 파울러 부인은 자진해서 기억이며 억측이며 소문 따위를 이것저것 이야기해 주었다. 애셔 부인은 사람들을 멀리하며 살았다. 이웃과 어울리는 일이 거의 없었고, 그 가엾은 d자에게는 여러 가지 근심거리가 있었다. 그것은 누구나 모두 잘 알고 있는 일이었다. 프란츠 애셔는 벌써 형무소에 처넣어야 마땅할 그런 남자였다. 그러나 애셔 부인이 그를 무서워하고 있었던 건 아니다. 그녀가 화를 내면 굉장했다. 그리고 언제나 솜씨 있게 잘 응수해 왔다. 하지만 그런 일이 일어나다니……. 즉 일이 되어갈 데까지 가버린 것이다. 파울러 부인은 몇 번이고 되풀이 그녀에게 이야기했었다. “그 남자는 언젠가 당신에게 무서운 짓을 할 거예요. 내 말을 잘 기억해 둬요.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/169       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/168       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/167       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/166       \n",
            "Negative tokens: ['재치있게' '이야기입니다만,' '얼른' '사실' '자기' '이에' '그녀가' '부인은' '잘']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 167/168       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 166/167       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 165/166       \n",
            "Peak count: 9\n",
            "Frame tokens: 말솜씨가 금방 부인은 주었다. 잘 있는 응수해 일어나다니……. 둬요. \n",
            "\n",
            "Similarity : 0.47425714093623694\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.2098069190979004 Generator / grammar loss:-0.19593197107315063   similarity loss:-0.17487381398677826\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "말솜씨가 금방 그 부인은 안심시키듯 사실 따위를 이것저것 주었다. 살았다. 그 d자에게는 잘 있는 일이었다. 아니다. 그녀가 굉장했다. 응수해 왔다. 일어나다니……. 즉 일이 부인은 그녀에게 짓을 둬요.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.164964  0.616693  0.475977  0.628307  0.004793  0.549819   \n",
            "\n",
            "    grammar  \n",
            "0  0.976641  \n",
            "Current result ==================================================\n",
            "Sample count: 46\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.174481  0.523681  0.511653    0.534628  0.004947     0.520951   \n",
            "\n",
            "   grammarity  \n",
            "0    0.970707  \n",
            "==================================================\n",
            "47 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "” 마침내 그 남자는 일을 저지르고 말았다. 그리고 그녀, 파울러 부인은 바로 이웃에 살면서 아무 소리도 못 들은 것이다. 잠시 사이를 두고 나서 포아로가 물었다. 애셔 부인은 어떤 이상한 편지?이를테면 개인적인 서명이 없는?예를 들어 ABC라는 서명이 든 편지를 받은 일이 없는가?파울러 부인은 유감스러운 듯 없다고 대답했다. “당신이 이야기하시는 그런 일은 저도 알고 있어요. 익명 편지라는 거지요. 큰소리로 말하기가 뭣할 정도로 창피스러운 게 가득 씌어 있는……네, 물론 프란츠 애셔가 그런 것을 썼는지 어떤지 저는 몰라요. 물론 썼다고 해도 애셔 부인이 제게 말했을 리 없고요. 뭐라고요? 철도 안내, ABC 철도 안내서라고요? 아니오, 그런 건 못 보았어요. 그리고 만일 애셔 부인에게 그런 게 보내져 왔다면 저한테 꼭 말해 줬을 거예요. 이번 사건을 들었을 때 전 하마터면 쓰러질 뻔했어요. 딸 에디가 알려 줬지요. ‘엄마, 옆 가게에 순경들이 많이 와 있어. ’라고 말예요. 정말 놀랐어요. 그 말을 듣고 전 말했지요. ‘저 아주머니는 그 집에 혼자 사는 게 아니었어.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/144       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/143       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/142       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/141       \n",
            "Negative tokens: ['애셔' '없는?예를' '받은' '“당신이' '익명' '뭐라고요?' '이번' '‘저' '혼자']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 142/143       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 141/142       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 140/141       \n",
            "Peak count: 8\n",
            "Frame tokens: 말았다. 것이다. 애셔 몰라요. 그리고 놀랐어요. ‘저 아니었어. \n",
            "\n",
            "Similarity : 0.49907813827162095\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.382920026779175 Generator / grammar loss:-0.20916222035884857   similarity loss:-0.1703917682170868\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "그 남자는 말았다. 애셔 “당신이 알고 있는……네, 것을 몰라요. 리 안내, 그리고 그런 사건을 ’라고 놀랐어요. 그 ‘저 아주머니는 집에 혼자 사는 게 아니었어.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var    total  \\\n",
            "0  SAM+WGAN    0.165455  0.578978  0.454916  0.549987  0.002808  0.50825   \n",
            "\n",
            "    grammar  \n",
            "0  0.993001  \n",
            "Current result ==================================================\n",
            "Sample count: 47\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.174289  0.524857  0.510446    0.534955  0.004901     0.520681   \n",
            "\n",
            "   grammarity  \n",
            "0    0.971181  \n",
            "==================================================\n",
            "48 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "그 조카딸이라도 함께 있었더라면 좋았을걸. 주정꾼 남자란 정말 허기진 늑대나 다름없으니까. 그 아주머니 남편은 짐승과 다를 바 없어. 나는 그 아주머니한테 몇 번이나 말했었는데, 결국 내 말대로 되어 버렸구나!그 남자는 언젠가 심한 짓을 할 거라고 말했는데. ’ 그 남자는 진짜로 해치운 거예요. 남자란 술을 마시면 무슨 짓을 할지 모르니까요. 이 살인이 그걸 말해 주고 있잖아요. ” 그녀는 숨을 헐떡이며 이야기를 끝냈다. 포아로가 물었다. “그 애셔라는 남자가 가게로 들어가는 것은 아무도 못 본 셈이군요?” 파울러 부인은 경멸하는 듯 콧방귀를 뀌며 말했다. “그야 아무도 못 보도록 들어가는 게 당연하지요. “ 그러나 그녀는 애셔가 어떻게 남의 눈에 띄지 않고 들어갈 수 있었는지에 대해선 설명해 주지 못했다. 그 집에는 뒷문이 없다고 그녀는 말했다. 또 애셔가 이 가까이에 잘 알려져 있다는 데에도 동의했다. “그렇지만 그는 교수형에 처해지기 싫으니까 용케 숨어 들어간 거예요. ” 포아로는 얼마동안 이야기를 이끌어 나가다가 파울러 부인이 알고 있는 이야기를 몇 번이나 되풀이하는 것을 깨닫자 그 면담을 끝내고 약속한 돈을 주었다. 길을 나서자 나는 말했다. “5파운드는 너무 비싼데, 포아로. ” “그렇지, 그것만으로는.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/161       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/160       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/159       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/158       \n",
            "Negative tokens: ['주정꾼' '결국' '해치운' '그녀는' '못' '셈이군요?”' '“그야' '애셔가' '얼마동안' '나가다가' '약속한']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 159/160       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 158/159       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 157/158       \n",
            "Peak count: 9\n",
            "Frame tokens: 주정꾼 정말 다름없으니까. 없어. 모르니까요. 끝냈다. 그녀는 않고 그것만으로는. \n",
            "\n",
            "Similarity : 0.4211492793566485\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.6330721378326416 Generator / grammar loss:-0.20739278197288513   similarity loss:-0.14183422923088074\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "그 주정꾼 정말 다름없으니까. 없어. 번이나 내 그 거예요. 남자란 술을 마시면 모르니까요. 이 숨을 이야기를 끝냈다. 물었다. 그녀는 않고 있었는지에 설명해 잘 동의했다. 나는 너무 그것만으로는.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending      var     total  \\\n",
            "0  SAM+WGAN    0.173776  0.449665  0.535618  0.503903  0.00126  0.508913   \n",
            "\n",
            "    grammar  \n",
            "0  0.973036  \n",
            "Current result ==================================================\n",
            "Sample count: 48\n",
            "     method  comp rate     intro     body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.174278  0.523291  0.51097    0.534308  0.004825     0.520436   \n",
            "\n",
            "   grammarity  \n",
            "0     0.97122  \n",
            "==================================================\n",
            "49 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "‘ “자네는 그녀가 이야기한 이상의 것을 알고 있다고 생각하나?” “우리는 지금 무엇을 물어야 좋을지 모르는 기묘한 위치에 놓여 있네. 우리는 어둠 속에서 숨바꼭질하고 있는 어린이들과도 같은 걸세. 우리는 손을 내밀어 찾고 있지. 파울러 부인은 자기가 알고 있다고 여기는 일들을 우리에게 말해줬네. 더욱이 꽤 억측을 해가면서. 그러나 언젠가 그 진술이 쓸모 있게 될 걸세. 5파운드를 투자한 건 결국 그 언젠가를 위해서라네. ” 나는 요점을 잘 잡을 수가 없었다. 그리고 마침 그때 우리는 글렌 형사와 마주쳤다. < 두 증인 > 글렌 형사는 좀 핼쑥해져 있는 듯 했다. 그는 오후 내내 담배 가게에 들어간 사람들 리스트를 만들고 있었던 모양이다. 포아로가 물었다. “결국은 눈에 띈 사람이 아무도 없다는 거로군요?” “아니, 보기는 본 모양입니다. 흘끔거리는 것 같은 느낌의 키가 큰 남자 셋, 시커먼 수염의 키 작은 남자 넷, 턱수염이 있는 사람 둘, 뚱뚱한 사람 셋, 모두 낯선 사람들로, 증언을 믿는다면 다 어딘지 수상쩍은 데가 있는 이들뿐입니다. 권총을 든 복면한 갱 한 무리가 범행을 저지르는 걸 보았다는 사람이 없는 게 이상할 정도입니다. ” 포아로는 동정적인 미소를 지었다. “애셔라는 사나이를 본 사람은 없던가요?” “없습니다. 이것도 그에게 유리한 점입니다. 저는 지금 막 서장님에게, 이것을 런던 경찰국에서 맡아야 할 일이라고 이야기하고 오는 참입니다. 이건 지방적인 범죄가 아닙니다.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/189       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/188       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/187       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/186       \n",
            "Negative tokens: ['모르는' '억측을' '언젠가를' '좀' '듯' '흘끔거리는' '수상쩍은' '“애셔라는']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 187/188       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 186/187       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 185/186       \n",
            "Peak count: 10\n",
            "Frame tokens: 있지. 그러나 걸세. 그리고 있는 포아로가 믿는다면 이들뿐입니다. “없습니다. 아닙니다. \n",
            "\n",
            "Similarity : 0.41135330821713934\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.6032652854919434 Generator / grammar loss:-0.16658322513103485   similarity loss:-0.10432028025388718\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            " 그녀가 이야기한 “우리는 좋을지 우리는 속에서 내밀어 찾고 있지. 파울러 부인은 여기는 일들을 말해줬네. 그러나 언젠가 쓸모 걸세. 잘 것 “없습니다. 저는 서장님에게, 런던 맡아야 참입니다. 이건 아닙니다. \n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body   ending       var     total  \\\n",
            "0  SAM+WGAN    0.161866  0.548167  0.468529  0.44896  0.001841  0.478586   \n",
            "\n",
            "    grammar  \n",
            "0  0.987883  \n",
            "Current result ==================================================\n",
            "Sample count: 49\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.174025  0.523799  0.510104    0.532566  0.004764     0.519582   \n",
            "\n",
            "   grammarity  \n",
            "0     0.97156  \n",
            "==================================================\n",
            "50 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "” 포아로는 신중하게 말했다. “나도 그렇게 생각하오. ” “포아로 씨, 싫은 사건입니다. 참으로 싫은 사건입니다. 저는 아무래도 마음에 들지 않습니다. ” 우리는 런던으로 돌아가기 전에 두 사람을 더 만났다. 하나는 제임즈 패트리지라는 인물이었다. 패트리지 씨는 애셔 부인이 살아 있는 동안 맨 마지막으로 만난 사람이었다. 그는 5시 30분에 그녀 가게에서 물건을 샀던 것이다. 페트리지 씨는 몸집 작은 빈약한 남자로 은행원이었다. 코안경은 걸친 무뚝뚝하고 빼빼 마른 느낌의 사나이였으나 말씨는 또박또박했다. 그는 자기에게 잘 어울리는 깨끗한 작은 집에 살고 있었다. 내 친구가 내민 명함을 보며 그는 말했다. “네, 포아로 씨. 글렌 형사에게서 들으셨습니까? 무슨 도움이 될까요, 포아로 씨?” “패트리지 씨, 당신은 살아있는 애셔 부인을 맨 마지막으로 만난 분이시니까요. ” 패트리지 씨는 두 손을 마주대고 미심쩍은 수표라도 들여다보듯 포아로를 보았다. “그것이 토론의 여지가 있는 점입니다, 포아로 씨. 제 다음에도 더 많은 손님이 애셔 부인한테서 물건을 샀을지 모르니 말입니다. ” “그렇더라도 지금으로선 아직 신고해 온 사람이 없습니다.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/143       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/142       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/141       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/140       \n",
            "Negative tokens: ['”' '씨,' '전에' '내' '수표라도' '포아로를']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 141/142       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 140/141       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 139/140       \n",
            "Peak count: 8\n",
            "Frame tokens: 신중하게 참으로 씨는 페트리지 그는 씨. 씨. 없습니다. \n",
            "\n",
            "Similarity : 0.40710856216577507\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.588862419128418 Generator / grammar loss:-0.17765600979328156   similarity loss:-0.11697377264499664\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "” 신중하게 생각하오. 싫은 사건입니다. 참으로 우리는 돌아가기 만났다. 맨 것이다. “패트리지 두 씨. 애셔 물건을 샀을지 모르니 “그렇더라도 지금으로선 신고해 사람이 없습니다. \n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro     body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.172355  0.781302  0.47915  0.472058  0.020775  0.537453   \n",
            "\n",
            "    grammar  \n",
            "0  0.983295  \n",
            "Current result ==================================================\n",
            "Sample count: 50\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.173992  0.528949  0.509485    0.531356  0.005085     0.519939   \n",
            "\n",
            "   grammarity  \n",
            "0    0.971795  \n",
            "==================================================\n",
            "51 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "” 패트리지 씨는 헛기침을 했다. “그들은 시민의 의무에 대한 관념을 갖고 있지 않으니까요, 포아로 씨. ” 그는 부엉이처럼 코안경 너머로 우리를 보았다. 포아로가 중얼거렸다. “정말 그렇습니다. 당신은 자진해서 경찰에 신고하셨습니까?” “그럼요, 저는 이 무서운 사건 이야기를 듣자 곧 제 진술이 도움이 될지도 모른다고 생각했습니다. 그래서 바로 신고했지요. ” 포아로는 엄숙하게 말했다. “정말 훌륭한 마음씨입니다. 저에게도 그 이야기를 되풀이 들려주실 수 있으시겠지요?” “알겠습니다. 저는 집으로 돌아오는 길이었는데, 정각 5시 30분에……. ” “실례입니다만, 어째서 그토록 정확하게 시간을 알고 계십니까?” 패트리지 씨는 방해를 받아 기분이 좀 상한 모양이었다. “교회 시계가 울렸습니다. 저는 제 시계를 보고 1분 늦는 것을 알았지요. 그때가 바로 애셔 부인 가게로 들어가기 직전이었습니다. ” “거기서 자주 물건을 사셨습니까?” “네, 자주 샀습니다. 집으로 돌아오는 길목이니까요. 1주일에 한두 번씩 저는 <존 코튼>을 순한 것으로 2온스씩 사고 있습니다. ” “애셔 부인을 알고 계셨습니까? 그녀의 가정에 대해서라든지 과거에 대해?” “전혀 모릅니다.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/138       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/137       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/136       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/135       \n",
            "Negative tokens: ['“그들은' '저는' '저에게도' '”' '시간을' '그때가' '번씩']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 136/137       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 135/136       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 134/135       \n",
            "Peak count: 7\n",
            "Frame tokens: 중얼거렸다. 신고했지요. 울렸습니다. 직전이었습니다. 샀습니다. ” 모릅니다. \n",
            "\n",
            "Similarity : 0.4762319230508373\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.502392530441284 Generator / grammar loss:-0.1911451816558838   similarity loss:-0.13980732858181\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "” 포아로가 중얼거렸다. 그렇습니다. “그럼요, 듣자 제 모른다고 신고했지요. 저에게도 “알겠습니다. 정각 어째서 좀 “교회 시계가 울렸습니다. 직전이었습니다. 샀습니다. ” 모릅니다.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.172241  0.521886  0.647314  0.495495  0.004386  0.576683   \n",
            "\n",
            "    grammar  \n",
            "0  0.979872  \n",
            "Current result ==================================================\n",
            "Sample count: 51\n",
            "     method  comp rate    intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.173957  0.52881  0.512188    0.530653  0.005071     0.521052   \n",
            "\n",
            "   grammarity  \n",
            "0    0.971953  \n",
            "==================================================\n",
            "52 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "사는 물건이나 또는 날씨에 대해 몇 마디 나눈 것 말고는 이야기한 일이 없습니다. ” “그녀의 생명을 위협하는 말을 자주 했었던 주정꾼 남편에 대해 알고 계셨습니까?” “아니오, 그 사람에 대해서는 아무것도 모릅니다. ” “그러나 당신도 얼굴은 알고 계셨겠지요. 어제 여느 때와 다른 어떤 기색은 없었습니까?흥분해 있었다던가, 화를 내고 있었다던가?” 패트리지 씨는 생각해 보더니 대답했다. “내가 아느 한에서는 여느 때와 다른 점이 전혀 없었습니다. ” 포아로는 일어섰다. “패트리지 씨, 질문에 대답해 주셔서 고맙습니다. 그런데 댁은 혹시 ABC가 없는지요? 런던으로 가는 시간표를 좀 보고 싶어서요. ” 그가 말한 선반 위에는 ABC와 함께 브래드쇼, 주식연감, 케리의 인명록 그리고 현대 인명록 및 지방 신사록 등이 있었다. 포아로는 ABC를 들고 기차시간을 살펴보는 시늉을 한 다음 패트리지 씨에게 고맙다는 인사를 하고 나왔다. 또 다른 한 사람은 앨버트 리딜로, 이 또한 색다른 사람이었다. 앨버트 리딜 씨는 철도 인부였다. 그의 신경질적인 아내가 접시 씻는 소리며, 그 집 개가 으르렁대는 소리며, 리딜 씨 자신의 노골적인 적의 등과 더불어 이야기가 진행되었다. 그는 넓적한 얼굴에 의심 많은 눈을 한 크고 우둥퉁한 거인으로, 고기든 파이를 아주 진한 차와 함께 집어삼키고 있었다. 그는 찻잔 가장자리께로부터 화난 듯한 얼굴로 우리를 노려보고 있었다. 그는 으르렁거렸다. “필요한 말은 다 한 줄로 아는데, 대체 나와 무슨 관계가 있다는거요? 나는 경찰에 다 말해줬소.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/195       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/194       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/193       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/192       \n",
            "Negative tokens: ['나눈' '이야기한' '주정꾼' '씨는' '씨,' '시간표를' '브래드쇼,' '인명록' '신사록' '앨버트' '그의' '씨'\n",
            " '“필요한' '아는데,' '나는']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 193/194       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 192/193       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 191/192       \n",
            "Peak count: 10\n",
            "Frame tokens: 했었던 계셨겠지요. 화를 대답했다. 일어섰다. 앨버트 진행되었다. 으르렁거렸다. 다 말해줬소. \n",
            "\n",
            "Similarity : 0.4473613274369119\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.4882073402404785 Generator / grammar loss:-0.1436653733253479   similarity loss:-0.09383872896432877\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "다른 화를 대답했다. 없었습니다. ” 포아로는 일어섰다. 질문에 고맙습니다. 런던으로 보고 위에는 ABC를 한 사람은 앨버트 인부였다. 진행되었다. 눈을 차와 집어삼키고 그는 으르렁거렸다. “필요한 다 나와 경찰에 다 말해줬소. \n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.165375  0.314666  0.530127  0.629918  0.017307  0.516972   \n",
            "\n",
            "    grammar  \n",
            "0  0.871154  \n",
            "Current result ==================================================\n",
            "Sample count: 52\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.173792  0.524692  0.512533    0.532562  0.005306     0.520973   \n",
            "\n",
            "   grammarity  \n",
            "0    0.970015  \n",
            "==================================================\n",
            "53 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "그런데 이번엔 또 외국놈 따위에게 다시 한 번 말하지 않으면 안 된다는 거요?” 포아로는 재빨리 내 쪽으로 흥미가 끌리는 듯한 눈짓을 보내고 나서 입을 열었다. “정말이지 안 되셨습니다. 그렇지만 할 수 없잖습니까? 어쨌든 살인 사건이니까요. 아주 신중하게 하지 않으면 안 되지요. ” 앨버트의 아내가 신경질적으로 말했다. “이분들이 듣고 싶어하시는 것을 모조리 이야기하는 게 좋아요. ” 거인이 소리쳤다. “잠자코 있어. ” 포아로가 솜씨 좋게 끼어들었다. “당신은 자진해서 경찰에 가신 게 아니잖습니까. ” “어째서 그런 짓을 해야 되는 거요? 그런 건 내 일이 아니잖소. ” 포아로는 아무렇지도 않게 말했다. “생각할 나름이지요. 살인이 일어났고 경찰은 가게에 왔던 사람을 알고 싶어했습니다. 저는 당신이 자진해서 신고하시는 편이 뭐랄까, 자연스럽다고 생각되는데요. ” “나한테는 일이 있소. 스스로 자진해서 가지 않았다니, 그렇게 말하면 곤란한데. ” “하지만 당신이 애셔 부인 가게로 들어가는 것을 본 사람이 있습니다.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/128       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/127       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/126       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/125       \n",
            "Negative tokens: ['거요?”' '없잖습니까?' '”' '스스로' '것을']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 126/127       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 125/126       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 124/125       \n",
            "Peak count: 7\n",
            "Frame tokens: 않으면 안 되지요. 끼어들었다. 나름이지요. ” 있습니다. \n",
            "\n",
            "Similarity : 0.47863085740565825\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.3712470531463623 Generator / grammar loss:-0.18148425221443176   similarity loss:-0.14392413198947906\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "그런데 번 내 안 되지요. 신경질적으로 모조리 게 소리쳤다. “잠자코 ” 포아로가 끼어들었다. 가신 건 내 아니잖소. ” “생각할 나름이지요. \n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro     body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.154143  0.544115  0.55261  0.412526  0.004112  0.508886   \n",
            "\n",
            "    grammar  \n",
            "0  0.988241  \n",
            "Current result ==================================================\n",
            "Sample count: 53\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.173422  0.525058  0.513289    0.530297  0.005284     0.520745   \n",
            "\n",
            "   grammarity  \n",
            "0    0.970359  \n",
            "==================================================\n",
            "54 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "그래서 경찰은 당신을 만나러 오지 않으면 안 되었던 것입니다. 그런데 경찰은 당신 이야기에 만족했습니까?” 앨버트는 사납게 되물었다. “어째서 만족하지 않겠소?” 포아로는 다만 어깨를 으쓱했을 뿐이었다. “대체 뭘 냄새 맡으려는 거요. 당신은. 나한테서 뭘 끄집어낼 수 있을 리 없잖소. 그 노파를 죽인 게 누군지 모두들 알고 있어. 그 남편이잖소?” “그렇지만 그날 밤 그는 그곳에 있지 않았고, 당신은 있었지요. ‘ “나한테 죄를 뒤집어씌우려는 거요, 당신? 잘됐어, 이거 잘해 봐야겠군. 대체 내가 그런 짓을 해야 될 이유가 어디 있소? 그 늙은이의 피로 얼룩진 담배를 한 갑 훔치려고? 내가 남들이 말하는 피에 굶주린 살인광이란 말이오? 이 내가?” 그는 위협하듯 의자에서 일어섰다. 그의 아내가 양 같은 소리를 질렀다. “버트, 버트, 그런 말을 해선 안 돼요. 버트, 그런 말을 하면 모두들……. ” 포아로가 말했다. “좀 침착하십시오. 저는 그저 당신이 그 가게에 가셨었다는 이야기를 듣고 싶었던 것뿐입니다.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/131       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/130       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/129       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/128       \n",
            "Negative tokens: ['“대체' '누군지' '그' '당신?' '있소?' '내가' '버트,' '싶었던']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 129/130       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 128/129       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 127/128       \n",
            "Peak count: 7\n",
            "Frame tokens: “어째서 당신은. 없잖소. 대체 이 “버트, 것뿐입니다. \n",
            "\n",
            "Similarity : 0.33850391566573057\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.295426845550537 Generator / grammar loss:-0.20319503545761108   similarity loss:-0.17343461513519287\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "그래서 당신을 오지 사납게 “어째서 뭘 맡으려는 거요. 당신은. 나한테서 없잖소. 게 있어. 당신? 대체 살인광이란 말이오? 이 “버트, 그저 것뿐입니다.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN     0.16699  0.425009  0.593759  0.482319  0.004909  0.526577   \n",
            "\n",
            "    grammar  \n",
            "0  0.990252  \n",
            "Current result ==================================================\n",
            "Sample count: 54\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.173302  0.523206  0.514779    0.529409  0.005277     0.520853   \n",
            "\n",
            "   grammarity  \n",
            "0    0.970727  \n",
            "==================================================\n",
            "55 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "그런데 그걸 거부하면 저에게는 어쩐지, 뭐라고 하나, 좀 이상한 기분이 드는군요. ” “내가 거부한다고 누가 말했소?” 리딜은 다시 의자에 앉았다. “이야기해 주겠소. ‘ “가게에 들어가셨던 게 6시였지요?” “그렇소. 사실은 1. 2분 지나 있었소. 골든 프레이크를 한 갑 사려고. 내가 문을 밀자……. ” “그러니까, 즉 가게 문이 닫혀 있었다는 거로군요?” “그렇소. 나는 벌써 가게를 닫았나 하고 생각했소. 그런데 그게 아니었소. 안으로 들어가니 아무도 없었소. 그래서 계산대를 쾅 두드리고는 잠시 기다려 보았소. 그래도 아무도 나오지 않길래 나는 밖으로 나왔소. ” “계산대 뒤에 쓰러져 있는 시체는 못 보셨군요?” “못 보았소. 다른 사람도 못 봤을 거요, 찾기라도 하지 않았다면. ” “철도 안내서는 있었습니까?” “있었소, 책장이 펼쳐져서 말이오. 그래서 나는 생각했소. 이 할머니, 너무 급하게 나가느라 문잠그는 걸 잊었나 보다고. ” “그래서 당신은 철도 안내서를 건드리거나 움직여 보셨군요?” “당치도 않소, 누가 그런 짓을 하겠소.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/130       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/129       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/128       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/127       \n",
            "Negative tokens: ['않길래' '“못' '잊었나']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 128/129       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 127/128       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 126/127       \n",
            "Peak count: 7\n",
            "Frame tokens: 밀자……. 생각했소. 없었소. 보았소. 않았다면. 생각했소. 하겠소. \n",
            "\n",
            "Similarity : 0.45211308684540485\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.4809763431549072 Generator / grammar loss:-0.1923356056213379   similarity loss:-0.14327718317508698\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "좀 누가 말했소?” “가게에 한 문을 밀자……. 생각했소. 들어가니 아무도 없었소. 보았소. 나왔소. “철도 있었습니까?” 생각했소. 문잠그는 잊었나 보다고. “당치도 하겠소.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.183865  0.492968  0.553699  0.598854  0.001882  0.555099   \n",
            "\n",
            "    grammar  \n",
            "0  0.980737  \n",
            "Current result ==================================================\n",
            "Sample count: 55\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.173494  0.522656  0.515487    0.530671  0.005215     0.521476   \n",
            "\n",
            "   grammarity  \n",
            "0    0.970909  \n",
            "==================================================\n",
            "56 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "지금 말한 일밖에 하지 않았소. ” “당신이 거기 들어가기 전에 누가 나오는 건 못 보셨습니까?” “못 보았소. 내가 말하고 싶은 건, 어째서 나에게 누명을 씌우려고……. ” 포아로는 일어섰다. “아무도 그러지 않습니다. 아직 지금 단계에서는. 그럼, 안녕히 계십시오. ” 그는 멍하니 입을 벌린 채 있는 사나이를 뒤에 남겨 두고 나왔다. 나는 그 뒤를 따랐다. 길로 나오자 그는 시계를 보았다. “빨리 가면 7시 2분 기차를 탈 수 있겠군. 자, 서둘러 가세. ‘ < 두 번째 편지 > 나는 열심히 물었다. “그래서?” 우리는 우리 말고는 아무도 없는 1등 차칸에 앉아 있었다. 기차는 급행으로 막 앤도버를 떠난 참이었다. 포아로가 말했다. “범죄는 빨강 머리에 왼쪽 눈이 사팔뜨기인 중키의 사나이에 의해 저질러졌네. 그 사나이는 오른쪽 다리를 조금 절고, 왼쪽 어때 밑에 점이 있지. ” 나는 소리쳤다. “포아로!” 한순간 나는 정말로 믿어 버렸던 것이다.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/128       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/127       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/126       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/125       \n",
            "Negative tokens: ['”' '내가' '건,' '”' '”' '“빨리' '“그래서?”' '막' '“포아로!”']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 126/127       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 125/126       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 124/125       \n",
            "Peak count: 7\n",
            "Frame tokens: ” 단계에서는. 나는 서둘러 말했다. 소리쳤다. 것이다. \n",
            "\n",
            "Similarity : 0.4536520907703009\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.3924455642700195 Generator / grammar loss:-0.18341046571731567   similarity loss:-0.14365026354789734\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "지금 말한 일밖에 하지 거기 “못 보았소. 나에게 씌우려고……. 나는 보았다. 서둘러 포아로가 말했다. 머리에 눈이 의해 조금 왼쪽 밑에 소리쳤다.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.170478  0.696886  0.534392  0.480712  0.008446  0.550787   \n",
            "\n",
            "    grammar  \n",
            "0  0.980763  \n",
            "Current result ==================================================\n",
            "Sample count: 56\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.173441  0.525767  0.515824    0.529779  0.005273     0.521999   \n",
            "\n",
            "   grammarity  \n",
            "0    0.971085  \n",
            "==================================================\n",
            "57 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "그러나 곧 이 친구의 장난기 어린 눈빛이 사실을 가르쳐 주었다. 나는 되풀이했다. 이번에는 나무라듯이 “포아로!” “자네는 어떻게 하자는 건가? 자네는 나에게 충성스러운 개같이 헌신적인 눈길을 보내면서 셜록 홈즈 같은 해결을 바라고 있네. 그런데 진상은 말일세. 살인범이 어떤 사니이며, 어디에 살고, 어떻게 하면 잡을 수 있는지 나는 도무지 알 수 없네. ” 나는 중얼거렸다. “녀석이 무슨 단서라도 남겨 줬더라면. ‘ 그렇지, 단서. 언제나 자네 마음을 끄는 건 그 단서라는 걸세. 유감스럽게도 그 사나이는 담배를 피워 담뱃재를 남겨 둬 주지도 않았고, 야릇한 모양의 징을 박은 신 자국도 남겨 주지 않았네. 그렇지, 그는 그리 친절하지 않았어. 그러나 적어도 철도 안내서가 있잖나. 그 ABC야말로 자네의 단서가 아니겠나!“ “그가 실수해서 그것을 남겨 뒀다고 생각하나?” “물론 그렇지는 않네. 일부러 두고 간 걸세. 지문 상태를 보면 알 수 있지. ” “지문이 없었잖나?” “바로 그걸세. 어제는 어떤 밤이었나? 더운 6월의 밤이었지. 이런 밤에 장갑을 끼고 다니는 사나이가 있을까?\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/141       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/140       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/139       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/138       \n",
            "Negative tokens: ['자네는' '보내면서' '알' '단서라도' '야릇한' '자국도']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 139/140       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 138/139       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 137/138       \n",
            "Peak count: 8\n",
            "Frame tokens: 되풀이했다. 말일세. 중얼거렸다. 걸세. 일부러 걸세. 없었잖나?” 있을까? \n",
            "\n",
            "Similarity : 0.47020314778208205\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.080526113510132 Generator / grammar loss:-0.14316314458847046   similarity loss:-0.1351061761379242\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "사실을 되풀이했다. 어떻게 자네는 눈길을 바라고 말일세. 어떤 잡을 중얼거렸다. 무슨 남겨 자네 마음을 끄는 그 걸세. 주지 ABC야말로 아니겠나!“ 일부러 걸세. 있지. 없었잖나?”\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.183784  0.513605  0.533124  0.400972  0.003392  0.489575   \n",
            "\n",
            "    grammar  \n",
            "0  0.866827  \n",
            "Current result ==================================================\n",
            "Sample count: 57\n",
            "     method  comp rate     intro      body  conclusion  isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.173622  0.525554  0.516128    0.527519  0.00524      0.52143   \n",
            "\n",
            "   grammarity  \n",
            "0    0.969256  \n",
            "==================================================\n",
            "58 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "그런 사나이는 곧 눈에 띄지. 그러니까 ABC에 지문이 없었다면 주의깊게 닦아 낸 게 틀림없네. 죄없는 남자라면 지문을 남겨 두겠지만, 죄가 있는 자는 남기지 않네. 그러므로 우리의 살인범은 그것을 일부러 남겨두고 간 걸세. 그러나 그 때문에 이것이 또 한 단서가 되지. ABC는 누군가가 사서 갖다 두었다……이런 가능성이 있는 셈일세. ” “그 방법으로 무엇을 알 수 있나?” “사실을 말하면, 헤이스팅즈. 나는 그리 희망을 가지고 있지 않네. 이 사나이, 이 미지의 X라는 사나이는 확실히 자기 능력에 자부심을 갖고 있네. 그는 뒤를 밟힐 그런 따위의 표시를 남겨 두지 않아. ” “그렇다면 ABC는 전혀 희망이 없는가?” “자네가 말하는 뜻에서는. ” “다른 뜻에서라면 있다는 건가?” 포아로는 곧바로 대답하지 않았다. 이윽고 그는 천천히 말했다. “그 답은 <있다>일세. 우리는 지금 미지의 인물과 마주하고 있네. 상대는 어둠 속에 있고, 언제까지나 어둠 속에 있으려 하지. 그러나 일의 성질로 보아 그는 자기에게 빛을 비추지 않고는 견디지 못할 걸세. 어떤 뜻에서는 이미 많은 것을 알고 있지. 나에게는 그의 모습이 흐릿하게 형태를 갖추어 오는 게 보인다네. 올바른 활자체를 달필로 쓸 수 있는 사나이?그것을 부당하게 느끼며 싸워 온 사나이가 내 눈에 보이네.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/169       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/168       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/167       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/166       \n",
            "Negative tokens: ['없었다면' '남자라면' '두겠지만,' '그러므로' '일부러' '그러나' '자기' '그는' '답은' '미지의' '그러나' '견디지'\n",
            " '그의']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 167/168       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 166/167       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 165/166       \n",
            "Peak count: 9\n",
            "Frame tokens: 곧 띄지. 되지. 있는 “사실을 이 않았다. 어떤 보이네. \n",
            "\n",
            "Similarity : 0.44337037015951364\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.3895671367645264 Generator / grammar loss:-0.17044749855995178   similarity loss:-0.13098657131195068\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "그런 사나이는 곧 눈에 띄지. 지문이 없었다면 낸 게 틀림없네. 죄없는 자는 남기지 않네. 살인범은 그것을 걸세. 또 되지. 않네. 사나이, 어떤 오는 올바른 달필로 보이네. \n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro     body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.149847  0.776146  0.49707  0.510894  0.016492  0.557033   \n",
            "\n",
            "    grammar  \n",
            "0  0.983684  \n",
            "Current result ==================================================\n",
            "Sample count: 58\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.173212  0.529874  0.515799    0.527233  0.005434     0.522044   \n",
            "\n",
            "   grammarity  \n",
            "0    0.969505  \n",
            "==================================================\n",
            "59 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "자신을 주장하고 남의 관심을 끌고 싶은 마음속 충동이 점점 강해지고, 사건이며 사물이 그것을 부숴 버려 한층 더 비굴한 감정을 쌓아 올려 간 것이 내 눈에는 보이네. 그리하여 내부의 성냥이 이 화약을 실은 열차에 불을 붙이게 된 걸세. ‘ 나는 반대했다. 그런 것은 모두 억측에 지나지 않잖나. 실제로는 아무 쓸모도 없어. “ “자네는 성냥 끄트러기라든가 담뱃재라든가 징 박은 구두 쪽이 마음에 드는가 보군. 그러나 적어도 우리는 스스로 실제적인 질문을 해보지 않으면 안 되네. 어째서 ABC인가? 어째서 애셔 부인인가? 어째서 앤도버인가?” “그 여자의 과거 생활은 아주 단순해 보이네. ” 나는 생각에 잠겼다. “그 두 남자와의 면담은 실망이었어. 그들은 우리가 이미 알고 있는 것 이상의 일은 아무것도 말하지 못했잖아. ” “사실을 말하면 나는 그들에게 큰 기대를 걸고 있지 않았네. 그러나 살인 후보자로서의 두 사람의 가능성을 무시할 수도 없었지. ‘ “자네는 정말로……. ” “적어도 범인이 앤도버 또는 그 언저리에 살고 있을 가능성은 있는 걸세. 그것이 어째서 앤도버인가 하는 우리들의 질문에 대한 가능한 대답이 되네. 게다가 그날 그 시간 가게에 있었던 것으로 알려진 사나이가 둘이나 있는 걸세. 그 어느 쪽이 범인일지도 모르잖나.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/168       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/167       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/166       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/165       \n",
            "Negative tokens: ['비굴한' '그리하여' '그러나' '스스로' '않으면' '애셔' '과거' '”' '생각에' '“자네는' '앤도버' '앤도버인가'\n",
            " '사나이가']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 166/167       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 165/166       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 164/165       \n",
            "Peak count: 9\n",
            "Frame tokens: 그런 지나지 “ ” 않았네. 수도 걸세. 그 모르잖나. \n",
            "\n",
            "Similarity : 0.373817312549104\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.3572068214416504 Generator / grammar loss:-0.17544713616371155   similarity loss:-0.13933919370174408\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "더 감정을 올려 보이네. 그런 지나지 “ 부인인가? “그 여자의 실망이었어. 우리가 말하지 ” 나는 큰 걸고 있지 않았네. 수도 정말로……. “적어도 범인이 살고 있을 걸세. 사나이가\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending      var     total  \\\n",
            "0  SAM+WGAN    0.159375  0.494473  0.535363  0.494593  0.00037  0.514954   \n",
            "\n",
            "    grammar  \n",
            "0  0.931043  \n",
            "Current result ==================================================\n",
            "Sample count: 59\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.172978  0.529274  0.516131    0.526679  0.005348     0.521924   \n",
            "\n",
            "   grammarity  \n",
            "0    0.968853  \n",
            "==================================================\n",
            "60 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "그리고 그 가운데 어느 쪽인가가 범인이 아님을 알리는 표시는 아직 아무것도 나타나지 않았으니 말일세. ‘ 나는 인정했다. “그 꼴사나운 짐승 같은 리딜일지도 모르지. ” “그런데 나는 리딜은 풀어줘도 좋다고 생각하고 있네. 그는 신경질적이고 큰소리를 치며 분명 초조해 있었어. ” “그러나 그것은 확실히……. ” “그 ABC 편지를 쓰는 그런 자와는 정반대의 성격일세. 자신감과 자부심이 우리가 찾고 있는 특징이지. ” “누군가 자신의 중대성을 알리고 싶어하는 사람이란 말인가?” “아마도 그럴 걸세. 그러나 어떤 종류의 사람들은 신경질적이고 겸손한 속에 오히려 크나큰 허영심과 자기만족을 숨기고 있기도 하지. ” “그 몸집 작은 패트리지 씨는 어떤가?” “그 사나이 쪽이 그런 타입에 가까워. 그 이상은 말할 수 없지만. 그는 마치 그 편지를 쓴 자가 할 것 같은 행동을 취하고 있었지. 바로 경찰에 갔고, 자기를 돋보이려 했으며, 자기 위치를 즐기고 있었거든. ” “정말로 그렇게 생각하나?” “아니, 헤이스팅즈. 내 개인적으로는 범인이 앤도버 밖에서 왔다고 생각하지만 어떤 수사도 소홀히 할 수 없네. 그리고 나는 언제나 <그>라고 말하고 있지만, 여성이 관계해 있을 가능성도 빼놓을 수 없네. ” “설마!” “물론 공격 수법으로 보아선 남자일세. 그러나 익명 편지란 남자보다 오히려 여자에 의해 잘 씌어지는 법이지. 이 사실을 머리에 넣어두지 않으면 안 되네.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/179       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/178       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/177       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/176       \n",
            "Negative tokens: ['그는' '싶어하는' '오히려' '자기만족을' '타입에' '개인적으로는' '생각하지만' '그리고' '<그>라고' '오히려'\n",
            " '씌어지는' '않으면']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 177/178       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 176/177       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 175/176       \n",
            "Peak count: 10\n",
            "Frame tokens: ” ” 걸세. 하지. 그는 것 했으며, 있었거든. 남자일세. 되네. \n",
            "\n",
            "Similarity : 0.42221730140770286\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.4664671421051025 Generator / grammar loss:-0.19652460515499115   similarity loss:-0.14900332689285278\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "아직 ‘ ” 그는 ” “그러나 그것은 그런 우리가 있는 걸세. 속에 있기도 하지. ” “정말로 생각하나?” 내 범인이 생각하지만 수사도 없네. 그리고 빼놓을 수 없네. “설마!” 남자일세. 법이지. 되네.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN     0.16079  0.587637  0.473116  0.436341  0.004151  0.484988   \n",
            "\n",
            "    grammar  \n",
            "0  0.993423  \n",
            "Current result ==================================================\n",
            "Sample count: 60\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.172774  0.530247  0.515414    0.525174  0.005328     0.521308   \n",
            "\n",
            "   grammarity  \n",
            "0    0.969262  \n",
            "==================================================\n",
            "61 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "” 나는 잠시 입을 다물고 있다가 말했다. “이번엔 뭘 하면 되나?” “오, 정력가 헤이스팅즈여. ” 포아로는 나에게 미소를 보냈다. “아니, 정말 뭘 하지?” “아무것도. ” “아무것도?” 내 목소리에는 뚜렷한 실망이 나타나 있었다. “내가 마술사인가, 마법사인가? 내게 뭘 시키고 싶은가?” 마음속으로 사태를 잘 생각해 보고 나서 나는 대답하기 힘들다는 것을 알았다. 그러나 나는 뭔가 하지 않으면 안 된다. 발밑에 풀이 나게 해선 안 된다는 확신을 가지고 있었다. 나는 말했다. “ABC도 있고, 편지지도 있고, 봉투도 있고……. ” “그 점에서는 물론 여러 가지 수배가 되어 있네. 경찰은 그런 종류의 수사를 하는 데 자유스러운 여러 가지 수단을 갖고 있지. 그런 점에서 무엇이 발견된다면 그들이 찾아내 줄 테니 걱정할 것 없네. ” 그래서 나 또한 만족하는 수밖에 없었다. 그뒤 며칠동안 이상하게도 포아로는 사건에 대한 토론을 피하는 듯했다. 내가 그 문제를 꺼내려 하면 못 참겠는지 손을 흔들며 말머리를 돌려 버렸다. 나는 그 까닭을 깊이 생각해 보기를 은근히 두려워하고 있었다. 애셔 부인 살해에 대해서는 포아로도 자신의 패배를 인정하고 있었다. ABC가 그에게 도전했고, 그리고 이겼다.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/158       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/157       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/156       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/155       \n",
            "Negative tokens: ['하면' '나에게' '하지?”' '내게' '대답하기' '그러나' '않으면' '그런' '수밖에' '하면' '생각해' '도전했고,']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 156/157       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 155/156       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 154/155       \n",
            "Peak count: 8\n",
            "Frame tokens: 잠시 말했다. 줄 ” 듯했다. 못 있었다. 이겼다. \n",
            "\n",
            "Similarity : 0.45429422364397454\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.352195978164673 Generator / grammar loss:-0.16251496970653534   similarity loss:-0.12692438066005707\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "” 잠시 말했다. “이번엔 하면 되나?” 헤이스팅즈여. 포아로는 정말 뭘 “아무것도. “내가 말했다. 있고……. 그들이 내가 못 말머리를 버렸다. 까닭을 깊이 두려워하고 있었다. 이겼다.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.168831  0.775396  0.452075  0.586753  0.017585  0.557143   \n",
            "\n",
            "    grammar  \n",
            "0  0.962373  \n",
            "Current result ==================================================\n",
            "Sample count: 61\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN    0.17271  0.534266  0.514376    0.526183  0.005529     0.521896   \n",
            "\n",
            "   grammarity  \n",
            "0    0.969149  \n",
            "==================================================\n",
            "62 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "잇따른 성공에 익숙해 있던 내 친구는 자기 실패에 민감했다. 그런 만큼 그는 이 문제에 대한 토론을 참을 수 없었던 것이다. 그것은 분명 위대한 남자의 왜소함을 나타내는 일임에 틀림없지만, 아무리 냉정한 사람일지라도 성공했을 때 흥분되는 것은 흔히 있는 일이다. 포아로의 경우는 그 흥분이 몇 해나 계속 되고 있었다. 포아로의 경우는 그 흥분이 몇 해나 계속 되고 있었다. 그 결과 지금에야 겨우 눈을 뜨게 되었다 해도 그리 이상할 것은 없으리라. 나는 친구의 약점을 존중하여 사건에 대해 그 이상 이야기하는 것을 삼갔다. 심문 보고는 신문에서 읽었다. 그것은 무척 간단한 것으로, ABC 편지에 대해서도 아무 언급이 없었다. 배심원 평결은 한 사람 또는 몇 사람의 알 수 없는 인물에 의한 살인으로 되어 있었다. 이 사건은 신문의 여러 기사들 속에서 그리 주의를 끌지 못했다. 이야깃거리가 될 듯한 데도, 구경거리가 될 듯한 데도 없었다. 뒷골목의 노파 살해 따윈 더 스릴 있는 화제 때문에 곧 흐지부지되어 버리는 것이다. 사실을 말하면, 사건은 내 머리 속에서도 사라져 가고 있었다. 그것은 포아로가 실패했다고 생각하는 게 싫었기 때문이라고도 할 수 있다. 그런데 7월 25일이 되어 사건은 갑자기 되살아났다. 나는 주말에 요크셔에 가 있었기 때문에 이틀쯤 포아로를 만나지 못했다. 월요일 오후에 돌아왔는데, 그 편지는 6시 우편으로 배달되었다. 나는 문제의 봉투를 뜯어 펼쳐 보았을 때 포아로가 갑자기 날카롭게 숨을 들이 쉬었던 것을 기억하고 있다. “왔네.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/202       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/201       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/200       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/199       \n",
            "Negative tokens: ['있던' '참을' '없었던' '틀림없지만,' '되고' '지금에야' '심문' '한' '몇' '이야깃거리가' '곧' '생각하는'\n",
            " '있었기' '갑자기']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 200/201       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 199/200       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 198/199       \n",
            "Peak count: 11\n",
            "Frame tokens: 익숙해 그런 흥분되는 흥분이 흥분이 없으리라. 그것은 못했다. 듯한 사실을 “왔네. \n",
            "\n",
            "Similarity : 0.44664290381411553\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.3889386653900146 Generator / grammar loss:-0.19312599301338196   similarity loss:-0.1537303924560547\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            " 익숙해 그런 그는 이 참을 틀림없지만, 사람일지라도 흥분되는 흥분이 흥분이 되었다 이상할 것은 없으리라. 것을 삼갔다. 심문 그것은 무척 ABC 편지에 한 없는 살인으로 되어 이 주의를 못했다. 될 것을 “왔네. \n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.157274  0.591591  0.475449  0.446702  0.003923  0.490053   \n",
            "\n",
            "    grammar  \n",
            "0  0.958307  \n",
            "Current result ==================================================\n",
            "Sample count: 62\n",
            "     method  comp rate    intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.172461  0.53519  0.513748    0.524901  0.005503     0.521382   \n",
            "\n",
            "   grammarity  \n",
            "0    0.968974  \n",
            "==================================================\n",
            "63 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "” 그가 말했다. 나는 그를 쳐다보았으나 잘 알 수가 없었다. “뭐가 왔다는 건가?” “ABC 사건의 제2장일세. ” 나는 잠시 멍해진 채 그를 보고 있엇다. 사건은 완전히 내 기억에서 떨어져 나가 있었던 것이다. “읽어보게. ” 포아로는 나에게 편지를 건네주었다. 전과 마찬가지로 좋은 편지지에 활자체로 씌어 있었다. 친애하는 포아로여, 대체 어떻게 된 건가? 첫 번째 게임은 내 승리다. 앤도버 사건은 실로 잘 되었잖은가? 그러나 재미는 이제 시작이다. 이번에는 벡스힐 바닷가로 주의를 돌리도록. 날짜는 오는 25일. 이 얼마나 유쾌한 일인가! 이만. ABC 나는 소리쳤다. “이런, 포아로! 이 미치광이는 또 다른 범죄를 저지르려는 게 아닌가?” “물론이지, 헤이스팅즈. 자네는 어떻게 생각하고 있었나?\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/98       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/97       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/96       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/95       \n",
            "Negative tokens: ['“뭐가' '”' '앤도버' '그러나' '이' '“이런,']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 96/97       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 95/96       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 94/95       \n",
            "Peak count: 5\n",
            "Frame tokens: 말했다. “뭐가 “읽어보게. “이런, 있었나? \n",
            "\n",
            "Similarity : 0.5197851191550267\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.0957515239715576 Generator / grammar loss:-0.14704933762550354   similarity loss:-0.13746686279773712\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            " 말했다. 수가 “뭐가 건가?” 제2장일세. 그를 내 “읽어보게. 실로 바닷가로 25일. “이런, 이 있었나?\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN     0.15404  0.678211  0.541072  0.553937  0.003824  0.572359   \n",
            "\n",
            "    grammar  \n",
            "0  0.942218  \n",
            "Current result ==================================================\n",
            "Sample count: 63\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.172168  0.537461  0.514181    0.525362  0.005476     0.522192   \n",
            "\n",
            "   grammarity  \n",
            "0     0.96855  \n",
            "==================================================\n",
            "64 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "앤도버 사건 하나로 끝날 줄 여겼나? 자네는 내가 말한 걸 기억하고 있겠지. <무서운 시작이다>라고. ” “무서운 일이군. ” “그렇네, 무서운 일일세. ” “우리는 살인광을 상대하고 있어. ” “바로 그렇네. ” 그의 냉정함은 어떤 극적인 태도보다도 인상적이었다. 나는 편지를 돌려주며 몸을 떨었다. 다음날 아침, 담당자들의 회의가 열렸다. 서섹스 주 경찰서장, 범죄 수사과장, 앤도버의 글렌 형사, 서섹스 경찰의 카터 경감, 재프 경감과 크롬이라는 이름의 젊은 형사, 그리고 저명한 정신과의 솜프슨 박사가 한 자리에 모였다. 편지의 소인은 햄스티드로 되어 있었지만, 포아로의 의견으로 이것은 그리 중요시되지 않았다. 사건은 충분히 검토되었다. 솜프슨 박사는 인상 좋은 중년 신사로, 그 풍부한 학식에도 불구하고 직업상의 전문용어를 피해 아주 평범한 말을 쓰도록 마음 쓰고 있었다. 범죄 수사과장이 말했다. “이 두 편지가 같은 필적임은 틀림없습니다. 둘 다 한 인물에 의해 씌어진 겁니다. 그리고 그 인물이 앤도버 살인 사건에 관련해 있는 것도 확실합니다. ” “과연 그렇습니다. 우리는 지금 또다른 명백한 두 번째의 계획적 살인 예고를 받고 있습니다.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/147       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/146       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/145       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/144       \n",
            "Negative tokens: ['서섹스' '사건은' '그' '불구하고' '예고를']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 145/146       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 144/145       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 143/144       \n",
            "Peak count: 8\n",
            "Frame tokens: <무서운 일이군. 무서운 범죄 겁니다. 확실합니다. “과연 있습니다. \n",
            "\n",
            "Similarity : 0.5045972926817466\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.4556665420532227 Generator / grammar loss:-0.19831813871860504   similarity loss:-0.1519375592470169\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            " 앤도버 <무서운 일이군. 무서운 일일세. 인상적이었다. 서섹스 주 앤도버의 이름의 한 모였다. 그리고 그 앤도버 살인 사건에 관련해 것도 확실합니다. “과연 살인 있습니다.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.164129  0.631892  0.518835  0.592333  0.002194  0.563496   \n",
            "\n",
            "    grammar  \n",
            "0  0.990562  \n",
            "Current result ==================================================\n",
            "Sample count: 64\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.172043  0.538936  0.514254    0.526409  0.005425     0.522837   \n",
            "\n",
            "   grammarity  \n",
            "0    0.968894  \n",
            "==================================================\n",
            "65 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "그 살인은 오는 25일 벡스힐로 예정되어 있습니다. 어떤 수단을 취하면 좋겠습니까?” 서섹스 주 경찰서장이 자기 경감 쪽을 보았다. “카터, 어떻게 해야 할까?” 카터 경감은 무겁게 고개를 저었다. “어렵군요, 예정된 피해자에 대한 최소한의 단서도 없습니다. 정직하고 공정하게 말해서 과연 우리가 어떤 수단을 취할 수 있겠습니까?” 포아로가 나섰다. “힌트가 있습니다. ” 모두의 얼굴이 그에게로 돌려졌다. “예정된 피해자의 이름은 B로 시작된다고 여겨집니다. ” 수사과장이 의심스러운 듯 말했다. “그것은 다만 생각일 따름이지요. ” 솜프슨 박사가 생각에 잠겨 말했다. “알파벡 콤플렉스군요. ” “나는 단지 가능성으로 말하고 있는 겁니다. 그 이상은 아니지요. 이 생각은 지난달에 살해된 그 불운한 여자의 가게 문에 애셔라는 글자가 씌어져 있는 것을 보았을 때 떠오른 겁니다. 벡스힐이라고 장소를 정한 편지를 보고 피해자도 알파벳순으로 정해지리라는 게 하나의 가능성으로 떠올랐지요. ” “그것은 확실히 가능성이 있습니다. 그러나 동시에 애셔라는 이름은 우연의 일치였는지도 모릅니다. 이번 피해자의 이름이 무엇이든 또 가게를 가진 노파일지도 모르지요. 알겠습니까?\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/139       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/138       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/137       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/136       \n",
            "Negative tokens: ['어떤' '예정된' '있겠습니까?”' '“힌트가' '“예정된' '가능성으로' '그러나' '애셔라는']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 137/138       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 136/137       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 135/136       \n",
            "Peak count: 8\n",
            "Frame tokens: 어떤 우리가 ” 말했다. 생각일 겁니다. 모릅니다. 알겠습니까? \n",
            "\n",
            "Similarity : 0.463486519064498\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.495910882949829 Generator / grammar loss:-0.21984577178955078   similarity loss:-0.1691991537809372\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "그 어떤 쪽을 정직하고 우리가 돌려졌다. ” 말했다. “그것은 생각일 ” 박사가 “알파벡 “나는 그 이상은 떠오른 겁니다. 장소를 정해지리라는 우연의 모릅니다. 이름이 알겠습니까?\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.167785  0.272601  0.513776  0.465137  0.010845  0.450949   \n",
            "\n",
            "    grammar  \n",
            "0  0.970015  \n",
            "Current result ==================================================\n",
            "Sample count: 65\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.171977  0.534839  0.514247    0.525466  0.005508     0.521731   \n",
            "\n",
            "   grammarity  \n",
            "0    0.968911  \n",
            "==================================================\n",
            "66 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "우리는 한 미치광이를 상대하고 있습니다. 상대는 동기에 대한 실마리를 아무것도 나타내 보이고 있지 않습니다. ” 카터 경감이 의심스러운 듯 물었다. “미치광이에게도 동기가 있을까요?” “물론 있습니다. 편집광의 특징 가운데 하나는 아주 논리적이라는 겁니다. 그는 자신이 목사, 의사 도는 담배 가게 노파라도 좋은데, 이들을 죽이게끔 신에 의해 정해져 있다고 믿지요. 그 배후에는 하나같이 어떤 완전하고도 타당한 이유가 있는 법입니다. 그러니 우리는 알파벳 같은 것에 정신을 팔면 안 됩니다. 앤도버 다음이 벡스힐인 것은 아마 우연의 일치에 지나지 않을 겁니다. ” “우리는 적어도 그 경계만은 할 수 있는 셈이네, 카터. 특히 조그만 가게의 B로 시작되는 이름에 주의하여, 혼자서 경영하는 조그만 담배 가게라든가 신문 가게를 지키게. 그 밖에는 달리 우리가 할 수 있는 일이 없어. 낯선 사람을 특히 주의해야 할 것은 물론이지만. ” 카터 경감은 신음 소리를 냈다. “학교의 방학이 시작되었는데 말입니까? 이번 주일에는 그곳으로 굉장한 인파가 몰릴 겁니다. ” 경찰서장이 날카롭게 말했다. “우리는 할 수 있는 건 다 해야만 되네. ” 이번에는 글렌 형사가 말했다. “애셔 사건에 관계있는 사람은 제가 지키지요.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/158       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/157       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/156       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/155       \n",
            "Negative tokens: ['편집광의' '이들을' '앤도버' '우연의' '셈이네,' '주의하여,' '그' '특히' '이번' '해야만']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 156/157       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 155/156       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 154/155       \n",
            "Peak count: 8\n",
            "Frame tokens: 있습니다. “미치광이에게도 겁니다. 믿지요. 법입니다. “우리는 “애셔 지키지요. \n",
            "\n",
            "Similarity : 0.41739711039546346\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.5273256301879883 Generator / grammar loss:-0.16648544371128082   similarity loss:-0.11247728019952774\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "상대하고 실마리를 나타내 물었다. 의사 좋은데, 정해져 믿지요. 그 타당한 이유가 있는 법입니다. 그러니 알파벳 것에 팔면 됩니다. 담배 카터 “우리는 할 “애셔 지키지요.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.154341  0.332685  0.434441  0.357114  0.001881  0.390892   \n",
            "\n",
            "    grammar  \n",
            "0  0.984078  \n",
            "Current result ==================================================\n",
            "Sample count: 66\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN    0.17171  0.531776  0.513038    0.522915  0.005453     0.519748   \n",
            "\n",
            "   grammarity  \n",
            "0    0.969141  \n",
            "==================================================\n",
            "67 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "패트리지와 리딜 두 증인과 물론 애셔도. 그들이 앤도버를 떠나면 미행시키겠습니다. ” 그리고 나서 두세 가지 제안과 얼마쯤 산만한 대화가 오간 뒤 회의는 끝났다. 나는 강을 따라 걸으며 말했다. “포아로, 이 범죄를 예방할 수 있을까?” 그는 야윈 얼굴을 내 쪽으로 돌렸다. “한 사람의 광기에 대해, 이렇게 사람 들끊는 정상적인 거리에서? 나는 걱정일세, 헤이스팅즈. 아주 걱정이네. 살인광 잭의 그 오래 계속된 성공을 기억하고 있겠지!” “무서운 일이군. ” “헤이스팅즈, 광기란 무서운 거라네. 나는 걱정일세. 아주 걱정이야. ” < 벡스힐 바닷가 살인 > 나는 지금도 7월 25일 아침, 잠에서 깨어난 무렵의 일을 기억하고 있다. 그것은 아마 7시 30분쯤이었다고 생각된다. 포아로가 침대 옆에 서서 가만히 내 어깨를 흔들고 있었다. 그의 얼굴을 한 번 보자 나는 곧 반쯤 잠든 상태에서 눈을 떴다. 나는 얼른 일어나면서 물었다. “무슨 일인가?” 그는 아주 간단하게 대답했지만, 그의 짧은 말 속에는 풍부한 감동이 담겨 있었다. “일어났네. ” 나는 소리쳤다.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/139       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/138       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/137       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/136       \n",
            "Negative tokens: ['산만한' '걱정일세,' '무렵의' '얼른' '대답했지만,']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 137/138       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 136/137       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 135/136       \n",
            "Peak count: 8\n",
            "Frame tokens: 그들이 미행시키겠습니다. 걱정이네. “무서운 걱정일세. 일어나면서 “일어났네. 소리쳤다. \n",
            "\n",
            "Similarity : 0.5651068921688067\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.468736410140991 Generator / grammar loss:-0.2224803864955902   similarity loss:-0.17471908032894135\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "두 물론 그들이 떠나면 미행시키겠습니다. 두세 헤이스팅즈. 걱정이네. 잭의 “무서운 광기란 걱정일세. 나는 아침, 기억하고 서서 곧 얼른 일어나면서 물었다. 일인가?” 대답했지만, “일어났네. 소리쳤다.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN    0.208487  0.477422  0.670586  0.649873  0.007498  0.625739   \n",
            "\n",
            "    grammar  \n",
            "0  0.982723  \n",
            "Current result ==================================================\n",
            "Sample count: 67\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.172259  0.530964  0.515389     0.52481  0.005484      0.52133   \n",
            "\n",
            "   grammarity  \n",
            "0    0.969343  \n",
            "==================================================\n",
            "68 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "“뭐라고? 하지만 오늘은 25일이잖나. ” “어젯밤. 아니, 오늘 아침 일찍 일어났네. ” 내가 침대에서 튀어 일어나 재빨리 옷을 입자 그는 지금 막 전화로 들은 이야기를 간간히 들려주었다. “젊은 여자의 시체가 벡스힐 바닷가에서 발견됐네. 그녀는 일리저버스 버너드(Barnard)라는 이름의 카페 여급사로 밝혀졌네. 이 아가씨는 최근 갓 지은 조그만 방갈로에서 부모와 함께 살고 있었지. 검시 결과에 의하면 12시에서 새벽 1시 사이에 살해된 모양일세. ” 나는 면도를 하며 물었다. “그러나 이것이 그 범죄라는 건 확실한가?” “벡스힐 행 기차 시간표가 있는 데가 펼쳐진 ABC 철도 안내서가 시체 밑에서 나왔네. ” 나는 손이 떨렸다. “무서운 이야기로군. ” “조심하게, 헤이스팅즈. 내 방에서 또 하나의 사건이 일어나면 참을 수 없으니까. ” 나는 얼마쯤 맥이 풀려 턱의 피를 닦았다. 그리고 물었다. “우리들의 전투 계획은?” “이제 곧 경찰차가 우리를 데리러 오게 되어 있네. 자네 커피는 이리로 가져오도록 시켰네. 출발을 늦출 수 없으니까.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/133       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/132       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/131       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/130       \n",
            "Negative tokens: ['막' '이' '최근' '일어나면' '“이제']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 131/132       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 130/131       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 129/130       \n",
            "Peak count: 7\n",
            "Frame tokens: ” 일어났네. 이 “그러나 헤이스팅즈. 물었다. 없으니까. \n",
            "\n",
            "Similarity : 0.3866438649937426\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.6891236305236816 Generator / grammar loss:-0.20159755647182465   similarity loss:-0.12974561750888824\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "25일이잖나. ” 아침 일어났네. ” 여급사로 밝혀졌네. 이 아가씨는 모양일세. 물었다. “그러나 건 기차 이야기로군. 헤이스팅즈. 그리고 물었다. 곧 오게 없으니까.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var    total  \\\n",
            "0  SAM+WGAN    0.174157  0.777859  0.487127  0.435648  0.022698  0.52983   \n",
            "\n",
            "    grammar  \n",
            "0  0.983995  \n",
            "Current result ==================================================\n",
            "Sample count: 68\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.172287  0.534595  0.514973    0.523499  0.005737     0.521455   \n",
            "\n",
            "   grammarity  \n",
            "0    0.969559  \n",
            "==================================================\n",
            "69 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "” 20분 뒤 우리는 속력 빠른 경찰차를 타고 템즈 강을 건너 런던을 벗어났다. 경찰차에는 크롬 형사가 함께 타고 있었다. 지난번 회의에 참석했었던 이 사건 담당 형사였다. 크롬 형사는 재프 경감과는 다른 타입의 경관이었다. 훨씬 젊고 말수가 적은 남자였다. 교양과 지식을 지녔으나, 내 기호로 말하면 얼마쯤 자기만족에 빠져있는 듯했다. 그는 최근에 있었던 일련의 어린이 살해 사건으로, 지금 브로드무어에 들어가 있는 범인을 끈질기게 추적해 이름을 떨친 참이었다. 그는 분명 이번 사건을 맡는 데 알맞은 인물이었으나, 그것을 그 자신이 지나치게 의식하고 있는 듯 생각되었다. 포아로를 대하는 그의 태도에는 좀 잘난 체하는 데가 있고, 얼마쯤 자의식적인 공립학교 식 방법으로 젊은 사람이 연장자를 대하는 것같이 그에게 복종하고 있었다. 그가 말했다. “저는 솜프슨 박사와 많은 이야기를 했습니다. 그분은 연속 살인 사건에 아주 흥미를 갖고 계시지요. 그것은 특수하게 비틀린 심성의 산물입니다. 물론 비전문가로서는 그가 의학적 견지에 대해 보이는 세부적인 점은 이해할 수 없지요. ” 그는 헛기침을 했다. “사실 저의 지난번 사건으로 말하면, 그 기사를 읽으셨는지 모르겠습니다만, 머스윌 힐 여학교 학생 메이벌 호머 사건 말입니다. 그 캐퍼라는 사나이는 무서운 녀석이었습니다. 그 범죄를 그의 짓이라고 밝혀내기까지 참으로 힘이 들었습니다. 아무튼 세 사람째였으니 말입니다. 아주 멀쩡해 보였지요.\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/178       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/177       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/176       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/175       \n",
            "Negative tokens: ['형사가' '교양과' '그는' '범인을' '인물이었으나,' '잘난' '자의식적인' '것같이' '많은' '그분은' '흥미를'\n",
            " '“사실' '사건으로' '그' '밝혀내기까지']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 176/177       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 175/176       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 174/175       \n",
            "Peak count: 9\n",
            "Frame tokens: 경관이었다. 남자였다. 자기만족에 참이었다. 자신이 있었다. 산물입니다. 그 보였지요. \n",
            "\n",
            "Similarity : 0.43295154619372567\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:2.579594135284424 Generator / grammar loss:-0.18990348279476166   similarity loss:-0.1302345246076584\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "경관이었다. 젊고 적은 남자였다. 자기만족에 추적해 떨친 참이었다. 인물이었으나, 자신이 있는 그의 식 있었다. 솜프슨 산물입니다. 저의 힐 학생 호머 캐퍼라는 그 참으로 아무튼 말입니다. 멀쩡해 보였지요.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending     var     total  \\\n",
            "0  SAM+WGAN    0.159059  0.384698  0.541563  0.634145  0.0106  0.537965   \n",
            "\n",
            "    grammar  \n",
            "0  0.977274  \n",
            "Current result ==================================================\n",
            "Sample count: 69\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.172095  0.532423  0.515359    0.525102  0.005808     0.521695   \n",
            "\n",
            "   grammarity  \n",
            "0    0.969671  \n",
            "==================================================\n",
            "70 / 70\n",
            "==================================================\n",
            "------------------------------------------------------------------\n",
            "하지만 여러 가지 테스트라는 게 있잖습니까. 아시겠지요, 유도심문이라는 것을. 물론 아주 새로운 것이어서 전에는 별로 문제되지 않았던 겁니다. 한 번 잡아들이면 이미 문제없습니다!이쪽이 알고 있다는 걸 알게 되면 상대방은 그만 손을 들지요. 상대는 완전히 이쪽이 하는 대로 따라옵니다. ” 포아로가 말했다. “우리 때에도 그런 일은 흔히 있었지요. ” 크롬 형사는 그를 돌아보고 대화를 계속하는 것처럼 입속으로 말했다. “아, 그렇습니까?” 잠시 침묵이 이어졌다. 뉴크로스 역을 지났을 즈음 크롬이 말했다. “이 사건에 대해 무언가 들어주고 싶은 게 있으시면 말씀하십시오. ” “그럼, 죽은 아가씨에 대해 이야기해 주겠소?” “그녀는 23살로 카페 <진저 캣>의 여급사로 일하고 있었습니다. ” “아니, 그런 점이 아니라, 이를테면 그녀는 아름다웠다든가……. ” 크롬 형사는 당할 수 없다는 투로 말했다. “그런 일에 대해서는 전혀 들은 게 없습니다. ” 그의 몸짓은 이렇게 말하고 있었다. 이거 정말, 외국인이란 모두 이렇다니까! 포아로의 눈에 재미있어 하는 빛이 슬며시 떠올랐다. “그런 사실이 당신에게는 중요하게 보이지 않소?\n",
            "------------------------------------------------------------------\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/140       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/139       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/138       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/137       \n",
            "Negative tokens: ['하는' '”' '“그런' '”' '“그런']\n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 138/139       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 137/138       \n",
            "Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 136/137       \n",
            "Peak count: 8\n",
            "Frame tokens: 한 말했다. “아, “이 ” 있었다. 포아로의 않소? \n",
            "\n",
            "Similarity : 0.4661321572977345\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, beta:1.9548474550247192 Generator / grammar loss:-0.13163864612579346   similarity loss:-0.1361546516418457\n",
            "--------------------------------------------------\n",
            "gold summary:\n",
            "\n",
            "--------------------------------------------------\n",
            "sam_wgan summary:\n",
            "테스트라는 게 한 알게 되면 상대방은 손을 완전히 따라옵니다. ” 말했다. 있었지요. “아, “이 대해 대해 아름다웠다든가……. 당할 수 없습니다. 정말, 포아로의 눈에 떠올랐다.\n",
            "--------------------------------------------------\n",
            "     method  comp ratio     intro      body    ending       var     total  \\\n",
            "0  SAM+WGAN     0.17301  0.492511  0.531212  0.578031  0.001223  0.537518   \n",
            "\n",
            "    grammar  \n",
            "0  0.970171  \n",
            "Current result ==================================================\n",
            "Sample count: 70\n",
            "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
            "0  SAM+WGAN   0.172108  0.531853  0.515585    0.525859  0.005742     0.521921   \n",
            "\n",
            "   grammarity  \n",
            "0    0.969678  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>method</th>\n",
              "      <th>comp rate</th>\n",
              "      <th>intro</th>\n",
              "      <th>body</th>\n",
              "      <th>conclusion</th>\n",
              "      <th>isthmus</th>\n",
              "      <th>simlirality</th>\n",
              "      <th>grammarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SAM+WGAN</td>\n",
              "      <td>0.172108</td>\n",
              "      <td>0.531853</td>\n",
              "      <td>0.515585</td>\n",
              "      <td>0.525859</td>\n",
              "      <td>0.005742</td>\n",
              "      <td>0.521921</td>\n",
              "      <td>0.969678</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     method  comp rate     intro      body  conclusion   isthmus  simlirality  \\\n",
              "0  SAM+WGAN   0.172108  0.531853  0.515585    0.525859  0.005742     0.521921   \n",
              "\n",
              "   grammarity  \n",
              "0    0.969678  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9Rgq4SHvqOD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}