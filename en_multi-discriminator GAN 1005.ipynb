{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "korean_frame_token_0_1.0_gamma_10.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dolmani38/Summary2/blob/main/en_multi-discriminator%20GAN%201005.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkQCxNatSIOk"
      },
      "source": [
        "# English Multi-Discriminator GAN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZjIW9VwyjDf"
      },
      "source": [
        "ABSTRACT\n",
        "\n",
        "----\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K87VNBbeRLFF"
      },
      "source": [
        "#4. Implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQZeBAf8NxAR"
      },
      "source": [
        "## 4.1 기본 설정..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAdXzWGuKSBT",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f8e6084-e87d-43c4-a7ef-c57036f6f46f"
      },
      "source": [
        "if True:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "newO0mBXKVnE",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2c2f8a4-8974-4097-e991-d2f723c80713"
      },
      "source": [
        "#!pip install keybert\n",
        "!pip3 install transformers\n",
        "!pip3 install sentence-transformers\n",
        "\n",
        "#!pip install sentence-transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.11.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.18)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: ruamel.yaml==0.17.16 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (0.17.16)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.1.2 in /usr/local/lib/python3.7/dist-packages (from ruamel.yaml==0.17.16->huggingface-hub>=0.0.17->transformers) (0.2.6)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.10.0+cu102)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.0.18)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.9.0+cu102)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.62.3)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.11.2)\n",
            "Requirement already satisfied: tokenizers>=0.10.3 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.10.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.96)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.8.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.46)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (5.4.1)\n",
            "Requirement already satisfied: ruamel.yaml==0.17.16 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->sentence-transformers) (0.17.16)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.1.2 in /usr/local/lib/python3.7/dist-packages (from ruamel.yaml==0.17.16->huggingface-hub->sentence-transformers) (0.2.6)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmIxp0FnKXif",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "812d48a0-c3a2-4a4b-9b02-141d6a72c73f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# set seeds for reproducability\n",
        "from numpy.random import seed\n",
        "#seed(1)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string, os \n",
        "\n",
        "import urllib.request\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3J0n_lhKcgm",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "733a4004-b340-476d-c019-467dbf263b5c"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "logout = True"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ue_4ZfdRKfdX",
        "trusted": true
      },
      "source": [
        "# Print iterations progress\n",
        "class ProgressBar:\n",
        "\n",
        "    def __init__(self,total=20, prefix = '', suffix = '', decimals = 1, length = 20, fill = '|', printEnd = \"\\r\"):\n",
        "        self.total = total\n",
        "        self.prefix = prefix\n",
        "        self.suffix = suffix\n",
        "        self.decimals = decimals\n",
        "        self.length = length\n",
        "        self.fill = fill\n",
        "        self.printEnd = printEnd\n",
        "        self.ite = 0\n",
        "        self.back_filledLength = 0\n",
        "\n",
        "    def printProgress(self,iteration, text):\n",
        "        self.ite += iteration\n",
        "        percent = (\"{0:.\" + str(self.decimals) + \"f}\").format(100 * (self.ite / float(self.total)))\n",
        "        filledLength = int(self.length * self.ite // self.total)\n",
        "        bar = self.fill * filledLength + '.' * (self.length - filledLength)\n",
        "        if filledLength > self.back_filledLength or percent == 100:\n",
        "            if logout:\n",
        "                print(f'\\r{self.prefix} |{bar}| {percent}% {self.suffix}  {text}', end=\"\", flush=True)\n",
        "            # Print New Line on Complete\n",
        "            if self.ite == self.total: \n",
        "                if logout:\n",
        "                    print()\n",
        "        self.back_filledLength = filledLength    "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNHI0G6JKc5h",
        "trusted": true
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Zsv-LVkKmfL"
      },
      "source": [
        "##4.2 Grammar Discriminator Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VQdGLciKc_y",
        "trusted": true
      },
      "source": [
        "from transformers import BertTokenizer, BertTokenizerFast,AutoTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset, random_split\n",
        "\n",
        "import time\n",
        "import random\n",
        "import datetime\n",
        "import pickle\n",
        "\n",
        "# 간단한 전처리\n",
        "def clean_text(txt):\n",
        "    txt = txt.replace('\\n',' ')\n",
        "    txt = txt.replace('\\r',' ')    \n",
        "    txt = txt.replace('=','')\n",
        "    txt = txt.replace('\\\"','')   \n",
        "    txt = txt.replace('\\'','')\n",
        "    #txt = txt.replace(',','')\n",
        "    txt = txt.replace('..','')\n",
        "    txt = txt.replace('...','')\n",
        "    txt = txt.replace(' .','.')\n",
        "    txt = txt.replace('.','. ')\n",
        "    txt = txt.replace('  ',' ')\n",
        "    txt = txt.replace('  ',' ')    \n",
        "    txt = txt.replace('  ',' ')   \n",
        "    txt = txt.replace('  ',' ')           \n",
        "    txt = txt.replace('  ',' ')\n",
        "    txt = txt.replace('  ',' ')    \n",
        "    txt = txt.replace('  ',' ')   \n",
        "    txt = txt.replace('  ',' ')             \n",
        "    return txt.strip()\n",
        "\n",
        "def shuffling(txt):\n",
        "    txt_list = txt.split(' ')\n",
        "    random.shuffle(txt_list)\n",
        "    return ' '.join(txt_list)\n",
        "\n",
        "def collect_training_dataset_for_grammar_discriminator(sentences_dataset):\n",
        "\n",
        "    sentences = []\n",
        "    labels = []\n",
        "\n",
        "    for txtss in sentences_dataset:\n",
        "        txtss = clean_text(txtss)\n",
        "        txts = txtss.strip().split('.')\n",
        "        for txt in txts:  \n",
        "            txt = txt.strip()\n",
        "            if len(txt) > 10:\n",
        "                #ko_grammar_dataset.append([txt,1])\n",
        "                txt = txt.replace('.','')\n",
        "                tf = random.choice([True,False])\n",
        "                # 정상 또는 비정상 둘중에 하나만 데이터셋에 추가\n",
        "                if (tf):\n",
        "                    sentences.append(txt) # '.'의 위치를 보고 True, False를 판단 하기 땜에...\n",
        "                    labels.append(1)\n",
        "                else:\n",
        "                    sentences.append(shuffling(txt))\n",
        "                    labels.append(0)\n",
        "\n",
        "    return sentences,labels\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "class Grammar_Discriminator:\n",
        "\n",
        "\n",
        "    def __init__(self, pretraoned_kobert_model_name='bert-base-v2', input_dir=None):\n",
        "\n",
        "        if input_dir is None:\n",
        "            self.tokenizer = BertTokenizerFast.from_pretrained(pretraoned_kobert_model_name)\n",
        "            self.discriminator = BertForSequenceClassification.from_pretrained(\n",
        "                                    pretraoned_kobert_model_name, # Use the 12-layer BERT model, with an uncased vocab.\n",
        "                                    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                                                    # You can increase this for multi-class tasks.   \n",
        "                                    output_attentions = False, # Whether the model returns attentions weights.\n",
        "                                    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "                                )            \n",
        "        else:\n",
        "            self.__load_model(input_dir)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def set_dataset(self, sentences,labels):\n",
        "        # Print the original sentence.\n",
        "        print(' Original: ', sentences[0])\n",
        "\n",
        "        # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "        input_ids = []\n",
        "        attention_masks = []\n",
        "\n",
        "        # For every sentence...\n",
        "        for i, sent in enumerate(sentences):\n",
        "            print(f'\\r Tokenize {i+1}/{len(sentences)}', end=\"\", flush=True)            \n",
        "            # `encode_plus` will:\n",
        "            #   (1) Tokenize the sentence.\n",
        "            #   (2) Prepend the `[CLS]` token to the start.\n",
        "            #   (3) Append the `[SEP]` token to the end.\n",
        "            #   (4) Map tokens to their IDs.\n",
        "            #   (5) Pad or truncate the sentence to `max_length`\n",
        "            #   (6) Create attention masks for [PAD] tokens.\n",
        "            encoded_dict = self.tokenizer.encode_plus(\n",
        "                                sent,                      # Sentence to encode.\n",
        "                                add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                                max_length = 64,           # Pad & truncate all sentences.\n",
        "                                pad_to_max_length = True,\n",
        "                                return_attention_mask = True,   # Construct attn. masks.\n",
        "                                return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                                truncation = True,\n",
        "                        )\n",
        "            \n",
        "            # Add the encoded sentence to the list.    \n",
        "            input_ids.append(encoded_dict['input_ids'])\n",
        "            \n",
        "            # And its attention mask (simply differentiates padding from non-padding).\n",
        "            attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "        # Convert the lists into tensors.\n",
        "        input_ids = torch.cat(input_ids, dim=0)\n",
        "        attention_masks = torch.cat(attention_masks, dim=0)\n",
        "        labels = torch.tensor(labels)\n",
        "\n",
        "        # Print sentence 0, now as a list of IDs.\n",
        "        print('Original: ', sentences[0])\n",
        "        print('Token IDs:', input_ids[0])\n",
        "\n",
        "        # Training & Validation Split\n",
        "        # Divide up our training set to use 90% for training and 10% for validation.\n",
        "\n",
        "        # Combine the training inputs into a TensorDataset.\n",
        "        dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "        # Create a 90-10 train-validation split.\n",
        "\n",
        "        # Calculate the number of samples to include in each set.\n",
        "        train_size = int(0.9 * len(dataset))\n",
        "        val_size = len(dataset) - train_size\n",
        "\n",
        "        # Divide the dataset by randomly selecting samples.\n",
        "        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "        print('{:>5,} training samples'.format(train_size))\n",
        "        print('{:>5,} validation samples'.format(val_size))\n",
        "\n",
        "        # The DataLoader needs to know our batch size for training, so we specify it \n",
        "        # here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "        # size of 16 or 32.\n",
        "        self.batch_size = 32\n",
        "\n",
        "        # Create the DataLoaders for our training and validation sets.\n",
        "        # We'll take training samples in random order. \n",
        "        self.train_dataloader = DataLoader(\n",
        "                    train_dataset,  # The training samples.\n",
        "                    sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "                    batch_size = self.batch_size # Trains with this batch size.\n",
        "                )\n",
        "\n",
        "        # For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "        self.validation_dataloader = DataLoader(\n",
        "                    val_dataset, # The validation samples.\n",
        "                    sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "                    batch_size = self.batch_size # Evaluate with this batch size.\n",
        "                )        \n",
        "\n",
        "\n",
        "    def train(self,epochs=4):\n",
        "        # Tell pytorch to run this model on the GPU.\n",
        "        self.discriminator.cuda()\n",
        "\n",
        "        # Get all of the model's parameters as a list of tuples.\n",
        "        params = list(self.discriminator.named_parameters())\n",
        "\n",
        "        print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "        print('==== Embedding Layer ====\\n')\n",
        "\n",
        "        for p in params[0:5]:\n",
        "            print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "        print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "        for p in params[5:21]:\n",
        "            print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "        print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "        for p in params[-4:]:\n",
        "            print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))  \n",
        "\n",
        "        # Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "        # I believe the 'W' stands for 'Weight Decay fix\"\n",
        "        self.optimizer = AdamW(self.discriminator.parameters(),\n",
        "                        lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                        eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                        )\n",
        "\n",
        "        # Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "        # We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "        # training data.\n",
        "        #epochs = 2\n",
        "\n",
        "        # Total number of training steps is [number of batches] x [number of epochs]. \n",
        "        # (Note that this is not the same as the number of training samples).\n",
        "        total_steps = len(self.train_dataloader) * epochs\n",
        "\n",
        "        # Create the learning rate scheduler.\n",
        "        scheduler = get_linear_schedule_with_warmup(self.optimizer, \n",
        "                                                    num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                                    num_training_steps = total_steps)\n",
        "            \n",
        "        # This training code is based on the `run_glue.py` script here:\n",
        "        # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "        # Set the seed value all over the place to make this reproducible.\n",
        "        seed_val = 42\n",
        "\n",
        "        random.seed(seed_val)\n",
        "        np.random.seed(seed_val)\n",
        "        torch.manual_seed(seed_val)\n",
        "        torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "        # We'll store a number of quantities such as training and validation loss, \n",
        "        # validation accuracy, and timings.\n",
        "        training_stats = []\n",
        "\n",
        "        # Measure the total training time for the whole run.\n",
        "        total_t0 = time.time()\n",
        "\n",
        "        # For each epoch...\n",
        "        for epoch_i in range(0, epochs):\n",
        "            \n",
        "            # ========================================\n",
        "            #               Training\n",
        "            # ========================================\n",
        "            \n",
        "            # Perform one full pass over the training set.\n",
        "\n",
        "            print(\"\")\n",
        "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "            print('Training...')\n",
        "\n",
        "            # Measure how long the training epoch takes.\n",
        "            t0 = time.time()\n",
        "\n",
        "            # Reset the total loss for this epoch.\n",
        "            total_train_loss = 0\n",
        "\n",
        "            # Put the model into training mode. Don't be mislead--the call to \n",
        "            # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "            # `dropout` and `batchnorm` layers behave differently during training\n",
        "            # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "            self.discriminator.train()\n",
        "\n",
        "            # For each batch of training data...\n",
        "            for step, batch in enumerate(self.train_dataloader):\n",
        "\n",
        "                # Progress update every 40 batches.\n",
        "                if step % 40 == 0 and not step == 0:\n",
        "                    # Calculate elapsed time in minutes.\n",
        "                    elapsed = format_time(time.time() - t0)\n",
        "                    \n",
        "                    # Report progress.\n",
        "                    print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(self.train_dataloader), elapsed))\n",
        "\n",
        "                # Unpack this training batch from our dataloader. \n",
        "                #\n",
        "                # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "                # `to` method.\n",
        "                #\n",
        "                # `batch` contains three pytorch tensors:\n",
        "                #   [0]: input ids \n",
        "                #   [1]: attention masks\n",
        "                #   [2]: labels \n",
        "                b_input_ids = batch[0].to(device)\n",
        "                b_input_mask = batch[1].to(device)\n",
        "                b_labels = batch[2].to(device)\n",
        "\n",
        "                # Always clear any previously calculated gradients before performing a\n",
        "                # backward pass. PyTorch doesn't do this automatically because \n",
        "                # accumulating the gradients is \"convenient while training RNNs\". \n",
        "                # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "                self.discriminator.zero_grad()        \n",
        "\n",
        "                # Perform a forward pass (evaluate the model on this training batch).\n",
        "                # The documentation for this `model` function is here: \n",
        "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "                # It returns different numbers of parameters depending on what arguments\n",
        "                # arge given and what flags are set. For our useage here, it returns\n",
        "                # the loss (because we provided labels) and the \"logits\"--the model\n",
        "                # outputs prior to activation.\n",
        "                outputs = self.discriminator(b_input_ids, \n",
        "                                    token_type_ids=None, \n",
        "                                    attention_mask=b_input_mask, \n",
        "                                    labels=b_labels)\n",
        "                loss, logits = outputs.loss, outputs.logits\n",
        "                # Accumulate the training loss over all of the batches so that we can\n",
        "                # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "                # single value; the `.item()` function just returns the Python value \n",
        "                # from the tensor.\n",
        "                total_train_loss += loss.item()\n",
        "\n",
        "                # Perform a backward pass to calculate the gradients.\n",
        "                loss.backward()\n",
        "\n",
        "                # Clip the norm of the gradients to 1.0.\n",
        "                # This is to help prevent the \"exploding gradients\" problem.\n",
        "                torch.nn.utils.clip_grad_norm_(self.discriminator.parameters(), 1.0)\n",
        "\n",
        "                # Update parameters and take a step using the computed gradient.\n",
        "                # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "                # modified based on their gradients, the learning rate, etc.\n",
        "                self.optimizer.step()\n",
        "\n",
        "                # Update the learning rate.\n",
        "                scheduler.step()\n",
        "\n",
        "            # Calculate the average loss over all of the batches.\n",
        "            avg_train_loss = total_train_loss / len(self.train_dataloader)            \n",
        "            \n",
        "            # Measure how long this epoch took.\n",
        "            training_time = format_time(time.time() - t0)\n",
        "\n",
        "            print(\"\")\n",
        "            print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "            print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "                \n",
        "            # ========================================\n",
        "            #               Validation\n",
        "            # ========================================\n",
        "            # After the completion of each training epoch, measure our performance on\n",
        "            # our validation set.\n",
        "\n",
        "            print(\"\")\n",
        "            print(\"Running Validation...\")\n",
        "\n",
        "            t0 = time.time()\n",
        "\n",
        "            # Put the model in evaluation mode--the dropout layers behave differently\n",
        "            # during evaluation.\n",
        "            self.discriminator.eval()\n",
        "\n",
        "            # Tracking variables \n",
        "            total_eval_accuracy = 0\n",
        "            total_eval_loss = 0\n",
        "            nb_eval_steps = 0\n",
        "\n",
        "            # Evaluate data for one epoch\n",
        "            for batch in self.validation_dataloader:\n",
        "                \n",
        "                # Unpack this training batch from our dataloader. \n",
        "                #\n",
        "                # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "                # the `to` method.\n",
        "                #\n",
        "                # `batch` contains three pytorch tensors:\n",
        "                #   [0]: input ids \n",
        "                #   [1]: attention masks\n",
        "                #   [2]: labels \n",
        "                b_input_ids = batch[0].to(device)\n",
        "                b_input_mask = batch[1].to(device)\n",
        "                b_labels = batch[2].to(device)\n",
        "                \n",
        "                # Tell pytorch not to bother with constructing the compute graph during\n",
        "                # the forward pass, since this is only needed for backprop (training).\n",
        "                with torch.no_grad():        \n",
        "\n",
        "                    # Forward pass, calculate logit predictions.\n",
        "                    # token_type_ids is the same as the \"segment ids\", which \n",
        "                    # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "                    # The documentation for this `model` function is here: \n",
        "                    # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "                    # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "                    # values prior to applying an activation function like the softmax.\n",
        "                    outputs = self.discriminator(b_input_ids, \n",
        "                                        token_type_ids=None, \n",
        "                                        attention_mask=b_input_mask,\n",
        "                                        labels=b_labels)\n",
        "                loss, logits = outputs.loss, outputs.logits\n",
        "                # Accumulate the validation loss.\n",
        "                total_eval_loss += loss.item()\n",
        "\n",
        "                # Move logits and labels to CPU\n",
        "                logits = logits.detach().cpu().numpy()\n",
        "                label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "                # Calculate the accuracy for this batch of test sentences, and\n",
        "                # accumulate it over all batches.\n",
        "                total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "                \n",
        "\n",
        "            # Report the final accuracy for this validation run.\n",
        "            avg_val_accuracy = total_eval_accuracy / len(self.validation_dataloader)\n",
        "            print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "            # Calculate the average loss over all of the batches.\n",
        "            avg_val_loss = total_eval_loss / len(self.validation_dataloader)\n",
        "            \n",
        "            # Measure how long the validation run took.\n",
        "            validation_time = format_time(time.time() - t0)\n",
        "            \n",
        "            print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "            print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "            # Record all statistics from this epoch.\n",
        "            training_stats.append(\n",
        "                {\n",
        "                    'epoch': epoch_i + 1,\n",
        "                    'Training Loss': avg_train_loss,\n",
        "                    'Valid. Loss': avg_val_loss,\n",
        "                    'Valid. Accur.': avg_val_accuracy,\n",
        "                    'Training Time': training_time,\n",
        "                    'Validation Time': validation_time\n",
        "                }\n",
        "            )\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"Training complete!\")\n",
        "\n",
        "        print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "            \n",
        "\n",
        "        return training_stats\n",
        "\n",
        "    def save_model(self, output_dir = './model_save/'):\n",
        "        # Create output directory if needed\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "\n",
        "        print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "        # They can then be reloaded using `from_pretrained()`\n",
        "        model_to_save = self.discriminator.module if hasattr(self.discriminator, 'module') else self.discriminator  # Take care of distributed/parallel training\n",
        "        model_to_save.save_pretrained(output_dir)\n",
        "        self.tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "        # Good practice: save your training arguments together with the trained model\n",
        "        # torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n",
        "\n",
        "    def __load_model(self, input_dir = './drive/MyDrive/Colab Notebooks/summary/en_grammar_check_model'):\n",
        "        print('Loading BERT tokenizer...')\n",
        "        self.tokenizer = BertTokenizerFast.from_pretrained(input_dir)\n",
        "        self.discriminator = BertForSequenceClassification.from_pretrained(input_dir)\n",
        "\n",
        "    def transfer_learning(self, sentences, train_for = True):\n",
        "        \n",
        "        input_ids = []\n",
        "        attention_masks = []\n",
        "\n",
        "        # For every sentence...\n",
        "        for sent in sentences:\n",
        "            # `encode_plus` will:\n",
        "            #   (1) Tokenize the sentence.\n",
        "            #   (2) Prepend the `[CLS]` token to the start.\n",
        "            #   (3) Append the `[SEP]` token to the end.\n",
        "            #   (4) Map tokens to their IDs.\n",
        "            #   (5) Pad or truncate the sentence to `max_length`\n",
        "            #   (6) Create attention masks for [PAD] tokens.\n",
        "            encoded_dict = self.tokenizer.encode_plus(\n",
        "                                sent,                      # Sentence to encode.\n",
        "                                add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                                max_length = 64,           # Pad & truncate all sentences.\n",
        "                                pad_to_max_length = True,\n",
        "                                return_attention_mask = True,   # Construct attn. masks.\n",
        "                                return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                                truncation = True,\n",
        "                        )\n",
        "            # Add the encoded sentence to the list.    \n",
        "            input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "            # And its attention mask (simply differentiates padding from non-padding).\n",
        "            attention_masks.append(encoded_dict['attention_mask'])\n",
        "        \n",
        "        if train_for:\n",
        "            b_labels = torch.ones(len(sentences),dtype=torch.long).to(device)\n",
        "        else:\n",
        "            b_labels = torch.zeros(len(sentences),dtype=torch.long).to(device)\n",
        "        #print(b_labels)\n",
        "        # Convert the lists into tensors.\n",
        "        input_ids = torch.cat(input_ids, dim=0).to(device)\n",
        "        attention_masks = torch.cat(attention_masks, dim=0).to(device)    \n",
        "        #if str(discriminator1.device) == 'cpu':\n",
        "        #    pass\n",
        "        #else:\n",
        "        #    input_ids = input_ids.to(device)\n",
        "        #    attention_masks = attention_masks.to(device)        \n",
        "\n",
        "        outputs = self.discriminator(input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=attention_masks, \n",
        "                                labels=b_labels)\n",
        "\n",
        "        #print(outputs)\n",
        "        #return torch.sigmoid(outputs[0][:,1])\n",
        "        #return outputs[0][:,1]\n",
        "        return outputs['loss'], outputs['logits']\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4zeEb0NR2QH"
      },
      "source": [
        "# 문법 discriminator 활용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7Zf2oRMMXmH",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b28720ec-5ff8-4ea9-90bd-f94b39375976"
      },
      "source": [
        "g_discriminator = Grammar_Discriminator(input_dir = '/content/drive/MyDrive/Colab Notebooks/summary/en_grammar_model')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BERT tokenizer...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEXRsgqlXkpf",
        "outputId": "fd6d1959-265b-4799-94c4-b3eaf62e2e79"
      },
      "source": [
        "txt = ['Her friends sadly never heard from her after they parted company.','Her friends sadly never from her after heard they parted company.']\n",
        "g_discriminator.discriminator.to(device)\n",
        "g_discriminator.transfer_learning(txt)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(2.1402, device='cuda:0', grad_fn=<NllLossBackward>),\n",
              " tensor([[-5.8312,  5.9503],\n",
              "         [ 2.0386, -2.2279]], device='cuda:0', grad_fn=<AddmmBackward>))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d96kaCAHKuUc"
      },
      "source": [
        "##4.3 Static similarity discriminator class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZDpXe7XKxeg",
        "trusted": true
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import BertTokenizer\n",
        "from scipy.signal import find_peaks\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.misc import electrocardiogram\n",
        "import scipy\n",
        "\n",
        "\n",
        "class Similarity_Discriminator:\n",
        "    '''\n",
        "    _instance = None\n",
        "    _embedder = None\n",
        "    def __new__(cls,pre_trained_model_name='stsb-roberta-large'):\n",
        "        if cls._instance is None:\n",
        "            print('Creating Similarity_Discriminator object')\n",
        "            cls._instance = super(Similarity_Discriminator, cls).__new__(cls)\n",
        "            # Put any initialization here.\n",
        "            cls._embedder = SentenceTransformer(pre_trained_model_name)\n",
        "        return cls._instance\n",
        "\n",
        "    '''\n",
        "\n",
        "    def __init__(self,pre_trained_model_name='stsb-roberta-large'): #'roberta-large-nli-stsb-mean-tokens'):\n",
        "        print('Creating Similarity_Discriminator object')\n",
        "        # Put any initialization here.\n",
        "        self._embedder = SentenceTransformer(pre_trained_model_name,device=device)  \n",
        "        #self.cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "    def encode(self,texts):\n",
        "        return self._embedder.encode(texts,show_progress_bar=False)\n",
        "\n",
        "    def similarity(self, query_text, org_text_emb):\n",
        "        queries = nltk.sent_tokenize(query_text)\n",
        "        query_embeddings = self._embedder.encode(queries,show_progress_bar=False)\n",
        "        #query_embeddings = self._embedder.encode(queries,show_progress_bar=False)\n",
        "        #print(queries)\n",
        "        #print(org_text_emb)\n",
        "        \n",
        "        if len(query_embeddings) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        cos_scores = scipy.spatial.distance.cdist(query_embeddings, org_text_emb, \"cosine\")\n",
        "        similarity_score = 1.0 - np.mean(np.min(cos_scores,axis=1))\n",
        "        '''\n",
        "        for query, query_embedding in zip(queries, query_embeddings):\n",
        "            distances = scipy.spatial.distance.cdist([query_embedding], [org_text_emb], \"cosine\")[0]\n",
        "            results = zip(range(len(distances)), distances)\n",
        "            for idx, distance in results:\n",
        "                scores.append(1-distance)\n",
        "        '''\n",
        "        return similarity_score  \n",
        " "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sQZ36GuMumP"
      },
      "source": [
        "###4.3.1 영어 문장 유사도 pre-trained model 적용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9Miao14Muww",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81487b22-f92b-4e4d-c407-1d3d0a6169fe"
      },
      "source": [
        "#del s_discriminator\n",
        "\n",
        "s_discriminator = Similarity_Discriminator()\n",
        "#s_discriminator = Similarity_Discriminator()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Similarity_Discriminator object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NGqfC0qL1JU"
      },
      "source": [
        "# ExtactiveSummarizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-NxbjyjT9k-"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import re\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import BertTokenizer, BertModel"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpwkLTqaUApn"
      },
      "source": [
        "\n",
        "class ExtactiveSummarizer:\n",
        "    # 한국어의 경우, 'kykim/bert-kor-base'\n",
        "    def __init__(self,model_name='bert-base-uncased'):\n",
        "        \n",
        "        #nltk.download('stopwords')\n",
        "        nltk.download('punkt')\n",
        "\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "        self.model = BertModel.from_pretrained(model_name, return_dict=True, output_attentions=True)\n",
        "        self.cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "        # If there's a GPU available...\n",
        "        if torch.cuda.is_available():    \n",
        "            # Tell PyTorch to use the GPU.    \n",
        "            self.device = torch.device(\"cuda\")\n",
        "        # If not...\n",
        "        else:\n",
        "            self.device = torch.device(\"cpu\")\n",
        "\n",
        "        self.model.to(self.device)\n",
        "        #self.cos.to(self.device)\n",
        "        \n",
        "    def read_article(self,text):        \n",
        "        sentences =[]        \n",
        "        sentences = sent_tokenize(text)    \n",
        "        for sentence in sentences:        \n",
        "            sentence.replace(\"[^a-zA-Z0-9]\",\" \")     \n",
        "        return sentences\n",
        "\n",
        "    def sentence_similarity(self,sent1,sent2):\n",
        "\n",
        "        sent_1_pooler_output = s_discriminator.encode([sent1])\n",
        "        #print(sent_1_pooler_output)\n",
        "        sent_2_pooler_output = s_discriminator.encode([sent2])\n",
        "        '''\n",
        "        tok_sent1 = self.tokenizer(sent1, return_tensors=\"pt\")\n",
        "        tok_sent2 = self.tokenizer(sent2, return_tensors=\"pt\")\n",
        "        tok_sent1.to(self.device)\n",
        "        tok_sent2.to(self.device)\n",
        "        outputs = self.model(**tok_sent1)\n",
        "        sent_1_pooler_output = outputs.pooler_output\n",
        "\n",
        "        outputs = self.model(**tok_sent2)\n",
        "        sent_2_pooler_output = outputs.pooler_output\n",
        "        '''\n",
        "        cos_scores = scipy.spatial.distance.cdist(sent_1_pooler_output, sent_2_pooler_output, \"cosine\")\n",
        "        return 1 - cos_scores[0] #self.cos(sent_1_pooler_output, sent_2_pooler_output) #.cpu().detach().numpy()\n",
        "\n",
        "    def get_self_attention_weight(self,sentence):\n",
        "        tok_sent = self.tokenizer(sentence, return_tensors=\"pt\")\n",
        "        tok_sent.to(self.device)\n",
        "        outputs = self.model(**tok_sent)\n",
        "        \n",
        "        attentions = torch.stack(outputs.attentions)\n",
        "        #attention = outputs[-1]  # Output includes attention weights when output_attentions=True\n",
        "        last_attentions = attentions[11][0][11]\n",
        "        #print(last_attentions.shape)\n",
        "        tokens = [self.tokenizer.convert_ids_to_tokens(s) for s in tok_sent['input_ids'].tolist()[0]]\n",
        "        #print(tokens)\n",
        "        attention_map = []\n",
        "        for i,token in enumerate(tokens):\n",
        "            if token.startswith('##'):\n",
        "                (ii,tt,s) = attention_map[-1]\n",
        "                tt += token.replace('##','')\n",
        "                s0 = torch.sum(last_attentions[:,i]).item()\n",
        "                s1 = s if s > s0 else s0\n",
        "                attention_map[len(attention_map)-1] = (ii,tt,s1)\n",
        "            else:\n",
        "                attention_map.append((i,token,torch.sum(last_attentions[:,i]).item()))\n",
        "            \n",
        "        return attention_map\n",
        "\n",
        "    # Create similarity matrix among all sentences\n",
        "    def build_similarity_matrix(self,sentences):\n",
        "        #create an empty similarity matrix\n",
        "        pooler_output = s_discriminator.encode(sentences)\n",
        "\n",
        "        cos_scores = scipy.spatial.distance.cdist(pooler_output, pooler_output, \"cosine\")\n",
        "\n",
        "        similarity_matrix = np.ones((len(sentences),len(sentences)))\n",
        "        similarity_matrix = similarity_matrix - cos_scores\n",
        "        #print(similarity_matrix)        \n",
        "        '''\n",
        "        for idx1 in range(len(sentences)):\n",
        "            for idx2 in range(len(sentences)):\n",
        "                if idx1!=idx2:\n",
        "                    similarity_matrix[idx1][idx2] = self.sentence_similarity(sentences[idx1],sentences[idx2])\n",
        "        '''      \n",
        "        return similarity_matrix\n",
        "\n",
        "    # Generate and return text summary\n",
        "    def generate_summary(self,text,top_n,min_length=30):\n",
        "        \n",
        "        ft = []\n",
        "        org_sentences = np.array(nltk.sent_tokenize(text))\n",
        "        for txt in org_sentences:\n",
        "            if len(txt) > min_length:\n",
        "                ft.append(txt)\n",
        "        text = ' '.join(ft)\n",
        "\n",
        "        #stop_words = stopwords.words('english')\n",
        "        summarize_text = []\n",
        "        \n",
        "        # Step1: read text and tokenize\n",
        "        sentences = self.read_article(text)\n",
        "        \n",
        "        # Steo2: generate similarity matrix across sentences\n",
        "        sentence_similarity_matrix = self.build_similarity_matrix(sentences)\n",
        "        \n",
        "        # Step3: Rank sentences in similarirty matrix\n",
        "        sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_matrix)\n",
        "        scores = nx.pagerank(sentence_similarity_graph)\n",
        "        #print(scores)\n",
        "        orderd = [(o,scores[o]) for o in scores.keys()]\n",
        "        orderd.sort(key=lambda e: e[1],reverse=True)\n",
        "\n",
        "        #print(orderd)\n",
        "\n",
        "        top_n = top_n if len(orderd) > top_n else len(orderd)\n",
        "        a = [orderd[i][0] for i in range(0,top_n)]\n",
        "        a.sort()\n",
        "        summ_text = \" \".join([sentences[i] for i in a])\n",
        "\n",
        "        '''\n",
        "        #Step4: sort the rank and place top sentences\n",
        "        ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)),reverse=True)\n",
        "        \n",
        "        #print(ranked_sentences)\n",
        "        # Step 5: get the top n number of sentences based on rank    \n",
        "        for i in range(top_n if top_n < len(ranked_sentences) else len(ranked_sentences)):\n",
        "            summarize_text.append(ranked_sentences[i][1])\n",
        "        \n",
        "        orderd = [(o,s) for o,s in enumerate(scores)]\n",
        "        orderd.sort(key=lambda e: e[1],reverse=True)\n",
        "        top_n = top_n if len(orderd) > top_n else len(orderd)\n",
        "        a = [orderd[i][0] for i in range(0,top_n)]\n",
        "        a.sort()\n",
        "        summ_text = \" \".join([sentences[i] for i in a])\n",
        "        '''\n",
        "        # Step 6 : outpur the summarized version\n",
        "        return summ_text,len(sentences)   # \" \".join(summarize_text), len(sentences)  #"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVsoAq3hz3mr",
        "outputId": "0174e4c0-903c-4962-937b-7c9522eb3f02"
      },
      "source": [
        "es = ExtactiveSummarizer()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVWEePCD0VTr",
        "outputId": "af42428f-2eda-47db-e7be-aefc6149a3d1"
      },
      "source": [
        "es.sentence_similarity('This IS expected if you are initializing from the checkpoint','This IS expected if you are initializing BertModel from the checkpoint')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.75140336])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qG4aL4oRz8mu",
        "outputId": "230a5464-aa35-4a8a-c770-3b714ddd9f13"
      },
      "source": [
        "es.get_self_attention_weight('The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo Grand Forks and Jamestown to the hepatitis A virus in late September and early October.')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, '[CLS]', 0.18557289242744446),\n",
              " (1, 'the', 0.07899213582277298),\n",
              " (2, 'bishop', 0.3666439354419708),\n",
              " (3, 'of', 0.05223364382982254),\n",
              " (4, 'the', 0.10034431517124176),\n",
              " (5, 'fargo', 0.0550147145986557),\n",
              " (6, 'catholic', 0.3552972078323364),\n",
              " (7, 'diocese', 0.11490436643362045),\n",
              " (8, 'in', 0.12254498898983002),\n",
              " (9, 'north', 0.009641865268349648),\n",
              " (10, 'dakota', 0.04165465384721756),\n",
              " (11, 'has', 0.11064591258764267),\n",
              " (12, 'exposed', 0.2776542901992798),\n",
              " (13, 'potentially', 0.10957956314086914),\n",
              " (14, 'hundreds', 0.1402682662010193),\n",
              " (15, 'of', 0.19087064266204834),\n",
              " (16, 'church', 0.38132524490356445),\n",
              " (17, 'members', 0.20232832431793213),\n",
              " (18, 'in', 0.20666250586509705),\n",
              " (19, 'fargo', 0.12040776759386063),\n",
              " (20, 'grand', 0.03221951425075531),\n",
              " (21, 'forks', 0.06993243098258972),\n",
              " (22, 'and', 0.13787443935871124),\n",
              " (23, 'jamestown', 0.29331332445144653),\n",
              " (24, 'to', 0.25634700059890747),\n",
              " (25, 'the', 0.23425814509391785),\n",
              " (26, 'hepatitis', 0.4447019398212433),\n",
              " (27, 'a', 0.09705034643411636),\n",
              " (28, 'virus', 0.23700100183486938),\n",
              " (29, 'in', 0.16439568996429443),\n",
              " (30, 'late', 0.10334088653326035),\n",
              " (31, 'september', 0.12408782541751862),\n",
              " (32, 'and', 0.15648669004440308),\n",
              " (33, 'early', 0.05973568558692932),\n",
              " (34, 'october', 0.09182393550872803),\n",
              " (35, '.', 17.814111709594727),\n",
              " (36, '[SEP]', 13.460731506347656)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xnk9GsQ0K1t1"
      },
      "source": [
        "# 4.4 Document source class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYs__02JjKjT"
      },
      "source": [
        "## CNN/Daily mail Sample data 수집"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCOWg1jX-OKH"
      },
      "source": [
        "import pickle\n",
        "if False:\n",
        "    with open(\"/content/drive/MyDrive/Colab Notebooks/summary/data/dnn_daily_mail_sample.bin\", \"rb\") as fp:\n",
        "        dt = pickle.load(fp)\n",
        "    sentences_dataset = dt[0]\n",
        "    gold_summary = dt[1]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftdYeFqbhlTK"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "if False:\n",
        "    train_data, test_data = tfds.load(name=\"cnn_dailymail\",split=(tfds.Split.TRAIN,tfds.Split.TEST),with_info=True)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkiRPhb5iC7M"
      },
      "source": [
        "if False:\n",
        "    sentences_dataset = []\n",
        "    gold_summary = []\n",
        "    iterator = iter(train_data[1])\n",
        "    cnt = 0\n",
        "    for data in iterator:\n",
        "        cnt += 1\n",
        "        if cnt < 1000:\n",
        "            sentences_dataset.append(data['article'].numpy().decode('UTF-8'))\n",
        "            gold_summary.append(data['highlights'].numpy().decode('UTF-8'))\n",
        "        else:\n",
        "            break\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khvoe_AHiqA0"
      },
      "source": [
        "import pickle\n",
        "if False:\n",
        "    with open(\"/content/drive/MyDrive/Colab Notebooks/summary/data/dnn_daily_mail_test.bin\", \"wb\") as fp:\n",
        "        pickle.dump([sentences_dataset,gold_summary],fp)\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJ1lRv75i4Gx"
      },
      "source": [
        "import pickle\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/summary/data/dnn_daily_mail_test.bin\", \"rb\") as fp:\n",
        "    dt = pickle.load(fp)\n",
        "sentences_dataset = dt[0]\n",
        "gold_summary = dt[1]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "yHlsQPhWj8JX",
        "outputId": "f7f88269-773a-4794-df1b-8d175d005906"
      },
      "source": [
        "gold_summary[0]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Experts question if  packed out planes are putting passengers at risk .\\nU.S consumer advisory group says minimum space must be stipulated .\\nSafety tests conducted on planes with more leg room than airlines offer .'"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnBm6RCvNIWG"
      },
      "source": [
        "## 4.4.2 source class 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYHIkaar2zb-"
      },
      "source": [
        "# 간단한 전처리\n",
        "def __clean_text(txt):\n",
        "    txt = txt.replace('\\n',' ')\n",
        "    txt = txt.replace('\\r',' ')   \n",
        "    txt = txt.replace('·',' ')       \n",
        "    #txt = txt.replace('=','')\n",
        "    #txt = txt.replace('\\\"','')   \n",
        "    #txt = txt.replace('\\'','')\n",
        "    #txt = txt.replace(',','')\n",
        "    #txt = txt.replace('..','')\n",
        "    #txt = txt.replace('...','')\n",
        "    txt = txt.replace('.',' ')\n",
        "    #txt = txt.replace('.','. ')\n",
        "    txt = txt.replace('  ',' ')\n",
        "    txt = txt.replace('  ',' ')    \n",
        "    txt = txt.replace('  ',' ')   \n",
        "    txt = txt.replace('  ',' ')           \n",
        "    txt = txt.replace('  ',' ')\n",
        "    txt = txt.replace('  ',' ')    \n",
        "    txt = txt.replace('  ',' ')   \n",
        "    txt = txt.replace('  ',' ')             \n",
        "    return txt.strip()\n",
        "\n",
        "def get_prepared_doc(txt):\n",
        "    docs = []\n",
        "    sentences = np.array(nltk.sent_tokenize(txt))\n",
        "    for sen in sentences:\n",
        "        docs.append(__clean_text(sen) +'.')\n",
        "    return (' '.join(docs)).strip()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsJKbtc2K4xN",
        "trusted": true
      },
      "source": [
        "\n",
        "\n",
        "class Source:\n",
        "\n",
        "    def __init__(self,full_text,org_text,delete_ending = False,attendtion_rate=0.3):\n",
        "        self.full_text = full_text\n",
        "        self.org_text = org_text\n",
        "        self.delete_ending = delete_ending\n",
        "        self.attendtion_rate = attendtion_rate\n",
        "\n",
        "    def __crean_text(self, txt):\n",
        "        txt = txt.replace('\\n',' ')\n",
        "        txt = txt.replace('\\r',' ')    \n",
        "        txt = txt.replace('=','')\n",
        "        txt = txt.replace('\\\"','')   \n",
        "        txt = txt.replace('\\'','')\n",
        "        txt = txt.replace(',','')\n",
        "        txt = txt.replace('..','')\n",
        "        txt = txt.replace('...','')\n",
        "        txt = txt.replace(' .','.')\n",
        "        txt = txt.replace('.','. ')\n",
        "        txt = txt.replace('  ',' ')\n",
        "        txt = txt.replace('  ',' ')    \n",
        "        txt = txt.replace('  ',' ')   \n",
        "        txt = txt.replace('  ',' ')           \n",
        "        txt = txt.replace('  ',' ')\n",
        "        txt = txt.replace('  ',' ')    \n",
        "        txt = txt.replace('  ',' ')   \n",
        "        txt = txt.replace('  ',' ')           \n",
        "        return txt.strip()\n",
        "\n",
        "    def get_similarity(self,sent):\n",
        "        pooler_output = s_discriminator.encode([sent])\n",
        "        cos_scores = scipy.spatial.distance.cdist(self.org_text_emb, pooler_output, \"cosine\")\n",
        "        a = np.ones(cos_scores.shape) - cos_scores\n",
        "        #print(cos_scores)\n",
        "        #print(np.mean(cos_scores))\n",
        "        return np.mean(a)-np.std(a) #self.cos(sent_1_pooler_output, sent_2_pooler_output) #.cpu().detach().numpy()\n",
        "\n",
        "    def set_key_rate(self,s_discriminator):\n",
        "        # full_text에 대한 처리...\n",
        "        self.full_text = self.__crean_text(self.full_text.strip())\n",
        "        self.full_sentences = np.array(nltk.sent_tokenize(self.full_text))\n",
        "        self.s_discriminator = s_discriminator\n",
        "        self.full_text_emb = self.s_discriminator.encode(self.full_sentences)   \n",
        "\n",
        "        # original sentance, 즉 source sentence에 대한 처리\n",
        "        self.org_text = self.__crean_text(self.org_text.strip())\n",
        "        if logout:\n",
        "            print('-'*50)\n",
        "            print(self.org_text)\n",
        "            print('-'*50)  \n",
        "\n",
        "        # 두개 이상의 문장이 있는 경우, 중간 문자의 마침표를 지우고,\n",
        "        # ' and'를 넣어서, 연결한다.\n",
        "        self.org_sentences = np.array(nltk.sent_tokenize(self.org_text))\n",
        "        self.org_text_emb = self.s_discriminator.encode(self.org_sentences)\n",
        "        s = []\n",
        "        for i,sents in enumerate(self.org_sentences):\n",
        "            if sents.endswith('.') and i < len(self.org_sentences)-1:\n",
        "                s.append(sents[:-1].strip() + \" and\")\n",
        "            else:\n",
        "                s.append(sents)\n",
        "                #self.org_sentences[i] = sents[:-1].strip() + \" and\"\n",
        "                #print(self.org_sentences[i])\n",
        "        self.org_sentences = s\n",
        "        #print(s)\n",
        "        #print(self.org_sentences)\n",
        "        # 하나의 문장을 token 단위로 잘라서 {index:token} dict을 만든다.\n",
        "        # 또한, 각 token의 attention을 설정한다.\n",
        "        self.org_term_set = (' '.join(self.org_sentences)).strip().split(' ')\n",
        "        self.org_source_length = len(self.org_term_set)\n",
        "        self.term_table = {}\n",
        "\n",
        "        self.seps = []\n",
        "        self.bias_table = {}\n",
        "        #morp_table = {}\n",
        "        aw = 0.0\n",
        "        attentions = []\n",
        "        self_attentions = es.get_self_attention_weight(self.org_text)\n",
        "        self_attentions_max = 0\n",
        "        self_attentions_map = {}\n",
        "        for index, word in enumerate(self.org_term_set):\n",
        "            self.term_table[index] = word\n",
        "            #attention = cosine_similarity(self.full_text,word)\n",
        "            #attentions.append(attention)\n",
        "            sa = 0\n",
        "            for u in range(index,len(self_attentions)):\n",
        "                if word.lower().replace('.','') == self_attentions[u][1]:\n",
        "                    sa = self_attentions[u][2]\n",
        "                    if sa > self_attentions_max:\n",
        "                        self_attentions_max = sa\n",
        "\n",
        "            self_attentions_map[index] = sa\n",
        "            #print(f'{word} \\t\\t {attention:.4f} {sa:.4f}')\n",
        "            \n",
        "        #print('self_attentions_max',self_attentions_max)\n",
        "        attentions = list(self_attentions_map.values())\n",
        "        attentions.sort(reverse=True)\n",
        "        #문장 전체 token의 30%에 attention을 준다.\n",
        "        trs = attentions[int(len(attentions)*self.attendtion_rate + 0.5)]\n",
        "\n",
        "        for index, word in enumerate(self.org_term_set):\n",
        "            self.term_table[index] = word\n",
        "            self.bias_table[index] = self_attentions_map[index] - trs #self_attentions_max\n",
        "            #attention = self_attentions_map[index] #cosine_similarity(self.full_text,word)\n",
        "            self.bias_table[index] = (0.0 if self.bias_table[index] > 0 else -1.0)\n",
        "            #self.bias_table[index] = (self.bias_table[index] if self.bias_table[index] > 0 else -1.0)\n",
        "            #if len(self.org_term_set) - 1 == index:\n",
        "            #        self.bias_table[index] = 0.1\n",
        "            '''\n",
        "            if attention >= trs or index == len(self.org_term_set)-1:\n",
        "                self.bias_table[index] = self_attentions_map[index] - trs #0.0 #attention\n",
        "            else:\n",
        "                self.bias_table[index] = -1.0 #attention #-cosine_similarity(self.full_text,word)\n",
        "            '''\n",
        "            '''\n",
        "            if word.endswith(('.','?')):\n",
        "                self.seps.append(index)\n",
        "                if self.org_source_length - 1 == index:\n",
        "                    pass\n",
        "                else:\n",
        "                    self.term_table[index] = combine_sentence(word)\n",
        "            '''\n",
        "            if logout:\n",
        "                print(f'{index} \\t {self_attentions_map[index]:.4f} \\t {self.bias_table[index]} \\t{word} ')            \n",
        "        #print(list(self.bias_table.values()))\n",
        "        # 또 다른 token 단위의 {index:token} dict을 만드는데, 이는 generator의 조합이\n",
        "        # 문법적으로 부실할 경우, corrector가 보정할때 '~~고'의 중간 연결문을 \n",
        "        # 부드럽게 만들기 위해 중간 문장의 '~다.'를 삭제한 dict에 해당한다.\n",
        "        self.combination_table = {}\n",
        "        for index, word in enumerate(self.org_term_set):\n",
        "            self.combination_table[index] = word\n",
        "            if index < len(self.org_term_set)-1: #중간 문장의 '~다.'를 삭제한다.\n",
        "                if self.org_term_set[index].endswith('다.'):\n",
        "                    self.combination_table[index] = word[0:len(word)-2]\n",
        "\n",
        "        #print(self.combination_table)\n",
        "        if len(self.term_table) > 128:\n",
        "            raise Exception(\"Too much sentence length.\")\n",
        "\n",
        "    def get_org_sample(self, num):\n",
        "        return self.org_sentences[np.random.choice(len(self.org_sentences), num)]\n",
        "\n",
        "    def get_source_embedded_code(self):\n",
        "        return self.org_text_emb\n",
        "\n",
        "    def get_random_text(self,rate=0.5):\n",
        "        cnt = int(len(self.term_table) * rate)\n",
        "        a = list(self.term_table.keys())\n",
        "        b = np.random.choice(a, cnt)\n",
        "        c = [fruit for fruit in a if fruit not in b]\n",
        "        txt = []\n",
        "        for i in c:\n",
        "            txt.append(self.term_table[i])\n",
        "        return ' '.join(txt).strip(), hash(tuple(b))"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af5b1VF7poE2"
      },
      "source": [
        "## N-Gram Similarity Comparison\n",
        "\n",
        "https://gist.github.com/gaulinmp/da5825de975ed0ea6a24186434c24fe4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAfA5fHxBoGW",
        "outputId": "6f706094-99e6-4044-878f-b09157ad8d15"
      },
      "source": [
        "# Get Tuple algorithms \n",
        "import re\n",
        "import math\n",
        "import numpy as np\n",
        "from itertools import chain\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.util import ngrams # This is the ngram magic.\n",
        "from textblob import TextBlob\n",
        "\n",
        "NGRAM = 4\n",
        "\n",
        "re_sent_ends_naive = re.compile(r'[.\\n]')\n",
        "re_stripper_alpha = re.compile('[^a-zA-Z]+')\n",
        "re_stripper_naive = re.compile('[^a-zA-Z\\.\\n]')\n",
        "\n",
        "splitter_naive = lambda x: re_sent_ends_naive.split(re_stripper_naive.sub(' ', x))\n",
        "\n",
        "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "def get_tuples_nosentences(txt):\n",
        "    \"\"\"Get tuples that ignores all punctuation (including sentences).\"\"\"\n",
        "    if not txt: return None\n",
        "    #ng = ngrams(re_stripper_alpha.sub(' ', txt).split(), NGRAM)\n",
        "    ng = ngrams(txt, NGRAM)\n",
        "    return list(ng)\n",
        "\n",
        "def get_tuples_manual_sentences(txt):\n",
        "    \"\"\"Naive get tuples that uses periods or newlines to denote sentences.\"\"\"\n",
        "    if not txt: return None\n",
        "    sentences = (x.split() for x in splitter_naive(txt) if x)\n",
        "    ng = (ngrams(x, NGRAM) for x in sentences if len(x) >= NGRAM)\n",
        "    return list(chain(*ng))\n",
        "\n",
        "def get_tuples_nltk_punkt_sentences(txt):\n",
        "    \"\"\"Get tuples that doesn't use textblob.\"\"\"\n",
        "    if not txt: return None\n",
        "    sentences = (re_stripper_alpha.split(x) for x in sent_detector.tokenize(txt) if x)\n",
        "    # Need to filter X because of empty 'words' from punctuation split\n",
        "    ng = (ngrams(filter(None, x), NGRAM) for x in sentences if len(x) >= NGRAM)\n",
        "    return list(chain(*ng))\n",
        "\n",
        "def get_tuples_textblob_sentences(txt):\n",
        "    \"\"\"New get_tuples that does use textblob.\"\"\"\n",
        "    if not txt: return None\n",
        "    tb = TextBlob(txt)\n",
        "    ng = (ngrams(x.words, NGRAM) for x in tb.sentences if len(x.words) > NGRAM)\n",
        "    return [item for sublist in ng for item in sublist]\n",
        "\n",
        "def jaccard_distance(a, b):\n",
        "    \"\"\"Calculate the jaccard distance between sets A and B\"\"\"\n",
        "    a = set(a)\n",
        "    b = set(b)\n",
        "    return 1.0 * len(a&b)/len(a|b)\n",
        "\n",
        "def cosine_similarity_ngrams(a, b):\n",
        "    vec1 = Counter(a)\n",
        "    vec2 = Counter(b)\n",
        "    \n",
        "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
        "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
        "\n",
        "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
        "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
        "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
        "\n",
        "    if not denominator:\n",
        "        return 0.0\n",
        "    return float(numerator) / denominator\n",
        "\n",
        "\n",
        "def test():\n",
        "    paragraph = \"\"\"It was the best of times, it was the worst of times.\n",
        "               It was the age of wisdom? It was the age of foolishness!\n",
        "               I first met Dr. Frankenstein in Munich; his monster was, presumably, at home.\"\"\"\n",
        "    print(paragraph)\n",
        "    _ = get_tuples_nosentences(paragraph);print(\"Number of N-grams (no sentences):\", len(_));_\n",
        "\n",
        "    _ = get_tuples_manual_sentences(paragraph);print(\"Number of N-grams (naive sentences):\", len(_));_\n",
        "\n",
        "    _ = get_tuples_nltk_punkt_sentences(paragraph);print(\"Number of N-grams (nltk sentences):\", len(_));_\n",
        "\n",
        "    _ = get_tuples_textblob_sentences(paragraph);print(\"Number of N-grams (TextBlob sentences):\", len(_));_\n",
        "\n",
        "    a = get_tuples_nosentences(\"It was the best of times.\")\n",
        "    b = get_tuples_nosentences(\"It was the worst of times.\")\n",
        "    print(\"Jaccard: {}   Cosine: {}\".format(jaccard_distance(a,b), cosine_similarity_ngrams(a,b)))\n",
        "\n",
        "    a = get_tuples_nosentences(\"Above is a bad example of four-gram similarity.\")\n",
        "    b = get_tuples_nosentences(\"This is a better example of four-gram similarity.\")\n",
        "    print(\"Jaccard: {}   Cosine: {}\".format(jaccard_distance(a,b), cosine_similarity_ngrams(a,b)))\n",
        "\n",
        "    a = get_tuples_nosentences(\"Jaccard Index ignores repetition repetition repetition repetition repetition.\")\n",
        "    b = get_tuples_nosentences(\"Cosine similarity weighs repetition repetition repetition repetition repetition.\")\n",
        "    print(\"Jaccard: {}   Cosine: {}\".format(jaccard_distance(a,b), cosine_similarity_ngrams(a,b)))\n",
        "test()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It was the best of times, it was the worst of times.\n",
            "               It was the age of wisdom? It was the age of foolishness!\n",
            "               I first met Dr. Frankenstein in Munich; his monster was, presumably, at home.\n",
            "Number of N-grams (no sentences): 214\n",
            "Number of N-grams (naive sentences): 25\n",
            "Number of N-grams (nltk sentences): 25\n",
            "Number of N-grams (TextBlob sentences): 25\n",
            "Jaccard: 0.6071428571428571   Cosine: 0.755742181606458\n",
            "Jaccard: 0.6071428571428571   Cosine: 0.755742181606458\n",
            "Jaccard: 0.23214285714285715   Cosine: 0.9208243668497166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akhPuNZHBx4w"
      },
      "source": [
        "def cosine_similarity(src_txt,trg_txt):\n",
        "    try:\n",
        "        if src_txt == None or src_txt.strip() == '':\n",
        "            return 0.0\n",
        "        if trg_txt == None or trg_txt.strip() == '':\n",
        "            return 0.0\n",
        "\n",
        "        a = get_tuples_nosentences(src_txt)\n",
        "        b = get_tuples_nosentences(trg_txt)\n",
        "        return cosine_similarity_ngrams(a,b)\n",
        "    except Exception as ex:\n",
        "        #print(src_txt,trg_txt)\n",
        "        return 0.0"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UAeFBYMMxKY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ae1cd6e-d406-40c1-8e36-e0baeccb197f"
      },
      "source": [
        "txt = \"\"\"\n",
        "The judge agreed with police that he would have been over the limit at the time his red Citroen hit Miss Titley’s blue Daihatsu Cuore on a road near Yarmouth, Isle of Wight, on October 11, 2013.\n",
        "His phone records showed he was also texting around the time of the crash.\n",
        "\"\"\"\n",
        "s = Source(txt,txt)\n",
        "s.set_key_rate(s_discriminator)\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "The judge agreed with police that he would have been over the limit at the time his red Citroen hit Miss Titley’s blue Daihatsu Cuore on a road near Yarmouth Isle of Wight on October 11 2013. His phone records showed he was also texting around the time of the crash.\n",
            "--------------------------------------------------\n",
            "0 \t 0.0952 \t -1.0 \tThe \n",
            "1 \t 0.0642 \t -1.0 \tjudge \n",
            "2 \t 0.0412 \t -1.0 \tagreed \n",
            "3 \t 0.0220 \t -1.0 \twith \n",
            "4 \t 0.1507 \t -1.0 \tpolice \n",
            "5 \t 0.0306 \t -1.0 \tthat \n",
            "6 \t 0.2372 \t 0.0 \the \n",
            "7 \t 0.0633 \t -1.0 \twould \n",
            "8 \t 0.0910 \t -1.0 \thave \n",
            "9 \t 0.2784 \t 0.0 \tbeen \n",
            "10 \t 0.3194 \t 0.0 \tover \n",
            "11 \t 0.0952 \t -1.0 \tthe \n",
            "12 \t 0.4084 \t 0.0 \tlimit \n",
            "13 \t 0.0320 \t -1.0 \tat \n",
            "14 \t 0.0952 \t -1.0 \tthe \n",
            "15 \t 0.0217 \t -1.0 \ttime \n",
            "16 \t 0.2955 \t 0.0 \this \n",
            "17 \t 0.1724 \t -1.0 \tred \n",
            "18 \t 0.2191 \t 0.0 \tCitroen \n",
            "19 \t 0.3664 \t 0.0 \thit \n",
            "20 \t 0.4336 \t 0.0 \tMiss \n",
            "21 \t 0.0000 \t -1.0 \tTitley’s \n",
            "22 \t 0.1807 \t -1.0 \tblue \n",
            "23 \t 0.4862 \t 0.0 \tDaihatsu \n",
            "24 \t 0.2958 \t 0.0 \tCuore \n",
            "25 \t 0.1166 \t -1.0 \ton \n",
            "26 \t 0.1323 \t -1.0 \ta \n",
            "27 \t 0.1636 \t -1.0 \troad \n",
            "28 \t 0.1093 \t -1.0 \tnear \n",
            "29 \t 0.2713 \t 0.0 \tYarmouth \n",
            "30 \t 0.0761 \t -1.0 \tIsle \n",
            "31 \t 0.0250 \t -1.0 \tof \n",
            "32 \t 0.1073 \t -1.0 \tWight \n",
            "33 \t 0.1166 \t -1.0 \ton \n",
            "34 \t 0.0968 \t -1.0 \tOctober \n",
            "35 \t 0.0547 \t -1.0 \t11 \n",
            "36 \t 0.1783 \t -1.0 \t2013 \n",
            "37 \t 0.0000 \t -1.0 \tand \n",
            "38 \t 0.2955 \t 0.0 \tHis \n",
            "39 \t 0.0893 \t -1.0 \tphone \n",
            "40 \t 0.0756 \t -1.0 \trecords \n",
            "41 \t 0.1856 \t 0.0 \tshowed \n",
            "42 \t 0.2372 \t 0.0 \the \n",
            "43 \t 0.0921 \t -1.0 \twas \n",
            "44 \t 0.1201 \t -1.0 \talso \n",
            "45 \t 0.8318 \t 0.0 \ttexting \n",
            "46 \t 0.0698 \t -1.0 \taround \n",
            "47 \t 0.0952 \t -1.0 \tthe \n",
            "48 \t 0.0217 \t -1.0 \ttime \n",
            "49 \t 0.0250 \t -1.0 \tof \n",
            "50 \t 0.0952 \t -1.0 \tthe \n",
            "51 \t 0.3884 \t 0.0 \tcrash. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lH9NLi9koYvo"
      },
      "source": [
        "def besm(full_text,top_rank=2):\n",
        "    '''\n",
        "    ft = []\n",
        "    org_sentences = np.array(nltk.sent_tokenize(full_text))\n",
        "    for txt in org_sentences:\n",
        "        if len(txt) > 30:\n",
        "            ft.append(txt)\n",
        "    full_text = ' '.join(ft)\n",
        "    '''\n",
        "    queries = nltk.sent_tokenize(full_text)\n",
        "    src_sentences = nltk.sent_tokenize(full_text)\n",
        "    query_embeddings = s_discriminator._embedder.encode(queries,show_progress_bar=False)\n",
        "    full_text_embeddings = s_discriminator._embedder.encode(src_sentences,show_progress_bar=False)\n",
        "    #print(queries)\n",
        "    #print(org_text_emb)\n",
        "    \n",
        "    if len(query_embeddings) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    cos_scores = scipy.spatial.distance.cdist(query_embeddings, full_text_embeddings, \"cosine\")\n",
        "    scores = np.max(cos_scores,axis=1)\n",
        "    orderd = [(o,s) for o,s in enumerate(scores)]\n",
        "    orderd.sort(key=lambda e: e[1],reverse=True)\n",
        "    top_rank = top_rank if len(orderd) > top_rank else len(orderd)\n",
        "    a = [orderd[i][0] for i in range(0,top_rank)]\n",
        "    a.sort()\n",
        "    summ_text = \" \".join([queries[i] for i in a])\n",
        "\n",
        "    return summ_text"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YA0YDeInoh7z",
        "outputId": "554bd635-a636-4195-9ccb-494f6ce0668a"
      },
      "source": [
        "full_text = get_prepared_doc(sentences_dataset[0])\n",
        "org_sentences = np.array(nltk.sent_tokenize(full_text))\n",
        "for txt in org_sentences:\n",
        "    print(txt)\n",
        "print()\n",
        "\n",
        "org_sentences = np.array(nltk.sent_tokenize(besm(full_text,top_rank=6)))\n",
        "for txt in org_sentences:\n",
        "    print(txt)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ever noticed how plane seats appear to be getting smaller and smaller?.\n",
            "With increasing numbers of people taking to the skies, some experts are questioning if having such packed out planes is putting passengers at risk.\n",
            "They say that the shrinking space on aeroplanes is not only uncomfortable - it's putting our health and safety in danger.\n",
            "More than squabbling over the arm rest, shrinking space on planes putting our health and safety in danger?.\n",
            "This week, a U S consumer advisory group set up by the Department of Transportation said at a public hearing that while the government is happy to set standards for animals flying on planes, it doesn't stipulate a minimum amount of space for humans.\n",
            "'In a world where animals have more rights to space and food than humans,' said Charlie Leocha, consumer representative on the committee.\n",
            "'It is time that the DOT and FAA take a stand for humane treatment of passengers '.\n",
            "But could crowding on planes lead to more serious issues than fighting for space in the overhead lockers, crashing elbows and seat back kicking?.\n",
            "Tests conducted by the FAA use planes with a 31 inch pitch, a standard which on some airlines has decreased.\n",
            "Many economy seats on United Airlines have 30 inches of room, while some airlines offer as little as 28 inches.\n",
            "Cynthia Corbertt, a human factors researcher with the Federal Aviation Administration, that it conducts tests on how quickly passengers can leave a plane.\n",
            "But these tests are conducted using planes with 31 inches between each row of seats, a standard which on some airlines has decreased, reported the Detroit News.\n",
            "The distance between two seats from one point on a seat to the same point on the seat behind it is known as the pitch.\n",
            "While most airlines stick to a pitch of 31 inches or above, some fall below this.\n",
            "While United Airlines has 30 inches of space, Gulf Air economy seats have between 29 and 32 inches, Air Asia offers 29 inches and Spirit Airlines offers just 28 inches.\n",
            "British Airways has a seat pitch of 31 inches, while easyJet has 29 inches, Thomson's short haul seat pitch is 28 inches, and Virgin Atlantic's is 30-31.\n",
            "\n",
            "Ever noticed how plane seats appear to be getting smaller and smaller?.\n",
            "With increasing numbers of people taking to the skies, some experts are questioning if having such packed out planes is putting passengers at risk.\n",
            "They say that the shrinking space on aeroplanes is not only uncomfortable - it's putting our health and safety in danger.\n",
            "This week, a U S consumer advisory group set up by the Department of Transportation said at a public hearing that while the government is happy to set standards for animals flying on planes, it doesn't stipulate a minimum amount of space for humans.\n",
            "Cynthia Corbertt, a human factors researcher with the Federal Aviation Administration, that it conducts tests on how quickly passengers can leave a plane.\n",
            "The distance between two seats from one point on a seat to the same point on the seat behind it is known as the pitch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGTFQU_PqaLk",
        "outputId": "7475eb22-62b8-4039-cb01-1b122b306c76"
      },
      "source": [
        "sum1 = es.generate_summary(full_text,top_n=9,min_length=0)[0]\n",
        "org_sentences = np.array(nltk.sent_tokenize(sum1))\n",
        "for txt in org_sentences:\n",
        "    print(txt)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ever noticed how plane seats appear to be getting smaller and smaller?.\n",
            "With increasing numbers of people taking to the skies, some experts are questioning if having such packed out planes is putting passengers at risk.\n",
            "They say that the shrinking space on aeroplanes is not only uncomfortable - it's putting our health and safety in danger.\n",
            "More than squabbling over the arm rest, shrinking space on planes putting our health and safety in danger?.\n",
            "Tests conducted by the FAA use planes with a 31 inch pitch, a standard which on some airlines has decreased.\n",
            "Many economy seats on United Airlines have 30 inches of room, while some airlines offer as little as 28 inches.\n",
            "But these tests are conducted using planes with 31 inches between each row of seats, a standard which on some airlines has decreased, reported the Detroit News.\n",
            "While most airlines stick to a pitch of 31 inches or above, some fall below this.\n",
            "British Airways has a seat pitch of 31 inches, while easyJet has 29 inches, Thomson's short haul seat pitch is 28 inches, and Virgin Atlantic's is 30-31.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5ng68INoY4D"
      },
      "source": [
        "\n",
        "def besm2(full_text,text,top_rank=2):\n",
        "    scores = []\n",
        "    queries = nltk.sent_tokenize(text)\n",
        "    for sen in queries:\n",
        "        s = cosine_similarity(sen,full_text)\n",
        "        scores.append(s)\n",
        "        #print(s,sen)\n",
        "    orderd = [(o,s) for o,s in enumerate(scores)]\n",
        "    orderd.sort(key=lambda e: e[1],reverse=True)\n",
        "    a = [orderd[i][0] for i in range(0,top_rank)]\n",
        "    a.sort()\n",
        "    summ_text = \" \".join([queries[i] for i in a])\n",
        "\n",
        "    return summ_text"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XY59mdNK8ub"
      },
      "source": [
        "# 4.5 Generator class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5CLF3WcK6lp",
        "trusted": true
      },
      "source": [
        "from functools import reduce\n",
        "\n",
        "\n",
        "# custom weights initialization called on netG and netD\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Linear') != -1:\n",
        "        m.weight.data.normal_(0.02, 0.08)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.05)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "        Simple Generator w/ MLP\n",
        "    \"\"\"\n",
        "    '''\n",
        "    def __init__(self, input_size=1024):\n",
        "        super(Generator, self).__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(input_size, input_size*2),\n",
        "            nn.BatchNorm1d(input_size*2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(input_size*2, input_size*3),\n",
        "            nn.BatchNorm1d(input_size*3),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(input_size*3, input_size*3),\n",
        "            nn.BatchNorm1d(input_size*3),\n",
        "            nn.ReLU(True),            \n",
        "            nn.Linear(input_size*3, input_size*2),\n",
        "            nn.BatchNorm1d(input_size*2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(input_size*2, input_size),\n",
        "            #nn.BatchNorm1d(term_length*4),\n",
        "            nn.Tanh() # -1 ~ 1\n",
        "        )\n",
        "\n",
        "    \n",
        "    def __init__(self, input_size=1024):\n",
        "        super(Generator, self).__init__()\n",
        "        l1 = nn.Linear(input_size, input_size*4)\n",
        "        l1.weight.data.normal_(0.0, 0.01)\n",
        "        bn = nn.BatchNorm1d(input_size*4)\n",
        "        bn.weight.data.normal_(0.0, 0.01)\n",
        "        bn.bias.data.fill_(0)        \n",
        "        l2 = nn.Linear(input_size*4, input_size)\n",
        "        l2.weight.data.normal_(0.05, 0.01)\n",
        "        self.layer = nn.Sequential(\n",
        "            l1,\n",
        "            bn,\n",
        "            nn.ReLU(True), #nn.LeakyReLU(0.2),\n",
        "            l2,\n",
        "            #nn.BatchNorm1d(term_length*4),\n",
        "            nn.Tanh() # -1 ~ 1\n",
        "        )\n",
        "    '''\n",
        "\n",
        "    def __init__(self, input_size=1024):\n",
        "        super(Generator, self).__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(input_size, input_size*4),\n",
        "            nn.BatchNorm1d(input_size*4),\n",
        "            nn.ReLU(True), #nn.LeakyReLU(0.2),\n",
        "            nn.Linear(input_size*4, input_size),\n",
        "            #nn.BatchNorm1d(input_size*4),\n",
        "            #nn.ReLU(True), #nn.LeakyReLU(0.2),            \n",
        "            #nn.Linear(input_size*4, input_size),\n",
        "            #nn.BatchNorm1d(input_size),\n",
        "            #nn.ReLU(True), #nn.LeakyReLU(0.2),\n",
        "            nn.Tanh() # -1 ~ 1\n",
        "        )\n",
        "\n",
        "    def forward(self, x, bias):\n",
        "        #biased_noise = torch.randn(N,_NOISE_DIM)\n",
        "        # stroy peak에 해당하는 term에게 평균값에 해당하는 bias를 추가 한다.\n",
        "                 \n",
        "        y_ = self.layer(x)\n",
        "        y = torch.add(y_,bias)\n",
        "        #y = nn.Sigmoid()(y)\n",
        "\n",
        "        return y, y_"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0RQOPpQgUTE"
      },
      "source": [
        "# SAM_Summarizer 학습기..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nd8GTS7HKz1H",
        "trusted": true
      },
      "source": [
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "from scipy.special import expit\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SAM_Summarizer:\n",
        "\n",
        "    def __init__(self,g_discriminator,s_discriminator):\n",
        "        self.g_discriminator = g_discriminator\n",
        "        #self.c_discriminator = c_discriminator\n",
        "        self.s_discriminator = s_discriminator\n",
        "        self.m = nn.Sigmoid()\n",
        "        self.with_bias = True\n",
        "\n",
        "    def ready(self,source):\n",
        "        self.source = source  \n",
        "        #self.source.analysis_frame_terms(self.s_discriminator)\n",
        "        self.generator = Generator(input_size=self.source.org_source_length)\n",
        "        self.generator.apply(weights_init)\n",
        "        return self\n",
        "\n",
        "    def summarize(self,epochs=10,batch_size=1,learning_rate=2e-4, display = False,comp_rate=1.0):\n",
        "        history = self.__train(epochs,batch_size,learning_rate,display,comp_rate)\n",
        "\n",
        "        if display and history is not None:\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            plt.plot(history['gen_g_loss'],label='grammar loss')\n",
        "            plt.plot(history['gen_l_loss'],label='compression loss')\n",
        "            plt.plot(history['gen_s_loss'],label='n-gram similarity loss')\n",
        "            #plt.plot(history['gen_c_loss'],label='context similarity loss')\n",
        "            #plt.plot(history['total loss'],label='total loss')\n",
        "            plt.plot(history['losses std'],label='standard deviation of losses')\n",
        "            \n",
        "            #if 'dis_loss' in history:\n",
        "            #    plt.plot(history['dis_loss'],label='discriminator grammar loss')\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "        return history\n",
        "\n",
        "    # text의 생성 for torch\n",
        "    def __text_gen2(self, p_txt, gen_length):\n",
        "        gtext = []\n",
        "        sorted_noise, i = torch.sort(p_txt, descending=True)\n",
        "        order, i = torch.sort(i[:gen_length], descending=False)\n",
        "        #print(len(order))\n",
        "        #print(gen_length)\n",
        "        assert len(order) == gen_length\n",
        "        order = order.cpu().detach().numpy()\n",
        "        for k in order:\n",
        "            gtext.append((self.source.term_table[k],k))\n",
        "        return gtext\n",
        "\n",
        "    def __text_gen3(self, p_txt):\n",
        "        gtext = []\n",
        "\n",
        "        for order,p in enumerate(p_txt):\n",
        "            if p > 0.0:\n",
        "                gtext.append(self.source.term_table[order])\n",
        "        return gtext\n",
        "\n",
        "    def __text_gen5(self, p_txt):\n",
        "        gtext = []\n",
        "\n",
        "        for order,p in enumerate(p_txt):\n",
        "            if p > 0.0:\n",
        "                gtext.append(self.source.combination_table[order])\n",
        "        return gtext\n",
        "\n",
        "    def __text_hash(self, p_txt):\n",
        "        b = []\n",
        "        #hash(tuple(b))\n",
        "        for order,p in enumerate(p_txt):\n",
        "            if p > 0.0:\n",
        "                b.append(order)\n",
        "        return hash(tuple(b))\n",
        "\n",
        "    def __text_gen4(self, p_txt):\n",
        "        gtext = \"\"\n",
        "        indexs = []\n",
        "        for order,p in enumerate(p_txt):\n",
        "            if p > 0.0:\n",
        "                gtext += self.source.term_table[order] + ' '\n",
        "                indexs.append(order)\n",
        "        return gtext.strip(),indexs\n",
        "\n",
        "    def __train(self, epochs=10,batch_size=10,learning_rate=2e-4,display = False,comp_rate=1.0):\n",
        "        # In the Deepmind paper they use RMSProp however then Adam optimizer\n",
        "        # improves training time\n",
        "        #generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "        # This method returns a helper function to compute cross entropy loss\n",
        "        #cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "        # Set the seed value all over the place to make this reproducible.\n",
        "        seed_val = int(random.random()*100)\n",
        "\n",
        "        random.seed(seed_val)\n",
        "        np.random.seed(seed_val)\n",
        "        torch.manual_seed(seed_val)\n",
        "        torch.cuda.manual_seed_all(seed_val)\n",
        "        \n",
        "        criterion = nn.MSELoss()\n",
        "        #D_opt = torch.optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "        G_opt = AdamW(self.generator.parameters(),\n",
        "                        lr = 2e-3, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                        eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                        )\n",
        "        # Create the learning rate scheduler.\n",
        "        scheduler = get_linear_schedule_with_warmup(G_opt, \n",
        "                                                    num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                                    num_training_steps = epochs)\n",
        "        \n",
        "        #gen_length = len(self.source.story_peaks) + int(len(self.source.story_peaks)*self.frame_expansion_ratio)\n",
        "        pb = ProgressBar(epochs,prefix='Train...')\n",
        "        gen_gmr_loss_history = []\n",
        "        gen_len_loss_history = []\n",
        "        gen_sim_loss_history = []\n",
        "        #gen_cos_loss_history = []\n",
        "        dis_loss_history = []    \n",
        "        total_loss_history = []\n",
        "        losses_std_history = []\n",
        "\n",
        "        #model 들은 cuda로 보낸다.\n",
        "        self.g_discriminator.discriminator.to(device)\n",
        "        self.g_discriminator.discriminator.eval() # 학습하지 않는다...\n",
        "        #self.c_discriminator.discriminator.to(device)\n",
        "        #self.c_discriminator.discriminator.eval() # 학습하지 않는다...\n",
        "\n",
        "        self.generator.to(device)       \n",
        "        self.generator.train()\n",
        "\n",
        "        #self.bias_w = init_bias\n",
        "        initial_bias = 0\n",
        "        #G_s_loss = torch.tensor(0)\n",
        "        #G_c_loss = torch.tensor(0)\n",
        "        #G_g_loss = torch.tensor(0)\n",
        "\n",
        "        '''\n",
        "        epsilon = 1 # Epsilon-greedy algorithm in initialized at 1 meaning every step is random at the start\n",
        "        max_epsilon = 1 # You can't explore more than 100% of the time\n",
        "        min_epsilon = 0.001 # At a minimum, we'll always explore 1% of the time\n",
        "        decay = 10/epochs\n",
        "        '''\n",
        "\n",
        "        dfs = torch.tensor([ 1.0, similarity, comp_rate], device=device, dtype=torch.float, requires_grad=True)\n",
        "        target = torch.tensor([list(self.source.bias_table.values()) for u in range(batch_size)],dtype=torch.float).to(device)\n",
        "        #print(target)\n",
        "        #noise = torch.randn(batch_size,self.source.org_source_length).to(device) \n",
        "        #a_w = 1.0\n",
        "        for i in range(epochs):\n",
        "   \n",
        "            if True:\n",
        "                noise = torch.randn(batch_size,self.source.org_source_length).to(device) \n",
        "                '''\n",
        "                random_number = np.random.rand()\n",
        "                # 2. Explore using the Epsilon Greedy Exploration Strategy\n",
        "                if random_number <= epsilon:\n",
        "                    # Explore\n",
        "                    bias = torch.randn(batch_size,self.source.org_source_length).to(device) * epsilon\n",
        "                    #b = torch.tensor([list(self.source.bias_table.values()) for u in range(batch_size)]).to(device)\n",
        "                    #bias = torch.add(a,b)\n",
        "                    #noise = torch.randn(batch_size,self.source.org_source_length).to(device) \n",
        "                else:\n",
        "                    #bias = torch.tensor([list(self.source.bias_table.values()) for u in range(batch_size)]).to(device)\n",
        "                    bias = torch.zeros_like(noise).to(device)\n",
        "                '''\n",
        "                bias = torch.zeros_like(noise).to(device)\n",
        "\n",
        "\n",
        "                #if self.with_bias:\n",
        "                #    bias[:,noise.shape[1]-1] = 0.1\n",
        "                #bias[:,noise.shape[1]-1] = 0.5\n",
        "                #if i < epochs/4:\n",
        "                #bias = torch.randn(batch_size,self.source.org_source_length).to(device) / 4                 \n",
        "                #bias = torch.randn(batch_size,self.source.org_source_length).to(device) \n",
        "\n",
        "                sw, sw0 = self.generator(noise,bias)\n",
        "                # sw를 복제하고 gradient 안되도록 detach 시키고... cpu에서 작업\n",
        "                sw_relu = F.relu(sw.clone().detach().cpu())\n",
        "                sw_relu_indexs = (sw_relu > 0).nonzero()\n",
        "                grammar_filter = torch.where(sw > 0.0, 1.0, 0.0)\n",
        "                similar_filter = torch.where(sw > 0.0, 1.0, 0.0)\n",
        "                for j in range(sw.shape[0]): # batch size\n",
        "                    tm1 = [v[1].item() for v in sw_relu_indexs if v[0]==j]\n",
        "                    text = ' '.join([self.source.term_table[x] for x in tm1])\n",
        "                    #print('text',text)\n",
        "                    # 문법성에 대해서...\n",
        "                    loss, out=self.g_discriminator.transfer_learning([text],train_for = False)\n",
        "                    grammar_rate = out[0,1].item()\n",
        "                    #print('grammar_rate',grammar_rate)               \n",
        "                    grammar_filter[j] = grammar_filter[j] * np.tanh(grammar_rate)\n",
        "                    # 유사성에 대해서...\n",
        "                    sim_rate = self.source.get_similarity(text) #cosine_similarity(self.source.full_text,text)  \n",
        "                    similar_filter[j] = similar_filter[j] * sim_rate\n",
        "                #print('grammar_filter',grammar_filter)  \n",
        "                G_g_loss = -torch.mean(sw*grammar_filter)\n",
        "                #print('G_g_loss',G_g_loss)  \n",
        "                G_s_loss = -torch.mean(sw*similar_filter)\n",
        "                G_l_loss = (criterion(sw,target) - 1)\n",
        "                '''\n",
        "                #print(sw)\n",
        "                with torch.no_grad():                \n",
        "                    fake_gmr_out, fake_sim_out, fake_cos_out, fake_len_out = self.__discrete_gradient(sw)\n",
        "\n",
        "                #print(fake_len_out)\n",
        "                #print(fake_gmr_out)\n",
        "                sw2 = sw * fake_gmr_out\n",
        "                #print(sw2)\n",
        "                G_g_loss = -torch.mean(sw2)\n",
        "                #print(G_g_loss)\n",
        "                sw1 = sw * fake_sim_out\n",
        "                G_s_loss = -torch.mean(sw1)\n",
        "\n",
        "                sw4 = sw * fake_cos_out\n",
        "                G_c_loss = -torch.mean(sw4) \n",
        "\n",
        "                #sw3 = sw * fake_len_out\n",
        "                #G_l_loss = -torch.mean(sw3)\n",
        "\n",
        "                G_l_loss = (criterion(sw,target) - 1) #* (1-epsilon)\n",
        "                '''\n",
        "\n",
        "                dsc_loss = torch.stack([G_g_loss,G_s_loss,G_l_loss])\n",
        "\n",
        "                G_loss = torch.dot(dfs,dsc_loss) + torch.std(dsc_loss)*std_factor\n",
        "                #G_loss =  G_g_loss  + G_s_loss + G_l_loss * comp_rate\n",
        "                #G_loss = G_l_loss\n",
        "\n",
        "                #print(G_loss)\n",
        "                \n",
        "                self.generator.zero_grad()\n",
        "                G_loss.backward()\n",
        "                #print('backward:')\n",
        "                G_opt.step()\n",
        "                scheduler.step()\n",
        "                '''\n",
        "                learning_rate = 0.02\n",
        "                with torch.no_grad():\n",
        "                    dfs += learning_rate * dfs.grad\n",
        "                    dfs.grad = None                    \n",
        "                    dfs[dfs < 0] = 0.1                \n",
        "                '''\n",
        "                #if G_g_loss == 0:# or (i > 100 and G_g_loss > 0):\n",
        "                #    return None\n",
        "\n",
        "                #if G_g_loss > 0:\n",
        "                #    a_w += 0.4\n",
        "            \n",
        "            gen_gmr_loss_history.append(G_g_loss.item())\n",
        "            #gen_cos_loss_history.append(G_c_loss.cpu().detach().numpy())\n",
        "            gen_sim_loss_history.append(G_s_loss.item())\n",
        "            #dis_loss_history.append(D_loss.cpu().detach().numpy())\n",
        "            gen_len_loss_history.append(G_l_loss.item())\n",
        "\n",
        "            #pb.printProgress(+1,f'{i+1}/{epochs} epochs, beta:{dfs} Generator / grammar loss:{G_g_loss}  similarity loss:{G_s_loss}') #,   Discriminator grammar_loss:{D_loss}        ')\n",
        "            #pb.printProgress(+1,'{}/{} epochs, beta:{}, grammar loss:{:.4f}  similarity loss:{:.4f} length loss:{:.4f}'.format(i+1,epochs,dfs,G_g_loss,G_s_loss,G_l_loss)) #,   Discriminator grammar_loss:{D_loss}        ')\n",
        "            pb.printProgress(+1,'{}/{} epochs, gl:{:.8f}  sl:{:.4f} ll:{:.4f}'.format(i+1,epochs, G_g_loss,G_s_loss,G_l_loss)) #,   Discriminator grammar_loss:{D_loss}        ')\n",
        "            \n",
        "            total_loss_history.append(torch.sum(dsc_loss).item())\n",
        "            losses_std_history.append(torch.std(dsc_loss).item())\n",
        "\n",
        "            #epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay * i)\n",
        "            \n",
        "            del G_g_loss\n",
        "            del G_s_loss\n",
        "            del G_l_loss\n",
        "\n",
        "        self.generator.eval()\n",
        "\n",
        "        if np.min(gen_gmr_loss_history[-10:]) > -0.10:\n",
        "            return None\n",
        "        #self.g_discriminator.discriminator.eval()\n",
        "\n",
        "        if display:\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            xs = np.arange(self.source.org_source_length)\n",
        "            #plt.bar(xs+0.0,sw0[0].cpu().detach().numpy(),label='before activation weights',width=0.2)\n",
        "            plt.bar(xs+0.0,sw[0].cpu().detach().numpy(),label='generated value',width=0.2)\n",
        "            plt.bar(xs+0.2,list(self.source.bias_table.values()),label='-self_attention',width=0.2)         \n",
        "            plt.legend()        \n",
        "            plt.show()\n",
        "\n",
        "        return  {'gen_g_loss':gen_gmr_loss_history,'gen_s_loss':gen_sim_loss_history,'gen_l_loss':gen_len_loss_history,'total loss':total_loss_history,'losses std':losses_std_history} #,'dis_loss':dis_loss_history }\n",
        "    '''\n",
        "    def get_summary(self, count):\n",
        "        #texts = []\n",
        "        self.generator.cpu()\n",
        "        self.generator.eval()\n",
        "        #gen_length = len(self.source.story_peaks) + int(len(self.source.story_peaks)*self.frame_expansion_ratio)\n",
        "        noise = torch.randn(count,self.source.org_source_length)\n",
        "        bias = torch.zeros_like(noise)\n",
        "        if self.with_bias:\n",
        "            bias[:,noise.shape[1]-1] = 1\n",
        "        with torch.no_grad():\n",
        "            sw,sw0 = self.generator(noise,bias)\n",
        "            #sw,sw0 = self.generator(noise)\n",
        "\n",
        "        max_score = 0\n",
        "        max_sim = 0\n",
        "        comp_rate = 0\n",
        "        best_text = \"\"\n",
        "\n",
        "        for p_txt in sw:\n",
        "            gtext = self.__text_gen3(p_txt)\n",
        "            text = ' '.join(gtext)\n",
        "            \n",
        "            #print('>>',text)\n",
        "            sim_score = self.s_discriminator.similarity(text,self.source.full_text_emb)\n",
        "            if sim_score > max_sim:\n",
        "                best_text = text.strip()\n",
        "                loss, out=self.g_discriminator.transfer_learning([text],train_for = False)\n",
        "                max_score = out[0,1].item()\n",
        "                comp_rate = 1 - len(best_text)/len(self.source.org_text)\n",
        "                max_sim = sim_score\n",
        "            #texts.append([text.strip(),out,sim_score])\n",
        "        return best_text, max_score, max_sim, comp_rate\n",
        "    '''\n",
        "    def get_samples(self,count):\n",
        "        self.generator.cpu()\n",
        "        self.generator.eval()\n",
        "        noise = torch.randn(count,self.source.org_source_length)\n",
        "        bias = torch.zeros_like(noise)\n",
        "        #if self.with_bias:\n",
        "        #    bias[:,noise.shape[1]-1] = 1\n",
        "        with torch.no_grad():\n",
        "            sw,sw0 = self.generator(noise,bias)\n",
        "        #samples = []\n",
        "        best_p_txt = None\n",
        "        best_text = \"\"\n",
        "        best_grammar_rate = 0\n",
        "        best_sim_rate = 0\n",
        "        best_comp_rate = 0\n",
        "        max_score = 0\n",
        "       \n",
        "        hash_list = []\n",
        "        for p_txt in sw:\n",
        "            gtext = self.__text_gen3(p_txt)\n",
        "            h = self.__text_hash(p_txt)\n",
        "            if h in hash_list:\n",
        "                pass\n",
        "            else:\n",
        "                hash_list.append(h)\n",
        "                text = (' '.join(gtext).strip())\n",
        "                loss, out=self.g_discriminator.transfer_learning([text],train_for = False)\n",
        "                grammar_rate = out[0,1].item()\n",
        "                #sim_score = self.s_discriminator.similarity(text,self.source.org_text_emb)\n",
        "                sim_rate = es.sentence_similarity(self.source.org_text,text)[0]    \n",
        "                comp_rate = 1 - len(text)/len(self.source.org_text)\n",
        "\n",
        "                #samples.append((text,out[0,1].item(),sim_score,comp_rate))\n",
        "                #score = out[0,1].item() + sim_score + comp_rate*2\n",
        "                score = grammar_rate/6 + sim_rate + (1- np.abs(0.5 - comp_rate))\n",
        "                if logout:\n",
        "                    print('g {:.4f} \\ts {:.4f} \\tc {:.4f}, score {:.4f}, [{}]'.format(grammar_rate,sim_rate,comp_rate,score,text))\n",
        "                if max_score < score and (comp_rate > 0.4 and comp_rate < 0.6):\n",
        "                    best_p_txt = p_txt\n",
        "                    max_score = score\n",
        "                    best_text = text\n",
        "                    best_grammar_rate = grammar_rate\n",
        "                    best_sim_rate = sim_rate\n",
        "                    best_comp_rate = comp_rate\n",
        "\n",
        "        if best_text.endswith('.'):\n",
        "            pass\n",
        "        else:\n",
        "            best_text += '.'\n",
        "            \n",
        "        if logout:\n",
        "            print(f'요약률 {best_comp_rate:.4f} 유사성 {best_sim_rate:.4f} 문법성 {best_grammar_rate:.4f} 요약 [{best_text}]')             \n",
        "        #return [best_text for i in range(count)], max_score\n",
        "        '''\n",
        "        correct_best_text = sentence_correct(' '.join(self.__text_gen5(best_p_txt))) #' '.join(self.__text_gen5(best_p_txt))\n",
        "        loss, out=self.g_discriminator.transfer_learning([best_text],train_for = False)\n",
        "        best_grammar_score = out[0,1].item()\n",
        "        loss, out=self.g_discriminator.transfer_learning([correct_best_text],train_for = False)\n",
        "        correct_best_grammar_score = out[0,1].item()\n",
        "        if best_grammar_score < 5.0 and correct_best_grammar_score > best_grammar_score:\n",
        "            if logout:\n",
        "                print('correct_grammar_score:{:.4f} best_grammar_score:{:.4f}'.format(correct_best_grammar_score,best_grammar_score))\n",
        "                print(best_text)\n",
        "                print(correct_best_text)\n",
        "            best_text = correct_best_text\n",
        "            best_grammar_score = correct_best_grammar_score\n",
        "        '''\n",
        "        return best_text, max_score, best_grammar_rate\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCdfO9iuLH6D"
      },
      "source": [
        "#5. Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YWDU3PbMo1T",
        "outputId": "7df1112b-7079-4bda-8b79-3d50143dba54"
      },
      "source": [
        "txt = \"\"\"\n",
        "The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo Grand Forks and Jamestown to the hepatitis A virus in late September and early October.\n",
        "\"\"\"\n",
        "\n",
        "source = Source(get_prepared_doc(sentences_dataset[0]),txt)\n",
        "source.set_key_rate(s_discriminator)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo Grand Forks and Jamestown to the hepatitis A virus in late September and early October.\n",
            "--------------------------------------------------\n",
            "0 \t 0.2343 \t -1.0 \tThe \n",
            "1 \t 0.3666 \t 0.0 \tbishop \n",
            "2 \t 0.1909 \t -1.0 \tof \n",
            "3 \t 0.2343 \t -1.0 \tthe \n",
            "4 \t 0.1204 \t -1.0 \tFargo \n",
            "5 \t 0.3553 \t 0.0 \tCatholic \n",
            "6 \t 0.1149 \t -1.0 \tDiocese \n",
            "7 \t 0.1644 \t -1.0 \tin \n",
            "8 \t 0.0096 \t -1.0 \tNorth \n",
            "9 \t 0.0417 \t -1.0 \tDakota \n",
            "10 \t 0.1106 \t -1.0 \thas \n",
            "11 \t 0.2777 \t 0.0 \texposed \n",
            "12 \t 0.1096 \t -1.0 \tpotentially \n",
            "13 \t 0.1403 \t -1.0 \thundreds \n",
            "14 \t 0.1909 \t -1.0 \tof \n",
            "15 \t 0.3813 \t 0.0 \tchurch \n",
            "16 \t 0.2023 \t -1.0 \tmembers \n",
            "17 \t 0.1644 \t -1.0 \tin \n",
            "18 \t 0.1204 \t -1.0 \tFargo \n",
            "19 \t 0.0322 \t -1.0 \tGrand \n",
            "20 \t 0.0699 \t -1.0 \tForks \n",
            "21 \t 0.1565 \t -1.0 \tand \n",
            "22 \t 0.2933 \t 0.0 \tJamestown \n",
            "23 \t 0.2563 \t 0.0 \tto \n",
            "24 \t 0.2343 \t -1.0 \tthe \n",
            "25 \t 0.4447 \t 0.0 \thepatitis \n",
            "26 \t 0.0971 \t -1.0 \tA \n",
            "27 \t 0.2370 \t 0.0 \tvirus \n",
            "28 \t 0.1644 \t -1.0 \tin \n",
            "29 \t 0.1033 \t -1.0 \tlate \n",
            "30 \t 0.1241 \t -1.0 \tSeptember \n",
            "31 \t 0.1565 \t -1.0 \tand \n",
            "32 \t 0.0597 \t -1.0 \tearly \n",
            "33 \t 0.0918 \t -1.0 \tOctober. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_eAwIPLb4aj"
      },
      "source": [
        "# sam_wgan4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhcoXuPMGy09"
      },
      "source": [
        "def sam_wgan4(full_text,text, epochs=50, batch_size=100,display=False, retry = True, retry_count = 0,comp_rate=1.0):\n",
        "    if retry_count > 30:\n",
        "        raise Exception(\"Can't summarize the text\")\n",
        "    if len(text) < 10:\n",
        "        return None\n",
        "    source = Source(get_prepared_doc(full_text),get_prepared_doc(text),delete_ending = False,attendtion_rate=atten_rate)\n",
        "    source.set_key_rate(s_discriminator)\n",
        "    summarizer = SAM_Summarizer(g_discriminator,s_discriminator)\n",
        "    summarizer.ready(source)\n",
        "    hist = summarizer.summarize(epochs,batch_size=2,learning_rate=5e-3,display=display,comp_rate=comp_rate)\n",
        "    if retry and hist == None and retry_count < 10:\n",
        "        print('\\n')\n",
        "        return sam_wgan4(full_text,text, epochs+10, batch_size,display=display,retry_count=retry_count+1)\n",
        "    samples, max_score, best_grammar_rate = summarizer.get_samples(batch_size)\n",
        "    #print(samples)\n",
        "    \n",
        "    if retry and best_grammar_rate < (3.0 - retry_count*0.1):\n",
        "        if logout:\n",
        "            print('재시도 max score:{} grammar:{} text:{}'.format(max_score,best_grammar_rate,samples))\n",
        "        del source\n",
        "        del summarizer            \n",
        "        return sam_wgan4(full_text,text, epochs+10, batch_size,display=display,retry_count=retry_count+1)\n",
        "\n",
        "    del source\n",
        "    del summarizer\n",
        "\n",
        "    return samples, max_score, best_grammar_rate"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "-pJUPNgwmSKB",
        "outputId": "48f30fdb-f3a4-4449-ad0a-3b7ae95e364e"
      },
      "source": [
        "sentences_dataset[0]"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Ever noticed how plane seats appear to be getting smaller and smaller? With increasing numbers of people taking to the skies, some experts are questioning if having such packed out planes is putting passengers at risk. They say that the shrinking space on aeroplanes is not only uncomfortable - it's putting our health and safety in danger. More than squabbling over the arm rest, shrinking space on planes putting our health and safety in danger? This week, a U.S consumer advisory group set up by the Department of Transportation said at a public hearing that while the government is happy to set standards for animals flying on planes, it doesn't stipulate a minimum amount of space for humans. 'In a world where animals have more rights to space and food than humans,' said Charlie Leocha, consumer representative on the committee.\\xa0'It is time that the DOT and FAA take a stand for humane treatment of passengers.' But could crowding on planes lead to more serious issues than fighting for space in the overhead lockers, crashing elbows and seat back kicking? Tests conducted by the FAA use planes with a 31 inch pitch, a standard which on some airlines has decreased . Many economy seats on United Airlines have 30 inches of room, while some airlines offer as little as 28 inches . Cynthia Corbertt, a human factors researcher with the Federal Aviation Administration, that it conducts tests on how quickly passengers can leave a plane. But these tests are conducted using planes with 31 inches between each row of seats, a standard which on some airlines has decreased, reported the Detroit News. The distance between two seats from one point on a seat to the same point on the seat behind it is known as the pitch. While most airlines stick to a pitch of 31 inches or above, some fall below this. While United Airlines has 30 inches of space, Gulf Air economy seats have between 29 and 32 inches, Air Asia offers 29 inches and Spirit Airlines offers just 28 inches. British Airways has a seat pitch of 31 inches, while easyJet has 29 inches, Thomson's short haul seat pitch is 28 inches, and Virgin Atlantic's is 30-31.\""
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6v9sEryZOLa"
      },
      "source": [
        "# Sentence Corrector (EncoderDecoderModel)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUe3ZCSIz8N8"
      },
      "source": [
        "from transformers import EncoderDecoderModel, BertTokenizerFast\n",
        "import torch\n",
        "\n",
        "pre_trained_kobert_model_name='bert-base-uncased'\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained(pre_trained_kobert_model_name)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euuB9E5uZ1j2"
      },
      "source": [
        "from transformers import EncoderDecoderModel, BertTokenizerFast\n",
        "try:\n",
        "    del model\n",
        "    print('delete model')\n",
        "except Exception as ex:\n",
        "    pass\n",
        "model = EncoderDecoderModel.from_pretrained(\"/content/drive/MyDrive/GAN_ENDE/en_sentence_complete_model\")"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr_6N_CYaKAI"
      },
      "source": [
        "def sentence_correct(text):\n",
        "    text = text.strip().lower()\n",
        "    text = text.replace('!','')\n",
        "    text = text.replace('?','')\n",
        "    w = text.split(' ')\n",
        "    last_token = w[-1]\n",
        "    if last_token.endswith(('.')):\n",
        "        last_token = w[-1][:-1]\n",
        "\n",
        "    last_character = w[len(w)-1][:-1]\n",
        "    inputs = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "    input_ids = inputs.input_ids.to(device)\n",
        "    attention_mask = inputs.attention_mask.to(device)\n",
        "    '''\n",
        "    v = torch.sum(attention_mask[0]).item()\n",
        "    c = random.sample([i for i in range(v)],int(v/2))\n",
        "    print(c)\n",
        "    #input_ids[0][c] = 0\n",
        "    attention_mask[0][c] = 0 #random.random()\n",
        "    attention_mask[0][0] = 1\n",
        "    attention_mask[0][v-1] = 1\n",
        "    \n",
        "    print(input_ids)    \n",
        "    print(attention_mask)\n",
        "    '''\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    outputs = model.generate(input_ids, attention_mask=attention_mask).cpu().detach().numpy()[0]\n",
        "    o=[]\n",
        "    for token in outputs:\n",
        "        if token == tokenizer.pad_token_id:\n",
        "            break\n",
        "        o.append(token)\n",
        "    output_str = tokenizer.batch_decode([o], skip_special_tokens=True)[0]\n",
        "    output_str = output_str.replace(' ’ ',\"'\") #' - '\n",
        "    output_str = output_str.replace(' - ',\"-\") #' - '\n",
        "    #if logout:\n",
        "    #    print('raw:',output_str)\n",
        "    \n",
        "    vb = [output_str.find('.'),output_str.find('?'),output_str.find('!')]\n",
        "    for v in range(len(vb)):\n",
        "        vb[v] = vb[v] if vb[v] >= 0 else 1000\n",
        "    eos = np.min(vb)\n",
        "    real_eos =  eos\n",
        "    if last_character.endswith('다'):\n",
        "        eos2 = output_str.find(last_character) \n",
        "        if eos2 > 0 and eos2 < eos:\n",
        "            real_eos = eos2 + len(last_character)\n",
        "    # 끝에 token matching을 위해, 적어도 3글자 이상의 token에 대하여...\n",
        "    elif len(last_token) > 3:\n",
        "        eos2 = output_str.find(last_token) \n",
        "        if eos2 > 0 and eos2 < eos:\n",
        "            real_eos = eos2 + len(last_token) \n",
        "            tmp = output_str[0:real_eos] + '.'\n",
        "            # 적어도 5어절 이상은 되어야 인정해 준다.\n",
        "            if len(tmp.split(' ')) > 5:\n",
        "                output_str = tmp\n",
        "    else:\n",
        "        output_str = output_str[0:real_eos] + '.'\n",
        "\n",
        "    if output_str.endswith('.'):\n",
        "        pass\n",
        "    else:\n",
        "        output_str += '.'\n",
        "    return output_str"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "GkdOT4m6Jlx9",
        "outputId": "76e8f2e0-eb9d-4bf3-c292-9f548f3250d5"
      },
      "source": [
        "txt = 'judge agreed with police that would have been over limit time Citroen Miss Titley’s blue Daihatsu Cuore road near Yarmouth Isle Wight October 2013 phone records showed also texting around time crash.'\n",
        "#txt = 'Cynthia human factors researcher Federal Aviation that it tests on how quickly passengers can leave a plane two seats from a seat on the seat behind it is the pitch crowding to more serious issues than fighting space in crashing elbows seat.'\n",
        "#txt = 'spin-off the E!'\n",
        "sentence_correct(txt)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"the judge agreed with police that he would have been over the limit of time to citroen miss titley's blue daihatsu cuore road near yarmouth on the isle of wight in october 2013 phone records showed he was also texting around the time of the crash.\""
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LKIH0nG31yx"
      },
      "source": [
        "\n",
        "def similarity3(full_text,org_text):\n",
        "    sentences = nltk.sent_tokenize(full_text)\n",
        "    #print(\"Num sentences:\", len(sentences))\n",
        "    querys = nltk.sent_tokenize(org_text)\n",
        "    #print(\"Num querys:\", len(querys))\n",
        "\n",
        "    #Compute the sentence embeddings\n",
        "    org_embeddings = s_discriminator._embedder.encode(sentences,show_progress_bar=False)\n",
        "    query_embeddings = s_discriminator._embedder.encode(querys,show_progress_bar=False)\n",
        "\n",
        "    #Compute the pair-wise cosine similarities\n",
        "    cos_scores = scipy.spatial.distance.cdist(query_embeddings, org_embeddings, \"cosine\")\n",
        "    similarity_score = 1.0 - np.mean(np.min(cos_scores,axis=0))\n",
        "\n",
        "    return similarity_score"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1L5pLIoV-62"
      },
      "source": [
        "def grammar3(full_text,org_text):\n",
        "    querys = nltk.sent_tokenize(org_text)\n",
        "    g = []\n",
        "    for txt in querys:\n",
        "        loss, out=g_discriminator.transfer_learning([txt],train_for = False)\n",
        "        g.append(out[0,1].item())\n",
        "    return np.tanh(np.mean(g))"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VadbdJnzL8Lq"
      },
      "source": [
        "# 실험 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFJ5v8fia6VD"
      },
      "source": [
        "def summary(ft,text,steps=4,top_rank=2,comp_rate=1.0):\n",
        "    org_sentences = np.array(nltk.sent_tokenize(text.strip()))\n",
        "    summary_text = []\n",
        "    g = []\n",
        "    sm = []\n",
        "    for i in range(0,len(org_sentences),steps):\n",
        "        txt = ''\n",
        "        cnt = 0\n",
        "        for s in range(i,i+steps):\n",
        "            if s < len(org_sentences):\n",
        "                txt +=  ' ' + org_sentences[s]\n",
        "                cnt +=1\n",
        "        #print(cnt,top_rank)\n",
        "        txt = txt.strip()\n",
        "        if cnt > top_rank:\n",
        "            txt = besm2(ft,txt,top_rank=top_rank)\n",
        "        #if len(txt.replace('\\n',' ').split(' ')) > 75:\n",
        "        #    raise Exception('Too much sentence length...')  \n",
        "        t,score, grammar = sam_wgan4(ft,txt.strip(),epochs=300,display=logout,comp_rate=comp_rate)\n",
        "        if logout:\n",
        "            print('-'*50)\n",
        "            print(t,score,grammar)\n",
        "        #t = sentence_correct(t)\n",
        "        #print(t)\n",
        "        summary_text.append(t)\n",
        "        g.append(grammar)\n",
        "        sm.append(similarity3(ft,t))\n",
        "\n",
        "    return ' '.join(summary_text).strip(),np.tanh(np.mean(g)),np.mean(sm)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irjx-TOnPBL4"
      },
      "source": [
        "## Main 실험"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FstAHWGQ8KR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "59ace783-fd44-4e46-e754-183608089853"
      },
      "source": [
        "logout = True\n",
        "atten_rate = 0.2\n",
        "similarity = 2.0\n",
        "std_factor = 3.0\n",
        "txt = \"\"\"\n",
        "The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo Grand Forks and Jamestown to the hepatitis A virus in late September and early October. The state Health Department has issued an advisory of exposure for anyone who attended five churches and took communion. Bishop John Folda (pictured) of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo Grand Forks and Jamestown to the hepatitis A.\n",
        "\"\"\"\n",
        "#The diocese says he contracted the infection through contaminated food while attending a conference for newly ordained bishops in Italy last month. Symptoms of hepatitis A include fever, tiredness, loss of appetite, nausea and abdominal discomfort. Fargo Catholic Diocese in North Dakota (pictured) is where the bishop is located.\n",
        "#The judge agreed with police that he would have been over the limit at the time his red Citroen hit Miss Titley’s blue Daihatsu Cuore on a road near Yarmouth, Isle of Wight, on October 11, 2013. His phone records showed he was also texting around the time of the crash.\n",
        "\n",
        "sam_wgan4(sentences_dataset[0],txt,epochs=300,display= True,retry = False,comp_rate= 1.5)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo Grand Forks and Jamestown to the hepatitis A virus in late September and early October. The state Health Department has issued an advisory of exposure for anyone who attended five churches and took communion. Bishop John Folda (pictured) of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo Grand Forks and Jamestown to the hepatitis A.\n",
            "--------------------------------------------------\n",
            "0 \t 0.1335 \t -1.0 \tThe \n",
            "1 \t 0.2536 \t 0.0 \tbishop \n",
            "2 \t 0.1254 \t -1.0 \tof \n",
            "3 \t 0.1335 \t -1.0 \tthe \n",
            "4 \t 0.0614 \t -1.0 \tFargo \n",
            "5 \t 0.1619 \t -1.0 \tCatholic \n",
            "6 \t 0.0799 \t -1.0 \tDiocese \n",
            "7 \t 0.1740 \t -1.0 \tin \n",
            "8 \t 0.0074 \t -1.0 \tNorth \n",
            "9 \t 0.0289 \t -1.0 \tDakota \n",
            "10 \t 0.0629 \t -1.0 \thas \n",
            "11 \t 0.1769 \t 0.0 \texposed \n",
            "12 \t 0.0823 \t -1.0 \tpotentially \n",
            "13 \t 0.0460 \t -1.0 \thundreds \n",
            "14 \t 0.1254 \t -1.0 \tof \n",
            "15 \t 0.1650 \t -1.0 \tchurch \n",
            "16 \t 0.1498 \t -1.0 \tmembers \n",
            "17 \t 0.1740 \t -1.0 \tin \n",
            "18 \t 0.0614 \t -1.0 \tFargo \n",
            "19 \t 0.0179 \t -1.0 \tGrand \n",
            "20 \t 0.0421 \t -1.0 \tForks \n",
            "21 \t 0.0616 \t -1.0 \tand \n",
            "22 \t 0.2777 \t 0.0 \tJamestown \n",
            "23 \t 0.2482 \t 0.0 \tto \n",
            "24 \t 0.1335 \t -1.0 \tthe \n",
            "25 \t 0.2717 \t 0.0 \thepatitis \n",
            "26 \t 0.0499 \t -1.0 \tA \n",
            "27 \t 0.2213 \t 0.0 \tvirus \n",
            "28 \t 0.1740 \t -1.0 \tin \n",
            "29 \t 0.0813 \t -1.0 \tlate \n",
            "30 \t 0.0786 \t -1.0 \tSeptember \n",
            "31 \t 0.0616 \t -1.0 \tand \n",
            "32 \t 0.0561 \t -1.0 \tearly \n",
            "33 \t 0.0732 \t -1.0 \tOctober \n",
            "34 \t 0.0616 \t -1.0 \tand \n",
            "35 \t 0.1335 \t -1.0 \tThe \n",
            "36 \t 0.1580 \t -1.0 \tstate \n",
            "37 \t 0.2483 \t 0.0 \tHealth \n",
            "38 \t 0.1342 \t -1.0 \tDepartment \n",
            "39 \t 0.0629 \t -1.0 \thas \n",
            "40 \t 0.0866 \t -1.0 \tissued \n",
            "41 \t 0.0490 \t -1.0 \tan \n",
            "42 \t 0.0456 \t -1.0 \tadvisory \n",
            "43 \t 0.1254 \t -1.0 \tof \n",
            "44 \t 0.0772 \t -1.0 \texposure \n",
            "45 \t 0.0569 \t -1.0 \tfor \n",
            "46 \t 0.0880 \t -1.0 \tanyone \n",
            "47 \t 0.0536 \t -1.0 \twho \n",
            "48 \t 0.2766 \t 0.0 \tattended \n",
            "49 \t 0.1601 \t -1.0 \tfive \n",
            "50 \t 0.4924 \t 0.0 \tchurches \n",
            "51 \t 0.0616 \t -1.0 \tand \n",
            "52 \t 0.1120 \t -1.0 \ttook \n",
            "53 \t 0.3064 \t 0.0 \tcommunion \n",
            "54 \t 0.0616 \t -1.0 \tand \n",
            "55 \t 0.2536 \t 0.0 \tBishop \n",
            "56 \t 0.1722 \t -1.0 \tJohn \n",
            "57 \t 0.0617 \t -1.0 \tFolda \n",
            "58 \t 0.0000 \t -1.0 \t(pictured) \n",
            "59 \t 0.1254 \t -1.0 \tof \n",
            "60 \t 0.1335 \t -1.0 \tthe \n",
            "61 \t 0.0614 \t -1.0 \tFargo \n",
            "62 \t 0.1619 \t -1.0 \tCatholic \n",
            "63 \t 0.0799 \t -1.0 \tDiocese \n",
            "64 \t 0.1740 \t -1.0 \tin \n",
            "65 \t 0.0074 \t -1.0 \tNorth \n",
            "66 \t 0.0289 \t -1.0 \tDakota \n",
            "67 \t 0.0629 \t -1.0 \thas \n",
            "68 \t 0.1769 \t 0.0 \texposed \n",
            "69 \t 0.0823 \t -1.0 \tpotentially \n",
            "70 \t 0.0460 \t -1.0 \thundreds \n",
            "71 \t 0.1254 \t -1.0 \tof \n",
            "72 \t 0.1650 \t -1.0 \tchurch \n",
            "73 \t 0.1498 \t -1.0 \tmembers \n",
            "74 \t 0.1740 \t -1.0 \tin \n",
            "75 \t 0.0614 \t -1.0 \tFargo \n",
            "76 \t 0.0179 \t -1.0 \tGrand \n",
            "77 \t 0.0421 \t -1.0 \tForks \n",
            "78 \t 0.0616 \t -1.0 \tand \n",
            "79 \t 0.2777 \t 0.0 \tJamestown \n",
            "80 \t 0.2482 \t 0.0 \tto \n",
            "81 \t 0.1335 \t -1.0 \tthe \n",
            "82 \t 0.2717 \t 0.0 \thepatitis \n",
            "83 \t 0.0499 \t -1.0 \tA. \n",
            "Train... |||||||||||||||||||||| 100.0%   300/300 epochs, gl:-0.26254559  sl:-0.1655 ll:-0.2492\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "g 3.3602 \ts 0.9211 \tc 0.4170, score 2.3982, [bishop Dakota has exposed potentially Grand Jamestown to the hepatitis virus in October The Health has issued an advisory of who attended five churches communion and Bishop (pictured) Fargo Catholic Diocese in North Dakota has exposed church members Fargo Forks and Jamestown to the hepatitis A.]\n",
            "g 3.5859 \ts 0.9222 \tc 0.4289, score 2.4487, [bishop Dakota has exposed potentially Grand Jamestown to the hepatitis virus in October The Health has issued an advisory of who attended five churches communion and Bishop (pictured) Fargo Catholic Diocese in North Dakota has exposed church members Fargo and Jamestown to the hepatitis A.]\n",
            "g 0.7817 \ts 0.9145 \tc 0.4387, score 1.9835, [bishop Dakota has exposed potentially Grand Jamestown to the hepatitis virus in October The Health has issued an advisory of who attended five churches communion and Bishop (pictured) Fargo Catholic North Dakota has exposed church members Fargo Forks and Jamestown to the hepatitis A.]\n",
            "g 3.5749 \ts 0.9174 \tc 0.4289, score 2.4421, [bishop Dakota has exposed potentially Grand Jamestown to the hepatitis virus in October The Health has issued an advisory who attended five churches communion and Bishop (pictured) Fargo Catholic Diocese in North Dakota has exposed church members Fargo Forks and Jamestown to the hepatitis]\n",
            "g -0.5368 \ts 0.9089 \tc 0.4625, score 1.7818, [bishop Dakota exposed potentially Grand Jamestown to the hepatitis virus in October The Health has issued an advisory of who attended five churches communion and Bishop (pictured) Fargo Diocese in North Dakota exposed church members Fargo and Jamestown to the hepatitis A.]\n",
            "g 3.3473 \ts 0.8373 \tc 0.4269, score 2.3221, [bishop Dakota has exposed potentially Grand Jamestown to the hepatitis virus in October The Health has issued an advisory of who attended churches communion and Bishop (pictured) Fargo Catholic Diocese in North Dakota has exposed church members Fargo Forks and Jamestown to the hepatitis A.]\n",
            "g 0.3188 \ts 0.9248 \tc 0.4368, score 1.9147, [bishop Dakota has exposed potentially Jamestown to the hepatitis virus in October The Health has issued an advisory of attended five churches communion and Bishop (pictured) Fargo Catholic Diocese in North Dakota has exposed church members Fargo Forks and Jamestown to the hepatitis A.]\n",
            "g 1.9384 \ts 0.9202 \tc 0.4229, score 2.1662, [bishop Dakota has exposed potentially Grand Jamestown to the hepatitis virus in October The Health has issued an advisory of who attended five churches communion and Bishop (pictured) Fargo Catholic Diocese North Dakota has exposed church members Fargo Forks and Jamestown to the hepatitis A.]\n",
            "g 1.3538 \ts 0.9250 \tc 0.4249, score 2.0755, [bishop Dakota has exposed potentially Grand Jamestown to the hepatitis virus in October The Health has issued an advisory of attended five churches communion and Bishop (pictured) Fargo Catholic Diocese in North Dakota has exposed church members Fargo Forks and Jamestown to the hepatitis A.]\n",
            "g 2.7318 \ts 0.9157 \tc 0.4289, score 2.2999, [bishop Dakota has exposed potentially Grand Jamestown to the hepatitis virus in October The Health has issued an advisory of who attended five churches communion and Bishop (pictured) Catholic Diocese in North Dakota has exposed church members Fargo Forks and Jamestown to the hepatitis A.]\n",
            "g 2.3547 \ts 0.9172 \tc 0.4328, score 2.2425, [bishop Dakota has exposed potentially Grand Jamestown to the hepatitis virus in October The Health has issued an advisory of who attended five churches communion and Bishop (pictured) Fargo Catholic in North Dakota has exposed church members Fargo Forks and Jamestown to the hepatitis A.]\n",
            "g 3.2644 \ts 0.9224 \tc 0.4249, score 2.3914, [bishop Dakota has exposed potentially Grand Jamestown to the hepatitis virus in October Health has issued an advisory of who attended five churches communion and Bishop (pictured) Fargo Catholic Diocese in North Dakota has exposed church members Fargo Forks and Jamestown to the hepatitis A.]\n",
            "g 1.5774 \ts 0.9117 \tc 0.4447, score 2.1192, [bishop Dakota has exposed potentially Grand Jamestown to the hepatitis virus in October The Health has issued an advisory of who attended five churches communion and Bishop (pictured) Catholic in North Dakota has exposed church members Fargo Forks and Jamestown to the hepatitis A.]\n",
            "g 2.0065 \ts 0.9182 \tc 0.4249, score 2.1775, [bishop Dakota has exposed potentially Grand Jamestown to the hepatitis virus in October The Health issued an advisory of who attended five churches communion and Bishop (pictured) Fargo Catholic Diocese in North Dakota has exposed church members Fargo Forks and Jamestown to the hepatitis A.]\n",
            "g 3.0973 \ts 0.9186 \tc 0.4328, score 2.3676, [bishop Dakota has exposed potentially Grand Jamestown to the hepatitis virus in The Health has issued an advisory of who attended five churches communion and Bishop (pictured) Fargo Catholic Diocese in North Dakota has exposed church members Fargo Forks and Jamestown to the hepatitis A.]\n",
            "g 3.2949 \ts 0.9168 \tc 0.4289, score 2.3948, [bishop Dakota has exposed potentially Grand Jamestown to the hepatitis virus in October The Health has issued an advisory of who attended five churches communion and Bishop (pictured) Fargo Catholic Diocese in North Dakota has exposed church members Forks and Jamestown to the hepatitis A.]\n",
            "g -1.7401 \ts 0.8966 \tc 0.4763, score 1.5828, [bishop Dakota exposed potentially Grand Jamestown to the hepatitis virus in October The Health has issued an advisory of who attended five churches communion and Bishop (pictured) Fargo Catholic in North Dakota has exposed church Forks and Jamestown to hepatitis A.]\n",
            "g 1.9012 \ts 0.9080 \tc 0.4427, score 2.1676, [bishop Dakota has exposed potentially Grand Jamestown to the hepatitis virus in October The Health has issued an advisory of who attended five churches communion Bishop (pictured) Fargo Diocese in North Dakota has exposed church members Fargo Forks and Jamestown to the hepatitis A.]\n",
            "g 2.5779 \ts 0.9091 \tc 0.4328, score 2.2715, [bishop Dakota has exposed potentially Grand Jamestown to the hepatitis virus in October The Health has issued an advisory of who attended five churches communion and Bishop (pictured) Fargo Catholic Diocese in North Dakota has exposed church Fargo Forks and Jamestown to the hepatitis A.]\n",
            "g 2.0724 \ts 0.9217 \tc 0.4249, score 2.1920, [bishop Dakota has exposed potentially Grand Jamestown to the hepatitis virus in October The Health has issued an advisory of who attended five churches communion and Bishop (pictured) Fargo Catholic Diocese in North Dakota has exposed church members Fargo Forks Jamestown to the hepatitis A.]\n",
            "g 3.6232 \ts 0.9193 \tc 0.4387, score 2.4619, [bishop Dakota has exposed potentially Grand Jamestown to the hepatitis virus in October The Health has issued an advisory of who attended five churches communion and Bishop Fargo Catholic Diocese in North Dakota has exposed church members Fargo Forks and Jamestown to the hepatitis A.]\n",
            "g 2.5899 \ts 0.9153 \tc 0.4249, score 2.2719, [bishop Dakota exposed potentially Grand Jamestown to the hepatitis virus in October The Health has issued an advisory of who attended five churches communion and Bishop (pictured) Fargo Catholic Diocese in North Dakota has exposed church members Fargo Forks and Jamestown to the hepatitis A.]\n",
            "g 2.5468 \ts 0.9106 \tc 0.4249, score 2.2599, [bishop Dakota has exposed potentially Grand Jamestown to the hepatitis virus in October The Health has issued an advisory of who attended five churches communion Bishop (pictured) Fargo Catholic Diocese in North Dakota has exposed church members Fargo Forks and Jamestown to the hepatitis A.]\n",
            "g -0.4979 \ts 0.9128 \tc 0.4407, score 1.7705, [bishop Dakota exposed potentially Grand Jamestown to the hepatitis virus in October The Health has issued an advisory of who attended five churches communion and Bishop (pictured) Fargo Catholic in North Dakota has exposed church members Fargo Forks and Jamestown to the hepatitis A.]\n",
            "요약률 0.4387 유사성 0.9193 문법성 3.6232 요약 [bishop Dakota has exposed potentially Grand Jamestown to the hepatitis virus in October The Health has issued an advisory of who attended five churches communion and Bishop Fargo Catholic Diocese in North Dakota has exposed church members Fargo Forks and Jamestown to the hepatitis A.]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('bishop Dakota has exposed potentially Grand Jamestown to the hepatitis virus in October The Health has issued an advisory of who attended five churches communion and Bishop Fargo Catholic Diocese in North Dakota has exposed church members Fargo Forks and Jamestown to the hepatitis A.',\n",
              " 2.4619078513842707,\n",
              " 3.6232330799102783)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "di_qtu_05Zcf"
      },
      "source": [
        "# 측정 도구..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFgYYBgh5b-P",
        "outputId": "563c51f1-5977-4f9a-8174-e40e95c5c07d"
      },
      "source": [
        "!pip install rouge-score"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.7/dist-packages (0.0.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge-score) (3.2.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.19.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.15.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge-score) (0.12.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqwC0uZrFAVo"
      },
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeL'], use_stemmer=True)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1gLnjcb937W"
      },
      "source": [
        "# 종합 Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUXcwD9OQeex"
      },
      "source": [
        "@@@"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZDibfTKqIJa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8747223-f625-4447-d3fc-91256740edb5"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Oct  5 13:27:26 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.74       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   71C    P0    31W /  70W |   4572MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_AzsvMX6KEq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0249fe82-431d-4070-fcf8-3ecad0053493"
      },
      "source": [
        "import io\n",
        "\n",
        "result_data = {}\n",
        "\n",
        "result_data['ID'] = []\n",
        "result_data['Original'] = []\n",
        "result_data['Step0'] = []\n",
        "result_data['Step1'] = []\n",
        "result_data['Step2'] = []\n",
        "result_data['Step3'] = []\n",
        "result_data['Ground Truth'] = []\n",
        "\n",
        "result_data['S0.R1'] = []\n",
        "result_data['S0.R2'] = []\n",
        "result_data['S0.RL'] = []\n",
        "result_data['S0.Similarity'] = []\n",
        "result_data['S0.Grammar'] = []\n",
        "\n",
        "result_data['S1.R1'] = []\n",
        "result_data['S1.R2'] = []\n",
        "result_data['S1.RL'] = []\n",
        "result_data['S1.Similarity'] = []\n",
        "result_data['S1.Grammar'] = []\n",
        "\n",
        "result_data['S2.R1'] = []\n",
        "result_data['S2.R2'] = []\n",
        "result_data['S2.RL'] = []\n",
        "result_data['S2.Similarity'] = []\n",
        "result_data['S2.Grammar'] = []\n",
        "\n",
        "result_data['S3.R1'] = []\n",
        "result_data['S3.R2'] = []\n",
        "result_data['S3.RL'] = []\n",
        "result_data['S3.Similarity'] = []\n",
        "result_data['S3.Grammar'] = []\n",
        "\n",
        "statistics_columns = ['S0.R1','S0.R2','S0.RL','S0.Similarity','S0.Grammar','S1.R1','S1.R2','S1.RL','S1.Similarity','S1.Grammar','S2.R1','S2.R2','S2.RL','S2.Similarity','S2.Grammar','S3.R1','S3.R2','S3.RL','S3.Similarity','S3.Grammar']\n",
        "\n",
        "atten_rate = 0.2\n",
        "similarity = 2.0\n",
        "std_factor = 3.0\n",
        "\n",
        "#es = ExtactiveSummarizer()\n",
        "\n",
        "pd.set_option('display.max_columns', 100)\n",
        "cnt = 0\n",
        "logout = False\n",
        "for rate in range(0,1):\n",
        "    comp_rate=1.5\n",
        "    #result_data['comp_rate'].append(comp_rate)\n",
        "    for try_count in range(1000):\n",
        "        \n",
        "        full_text = get_prepared_doc(sentences_dataset[try_count])\n",
        "        '''\n",
        "        try:\n",
        "            del model\n",
        "            print('delete model')\n",
        "        except Exception as ex:\n",
        "            pass\n",
        "        model = EncoderDecoderModel.from_pretrained(\"/content/drive/MyDrive/GAN_ENDE/en_sentence_complete_model\")\n",
        "        '''\n",
        "        try:\n",
        "        #if True:\n",
        "            #org_text = besm(full_text,num_sentences=9)\n",
        "            step0 = es.generate_summary(full_text,top_n=3,min_length=0)[0]\n",
        "\n",
        "            step1 = es.generate_summary(full_text,top_n=9,min_length=0)[0]\n",
        "            \n",
        "            org_sentences = np.array(nltk.sent_tokenize(step1.strip()))\n",
        "            if logout:\n",
        "                print('BESM Summary sentance length:',len(org_sentences))\n",
        "            step2,grammar,simil = summary(full_text,step1,steps=3,top_rank=3,comp_rate=comp_rate)\n",
        "            tmp = []\n",
        "            for txt in np.array(nltk.sent_tokenize(step2.strip())):\n",
        "                tmp.append(sentence_correct(txt))         \n",
        "            step3 = ' '.join(tmp)\n",
        "\n",
        "            ground_trouth = gold_summary[try_count]\n",
        "            \n",
        "            print('='*50 + ' Original Document ' + str(try_count) + '='*50)\n",
        "            for txt in np.array(nltk.sent_tokenize(full_text)):\n",
        "                print(txt)\n",
        "            print('-'*50 + ' Step0 ' + str(try_count) + '-'*50)\n",
        "            for txt in np.array(nltk.sent_tokenize(step0.strip())):\n",
        "                print(txt)                  \n",
        "            print('-'*50 + ' Step1 ' + str(try_count) + '-'*50)\n",
        "            for txt in np.array(nltk.sent_tokenize(step1.strip())):\n",
        "                print(txt)                \n",
        "            print('-'*50 + ' Step2 ' + str(try_count) + '-'*50)\n",
        "            for txt in np.array(nltk.sent_tokenize(step2.strip())):\n",
        "                print(txt)\n",
        "            print('-'*50 + ' Step3 ' + str(try_count) + '-'*50)\n",
        "            for txt in np.array(nltk.sent_tokenize(step3.strip())):\n",
        "                print(txt)                \n",
        "            print('-'*50 + ' Ground truth' + '-'*50)\n",
        "            print(ground_trouth)\n",
        "            print('-'*120)\n",
        "\n",
        "            #with io.open('/content/drive/MyDrive/GAN_ENDE/CNN_Daily_summary_result.txt','a',encoding='utf8') as f:\n",
        "            #    f.write(org_text + '\\r\\n\\r\\n')\n",
        "            s0_rouge = scorer.score(ground_trouth,step0)            \n",
        "            s1_rouge = scorer.score(ground_trouth,step1)\n",
        "            s2_rouge = scorer.score(ground_trouth,step2)\n",
        "            s3_rouge = scorer.score(ground_trouth,step3)\n",
        "            s0_simil = similarity3(full_text,step0)            \n",
        "            s1_simil = similarity3(full_text,step1)\n",
        "            s2_simil = similarity3(full_text,step2)\n",
        "            s3_simil = similarity3(full_text,step3)\n",
        "            s0_gramm = grammar3(full_text,step0)            \n",
        "            s1_gramm = grammar3(full_text,step1)\n",
        "            s2_gramm = grammar3(full_text,step2) \n",
        "            s3_gramm = grammar3(full_text,step3)  \n",
        "            #print(scores['rouge1'].fmeasure)\n",
        "            if s1_rouge['rouge1'].fmeasure > 0.1:\n",
        "\n",
        "                result_data['ID'].append('CNN/DM-'+str(try_count))\n",
        "                result_data['Original'].append(full_text)\n",
        "                result_data['Step0'].append(step1)\n",
        "                result_data['Step1'].append(step1)\n",
        "                result_data['Step2'].append(step2)\n",
        "                result_data['Step3'].append(step3)\n",
        "                result_data['Ground Truth'].append(ground_trouth)\n",
        "\n",
        "                result_data['S0.R1'].append(s0_rouge['rouge1'].fmeasure)\n",
        "                result_data['S0.R2'].append(s0_rouge['rouge2'].fmeasure)\n",
        "                result_data['S0.RL'].append(s0_rouge['rougeL'].fmeasure)\n",
        "                result_data['S0.Similarity'].append(s0_simil)\n",
        "                result_data['S0.Grammar'].append(s0_gramm)\n",
        "\n",
        "                result_data['S1.R1'].append(s1_rouge['rouge1'].fmeasure)\n",
        "                result_data['S1.R2'].append(s1_rouge['rouge2'].fmeasure)\n",
        "                result_data['S1.RL'].append(s1_rouge['rougeL'].fmeasure)\n",
        "                result_data['S1.Similarity'].append(s1_simil)\n",
        "                result_data['S1.Grammar'].append(s1_gramm)\n",
        "\n",
        "                result_data['S2.R1'].append(s2_rouge['rouge1'].fmeasure)\n",
        "                result_data['S2.R2'].append(s2_rouge['rouge2'].fmeasure)\n",
        "                result_data['S2.RL'].append(s2_rouge['rougeL'].fmeasure)\n",
        "                result_data['S2.Similarity'].append(s2_simil)\n",
        "                result_data['S2.Grammar'].append(s2_gramm)\n",
        "\n",
        "                result_data['S3.R1'].append(s3_rouge['rouge1'].fmeasure)\n",
        "                result_data['S3.R2'].append(s3_rouge['rouge2'].fmeasure)\n",
        "                result_data['S3.RL'].append(s3_rouge['rougeL'].fmeasure)\n",
        "                result_data['S3.Similarity'].append(s3_simil)\n",
        "                result_data['S3.Grammar'].append(s3_gramm)\n",
        "\n",
        "            print()\n",
        "            df_result_data = pd.DataFrame(result_data)\n",
        "            print(df_result_data[statistics_columns].iloc[-1])\n",
        "            print()\n",
        "            df_mean = df_result_data[statistics_columns].mean()\n",
        "            print('** Mean result **')\n",
        "            print(df_mean)\n",
        "            del df_mean\n",
        "            del df_result_data\n",
        "\n",
        "            cnt+=1\n",
        "            if cnt > 100:\n",
        "                break                \n",
        "        except Exception as ex:\n",
        "            print(ex)\n",
        "\n",
        "print('** Total mean result **')\n",
        "df_result_data = pd.DataFrame(result_data)\n",
        "df_mean = df_result_data[statistics_columns].mean()\n",
        "print(df_mean)\n",
        "df_result_data.to_pickle(\"/content/drive/MyDrive/GAN_ENDE/df_result_93.pkl\")\n",
        "\n",
        "df_result_data[['ID']+statistics_columns]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================== Original Document 0==================================================\n",
            "Ever noticed how plane seats appear to be getting smaller and smaller?.\n",
            "With increasing numbers of people taking to the skies, some experts are questioning if having such packed out planes is putting passengers at risk.\n",
            "They say that the shrinking space on aeroplanes is not only uncomfortable - it's putting our health and safety in danger.\n",
            "More than squabbling over the arm rest, shrinking space on planes putting our health and safety in danger?.\n",
            "This week, a U S consumer advisory group set up by the Department of Transportation said at a public hearing that while the government is happy to set standards for animals flying on planes, it doesn't stipulate a minimum amount of space for humans.\n",
            "'In a world where animals have more rights to space and food than humans,' said Charlie Leocha, consumer representative on the committee.\n",
            "'It is time that the DOT and FAA take a stand for humane treatment of passengers '.\n",
            "But could crowding on planes lead to more serious issues than fighting for space in the overhead lockers, crashing elbows and seat back kicking?.\n",
            "Tests conducted by the FAA use planes with a 31 inch pitch, a standard which on some airlines has decreased.\n",
            "Many economy seats on United Airlines have 30 inches of room, while some airlines offer as little as 28 inches.\n",
            "Cynthia Corbertt, a human factors researcher with the Federal Aviation Administration, that it conducts tests on how quickly passengers can leave a plane.\n",
            "But these tests are conducted using planes with 31 inches between each row of seats, a standard which on some airlines has decreased, reported the Detroit News.\n",
            "The distance between two seats from one point on a seat to the same point on the seat behind it is known as the pitch.\n",
            "While most airlines stick to a pitch of 31 inches or above, some fall below this.\n",
            "While United Airlines has 30 inches of space, Gulf Air economy seats have between 29 and 32 inches, Air Asia offers 29 inches and Spirit Airlines offers just 28 inches.\n",
            "British Airways has a seat pitch of 31 inches, while easyJet has 29 inches, Thomson's short haul seat pitch is 28 inches, and Virgin Atlantic's is 30-31.\n",
            "-------------------------------------------------- Step0 0--------------------------------------------------\n",
            "Tests conducted by the FAA use planes with a 31 inch pitch, a standard which on some airlines has decreased.\n",
            "But these tests are conducted using planes with 31 inches between each row of seats, a standard which on some airlines has decreased, reported the Detroit News.\n",
            "While most airlines stick to a pitch of 31 inches or above, some fall below this.\n",
            "-------------------------------------------------- Step1 0--------------------------------------------------\n",
            "Ever noticed how plane seats appear to be getting smaller and smaller?.\n",
            "With increasing numbers of people taking to the skies, some experts are questioning if having such packed out planes is putting passengers at risk.\n",
            "They say that the shrinking space on aeroplanes is not only uncomfortable - it's putting our health and safety in danger.\n",
            "More than squabbling over the arm rest, shrinking space on planes putting our health and safety in danger?.\n",
            "Tests conducted by the FAA use planes with a 31 inch pitch, a standard which on some airlines has decreased.\n",
            "Many economy seats on United Airlines have 30 inches of room, while some airlines offer as little as 28 inches.\n",
            "But these tests are conducted using planes with 31 inches between each row of seats, a standard which on some airlines has decreased, reported the Detroit News.\n",
            "While most airlines stick to a pitch of 31 inches or above, some fall below this.\n",
            "British Airways has a seat pitch of 31 inches, while easyJet has 29 inches, Thomson's short haul seat pitch is 28 inches, and Virgin Atlantic's is 30-31.\n",
            "-------------------------------------------------- Step2 0--------------------------------------------------\n",
            "noticed how seats appear to smaller?\n",
            "With numbers of people taking to skies some experts questioning having packed planes is putting passengers risk and They space on aeroplanes only - our health safety.\n",
            "squabbling the arm rest shrinking space our health danger?\n",
            "and with a 31 inch pitch standard on some airlines has Many seats on United Airlines have 30 inches room while some offer as 28 inches.\n",
            "tests are conducted using planes 31 between row seats standard on airlines While most airlines stick a pitch of 31 or above some fall below this and Airways has a seat 31 inches easyJet has 29 Thomsons short haul is 28 and Virgin is.\n",
            "-------------------------------------------------- Step3 0--------------------------------------------------\n",
            "noticed how seats appear to be smaller.\n",
            "with huge numbers of people taking to the skies and some experts questioning whether having packed planes is putting passengers at risk and they could lose space on aeroplanes only-to protect our health and safety.\n",
            "squabbling at the arm rest in shrinking space increases our health danger.\n",
            "and with a 31 inch pitch standard on offer, some airlines has many seats on the ground, while united airlines have 30 inches.\n",
            "tests are being conducted using planes 31 inches between row seats as standard on airlines while most airlines stick to a pitch of 31 or above and some airlines fall below this standard and british airways has a seat 31 inches below average, easyjet has 29 inches and thomsons short haul is 28 inches and virgin is.\n",
            "-------------------------------------------------- Ground truth--------------------------------------------------\n",
            "Experts question if  packed out planes are putting passengers at risk .\n",
            "U.S consumer advisory group says minimum space must be stipulated .\n",
            "Safety tests conducted on planes with more leg room than airlines offer .\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "S0.R1            0.164948\n",
            "S0.R2            0.042105\n",
            "S0.RL            0.123711\n",
            "S0.Similarity    0.509294\n",
            "S0.Grammar       0.999981\n",
            "S1.R1            0.234234\n",
            "S1.R2            0.090909\n",
            "S1.RL            0.180180\n",
            "S1.Similarity    0.751495\n",
            "S1.Grammar       0.999985\n",
            "S2.R1            0.248276\n",
            "S2.R2            0.027972\n",
            "S2.RL            0.179310\n",
            "S2.Similarity    0.626171\n",
            "S2.Grammar       0.951615\n",
            "S3.R1            0.224852\n",
            "S3.R2            0.047904\n",
            "S3.RL            0.165680\n",
            "S3.Similarity    0.619923\n",
            "S3.Grammar       0.999953\n",
            "Name: 0, dtype: float64\n",
            "\n",
            "** Mean result **\n",
            "S0.R1            0.164948\n",
            "S0.R2            0.042105\n",
            "S0.RL            0.123711\n",
            "S0.Similarity    0.509294\n",
            "S0.Grammar       0.999981\n",
            "S1.R1            0.234234\n",
            "S1.R2            0.090909\n",
            "S1.RL            0.180180\n",
            "S1.Similarity    0.751495\n",
            "S1.Grammar       0.999985\n",
            "S2.R1            0.248276\n",
            "S2.R2            0.027972\n",
            "S2.RL            0.179310\n",
            "S2.Similarity    0.626171\n",
            "S2.Grammar       0.951615\n",
            "S3.R1            0.224852\n",
            "S3.R2            0.047904\n",
            "S3.RL            0.165680\n",
            "S3.Similarity    0.619923\n",
            "S3.Grammar       0.999953\n",
            "dtype: float64\n",
            "================================================== Original Document 1==================================================\n",
            "A drunk teenage boy had to be rescued by security after jumping into a lions' enclosure at a zoo in western India.\n",
            "Rahul Kumar, 17, clambered over the enclosure fence at the Kamla Nehru Zoological Park in Ahmedabad, and began running towards the animals, shouting he would 'kill them'.\n",
            "Mr Kumar explained afterwards that he was drunk and 'thought I'd stand a good chance' against the predators.\n",
            "Next level drunk: Intoxicated Rahul Kumar, 17, climbed into the lions' enclosure at a zoo in Ahmedabad and began running towards the animals shouting 'Today I kill a lion!'.\n",
            "Mr Kumar had been sitting near the enclosure when he suddenly made a dash for the lions, surprising zoo security.\n",
            "The intoxicated teenager ran towards the lions, shouting: 'Today I kill a lion or a lion kills me!'.\n",
            "A zoo spokesman said: 'Guards had earlier spotted him close to the enclosure but had no idea he was planing to enter it.\n",
            "'Fortunately, there are eight moats to cross before getting to where the lions usually are and he fell into the second one, allowing guards to catch up with him and take him out.\n",
            "'We then handed him over to the police '.\n",
            "Brave fool: Fortunately, Mr Kumar fell into a moat as he ran towards the lions and could be rescued by zoo security staff before reaching the animals (stock image) Kumar later explained: 'I don't really know why I did it.\n",
            "'I was drunk and thought I'd stand a good chance '.\n",
            "A police spokesman said: 'He has been cautioned and will be sent for psychiatric evaluation.\n",
            "'Fortunately for him, the lions were asleep and the zoo guards acted quickly enough to prevent a tragedy similar to that in Delhi '.\n",
            "Last year a 20-year-old man was mauled to death by a tiger in the Indian capital after climbing into its enclosure at the city zoo.\n",
            "-------------------------------------------------- Step0 1--------------------------------------------------\n",
            "Next level drunk: Intoxicated Rahul Kumar, 17, climbed into the lions' enclosure at a zoo in Ahmedabad and began running towards the animals shouting 'Today I kill a lion!'.\n",
            "Mr Kumar had been sitting near the enclosure when he suddenly made a dash for the lions, surprising zoo security.\n",
            "Brave fool: Fortunately, Mr Kumar fell into a moat as he ran towards the lions and could be rescued by zoo security staff before reaching the animals (stock image) Kumar later explained: 'I don't really know why I did it.\n",
            "-------------------------------------------------- Step1 1--------------------------------------------------\n",
            "A drunk teenage boy had to be rescued by security after jumping into a lions' enclosure at a zoo in western India.\n",
            "Rahul Kumar, 17, clambered over the enclosure fence at the Kamla Nehru Zoological Park in Ahmedabad, and began running towards the animals, shouting he would 'kill them'.\n",
            "Mr Kumar explained afterwards that he was drunk and 'thought I'd stand a good chance' against the predators.\n",
            "Next level drunk: Intoxicated Rahul Kumar, 17, climbed into the lions' enclosure at a zoo in Ahmedabad and began running towards the animals shouting 'Today I kill a lion!'.\n",
            "Mr Kumar had been sitting near the enclosure when he suddenly made a dash for the lions, surprising zoo security.\n",
            "The intoxicated teenager ran towards the lions, shouting: 'Today I kill a lion or a lion kills me!'.\n",
            "'Fortunately, there are eight moats to cross before getting to where the lions usually are and he fell into the second one, allowing guards to catch up with him and take him out.\n",
            "Brave fool: Fortunately, Mr Kumar fell into a moat as he ran towards the lions and could be rescued by zoo security staff before reaching the animals (stock image) Kumar later explained: 'I don't really know why I did it.\n",
            "'Fortunately for him, the lions were asleep and the zoo guards acted quickly enough to prevent a tragedy similar to that in Delhi '.\n",
            "-------------------------------------------------- Step2 1--------------------------------------------------\n",
            "drunk boy had after jumping into lions enclosure zoo India Rahul Kumar 17 clambered over the enclosure at Nehru Zoological Park in Ahmedabad began running towards shouting kill Kumar he drunk Id good chance against predators.\n",
            "Next level drunk: Kumar climbed into the lions enclosure at a zoo and began running towards the animals kill Mr Kumar had sitting enclosure when made for the lions zoo intoxicated teenager lions kill a lion or lion kills me!\n",
            "and.\n",
            "Fortunately eight moats getting the lions and he fell the second one catch up with take him Fortunately Kumar fell a moat ran lions could rescued zoo security the image) Kumar dont really know why I did it for lions asleep zoo guards quickly enough to prevent tragedy in Delhi.\n",
            "-------------------------------------------------- Step3 1--------------------------------------------------\n",
            "a drunk boy had died after jumping into the lions enclosure at zoo india rahul kumar 17 clambered over the enclosure at indira gandhi zoological park in ahmedabad and began running towards him shouting'i kill kumar'as he was drunk with id and had a good chance against predators.\n",
            "next level drunk : kumar climbed into the lions enclosure at a zoo and began running towards the animals and threatened to kill mr kumar who had been sitting in the enclosure when he made his way to the lions zoo after becoming intoxicated teenager'the lions kill a lion or a lion kills me '.\n",
            "and, and.\n",
            "fortunately eight moats were getting through the lions and he fell in the second one to catch up with him'fortunately kumar fell down a moat and ran away, while the lions could be rescued by zoo security guards ( stock image ) kumar added :'i dont really know why i did it for the lions asleep behind the zoo guards quickly enough to prevent the tragedy in delhi.\n",
            "-------------------------------------------------- Ground truth--------------------------------------------------\n",
            "Drunk teenage boy climbed into lion enclosure at zoo in west India .\n",
            "Rahul Kumar, 17, ran towards animals shouting 'Today I kill a lion!'\n",
            "Fortunately he fell into a moat before reaching lions and was rescued .\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "S0.R1            0.492063\n",
            "S0.R2            0.290323\n",
            "S0.RL            0.380952\n",
            "S0.Similarity    0.621382\n",
            "S0.Grammar       0.999981\n",
            "S1.R1            0.261194\n",
            "S1.R2            0.165414\n",
            "S1.RL            0.231343\n",
            "S1.Similarity    0.822449\n",
            "S1.Grammar       0.999980\n",
            "S2.R1            0.380368\n",
            "S2.R2            0.149068\n",
            "S2.RL            0.269939\n",
            "S2.Similarity    0.586348\n",
            "S2.Grammar       0.992776\n",
            "S3.R1            0.306220\n",
            "S3.R2            0.125604\n",
            "S3.RL            0.229665\n",
            "S3.Similarity    0.597156\n",
            "S3.Grammar       0.999177\n",
            "Name: 1, dtype: float64\n",
            "\n",
            "** Mean result **\n",
            "S0.R1            0.328506\n",
            "S0.R2            0.166214\n",
            "S0.RL            0.252332\n",
            "S0.Similarity    0.565338\n",
            "S0.Grammar       0.999981\n",
            "S1.R1            0.247714\n",
            "S1.R2            0.128161\n",
            "S1.RL            0.205762\n",
            "S1.Similarity    0.786972\n",
            "S1.Grammar       0.999982\n",
            "S2.R1            0.314322\n",
            "S2.R2            0.088520\n",
            "S2.RL            0.224624\n",
            "S2.Similarity    0.606260\n",
            "S2.Grammar       0.972195\n",
            "S3.R1            0.265536\n",
            "S3.R2            0.086754\n",
            "S3.RL            0.197673\n",
            "S3.Similarity    0.608539\n",
            "S3.Grammar       0.999565\n",
            "dtype: float64\n",
            "================================================== Original Document 2==================================================\n",
            "Dougie Freedman is on the verge of agreeing a new two-year deal to remain at Nottingham Forest.\n",
            "Freedman has stabilised Forest since he replaced cult hero Stuart Pearce and the club's owners are pleased with the job he has done at the City Ground.\n",
            "Dougie Freedman is set to sign a new deal at Nottingham Forest.\n",
            "Freedman has impressed at the City Ground since replacing Stuart Pearce in February.\n",
            "They made an audacious attempt on the play-off places when Freedman replaced Pearce but have tailed off in recent weeks.\n",
            "That has not prevented Forest's ownership making moves to secure Freedman on a contract for the next two seasons.\n",
            "-------------------------------------------------- Step0 2--------------------------------------------------\n",
            "Dougie Freedman is on the verge of agreeing a new two-year deal to remain at Nottingham Forest.\n",
            "Dougie Freedman is set to sign a new deal at Nottingham Forest.\n",
            "That has not prevented Forest's ownership making moves to secure Freedman on a contract for the next two seasons.\n",
            "-------------------------------------------------- Step1 2--------------------------------------------------\n",
            "Dougie Freedman is on the verge of agreeing a new two-year deal to remain at Nottingham Forest.\n",
            "Freedman has stabilised Forest since he replaced cult hero Stuart Pearce and the club's owners are pleased with the job he has done at the City Ground.\n",
            "Dougie Freedman is set to sign a new deal at Nottingham Forest.\n",
            "Freedman has impressed at the City Ground since replacing Stuart Pearce in February.\n",
            "They made an audacious attempt on the play-off places when Freedman replaced Pearce but have tailed off in recent weeks.\n",
            "That has not prevented Forest's ownership making moves to secure Freedman on a contract for the next two seasons.\n",
            "-------------------------------------------------- Step2 2--------------------------------------------------\n",
            "Dougie Freedman is verge of agreeing a deal to Nottingham Forest and Freedman Forest since he replaced Stuart Pearce clubs are he City Ground and Dougie Freedman is sign new Forest.\n",
            "Freedman at the City since replacing Stuart Pearce They made Freedman replaced Pearce but recent weeks has prevented Forests ownership secure Freedman a contract the next seasons.\n",
            "-------------------------------------------------- Step3 2--------------------------------------------------\n",
            "dougie freedman is on the verge of agreeing a deal to join nottingham forest.\n",
            "freedman has been at the city ground since replacing stuart pearce when they made freedman replaced pearce but in recent weeks has prevented forest ownership to secure freedman a new contract for the next two seasons.\n",
            "-------------------------------------------------- Ground truth--------------------------------------------------\n",
            "Nottingham Forest are close to extending Dougie Freedman's contract .\n",
            "The Forest boss took over from former manager Stuart Pearce in February .\n",
            "Freedman has since lead the club to ninth in the Championship .\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "S0.R1            0.313253\n",
            "S0.R2            0.049383\n",
            "S0.RL            0.192771\n",
            "S0.Similarity    0.660138\n",
            "S0.Grammar       0.999983\n",
            "S1.R1            0.303448\n",
            "S1.R2            0.097902\n",
            "S1.RL            0.193103\n",
            "S1.Similarity    1.000000\n",
            "S1.Grammar       0.999983\n",
            "S2.R1            0.351648\n",
            "S2.R2            0.089888\n",
            "S2.RL            0.241758\n",
            "S2.Similarity    0.635039\n",
            "S2.Grammar       0.999675\n",
            "S3.R1            0.409639\n",
            "S3.R2            0.098765\n",
            "S3.RL            0.240964\n",
            "S3.Similarity    0.586770\n",
            "S3.Grammar       0.999982\n",
            "Name: 2, dtype: float64\n",
            "\n",
            "** Mean result **\n",
            "S0.R1            0.323422\n",
            "S0.R2            0.127270\n",
            "S0.RL            0.232478\n",
            "S0.Similarity    0.596938\n",
            "S0.Grammar       0.999982\n",
            "S1.R1            0.266292\n",
            "S1.R2            0.118075\n",
            "S1.RL            0.201542\n",
            "S1.Similarity    0.857981\n",
            "S1.Grammar       0.999983\n",
            "S2.R1            0.326764\n",
            "S2.R2            0.088976\n",
            "S2.RL            0.230336\n",
            "S2.Similarity    0.615853\n",
            "S2.Grammar       0.981355\n",
            "S3.R1            0.313570\n",
            "S3.R2            0.090758\n",
            "S3.RL            0.212103\n",
            "S3.Similarity    0.601283\n",
            "S3.Grammar       0.999704\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmtUXjmagINF"
      },
      "source": [
        "import io\n",
        "\n",
        "result_data = {}\n",
        "\n",
        "result_data['ID'] = []\n",
        "result_data['Original'] = []\n",
        "result_data['Step0'] = []\n",
        "result_data['Step1'] = []\n",
        "result_data['Step2'] = []\n",
        "result_data['Step3'] = []\n",
        "result_data['Ground Truth'] = []\n",
        "\n",
        "result_data['S0.R1'] = []\n",
        "result_data['S0.R2'] = []\n",
        "result_data['S0.RL'] = []\n",
        "result_data['S0.Similarity'] = []\n",
        "result_data['S0.Grammar'] = []\n",
        "\n",
        "result_data['S1.R1'] = []\n",
        "result_data['S1.R2'] = []\n",
        "result_data['S1.RL'] = []\n",
        "result_data['S1.Similarity'] = []\n",
        "result_data['S1.Grammar'] = []\n",
        "\n",
        "result_data['S2.R1'] = []\n",
        "result_data['S2.R2'] = []\n",
        "result_data['S2.RL'] = []\n",
        "result_data['S2.Similarity'] = []\n",
        "result_data['S2.Grammar'] = []\n",
        "\n",
        "result_data['S3.R1'] = []\n",
        "result_data['S3.R2'] = []\n",
        "result_data['S3.RL'] = []\n",
        "result_data['S3.Similarity'] = []\n",
        "result_data['S3.Grammar'] = []\n",
        "\n",
        "statistics_columns = ['S0.R1','S0.R2','S0.RL','S0.Similarity','S0.Grammar','S1.R1','S1.R2','S1.RL','S1.Similarity','S1.Grammar','S2.R1','S2.R2','S2.RL','S2.Similarity','S2.Grammar','S3.R1','S3.R2','S3.RL','S3.Similarity','S3.Grammar']\n",
        "\n",
        "atten_rate = 0.2\n",
        "similarity = 2.0\n",
        "std_factor = 3.0\n",
        "\n",
        "#es = ExtactiveSummarizer()\n",
        "\n",
        "pd.set_option('display.max_columns', 100)\n",
        "cnt = 0\n",
        "logout = False\n",
        "for rate in range(0,1):\n",
        "    comp_rate=1.5\n",
        "    #result_data['comp_rate'].append(comp_rate)\n",
        "    for try_count in range(1000):\n",
        "        \n",
        "        full_text = get_prepared_doc(sentences_dataset[try_count])\n",
        "        '''\n",
        "        try:\n",
        "            del model\n",
        "            print('delete model')\n",
        "        except Exception as ex:\n",
        "            pass\n",
        "        model = EncoderDecoderModel.from_pretrained(\"/content/drive/MyDrive/GAN_ENDE/en_sentence_complete_model\")\n",
        "        '''\n",
        "        try:\n",
        "\n",
        "            #org_text = besm(full_text,num_sentences=9)\n",
        "            step0 = es.generate_summary(full_text,top_n=3,min_length=0)[0]\n",
        "\n",
        "            step1 = es.generate_summary(full_text,top_n=6,min_length=0)[0]\n",
        "            \n",
        "            org_sentences = np.array(nltk.sent_tokenize(step1.strip()))\n",
        "            if logout:\n",
        "                print('BESM Summary sentance length:',len(org_sentences))\n",
        "            step2,grammar,simil = summary(full_text,step1,steps=2,top_rank=2,comp_rate=comp_rate)\n",
        "            tmp = []\n",
        "            for txt in np.array(nltk.sent_tokenize(step2.strip())):\n",
        "                tmp.append(sentence_correct(txt))         \n",
        "            step3 = ' '.join(tmp)\n",
        "\n",
        "            ground_trouth = gold_summary[try_count]\n",
        "            \n",
        "            print('='*50 + ' Original Document ' + str(try_count) + '='*50)\n",
        "            for txt in np.array(nltk.sent_tokenize(full_text)):\n",
        "                print(txt)\n",
        "            print('-'*50 + ' Step0 ' + str(try_count) + '-'*50)\n",
        "            for txt in np.array(nltk.sent_tokenize(step0.strip())):\n",
        "                print(txt)                  \n",
        "            print('-'*50 + ' Step1 ' + str(try_count) + '-'*50)\n",
        "            for txt in np.array(nltk.sent_tokenize(step1.strip())):\n",
        "                print(txt)                \n",
        "            print('-'*50 + ' Step2 ' + str(try_count) + '-'*50)\n",
        "            for txt in np.array(nltk.sent_tokenize(step2.strip())):\n",
        "                print(txt)\n",
        "            print('-'*50 + ' Step3 ' + str(try_count) + '-'*50)\n",
        "            for txt in np.array(nltk.sent_tokenize(step3.strip())):\n",
        "                print(txt)                \n",
        "            print('-'*50 + ' Ground truth' + '-'*50)\n",
        "            print(ground_trouth)\n",
        "            print('-'*120)\n",
        "\n",
        "            #with io.open('/content/drive/MyDrive/GAN_ENDE/CNN_Daily_summary_result.txt','a',encoding='utf8') as f:\n",
        "            #    f.write(org_text + '\\r\\n\\r\\n')\n",
        "            s0_rouge = scorer.score(ground_trouth,step0)            \n",
        "            s1_rouge = scorer.score(ground_trouth,step1)\n",
        "            s2_rouge = scorer.score(ground_trouth,step2)\n",
        "            s3_rouge = scorer.score(ground_trouth,step3)\n",
        "            s0_simil = similarity3(full_text,step0)            \n",
        "            s1_simil = similarity3(full_text,step1)\n",
        "            s2_simil = similarity3(full_text,step2)\n",
        "            s3_simil = similarity3(full_text,step3)\n",
        "            s0_gramm = grammar3(full_text,step0)            \n",
        "            s1_gramm = grammar3(full_text,step1)\n",
        "            s2_gramm = grammar3(full_text,step2) \n",
        "            s3_gramm = grammar3(full_text,step3)  \n",
        "            #print(scores['rouge1'].fmeasure)\n",
        "            if s1_rouge['rouge1'].fmeasure > 0.1:\n",
        "\n",
        "                result_data['ID'].append('CNN/DM-'+str(try_count))\n",
        "                result_data['Original'].append(full_text)\n",
        "                result_data['Step0'].append(step1)\n",
        "                result_data['Step1'].append(step1)\n",
        "                result_data['Step2'].append(step2)\n",
        "                result_data['Step3'].append(step3)\n",
        "                result_data['Ground Truth'].append(ground_trouth)\n",
        "\n",
        "                result_data['S0.R1'].append(s0_rouge['rouge1'].fmeasure)\n",
        "                result_data['S0.R2'].append(s0_rouge['rouge2'].fmeasure)\n",
        "                result_data['S0.RL'].append(s0_rouge['rougeL'].fmeasure)\n",
        "                result_data['S0.Similarity'].append(s0_simil)\n",
        "                result_data['S0.Grammar'].append(s0_gramm)\n",
        "\n",
        "                result_data['S1.R1'].append(s1_rouge['rouge1'].fmeasure)\n",
        "                result_data['S1.R2'].append(s1_rouge['rouge2'].fmeasure)\n",
        "                result_data['S1.RL'].append(s1_rouge['rougeL'].fmeasure)\n",
        "                result_data['S1.Similarity'].append(s1_simil)\n",
        "                result_data['S1.Grammar'].append(s1_gramm)\n",
        "\n",
        "                result_data['S2.R1'].append(s2_rouge['rouge1'].fmeasure)\n",
        "                result_data['S2.R2'].append(s2_rouge['rouge2'].fmeasure)\n",
        "                result_data['S2.RL'].append(s2_rouge['rougeL'].fmeasure)\n",
        "                result_data['S2.Similarity'].append(s2_simil)\n",
        "                result_data['S2.Grammar'].append(s2_gramm)\n",
        "\n",
        "                result_data['S3.R1'].append(s3_rouge['rouge1'].fmeasure)\n",
        "                result_data['S3.R2'].append(s3_rouge['rouge2'].fmeasure)\n",
        "                result_data['S3.RL'].append(s3_rouge['rougeL'].fmeasure)\n",
        "                result_data['S3.Similarity'].append(s3_simil)\n",
        "                result_data['S3.Grammar'].append(s3_gramm)\n",
        "\n",
        "            print()\n",
        "            df_result_data = pd.DataFrame(result_data)\n",
        "            print(df_result_data[statistics_columns].iloc[-1])\n",
        "            print()\n",
        "            df_mean = df_result_data[statistics_columns].mean()\n",
        "            print('** Mean result **')\n",
        "            print(df_mean)\n",
        "            del df_mean\n",
        "            del df_result_data\n",
        "\n",
        "            cnt+=1\n",
        "            if cnt > 100:\n",
        "                break            \n",
        "        except Exception as ex:\n",
        "            print(ex)\n",
        "\n",
        "print('** Total mean result **')\n",
        "df_result_data = pd.DataFrame(result_data)\n",
        "df_mean = df_result_data[statistics_columns].mean()\n",
        "print(df_mean)\n",
        "df_result_data.to_pickle(\"/content/drive/MyDrive/GAN_ENDE/df_result_93.pkl\")\n",
        "\n",
        "df_result_data[['ID']+statistics_columns]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}