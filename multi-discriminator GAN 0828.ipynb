{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "korean_frame_token_0_1.0_gamma_10.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dolmani38/Summary2/blob/main/multi-discriminator%20GAN%200828.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkQCxNatSIOk"
      },
      "source": [
        "# Multi-Discriminator GAN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZjIW9VwyjDf"
      },
      "source": [
        "ABSTRACT\n",
        "\n",
        "----\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c20I_OEErmP9"
      },
      "source": [
        "# Related works\n",
        "\n",
        "두개 이상의 discriminator를 사용하는 GAN 연구에 대하여 알아본다.\n",
        "\n",
        "어떤 목적으로 복수의 discriminator를 사용하고 그 효과는 무엇인지 알아본다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uFOIbzcFagq"
      },
      "source": [
        "## 0. N개의 discriminator를 활용한 연구 \n",
        "\n",
        "Generative adversarial networks (Goodfellow et al. (2014))\n",
        "\n",
        "Generator를 multi로 한 연구들...\n",
        "\n",
        "1) Q. Hoang, T. Dinh Nguyen, T. Le, and D. Phung, “Multi-Generator Generative Adversarial Nets,” ArXiv e-prints, Aug. 2017.\n",
        "\n",
        "2) Multi-Agent Diverse Generative Adversarial Networks\n",
        "\n",
        "Federated learning의 한 지류가 될수도 있을 것...\n",
        "\n",
        "H. B. McMahan, E. Moore, D. Ramage, and B. A. y Arcas, “Federated learning of deep networks using model averaging,” CoRR, vol. abs/1602.05629, 2016."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa-0-i84rqoX"
      },
      "source": [
        "## 1. Automatic Image Colorization based on Multi-Discriminators Generative Adversarial Networks [품질향상]\n",
        "\n",
        "GAN은 흑백의 이미지를 입력하여 Color화 된 이미지를 생성해 낼 수 있다.\n",
        "본 논문은 두개의 discriminator를 이용하여 더 produces\n",
        "more realistic quality results.\n",
        "\n",
        "Different from conventional GAN network architecture,\n",
        "Park et al. [13] (S.-J. Park, H. Son, S. Cho, K.-S. Hong, and S. Lee, “Srfeat: Single image super-resolution with feature discrimination,” in Proceedings of the European Conference on Computer Vision, 2018, pp. 439–455.) introduce architecture based on combination of one generator associated with two discriminators. For colorization task, we propose an extended model, illustrated in Fig.1, which uses two discriminators: <font color='red'><b>an image discriminator Di and a feature discriminator Df</b> </font>. The first one discriminates real images (RGB) from colorized images by inspecting their pixel values, while the second discriminates real images from colorized ones by inspecting their feature maps, noted respectively\n",
        "VGG(y) and VGG(G(x)) .\n",
        "\n",
        "본 논문의 Proposed Loss functions 중 GAN에 대한 Loss은 다음과 같다.\n",
        "\n",
        "$$ L_{M-dis}(G,D_i,D_f) = \\lambda_iL_{GAN}(G,D_i) + \\lambda_fL_{GAN}(G,D_f)  $$\n",
        "where lambda_i and lambda_f denote a defined weighting factors\n",
        "\n",
        "실험에 있어서도 lambda_i and lambda_f 의 값을 특정 값을 설정하여 실험 하였다.\n",
        "그 값은 최적의 값이 였을까??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxfz7tKoDcZQ"
      },
      "source": [
        "## 2. UMLE: Unsupervised Multi-discriminator Network for Low Light Enhancement [품질 향상]\n",
        "\n",
        "Low-light image enhancement, such as recovering color and texture details from low-light images, is a complex and vital task. For automated driving, low-light scenarios will have serious implications for vision-based applications. To address this problem, we propose a real-time unsupervised generative adversarial network (GAN) containing  <font color='red'><b>multiple discriminators, i.e. a multi-scale discriminator, a texture discriminator, and a color discriminator.</b></font>\n",
        "\n",
        "본 논문에서 loss function 은 Adversarial loss + Cycle loss + Color loss + Preserving Loss + Reconstruction loss 로 구성된다.\n",
        "\n",
        "$$ L_{all} = \\omega_1L_{adv}+\\omega_2L_{cyc}+\\omega_3L_{color}+\\omega_4L_{pre}+\\omega_5L_{idt}$$ \n",
        "\n",
        "하지만 각각의 omega는 huristic하게 특정 지었다. 최적화된 값인가??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jbt0ApbS8V_Q"
      },
      "source": [
        "## 3. GENERATIVE MULTI-ADVERSARIAL NETWORKS [품질 향상+mode collapse]\n",
        "\n",
        "N개의 복수 discriminator를 사용하여 안정적으로 더 빠르게 GAN 학습을 할 수 있다. 또한 mode collapse에도 robust 한 특성을 보인다.\n",
        "\n",
        "본 논문에서는 loss function을 three classical Pythagorean means 을 응용하여 정의하였다.\n",
        "\n",
        "$$ AM_{soft}(V, \\lambda) = \\sum_{i}^N \\omega_iV_i $$\n",
        "$$ GM_{soft}(V, \\lambda) = -exp(\\sum_{i}^N \\omega_ilog(-V_i)) $$\n",
        "$$ HM_{soft}(V, \\lambda) = (\\sum_{i}^N \\omega_iV_i^{-1})^{-1} $$\n",
        "\n",
        "하지만, 논문에서는 omega에 대하여 다루지 않았다.\n",
        "저 omega는 어떻게 최적화 할 수 있겠는가?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5qpgtDUySyC"
      },
      "source": [
        "## 4. Dual Discriminator Generative Adversarial Nets [mode collapse]\n",
        "\n",
        "GAN에서 발생하는 치명적인 mode collapse (https://developers.google.com/machine-learning/gan/problems) 현상을 개선하기 위해 두개의 discriminator를 사용한다. - dual discriminator generative adversarial network (D2GAN)\n",
        "\n",
        "it combines <font color='red'><b>the Kullback-Leibler (KL) and reverse KL divergences</b></font> into a unified objective function, thus it exploits the complementary statistical properties from these divergences to effectively diversify the estimated density in capturing multi-modes.\n",
        "\n",
        "본 논문에서 제안하는 D2GAN의 목적함수는 다음과 같다.\n",
        "\n",
        "$$ \\min_{G} \\max_{D_1,D_2} J (G,D_1,D_2) = \\alpha \\times E_{x \\sim P_{data}} [logD_1 (x)] + E_{z \\sim P_z} [-D_1 (G(z))] + E_{x \\sim P_{data}}[-D_2 (x)] + \\beta \\times E_{z \\sim P_z} [logD_2 (G(z))] $$\n",
        "\n",
        "여기서 alpha, beta는 hyperparameter로서, 본 논문의 실험에서는 다양한 값을 대입하여 각각의 성능을 확인하였다.\n",
        "\n",
        "이렇게 값을 찾아야만 하는가?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBQHO9hOBho-"
      },
      "source": [
        "## 5. MD-GAN: Multi-Discriminator Generative Adversarial Networks for Distributed Datasets [성능 향상]\n",
        "\n",
        "we address the problem of distributing GANs so that they are able to train over datasets that are spread on multiple workers. MD-GAN is exposed as the first solution for this problem: we propose a novel learning procedure for GANs\n",
        "so that they fit this distributed setup. We then compare the performance of MD-GAN to an adapted version of Federated Learning to GANs, using the MNIST and CIFAR10 datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPUM8QvQCWzi"
      },
      "source": [
        "## 6. ParallelWasserstein Generative Adversarial Nets with Multiple Discriminators [성능 향상]\n",
        "\n",
        "In this paper, we solve the computation cost problem by speeding up the Wasserstein GANs from a welldesigned communication efficient parallel architecture. 그리고 이것을 Multiple Discriminators 로 구성하였다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjztFvs812EB"
      },
      "source": [
        "## Proposed methods\n",
        "\n",
        "ref : https://realpython.com/python-ai-neural-network/\n",
        "\n",
        "colab 수식입력 : \n",
        "\n",
        "https://wikidocs.net/1679\n",
        "\n",
        "https://en.wikipedia.org/wiki/Help:Displaying_a_formula#Formatting_using_TeX\n",
        "\n",
        "Original GAN의 목적함수\n",
        "$$ \\min_{G}\\max_{D} V(D,G) = E_{x\\sim p_{data}(x)}[logD(x)] + E_{z\\sim p_{z}(z)}[log(1-D(G(z)))] $$\n",
        "\n",
        "Multi-Discriminator GAN은 discriminator가 각 목적에 의하여 여러개 (N개) 있다.\n",
        "MDGAN의 목적함수\n",
        "\n",
        "$$ \\min_{G}\\max_{D_{i\\sim N}} V(D_{i\\sim N},G) = \\sum_{i=1}^N \\{E_{x\\sim p_{data}(x)}[logD_i(x)] + E_{z\\sim p_{z}(z)}[log(1-D_i(G(z)))]\\} $$\n",
        "\n",
        "여기서\n",
        "\n",
        "$$ L(D_i,G) =  E_{x\\sim p_{data}(x)}[logD_i(x)] + E_{z\\sim p_{z}(z)}[log(1-D_i(G(z)))] $$\n",
        "\n",
        "이라하고 단순화 하면\n",
        "\n",
        "$$ \\min_{G}\\max_{D_{i\\sim N}} V(D_{i\\sim N},G) = \\sum_{i=1}^N L(D_i,G) $$\n",
        "\n",
        "와 같이 된다.\n",
        "\n",
        "문제점은 GAN의 특성상, 특정 Discriminator가 학습에 있어 지배적으로 loss 함수에 영향을 미치게 되어 각각의 Discriminator가 골고루 학습에 참여하지 못하고 의도하지 않은 결과를 만들게 된다. 이러한 문제점을 극복하기 위해 다음의 두가지 제안을 한다.\n",
        "\n",
        "1) 목적함수에 각 Loss 에 대한 표준편차 (standard-deviation) 를 반영하여 각 Discriminator에 대한 Loss가 상호 유사한 수준을 유지하면 학습이 진행되도록 한다.\n",
        "\n",
        "$$ \\min_{G}\\max_{D_{i\\sim N}} V(D_{i\\sim N},G) = \\sum_{i=1}^N L(D_i,G) + \\sigma(L(D_i,G))$$\n",
        "\n",
        "2) 각 discriminator에 의한 loss를 제어하기 위해, adaptive discriminant factor (ADF) 를 적용하고, 학습의 진행 과정에서 이를 최적화 한다. \n",
        "\n",
        "$$ \\min_{G}\\max_{D_{i\\sim N}} V(\\lambda_{i\\sim N},D_{i\\sim N},G) = \\sum_{i=1}^N \\lambda_iL(D_i,G)$$\n",
        "\n",
        "3) 1)의 제안에 2)의 제안을 추가한다.\n",
        "\n",
        "$$ \\min_{G}\\max_{D_{i\\sim N}} V(\\lambda_{i\\sim N},D_{i\\sim N},G) = \\sum_{i=1}^N \\lambda_iL(D_i,G) + \\sigma(L(D_i,G))$$\n",
        "\n",
        "\n",
        "여기서\n",
        "\n",
        "$$ \\lambda_i = adaptive\\ discriminant \\ factor \\ for \\ discriminator \\ i  $$\n",
        "\n",
        "중요한 것은, 학습과정에서 L_i을 작게 (학습의 방향)하기 위해서는 lambda_i는 역으로 커져야 한다. 그래야, 전체 Loss function에서 비중이 증대되어 더 적극적인 학습이 이루어 지게 된다. 따라서, lambda_i의 최적화 방향은 기존의 gradient decent와 반대 방향이 되어야 한다.\n",
        "\n",
        "$$ \\lambda_i^{t+1} = \\lambda_i^t + \\gamma \\nabla V(\\lambda_{i\\sim N}^t,D_{i\\sim N},G)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K87VNBbeRLFF"
      },
      "source": [
        "#4. Implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQZeBAf8NxAR"
      },
      "source": [
        "## 4.1 기본 설정..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAdXzWGuKSBT",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9cccdb7-b597-42c8-be6e-079701c29057"
      },
      "source": [
        "if True:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "newO0mBXKVnE",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b6eeddb-e3f3-4661-f5ef-ca7393107e9c"
      },
      "source": [
        "#!pip install keybert\n",
        "!pip3 install transformers\n",
        "!pip3 install sentence-transformers\n",
        "\n",
        "#!pip install sentence-transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.9.0+cu102)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.10.0+cu102)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.96)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.62.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.9.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (5.4.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.10.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.6.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmIxp0FnKXif",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b5440fd-d91f-4d46-ca9a-9fd92f93f24d"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# set seeds for reproducability\n",
        "from numpy.random import seed\n",
        "#seed(1)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string, os \n",
        "\n",
        "import urllib.request\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3J0n_lhKcgm",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f58dc1a2-4931-4d37-d9ca-fa04911dc4be"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ue_4ZfdRKfdX",
        "trusted": true
      },
      "source": [
        "# Print iterations progress\n",
        "class ProgressBar:\n",
        "\n",
        "    def __init__(self,total=20, prefix = '', suffix = '', decimals = 1, length = 20, fill = '|', printEnd = \"\\r\"):\n",
        "        self.total = total\n",
        "        self.prefix = prefix\n",
        "        self.suffix = suffix\n",
        "        self.decimals = decimals\n",
        "        self.length = length\n",
        "        self.fill = fill\n",
        "        self.printEnd = printEnd\n",
        "        self.ite = 0\n",
        "        self.back_filledLength = 0\n",
        "\n",
        "    def printProgress(self,iteration, text):\n",
        "        self.ite += iteration\n",
        "        percent = (\"{0:.\" + str(self.decimals) + \"f}\").format(100 * (self.ite / float(self.total)))\n",
        "        filledLength = int(self.length * self.ite // self.total)\n",
        "        bar = self.fill * filledLength + '.' * (self.length - filledLength)\n",
        "        if filledLength > self.back_filledLength or percent == 100:\n",
        "            print(f'\\r{self.prefix} |{bar}| {percent}% {self.suffix}  {text}', end=\"\", flush=True)\n",
        "            # Print New Line on Complete\n",
        "            if self.ite == self.total: \n",
        "                print()\n",
        "        self.back_filledLength = filledLength    "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNHI0G6JKc5h",
        "trusted": true
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Zsv-LVkKmfL"
      },
      "source": [
        "##4.2 Grammar Discriminator Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VQdGLciKc_y",
        "trusted": true
      },
      "source": [
        "from transformers import BertTokenizer, BertTokenizerFast,AutoTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset, random_split\n",
        "\n",
        "import time\n",
        "import random\n",
        "import datetime\n",
        "\n",
        "# 간단한 전처리\n",
        "def clean_text(txt):\n",
        "    txt = txt.replace('\\n',' ')\n",
        "    txt = txt.replace('\\r',' ')    \n",
        "    txt = txt.replace('=','')\n",
        "    txt = txt.replace('\\\"','')   \n",
        "    txt = txt.replace('\\'','')\n",
        "    #txt = txt.replace(',','')\n",
        "    txt = txt.replace('..','')\n",
        "    txt = txt.replace('...','')\n",
        "    txt = txt.replace(' .','.')\n",
        "    txt = txt.replace('.','. ')\n",
        "    txt = txt.replace('  ',' ')\n",
        "    txt = txt.replace('  ',' ')    \n",
        "    txt = txt.replace('  ',' ')   \n",
        "    txt = txt.replace('  ',' ')           \n",
        "    txt = txt.replace('  ',' ')\n",
        "    txt = txt.replace('  ',' ')    \n",
        "    txt = txt.replace('  ',' ')   \n",
        "    txt = txt.replace('  ',' ')             \n",
        "    return txt.strip()\n",
        "\n",
        "def shuffling(txt):\n",
        "    txt_list = txt.split(' ')\n",
        "    random.shuffle(txt_list)\n",
        "    return ' '.join(txt_list)\n",
        "\n",
        "def collect_training_dataset_for_grammar_discriminator(sentences_dataset):\n",
        "\n",
        "    sentences = []\n",
        "    labels = []\n",
        "\n",
        "    for txtss in sentences_dataset:\n",
        "        txtss = clean_text(txtss)\n",
        "        txts = txtss.strip().split('.')\n",
        "        for txt in txts:  \n",
        "            txt = txt.strip()\n",
        "            if len(txt) > 40:\n",
        "                #ko_grammar_dataset.append([txt,1])\n",
        "                txt = txt.replace('.','')\n",
        "                tf = random.choice([True,False])\n",
        "                # 정상 또는 비정상 둘중에 하나만 데이터셋에 추가\n",
        "                if (tf):\n",
        "                    sentences.append(txt) # '.'의 위치를 보고 True, False를 판단 하기 땜에...\n",
        "                    labels.append(1)\n",
        "                else:\n",
        "                    sentences.append(shuffling(txt))\n",
        "                    labels.append(0)\n",
        "\n",
        "    return sentences,labels\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "class Grammar_Discriminator:\n",
        "\n",
        "\n",
        "    def __init__(self, pretraoned_kobert_model_name='kykim/bert-kor-base', input_dir=None):\n",
        "\n",
        "        if input_dir is None:\n",
        "            self.tokenizer = BertTokenizerFast.from_pretrained(pretraoned_kobert_model_name)\n",
        "            self.discriminator = BertForSequenceClassification.from_pretrained(\n",
        "                                    pretraoned_kobert_model_name, # Use the 12-layer BERT model, with an uncased vocab.\n",
        "                                    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                                                    # You can increase this for multi-class tasks.   \n",
        "                                    output_attentions = False, # Whether the model returns attentions weights.\n",
        "                                    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "                                )            \n",
        "        else:\n",
        "            self.__load_model(input_dir)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def set_dataset(self, sentences,labels):\n",
        "        # Print the original sentence.\n",
        "        print(' Original: ', sentences[0])\n",
        "\n",
        "        # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "        input_ids = []\n",
        "        attention_masks = []\n",
        "\n",
        "        # For every sentence...\n",
        "        for sent in sentences:\n",
        "            # `encode_plus` will:\n",
        "            #   (1) Tokenize the sentence.\n",
        "            #   (2) Prepend the `[CLS]` token to the start.\n",
        "            #   (3) Append the `[SEP]` token to the end.\n",
        "            #   (4) Map tokens to their IDs.\n",
        "            #   (5) Pad or truncate the sentence to `max_length`\n",
        "            #   (6) Create attention masks for [PAD] tokens.\n",
        "            encoded_dict = self.tokenizer.encode_plus(\n",
        "                                sent,                      # Sentence to encode.\n",
        "                                add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                                max_length = 64,           # Pad & truncate all sentences.\n",
        "                                pad_to_max_length = True,\n",
        "                                return_attention_mask = True,   # Construct attn. masks.\n",
        "                                return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                                truncation = True,\n",
        "                        )\n",
        "            \n",
        "            # Add the encoded sentence to the list.    \n",
        "            input_ids.append(encoded_dict['input_ids'])\n",
        "            \n",
        "            # And its attention mask (simply differentiates padding from non-padding).\n",
        "            attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "        # Convert the lists into tensors.\n",
        "        input_ids = torch.cat(input_ids, dim=0)\n",
        "        attention_masks = torch.cat(attention_masks, dim=0)\n",
        "        labels = torch.tensor(labels)\n",
        "\n",
        "        # Print sentence 0, now as a list of IDs.\n",
        "        print('Original: ', sentences[0])\n",
        "        print('Token IDs:', input_ids[0])\n",
        "\n",
        "        # Training & Validation Split\n",
        "        # Divide up our training set to use 90% for training and 10% for validation.\n",
        "\n",
        "        # Combine the training inputs into a TensorDataset.\n",
        "        dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "        # Create a 90-10 train-validation split.\n",
        "\n",
        "        # Calculate the number of samples to include in each set.\n",
        "        train_size = int(0.9 * len(dataset))\n",
        "        val_size = len(dataset) - train_size\n",
        "\n",
        "        # Divide the dataset by randomly selecting samples.\n",
        "        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "        print('{:>5,} training samples'.format(train_size))\n",
        "        print('{:>5,} validation samples'.format(val_size))\n",
        "\n",
        "        # The DataLoader needs to know our batch size for training, so we specify it \n",
        "        # here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "        # size of 16 or 32.\n",
        "        self.batch_size = 32\n",
        "\n",
        "        # Create the DataLoaders for our training and validation sets.\n",
        "        # We'll take training samples in random order. \n",
        "        self.train_dataloader = DataLoader(\n",
        "                    train_dataset,  # The training samples.\n",
        "                    sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "                    batch_size = self.batch_size # Trains with this batch size.\n",
        "                )\n",
        "\n",
        "        # For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "        self.validation_dataloader = DataLoader(\n",
        "                    val_dataset, # The validation samples.\n",
        "                    sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "                    batch_size = self.batch_size # Evaluate with this batch size.\n",
        "                )        \n",
        "\n",
        "\n",
        "\n",
        "    def train(self,epochs=4):\n",
        "        # Tell pytorch to run this model on the GPU.\n",
        "        self.discriminator.cuda()\n",
        "\n",
        "        # Get all of the model's parameters as a list of tuples.\n",
        "        params = list(self.discriminator.named_parameters())\n",
        "\n",
        "        print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "        print('==== Embedding Layer ====\\n')\n",
        "\n",
        "        for p in params[0:5]:\n",
        "            print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "        print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "        for p in params[5:21]:\n",
        "            print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "        print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "        for p in params[-4:]:\n",
        "            print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))  \n",
        "\n",
        "        # Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "        # I believe the 'W' stands for 'Weight Decay fix\"\n",
        "        self.optimizer = AdamW(self.discriminator.parameters(),\n",
        "                        lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                        eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                        )\n",
        "\n",
        "        # Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "        # We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "        # training data.\n",
        "        #epochs = 2\n",
        "\n",
        "        # Total number of training steps is [number of batches] x [number of epochs]. \n",
        "        # (Note that this is not the same as the number of training samples).\n",
        "        total_steps = len(self.train_dataloader) * epochs\n",
        "\n",
        "        # Create the learning rate scheduler.\n",
        "        scheduler = get_linear_schedule_with_warmup(self.optimizer, \n",
        "                                                    num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                                    num_training_steps = total_steps)\n",
        "            \n",
        "        # This training code is based on the `run_glue.py` script here:\n",
        "        # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "        # Set the seed value all over the place to make this reproducible.\n",
        "        seed_val = 42\n",
        "\n",
        "        random.seed(seed_val)\n",
        "        np.random.seed(seed_val)\n",
        "        torch.manual_seed(seed_val)\n",
        "        torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "        # We'll store a number of quantities such as training and validation loss, \n",
        "        # validation accuracy, and timings.\n",
        "        training_stats = []\n",
        "\n",
        "        # Measure the total training time for the whole run.\n",
        "        total_t0 = time.time()\n",
        "\n",
        "        # For each epoch...\n",
        "        for epoch_i in range(0, epochs):\n",
        "            \n",
        "            # ========================================\n",
        "            #               Training\n",
        "            # ========================================\n",
        "            \n",
        "            # Perform one full pass over the training set.\n",
        "\n",
        "            print(\"\")\n",
        "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "            print('Training...')\n",
        "\n",
        "            # Measure how long the training epoch takes.\n",
        "            t0 = time.time()\n",
        "\n",
        "            # Reset the total loss for this epoch.\n",
        "            total_train_loss = 0\n",
        "\n",
        "            # Put the model into training mode. Don't be mislead--the call to \n",
        "            # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "            # `dropout` and `batchnorm` layers behave differently during training\n",
        "            # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "            self.discriminator.train()\n",
        "\n",
        "            # For each batch of training data...\n",
        "            for step, batch in enumerate(self.train_dataloader):\n",
        "\n",
        "                # Progress update every 40 batches.\n",
        "                if step % 40 == 0 and not step == 0:\n",
        "                    # Calculate elapsed time in minutes.\n",
        "                    elapsed = format_time(time.time() - t0)\n",
        "                    \n",
        "                    # Report progress.\n",
        "                    print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(self.train_dataloader), elapsed))\n",
        "\n",
        "                # Unpack this training batch from our dataloader. \n",
        "                #\n",
        "                # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "                # `to` method.\n",
        "                #\n",
        "                # `batch` contains three pytorch tensors:\n",
        "                #   [0]: input ids \n",
        "                #   [1]: attention masks\n",
        "                #   [2]: labels \n",
        "                b_input_ids = batch[0].to(device)\n",
        "                b_input_mask = batch[1].to(device)\n",
        "                b_labels = batch[2].to(device)\n",
        "\n",
        "                # Always clear any previously calculated gradients before performing a\n",
        "                # backward pass. PyTorch doesn't do this automatically because \n",
        "                # accumulating the gradients is \"convenient while training RNNs\". \n",
        "                # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "                self.discriminator.zero_grad()        \n",
        "\n",
        "                # Perform a forward pass (evaluate the model on this training batch).\n",
        "                # The documentation for this `model` function is here: \n",
        "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "                # It returns different numbers of parameters depending on what arguments\n",
        "                # arge given and what flags are set. For our useage here, it returns\n",
        "                # the loss (because we provided labels) and the \"logits\"--the model\n",
        "                # outputs prior to activation.\n",
        "                outputs = self.discriminator(b_input_ids, \n",
        "                                    token_type_ids=None, \n",
        "                                    attention_mask=b_input_mask, \n",
        "                                    labels=b_labels)\n",
        "                loss, logits = outputs.loss, outputs.logits\n",
        "                # Accumulate the training loss over all of the batches so that we can\n",
        "                # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "                # single value; the `.item()` function just returns the Python value \n",
        "                # from the tensor.\n",
        "                total_train_loss += loss.item()\n",
        "\n",
        "                # Perform a backward pass to calculate the gradients.\n",
        "                loss.backward()\n",
        "\n",
        "                # Clip the norm of the gradients to 1.0.\n",
        "                # This is to help prevent the \"exploding gradients\" problem.\n",
        "                torch.nn.utils.clip_grad_norm_(self.discriminator.parameters(), 1.0)\n",
        "\n",
        "                # Update parameters and take a step using the computed gradient.\n",
        "                # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "                # modified based on their gradients, the learning rate, etc.\n",
        "                self.optimizer.step()\n",
        "\n",
        "                # Update the learning rate.\n",
        "                scheduler.step()\n",
        "\n",
        "            # Calculate the average loss over all of the batches.\n",
        "            avg_train_loss = total_train_loss / len(self.train_dataloader)            \n",
        "            \n",
        "            # Measure how long this epoch took.\n",
        "            training_time = format_time(time.time() - t0)\n",
        "\n",
        "            print(\"\")\n",
        "            print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "            print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "                \n",
        "            # ========================================\n",
        "            #               Validation\n",
        "            # ========================================\n",
        "            # After the completion of each training epoch, measure our performance on\n",
        "            # our validation set.\n",
        "\n",
        "            print(\"\")\n",
        "            print(\"Running Validation...\")\n",
        "\n",
        "            t0 = time.time()\n",
        "\n",
        "            # Put the model in evaluation mode--the dropout layers behave differently\n",
        "            # during evaluation.\n",
        "            self.discriminator.eval()\n",
        "\n",
        "            # Tracking variables \n",
        "            total_eval_accuracy = 0\n",
        "            total_eval_loss = 0\n",
        "            nb_eval_steps = 0\n",
        "\n",
        "            # Evaluate data for one epoch\n",
        "            for batch in self.validation_dataloader:\n",
        "                \n",
        "                # Unpack this training batch from our dataloader. \n",
        "                #\n",
        "                # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "                # the `to` method.\n",
        "                #\n",
        "                # `batch` contains three pytorch tensors:\n",
        "                #   [0]: input ids \n",
        "                #   [1]: attention masks\n",
        "                #   [2]: labels \n",
        "                b_input_ids = batch[0].to(device)\n",
        "                b_input_mask = batch[1].to(device)\n",
        "                b_labels = batch[2].to(device)\n",
        "                \n",
        "                # Tell pytorch not to bother with constructing the compute graph during\n",
        "                # the forward pass, since this is only needed for backprop (training).\n",
        "                with torch.no_grad():        \n",
        "\n",
        "                    # Forward pass, calculate logit predictions.\n",
        "                    # token_type_ids is the same as the \"segment ids\", which \n",
        "                    # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "                    # The documentation for this `model` function is here: \n",
        "                    # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "                    # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "                    # values prior to applying an activation function like the softmax.\n",
        "                    outputs = self.discriminator(b_input_ids, \n",
        "                                        token_type_ids=None, \n",
        "                                        attention_mask=b_input_mask,\n",
        "                                        labels=b_labels)\n",
        "                loss, logits = outputs.loss, outputs.logits\n",
        "                # Accumulate the validation loss.\n",
        "                total_eval_loss += loss.item()\n",
        "\n",
        "                # Move logits and labels to CPU\n",
        "                logits = logits.detach().cpu().numpy()\n",
        "                label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "                # Calculate the accuracy for this batch of test sentences, and\n",
        "                # accumulate it over all batches.\n",
        "                total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "                \n",
        "\n",
        "            # Report the final accuracy for this validation run.\n",
        "            avg_val_accuracy = total_eval_accuracy / len(self.validation_dataloader)\n",
        "            print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "            # Calculate the average loss over all of the batches.\n",
        "            avg_val_loss = total_eval_loss / len(self.validation_dataloader)\n",
        "            \n",
        "            # Measure how long the validation run took.\n",
        "            validation_time = format_time(time.time() - t0)\n",
        "            \n",
        "            print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "            print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "            # Record all statistics from this epoch.\n",
        "            training_stats.append(\n",
        "                {\n",
        "                    'epoch': epoch_i + 1,\n",
        "                    'Training Loss': avg_train_loss,\n",
        "                    'Valid. Loss': avg_val_loss,\n",
        "                    'Valid. Accur.': avg_val_accuracy,\n",
        "                    'Training Time': training_time,\n",
        "                    'Validation Time': validation_time\n",
        "                }\n",
        "            )\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"Training complete!\")\n",
        "\n",
        "        print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "            \n",
        "\n",
        "        return training_stats\n",
        "\n",
        "    def save_model(self, output_dir = './model_save/'):\n",
        "        # Create output directory if needed\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "\n",
        "        print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "        # They can then be reloaded using `from_pretrained()`\n",
        "        model_to_save = self.discriminator.module if hasattr(self.discriminator, 'module') else self.discriminator  # Take care of distributed/parallel training\n",
        "        model_to_save.save_pretrained(output_dir)\n",
        "        self.tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "        # Good practice: save your training arguments together with the trained model\n",
        "        # torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n",
        "\n",
        "    def __load_model(self, input_dir = './drive/MyDrive/Colab Notebooks/summary/en_grammar_check_model'):\n",
        "        print('Loading BERT tokenizer...')\n",
        "        self.tokenizer = BertTokenizerFast.from_pretrained(input_dir)\n",
        "        self.discriminator = BertForSequenceClassification.from_pretrained(input_dir)\n",
        "\n",
        "    def transfer_learning(self, sentences, train_for = True):\n",
        "        \n",
        "        input_ids = []\n",
        "        attention_masks = []\n",
        "\n",
        "        # For every sentence...\n",
        "        for sent in sentences:\n",
        "            # `encode_plus` will:\n",
        "            #   (1) Tokenize the sentence.\n",
        "            #   (2) Prepend the `[CLS]` token to the start.\n",
        "            #   (3) Append the `[SEP]` token to the end.\n",
        "            #   (4) Map tokens to their IDs.\n",
        "            #   (5) Pad or truncate the sentence to `max_length`\n",
        "            #   (6) Create attention masks for [PAD] tokens.\n",
        "            encoded_dict = self.tokenizer.encode_plus(\n",
        "                                sent,                      # Sentence to encode.\n",
        "                                add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                                max_length = 64,           # Pad & truncate all sentences.\n",
        "                                pad_to_max_length = True,\n",
        "                                return_attention_mask = True,   # Construct attn. masks.\n",
        "                                return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                                truncation = True,\n",
        "                        )\n",
        "            # Add the encoded sentence to the list.    \n",
        "            input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "            # And its attention mask (simply differentiates padding from non-padding).\n",
        "            attention_masks.append(encoded_dict['attention_mask'])\n",
        "        \n",
        "        if train_for:\n",
        "            b_labels = torch.ones(len(sentences),dtype=torch.long).to(device)\n",
        "        else:\n",
        "            b_labels = torch.zeros(len(sentences),dtype=torch.long).to(device)\n",
        "        #print(b_labels)\n",
        "        # Convert the lists into tensors.\n",
        "        input_ids = torch.cat(input_ids, dim=0).to(device)\n",
        "        attention_masks = torch.cat(attention_masks, dim=0).to(device)    \n",
        "        #if str(discriminator1.device) == 'cpu':\n",
        "        #    pass\n",
        "        #else:\n",
        "        #    input_ids = input_ids.to(device)\n",
        "        #    attention_masks = attention_masks.to(device)        \n",
        "\n",
        "        outputs = self.discriminator(input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=attention_masks, \n",
        "                                labels=b_labels)\n",
        "\n",
        "        #print(outputs)\n",
        "        #return torch.sigmoid(outputs[0][:,1])\n",
        "        #return outputs[0][:,1]\n",
        "        return outputs['loss'], outputs['logits']\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjqloLeQpyxz"
      },
      "source": [
        "# 문법 학습을 위한 데이터셋 구성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "p-dd_3RRp2Qz",
        "outputId": "ccac1e20-57c7-4f53-ebef-cd1e9e393e03"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/summary/korean_news_corpus.csv')\n",
        "df"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>contents</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>문 대통령 변창흠 국토장관 사의표명 사실상 수용</td>\n",
              "      <td>정만호 국민소통수석이 12일 오후 청와대 춘추관 대브리핑룸에서 변창흠 국토부 장관 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>계급장 수여하는 문 대통령</td>\n",
              "      <td>(아산뉴스1) 이광호 기자 문재인 대통령이 12일 오후 충남 아산시 경찰대학에서 열...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>계급장 수여하는 문 대통령</td>\n",
              "      <td>(아산뉴스1) 이광호 기자 문재인 대통령이 12일 오후 충남 아산시 경찰대학에서 열...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>수상자 메달 걸어주는 문 대통령</td>\n",
              "      <td>(아산뉴스1) 이광호 기자 문재인 대통령이 12일 오후 충남 아산시 경찰대학에서 열...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>정몽구 서울아산병원에 50억 쾌척</td>\n",
              "      <td>인재 양성·소외 계층 지원 등 계획 “부친 질병·가난 악순환 끊기 원해 국내 최고 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140559</th>\n",
              "      <td>[건축과도시] 북한산을 캔버스 삼아. 미술관 또 하나의 작품이 되다</td>\n",
              "      <td>&lt;은평구 진관동 사비나 미술관&gt; 서울시 은평구 진관동에 자리잡은 사비나미술관. 삼각...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140560</th>\n",
              "      <td>조선후기 문인 김조순 별장 그린 옥호정도 첫 공개</td>\n",
              "      <td>국립중앙박물관 서화실 개편해 32점 새롭게 전시 옥호정도[국립중앙박물관 제공연합뉴스...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140561</th>\n",
              "      <td>안성 청룡사 대웅전에서 목재 곡자 발견</td>\n",
              "      <td>문화재청(청장 정재숙)의 국고보조와 기술지도로 안성시에서 시행하고 있는 안성 청룡사...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140562</th>\n",
              "      <td>156년전 ㄱ자 곡자 찾았다 안성 청룡사 기둥 밑에서</td>\n",
              "      <td>안성 청룡사 대웅전에서 발견된 곡자 【서울뉴시스】 이수지 기자 안성 청룡사 대웅전에...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140563</th>\n",
              "      <td>[김중기의 필름통] 새 영화 도굴 나인스 게이트 앙상블</td>\n",
              "      <td>영화 도굴 스틸컷 ◆도굴 감독: 박정배 출연: 이제훈 조우진 신혜선 도굴을 소재로 ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>140564 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        title                                           contents\n",
              "0                  문 대통령 변창흠 국토장관 사의표명 사실상 수용  정만호 국민소통수석이 12일 오후 청와대 춘추관 대브리핑룸에서 변창흠 국토부 장관 ...\n",
              "1                              계급장 수여하는 문 대통령  (아산뉴스1) 이광호 기자 문재인 대통령이 12일 오후 충남 아산시 경찰대학에서 열...\n",
              "2                              계급장 수여하는 문 대통령  (아산뉴스1) 이광호 기자 문재인 대통령이 12일 오후 충남 아산시 경찰대학에서 열...\n",
              "3                           수상자 메달 걸어주는 문 대통령  (아산뉴스1) 이광호 기자 문재인 대통령이 12일 오후 충남 아산시 경찰대학에서 열...\n",
              "4                          정몽구 서울아산병원에 50억 쾌척  인재 양성·소외 계층 지원 등 계획 “부친 질병·가난 악순환 끊기 원해 국내 최고 ...\n",
              "...                                       ...                                                ...\n",
              "140559  [건축과도시] 북한산을 캔버스 삼아. 미술관 또 하나의 작품이 되다  <은평구 진관동 사비나 미술관> 서울시 은평구 진관동에 자리잡은 사비나미술관. 삼각...\n",
              "140560            조선후기 문인 김조순 별장 그린 옥호정도 첫 공개  국립중앙박물관 서화실 개편해 32점 새롭게 전시 옥호정도[국립중앙박물관 제공연합뉴스...\n",
              "140561                  안성 청룡사 대웅전에서 목재 곡자 발견  문화재청(청장 정재숙)의 국고보조와 기술지도로 안성시에서 시행하고 있는 안성 청룡사...\n",
              "140562          156년전 ㄱ자 곡자 찾았다 안성 청룡사 기둥 밑에서  안성 청룡사 대웅전에서 발견된 곡자 【서울뉴시스】 이수지 기자 안성 청룡사 대웅전에...\n",
              "140563         [김중기의 필름통] 새 영화 도굴 나인스 게이트 앙상블  영화 도굴 스틸컷 ◆도굴 감독: 박정배 출연: 이제훈 조우진 신혜선 도굴을 소재로 ...\n",
              "\n",
              "[140564 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9ouEN_yp4uG",
        "outputId": "ac5c2610-5c95-4f45-8c10-3cc853d03356"
      },
      "source": [
        "df = df.dropna(axis=0)\n",
        "df['contents'].count()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "140536"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "-e484q4qqA74",
        "outputId": "0738f444-96b8-4b30-d106-bbbcadecd97c"
      },
      "source": [
        "import re\n",
        "import sys\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# 간단한 전처리\n",
        "def clean_text(txt):\n",
        "    txt = txt.replace('\\n',' ')\n",
        "    txt = txt.replace('\\r',' ')    \n",
        "    txt = txt.replace('=','')\n",
        "    txt = txt.replace('\\\"','')   \n",
        "    txt = txt.replace('\\'','')\n",
        "    txt = txt.replace('”','')\n",
        "    txt = txt.replace('“','')\n",
        "    txt = txt.replace('’','')\n",
        "    #txt = txt.replace(',','')\n",
        "    txt = txt.replace('..','')\n",
        "    txt = txt.replace('...','')\n",
        "    #txt = txt.replace('.','. ')\n",
        "    txt = txt.replace('.','. ')\n",
        "    txt = txt.replace('  ',' ')\n",
        "    txt = txt.replace('  ',' ')    \n",
        "    txt = txt.replace('  ',' ')   \n",
        "    txt = txt.replace('  ',' ')           \n",
        "    txt = txt.replace('  ',' ')\n",
        "    txt = txt.replace('  ',' ')    \n",
        "    txt = txt.replace('  ',' ')   \n",
        "    txt = txt.replace('  ',' ')             \n",
        "    return txt.strip()\n",
        "    \n",
        "# 검사...\n",
        "pattens = [\"[34569][0-9]{3}[\\;.\\;-\\; ][0-9]{4}[\\;.\\;-\\; ][0-9]{4}[\\;.\\;-\\; ][0-9]{4}\",\n",
        "           \"[0-9]{2,3}[\\:\\s\\;.\\;,\\;-;)][0-9]{3,4}[\\:\\s\\;.\\;,\\;-][0-9]{4}\",\n",
        "           \"[0-9]{1}[0-9]{1}[\\W]?[0-1]{1}[0-9]{1}[\\W]?[0-3]{1}[\\W]?[0-9]{1}[\\W]?[1-4]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}\",\n",
        "           \"[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{6}|[0-9]{3}[\\:\\s\\;.\\;,\\;-]([0-9]{5,6}[\\:\\s\\;.\\;,\\;-][0-9]{3}|[0-9]{6}[\\:\\s\\;.\\;,\\;-][0-9]{5}|[0-9]{2,3}[\\:\\s\\;.\\;,\\;-][0-9]{6}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{7}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{4,6}[\\:\\s\\;.\\;,\\;-][0-9]|[0-9]{5}[\\:\\s\\;.\\;,\\;-][0-9]{3}[\\:\\s\\;.\\;,\\;-][0-9]{2}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{5}[\\:\\s\\;.\\;,\\;-][0-9]{3}|[0-9]{4}[\\:\\s\\;.\\;,\\;-][0-9]{4}[\\:\\s\\;.\\;,\\;-][0-9]{3}|[0-9]{6}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{3}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{7})|[0-9]{4}[\\:\\s\\;.\\;,\\;-]([0-9]{3}[\\:\\s\\;.\\;,\\;-][0-9]{6}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{6}[\\:\\s\\;.\\;,\\;-][0-9])|[0-9]{5}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{6}|[0-9]{6}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{5,6}\"\n",
        "           ]\n",
        "\n",
        "filters = []\n",
        "for p in pattens:\n",
        "    filters.append(re.compile(p))\n",
        "\n",
        "sentences = []\n",
        "df = df.dropna(axis=0)\n",
        "cnt = df['contents'].count()\n",
        "#print('Total row count:',cnt)\n",
        "i=0\n",
        "for raw_text in df['contents']:\n",
        "    i=i+1\n",
        "    try:\n",
        "        if i%100 == 0:\n",
        "            percent = (\"{0:.2f}\").format(100 * (i / float(cnt)))\n",
        "            print(f'\\r {percent}% {i}/{str(cnt)}', end=\"\", flush=True)\n",
        "\n",
        "        docs = nltk.sent_tokenize(clean_text(raw_text))\n",
        "        for txt in docs:\n",
        "            if txt.find('▶') > -1 or txt.find('@') > -1 or txt.find('ⓒ') > -1: \n",
        "                pass\n",
        "            else:\n",
        "                txt = txt.strip()\n",
        "                if any(chr.isdigit() for chr in txt) :\n",
        "                    pass\n",
        "                else:\n",
        "                    sentences.append(txt)\n",
        "    except KeyboardInterrupt as ki:\n",
        "        raise ki        \n",
        "    except:\n",
        "        pass #print(\"Unexpected error:\", sys.exc_info()[0])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            " 23.84% 33500/140536"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-b09b8dcf4c1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m                     \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mki\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mki\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mpass\u001b[0m \u001b[0;31m#print(\"Unexpected error:\", sys.exc_info()[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-b09b8dcf4c1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\r {percent}% {i}/{str(cnt)}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtxt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'▶'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'@'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ⓒ'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[1;32m     94\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m# Standard word tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1235\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m         \"\"\"\n\u001b[0;32m-> 1237\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \"\"\"\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \"\"\"\n\u001b[1;32m   1315\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \"\"\"\n\u001b[1;32m    311\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1291\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_break\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'next_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtext_contains_sentbreak\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1335\u001b[0m         \"\"\"\n\u001b[1;32m   1336\u001b[0m         \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;31m# used to ignore last token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1337\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_annotate_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1338\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_annotate_second_pass\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         \u001b[0mheuristic\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4.1\u001b[0m\u001b[0;36m.2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfrequent\u001b[0m \u001b[0msentence\u001b[0m \u001b[0mstarter\u001b[0m \u001b[0mheuristic\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4.1\u001b[0m\u001b[0;36m.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1471\u001b[0m         \"\"\"\n\u001b[0;32m-> 1472\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1473\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_second_pass_annotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \"\"\"\n\u001b[1;32m    311\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_annotate_first_pass\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    579\u001b[0m           \u001b[0;34m-\u001b[0m \u001b[0mellipsis_toks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mindices\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mellipsis\u001b[0m \u001b[0mmarks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \"\"\"\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0maug_tok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_pass_annotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_tok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0maug_tok\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_tokenize_words\u001b[0;34m(self, plaintext)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m                 yield self._Token(next(line_toks),\n\u001b[0;32m--> 551\u001b[0;31m                         parastart=parastart, linestart=True)\n\u001b[0m\u001b[1;32m    552\u001b[0m                 \u001b[0mparastart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tok, **params)\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0m__slots__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'tok'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'type'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'period_final'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pM9ZrtFcqEn0"
      },
      "source": [
        "import re\n",
        "import sys\n",
        "import io\n",
        "\n",
        "#텍스트 정제(전처리)\n",
        "def cleanText(readData):\n",
        "    #텍스트에 포함되어 있는 특수 문자 제거\n",
        "    text = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》◆◇●🎧○▲\\t―△━▷]', '', readData)\n",
        "    return text.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piBg9aJkqHH-"
      },
      "source": [
        "c_sentences = []\n",
        "for sentence in sentences:\n",
        "    s = cleanText(sentence)\n",
        "    c = len(s.split())\n",
        "    if c >= 3 and c < 10 and s.find('재배포') < 0 and s.find('기자') < 0  and s.find('유투브') < 0 and s.find('www') < 0 and s.find('com') < 0 and s.find('접속하기') < 0 and s.find('http') < 0 and s.find('뉴스') < 0 and s.find('일보') < 0 :\n",
        "        if s.endswith(('다','요')):\n",
        "            c_sentences.append(s.strip())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ljb_u8pcqJxe"
      },
      "source": [
        "len(c_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TscPqOMtqKdB"
      },
      "source": [
        "c_sentences[:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51dbd376qMyB"
      },
      "source": [
        "ko_sentences_dataset = c_sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk93tR2Nuk8t"
      },
      "source": [
        "# 문법 discriminator의 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7Zf2oRMMXmH",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1489192-f5e8-430f-d34c-04adc970596b"
      },
      "source": [
        "use_pretrained_model = True\n",
        "\n",
        "if use_pretrained_model:\n",
        "    #g_discriminator = Grammar_Discriminator(input_dir = '/content/drive/MyDrive/Colab Notebooks/summary/model_save')\n",
        "    g_discriminator = Grammar_Discriminator(input_dir = '/content/drive/MyDrive/Colab Notebooks/summary/ko_grammar_model3')\n",
        "else:\n",
        "    sentences,labels = collect_training_dataset_for_grammar_discriminator(ko_sentences_dataset)\n",
        "    print(len(sentences))\n",
        "    g_discriminator = Grammar_Discriminator()\n",
        "    g_discriminator.set_dataset(sentences,labels)\n",
        "    g_discriminator.train(epochs=1)\n",
        "    g_discriminator.save_model(output_dir='/content/drive/MyDrive/Colab Notebooks/summary/ko_grammar_model3')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKRx8zA0uY-B"
      },
      "source": [
        "if False: ## 추가적인 fine-tuning\n",
        "    g_discriminator.train(epochs=10)\n",
        "    g_discriminator.save_model(output_dir='/content/drive/MyDrive/Colab Notebooks/summary/ko_grammar_model3')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-kyzdkT-G2m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab2ac494-cea5-4d1d-cfe0-c0db07a9be2a"
      },
      "source": [
        "txt = ['최근 날씨가 포근해지면서 산을 찾는 사람들도 늘고 있는데요','서비스를 음원 플랫폼 스포티파이가 국내 론칭한다']\n",
        "g_discriminator.discriminator.to(device)\n",
        "g_discriminator.transfer_learning(txt)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(6.3284, device='cuda:0', grad_fn=<NllLossBackward>),\n",
              " tensor([[-6.2094,  6.2351],\n",
              "         [ 6.4035, -6.2532]], device='cuda:0', grad_fn=<AddmmBackward>))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d96kaCAHKuUc"
      },
      "source": [
        "##4.3 Static similarity discriminator class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZDpXe7XKxeg",
        "trusted": true
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import BertTokenizer\n",
        "from scipy.signal import find_peaks\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.misc import electrocardiogram\n",
        "import scipy\n",
        "\n",
        "\n",
        "class Similarity_Discriminator:\n",
        "    '''\n",
        "    _instance = None\n",
        "    _embedder = None\n",
        "    def __new__(cls,pre_trained_model_name='stsb-roberta-large'):\n",
        "        if cls._instance is None:\n",
        "            print('Creating Similarity_Discriminator object')\n",
        "            cls._instance = super(Similarity_Discriminator, cls).__new__(cls)\n",
        "            # Put any initialization here.\n",
        "            cls._embedder = SentenceTransformer(pre_trained_model_name)\n",
        "        return cls._instance\n",
        "\n",
        "    '''\n",
        "\n",
        "    def __init__(self,pre_trained_model_name='xlm-r-large-en-ko-nli-ststb'):\n",
        "        print('Creating Similarity_Discriminator object')\n",
        "        # Put any initialization here.\n",
        "        self._embedder = SentenceTransformer(pre_trained_model_name)  \n",
        "        #self.cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "    def encode(self,texts):\n",
        "        return self._embedder.encode(texts,show_progress_bar=False)\n",
        "\n",
        "    def similarity(self, query_text, org_text_emb):\n",
        "        queries = nltk.sent_tokenize(query_text)\n",
        "        query_embeddings = self._embedder.encode(queries,show_progress_bar=False)\n",
        "        #query_embeddings = self._embedder.encode(queries,show_progress_bar=False)\n",
        "        #print(queries)\n",
        "        #print(org_text_emb)\n",
        "        \n",
        "        if len(query_embeddings) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        cos_scores = scipy.spatial.distance.cdist(query_embeddings, org_text_emb, \"cosine\")\n",
        "        similarity_score = 1.0 - np.mean(np.min(cos_scores,axis=0))\n",
        "        '''\n",
        "        for query, query_embedding in zip(queries, query_embeddings):\n",
        "            distances = scipy.spatial.distance.cdist([query_embedding], [org_text_emb], \"cosine\")[0]\n",
        "            results = zip(range(len(distances)), distances)\n",
        "            for idx, distance in results:\n",
        "                scores.append(1-distance)\n",
        "        '''\n",
        "        return similarity_score  \n",
        " "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sQZ36GuMumP"
      },
      "source": [
        "###4.3.1 한국어 문장 유사도 pre-trained model 적용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9Miao14Muww",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "454e494d-a212-48f1-e3c4-d7688a6c41c0"
      },
      "source": [
        "s_discriminator = Similarity_Discriminator()\n",
        "#s_discriminator = Similarity_Discriminator()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating Similarity_Discriminator object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xnk9GsQ0K1t1"
      },
      "source": [
        "# 4.4 Document source class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhvXMrSO-CiD"
      },
      "source": [
        "## 두 문장을 합치는 rule 변환기... --> 이거... ML로 나중에 바꿔야..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_W1Wqq26MjQ"
      },
      "source": [
        "combine_matching_table = {}\n",
        "\n",
        "combine_matching_table['어요.'] = '고'\n",
        "combine_matching_table['지요.'] = '고'\n",
        "combine_matching_table['답니다.'] = '고'\n",
        "combine_matching_table['보거라.'] = '봐,'\n",
        "combine_matching_table['간단다.'] = '가니,'\n",
        "combine_matching_table['돼.'] = '되,'\n",
        "combine_matching_table['해.'] = '하며,'\n",
        "combine_matching_table['다.'] = '고'\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giRiIsfR6DV6"
      },
      "source": [
        "def combine_sentence(txt):\n",
        "    for c in combine_matching_table.keys():\n",
        "        if txt.endswith(c):\n",
        "            txt = txt.replace(c,combine_matching_table[c])\n",
        "    return txt"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwhMHwwefy-N"
      },
      "source": [
        "\n",
        "conjunction_table = ['그러던','그래서','그러나','그런데','그리고','그랬더니','그러니까','하지만','그래서']"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnBm6RCvNIWG"
      },
      "source": [
        "## 4.4.2 source class 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsJKbtc2K4xN",
        "trusted": true
      },
      "source": [
        "\n",
        "\n",
        "class Source:\n",
        "\n",
        "    def __init__(self,org_text):\n",
        "        self.org_text = org_text\n",
        "\n",
        "    def __crean_text(self, txt):\n",
        "        txt = txt.replace('\\n',' ')\n",
        "        txt = txt.replace('\\r',' ')    \n",
        "        txt = txt.replace('=','')\n",
        "        txt = txt.replace('\\\"','')   \n",
        "        txt = txt.replace('\\'','')\n",
        "        txt = txt.replace(',','')\n",
        "        txt = txt.replace('..','')\n",
        "        txt = txt.replace('...','')\n",
        "        txt = txt.replace(' .','.')\n",
        "        txt = txt.replace('.','. ')\n",
        "        txt = txt.replace('  ',' ')\n",
        "        txt = txt.replace('  ',' ')    \n",
        "        txt = txt.replace('  ',' ')   \n",
        "        txt = txt.replace('  ',' ')           \n",
        "        txt = txt.replace('  ',' ')\n",
        "        txt = txt.replace('  ',' ')    \n",
        "        txt = txt.replace('  ',' ')   \n",
        "        txt = txt.replace('  ',' ')           \n",
        "        return txt.strip()\n",
        "\n",
        "    def set_key_rate(self,s_discriminator):\n",
        "        #self.full_text = self.__crean_text(self.full_text.strip())\n",
        "        self.org_text = self.__crean_text(self.org_text.strip())\n",
        "        print('-'*50)\n",
        "        print(self.org_text)\n",
        "        print('-'*50)\n",
        "        self.org_sentences = np.array(nltk.sent_tokenize(self.org_text))\n",
        "        for i,sents in enumerate(self.org_sentences):\n",
        "            for cj in conjunction_table: \n",
        "                if sents.startswith(cj):\n",
        "                    self.org_sentences[i] = sents[len(cj):].strip()\n",
        "\n",
        "        #self.full_sentences = np.array(nltk.sent_tokenize(self.full_text))\n",
        "        \n",
        "        #self.org_term_set = (' ' + self.org_text + ' ').split(' ')\n",
        "        self.org_term_set = (' '.join(self.org_sentences)).strip().split(' ')\n",
        "        self.org_source_length = len(self.org_term_set)\n",
        "        self.term_table = {}\n",
        "        self.seps = []\n",
        "        #morp_table = {}\n",
        "\n",
        "        for index, word in zip(range(len(self.org_term_set)),self.org_term_set):\n",
        "            self.term_table[index] = word\n",
        "            if word.endswith(('.','?')):\n",
        "                self.seps.append(index)\n",
        "                if self.org_source_length - 1 == index:\n",
        "                    pass\n",
        "                else:\n",
        "                    self.term_table[index] = combine_sentence(word)\n",
        "\n",
        "        #print(self.term_table.values())\n",
        "        #print('------------------------------------------------------------------')\n",
        "\n",
        "        self.s_discriminator = s_discriminator\n",
        "        # 원문의 embedding...\n",
        "        self.org_text_emb = self.s_discriminator.encode(self.org_sentences)\n",
        "        #self.full_text_emb = self.s_discriminator.encode(self.full_sentences)\n",
        "        #top_n = int(len(self.term_table) * comp_rate)\n",
        "        #print('top_n',top_n)\n",
        "        #self.story_peaks = [i+1 for i in range(top_n)]\n",
        "\n",
        "    def get_org_sample(self, num):\n",
        "        return self.org_sentences[np.random.choice(len(self.org_sentences), num)]\n",
        "\n",
        "    def get_source_embedded_code(self):\n",
        "        return self.org_text_emb\n",
        "\n",
        "    def get_random_text(self,rate=0.7):\n",
        "        cnt = int(len(self.term_table) * rate)\n",
        "        a = list(self.term_table.keys())\n",
        "        b = np.random.choice(a, cnt)\n",
        "        c = [fruit for fruit in a if fruit not in b]\n",
        "        txt = []\n",
        "        for i in c:\n",
        "            txt.append(self.term_table[i])\n",
        "        return ' '.join(txt).strip(), hash(tuple(b))"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UAeFBYMMxKY",
        "outputId": "c9265fe4-292f-4a28-9323-c53d72157fee"
      },
      "source": [
        "txt = \"\"\"\n",
        "황금마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼어요. \n",
        "그러니까 반드시 밤 열두시가 되기 전에 돌아와야 해요.\n",
        "\"\"\"\n",
        "s = Source(txt)\n",
        "s.set_key_rate(s_discriminator)\n",
        "s.get_random_text()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "황금마차는 호박으로 흰말은 생쥐로 마부는 도마뱀으로 변하게 돼어요. 그러니까 반드시 밤 열두시가 되기 전에 돌아와야 해요.\n",
            "--------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('황금마차는 흰말은 도마뱀으로 변하게 반드시 밤 되기 전에 돌아와야', 8003332976094919178)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XY59mdNK8ub"
      },
      "source": [
        "# 4.5 Generator class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5CLF3WcK6lp",
        "trusted": true
      },
      "source": [
        "from functools import reduce\n",
        "\n",
        "\n",
        "# custom weights initialization called on netG and netD\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Linear') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.06)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.05)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "        Simple Generator w/ MLP\n",
        "    \"\"\"\n",
        "    '''\n",
        "    def __init__(self, input_size=1024):\n",
        "        super(Generator, self).__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(input_size, input_size*2),\n",
        "            nn.BatchNorm1d(input_size*2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(input_size*2, input_size*3),\n",
        "            nn.BatchNorm1d(input_size*3),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(input_size*3, input_size*3),\n",
        "            nn.BatchNorm1d(input_size*3),\n",
        "            nn.LeakyReLU(0.2),            \n",
        "            nn.Linear(input_size*3, input_size*2),\n",
        "            nn.BatchNorm1d(input_size*2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(input_size*2, input_size),\n",
        "            #nn.BatchNorm1d(term_length*4),\n",
        "            nn.Tanh() # -1 ~ 1\n",
        "        )\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, input_size=1024):\n",
        "        super(Generator, self).__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(input_size, input_size*4),\n",
        "            nn.BatchNorm1d(input_size*4),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(input_size*4, input_size),\n",
        "            #nn.BatchNorm1d(term_length*4),\n",
        "            nn.Tanh() # -1 ~ 1\n",
        "        )\n",
        "\n",
        "    def forward(self, x, bias):\n",
        "        #biased_noise = torch.randn(N,_NOISE_DIM)\n",
        "        # stroy peak에 해당하는 term에게 평균값에 해당하는 bias를 추가 한다.\n",
        "                 \n",
        "        y_ = self.layer(x)\n",
        "        y = torch.add(y_,bias)\n",
        "        #y = nn.Sigmoid()(y)\n",
        "\n",
        "        return y, y_\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co1MnRG8a4Vq"
      },
      "source": [
        "# multi-discriminator에 대한 Adaptive discriminant factor 를 구하기 위한 학습\n",
        "\n",
        "ref : https://realpython.com/python-ai-neural-network/\n",
        "\n",
        "colab 수식입력 : \n",
        "\n",
        "https://wikidocs.net/1679\n",
        "\n",
        "https://en.wikipedia.org/wiki/Help:Displaying_a_formula#Formatting_using_TeX\n",
        "\n",
        "Original GAN의 목적함수\n",
        "$$ \\min_{G}\\max_{D} V(D,G) = E_{x\\sim p_{data}(x)}[logD(x)] + E_{z\\sim p_{z}(z)}[log(1-D(G(z)))] $$\n",
        "\n",
        "Multi-Discriminator GAN은 discriminator가 각 목적에 의하여 여러개 (N개) 있다.\n",
        "MDGAN의 목적함수\n",
        "\n",
        "$$ \\min_{G}\\max_{D_{i\\sim N}} V(D_{i\\sim N},G) = \\sum_{i=1}^N \\{E_{x\\sim p_{data}(x)}[logD_i(x)] + E_{z\\sim p_{z}(z)}[log(1-D_i(G(z)))]\\} $$\n",
        "\n",
        "여기서\n",
        "\n",
        "$$ L(D_i,G) =  E_{x\\sim p_{data}(x)}[logD_i(x)] + E_{z\\sim p_{z}(z)}[log(1-D_i(G(z)))] $$\n",
        "\n",
        "이라하고 단순화 하면\n",
        "\n",
        "$$ \\min_{G}\\max_{D_{i\\sim N}} V(D_{i\\sim N},G) = \\sum_{i=1}^N L(D_i,G) $$\n",
        "\n",
        "와 같이 된다.\n",
        "\n",
        "문제점은 GAN의 특성상, 특정 Discriminator가 학습에 있어 지배적으로 loss 함수에 영향을 미치게 되어 각각의 Discriminator가 골고루 학습에 참여하지 못하고 의도하지 않은 결과를 만들게 된다. 이러한 문제점을 극복하기 위해 다음의 두가지 제안을 한다.\n",
        "\n",
        "1) 목적함수에 각 Loss 에 대한 표준편차 (standard-deviation) 를 반영하여 각 Discriminator에 대한 Loss가 상호 유사한 수준을 유지하면 학습이 진행되도록 한다.\n",
        "\n",
        "$$ \\min_{G}\\max_{D_{i\\sim N}} V(D_{i\\sim N},G) = \\sum_{i=1}^N L(D_i,G) + STD_{i \\sim N}(L(D_i,G))$$\n",
        "\n",
        "2) 1)의 제안에 추가하여, 각 discriminator에 의한 loss를 제어하기 위해, adaptive discriminant factor (ADF) 를 적용하고, 학습의 진행 과정에서 이를 최적화 한다. \n",
        "\n",
        "$$ \\min_{G}\\max_{D_{i\\sim N}} V(D_{i\\sim N},G) = \\sum_{i=1}^N f_iL(D_i,G) + STD_{i \\sim N}(L(D_i,G))$$\n",
        "\n",
        "여서서 \n",
        "\n",
        "fi = adaptive discriminant factor for discriminator i \n",
        "\n",
        "중요한 것은, 학습과정에서 Li을 작게 (학습의 방향)하기 위해서는 fi는 역으로 커져야 한다. 그래야, 전체 Loss function에서 비중이 증대되어 더 적극적인 학습이 이루어 지게 된다. 따라서, fi의 최적화 방향은 기존의 gradient decent와 반대 방향이 되어야 한다.\n",
        "\n",
        "$$ f_i^{t+1} = f_i^t + \\alpha \\frac{\\partial V}{\\partial f_i}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLVVmQdxLBHZ"
      },
      "source": [
        "##4.6 Summarizer class (GAN training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0RQOPpQgUTE"
      },
      "source": [
        "# 학습기..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPOlFSlUXEvt",
        "outputId": "275ce0cd-4eb2-409a-ab7d-ff96abcc244e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "a = torch.tensor([1,2,3,4])\n",
        "a = a + 1\n",
        "a"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2, 3, 4, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nd8GTS7HKz1H",
        "trusted": true
      },
      "source": [
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "from scipy.special import expit\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "class SAM_Summarizer:\n",
        "\n",
        "    def __init__(self,g_discriminator,s_discriminator):\n",
        "        self.g_discriminator = g_discriminator\n",
        "        #self.c_discriminator = c_discriminator\n",
        "        self.s_discriminator = s_discriminator\n",
        "        self.m = nn.Sigmoid()\n",
        "\n",
        "    def ready(self,source):\n",
        "        self.source = source  \n",
        "        #self.source.analysis_frame_terms(self.s_discriminator)\n",
        "        self.generator = Generator(input_size=self.source.org_source_length)\n",
        "        self.generator.apply(weights_init)\n",
        "        return self\n",
        "\n",
        "    def summarize(self,epochs=10,batch_size=1,learning_rate=2e-4, display = False):\n",
        "        history = self.__train(epochs,batch_size,learning_rate,display)\n",
        "        if display:\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            plt.plot(history['gen_g_loss'],label='grammar loss')\n",
        "            plt.plot(history['gen_l_loss'],label='compression loss')\n",
        "            plt.plot(history['gen_s_loss'],label='similarity loss')\n",
        "\n",
        "            plt.plot(history['total loss'],label='total loss')\n",
        "            plt.plot(history['losses std'],label='standard deviation of losses')\n",
        "            \n",
        "            #if 'dis_loss' in history:\n",
        "            #    plt.plot(history['dis_loss'],label='discriminator grammar loss')\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "        return history\n",
        "\n",
        "    # text의 생성 for torch\n",
        "    def __text_gen2(self, p_txt, gen_length):\n",
        "        gtext = []\n",
        "        sorted_noise, i = torch.sort(p_txt, descending=True)\n",
        "        order, i = torch.sort(i[:gen_length], descending=False)\n",
        "        #print(len(order))\n",
        "        #print(gen_length)\n",
        "        assert len(order) == gen_length\n",
        "        order = order.cpu().detach().numpy()\n",
        "        for k in order:\n",
        "            gtext.append((self.source.term_table[k],k))\n",
        "        return gtext\n",
        "\n",
        "    def __text_gen3(self, p_txt):\n",
        "        gtext = []\n",
        "\n",
        "        for order,p in enumerate(p_txt):\n",
        "            if p > 0.0:\n",
        "                gtext.append(self.source.term_table[order])\n",
        "        return gtext\n",
        "\n",
        "    def __text_gen4(self, p_txt):\n",
        "        gtext = \"\"\n",
        "        indexs = []\n",
        "        for order,p in enumerate(p_txt):\n",
        "            if p > 0.0:\n",
        "                gtext += self.source.term_table[order] + ' '\n",
        "                indexs.append(order)\n",
        "        return gtext.strip(),indexs\n",
        "\n",
        "\n",
        "    def __discrete_gradient(self,weights,use_gpu=False, verbose=0):\n",
        "        fake_gen_out = torch.zeros(weights.shape).to(device)\n",
        "        #fake_com_out = torch.zeros(weights.shape).to(device)\n",
        "        fake_sim_out = torch.zeros(weights.shape).to(device)\n",
        "        fake_len_out = torch.zeros(weights.shape).to(device)\n",
        "\n",
        "        #real_text = self.source.get_org_sample(weights.shape[0])\n",
        "        fake_outs = []\n",
        "        #real_outs = []\n",
        "        apply_order = []\n",
        "        for i, noise in enumerate(weights):\n",
        "            #gtext = self.__text_gen2(noise,gen_length)\n",
        "            gtext,tk = self.__text_gen4(noise)\n",
        "            fake_outs.append(gtext)\n",
        "            apply_order.append((i,tk))\n",
        "            '''\n",
        "            tw = \"\"\n",
        "            tk = []\n",
        "            fake_scores = []\n",
        "            for (w,k) in gtext:\n",
        "                tw += w + ' '\n",
        "                tk.append(k)\n",
        "                if w.endswith('.'):\n",
        "                    fake_outs.append(tw.strip())\n",
        "                    real_outs.append(real_text[i])\n",
        "                    apply_order.append((i,tk))\n",
        "                    tw = \"\"\n",
        "                    tk = []\n",
        "                    \n",
        "            if len(tk) > 0:\n",
        "                fake_outs.append(tw.strip())\n",
        "                real_outs.append(real_text[i])\n",
        "                apply_order.append((i,tk))\n",
        "            '''\n",
        "        D_z_loss, fake_gmr_out=self.g_discriminator.transfer_learning(fake_outs,train_for = False)\n",
        "        #D_c_loss, fake_cpt_out=self.c_discriminator.transfer_learning(fake_outs,train_for = False)\n",
        "        #D_x_loss, real_gmr_out=self.g_discriminator.transfer_learning(real_outs,train_for = True)   # not use of 'real_gmr_out'\n",
        "\n",
        "        '''\n",
        "        f_sim_out = []\n",
        "        for fake_text in fake_outs:\n",
        "            f_sim_out.append(self.s_discriminator.similarity(fake_text,self.source.full_text_emb))\n",
        "        '''\n",
        "        o_sim_out = []\n",
        "        o_len_out = []\n",
        "        for fake_text in fake_outs:\n",
        "            o_sim_out.append(self.s_discriminator.similarity(fake_text,self.source.org_text_emb))\n",
        "            o_len_out.append(1 - len(fake_text.split(' '))/self.source.org_source_length)\n",
        "\n",
        "\n",
        "        #if use_gpu:\n",
        "        #    apply_order = torch.FloatTensor(apply_order).to(device)  \n",
        "        \n",
        "        #print(fake_dis_out)\n",
        "        \n",
        "        for j, (i,tk) in enumerate(apply_order):\n",
        "            #fake_gen_out[i,tk] += fake_dis_out[j].numpy() --> 이거는 tf 용...\n",
        "            #fake_gen_out[i,tk] += fake_dis_out[j] #.cpu().detach().numpy()\n",
        "            # \n",
        "            try:\n",
        "                #print('fake_gmr_out:',fake_gmr_out[j,1])\n",
        "                #print('real_gmr_out:',real_gmr_out[j,1])\n",
        "                #fake_gen_out[i,tk] += torch.sigmoid(fake_gmr_out[j,1])\n",
        "\n",
        "                fake_gen_out[i,tk] += torch.tanh( fake_gmr_out[j,1])\n",
        "                #fake_com_out[i,tk] += torch.tanh( fake_cpt_out[j,1])\n",
        "                #fake_sim_out[i,tk] += f_sim_out[j] + o_sim_out[j]\n",
        "                fake_sim_out[i,tk] += o_sim_out[j]\n",
        "                fake_len_out[i,tk] += o_len_out[j] #torch.tensor(fake_text_len/self.source.org_source_length).to(device)\n",
        "            except Exception as ex:\n",
        "                print(j,i,tk)\n",
        "                print(fake_gmr_out)\n",
        "                raise ex\n",
        "\n",
        "        return fake_gen_out, fake_sim_out, fake_len_out #fake_com_out, fake_sim_out #, D_z_loss, D_x_loss\n",
        "\n",
        "\n",
        "    def __train(self, epochs=10,batch_size=10,learning_rate=2e-4,display = False):\n",
        "        # In the Deepmind paper they use RMSProp however then Adam optimizer\n",
        "        # improves training time\n",
        "        #generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "        # This method returns a helper function to compute cross entropy loss\n",
        "        #cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "        # Set the seed value all over the place to make this reproducible.\n",
        "        seed_val = int(random.random()*100)\n",
        "\n",
        "        random.seed(seed_val)\n",
        "        np.random.seed(seed_val)\n",
        "        torch.manual_seed(seed_val)\n",
        "        torch.cuda.manual_seed_all(seed_val)\n",
        "        \n",
        "        criterion = nn.BCELoss()\n",
        "        #D_opt = torch.optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "        G_opt = AdamW(self.generator.parameters(),\n",
        "                        lr = 2e-3, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                        eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                        )\n",
        "        # Create the learning rate scheduler.\n",
        "        scheduler = get_linear_schedule_with_warmup(G_opt, \n",
        "                                                    num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                                    num_training_steps = epochs)\n",
        "        \n",
        "        #gen_length = len(self.source.story_peaks) + int(len(self.source.story_peaks)*self.frame_expansion_ratio)\n",
        "        pb = ProgressBar(epochs,prefix='Train...')\n",
        "        gen_gmr_loss_history = []\n",
        "        gen_len_loss_history = []\n",
        "        gen_sim_loss_history = []\n",
        "        dis_loss_history = []    \n",
        "        total_loss_history = []\n",
        "        losses_std_history = []\n",
        "\n",
        "        #model 들은 cuda로 보낸다.\n",
        "        self.g_discriminator.discriminator.to(device)\n",
        "        self.g_discriminator.discriminator.eval() # 학습하지 않는다...\n",
        "        #self.c_discriminator.discriminator.to(device)\n",
        "        #self.c_discriminator.discriminator.eval() # 학습하지 않는다...\n",
        "\n",
        "        self.generator.to(device)       \n",
        "        self.generator.train()\n",
        "\n",
        "        #self.bias_w = init_bias\n",
        "        initial_bias = 0\n",
        "        G_s_loss = torch.tensor(0)\n",
        "        #G_c_loss = torch.tensor(0)\n",
        "        G_g_loss = torch.tensor(0)\n",
        "\n",
        "\n",
        "        dfs = torch.tensor([ 1.0, 1.0, 1.0], device=device, dtype=torch.float, requires_grad=True)\n",
        "\n",
        "        for i in range(epochs):\n",
        "            '''\n",
        "            noise = torch.randn(batch_size,self.source.org_source_length).to(device)\n",
        "            bias = torch.zeros_like(noise).to(device)\n",
        "            bias[:,self.source.story_peaks] += self.bias_w \n",
        "            with torch.no_grad():        \n",
        "                sw, sw0 = self.generator(noise,bias)\n",
        "\n",
        "            self.g_discriminator.discriminator.train()          #discriminator는 evaluation 모드로 전환\n",
        "            fake_gmr_out, fake_sim_out, D_z_loss, D_x_loss = self.__discrete_gradient(sw,gen_length)\n",
        "            \n",
        "            D_loss = D_x_loss + D_z_loss      \n",
        "\n",
        "            self.g_discriminator.discriminator.zero_grad()\n",
        "            D_loss.backward()\n",
        "            D1_opt.step()\n",
        "            self.g_discriminator.discriminator.eval()\n",
        "            seps\n",
        "            '''\n",
        "            if True:\n",
        "                noise = torch.randn(batch_size,self.source.org_source_length).to(device) \n",
        "                bias = torch.zeros_like(noise).to(device)\n",
        "                #bias[:,self.source.seps] = -1.0 #+= self.bias_w  '~~~~.'에 해당하는 token은 제외 되도록 -1의 bias를 삽입\n",
        "                #bias[:,self.source.seps[len(self.source.seps)-1]] = 1.0 # 마지막  '~~~~.'에 해당하는 token은 반드시 포함 하도록 +1의  bias를 삽입\n",
        "\n",
        "                sw, sw0 = self.generator(noise,bias)\n",
        "                with torch.no_grad():                \n",
        "                    #fake_gmr_out, fake_com_out, fake_sim_out, D_z_loss, D_x_loss = self.__discrete_gradient(sw,gen_length,beta)\n",
        "                    fake_gmr_out, fake_sim_out, fake_len_out = self.__discrete_gradient(sw)\n",
        "                    #print(fake_gmr_out)\n",
        "                    #print(fake_sim_out)\n",
        "                    #print(fake_len_out)\n",
        "                    \n",
        "                '''\n",
        "                if int(i/10)%2 == 0:  # grammar와 similarity를 각각 한번씩 교대로 학습한다?\n",
        "                    sw1 = sw * fake_sim_out\n",
        "                    G_s_loss = -torch.mean(sw1)\n",
        "                    G_loss = G_s_loss    \n",
        "                else: #if i%2 == 1:\n",
        "                    sw1 = sw * fake_gmr_out\n",
        "                    G_g_loss = -torch.mean(sw1)\n",
        "                    G_loss = G_g_loss\n",
        "                '''\n",
        "\n",
        "                sw1 = sw * fake_sim_out\n",
        "                G_s_loss = -torch.mean(sw1) \n",
        "                sw2 = sw * fake_gmr_out\n",
        "                G_g_loss = -torch.mean(sw2) \n",
        "                sw3 = sw * fake_len_out\n",
        "                G_l_loss = -torch.mean(sw3)\n",
        "\n",
        "\n",
        "                #sw3 = sw * fake_com_out\n",
        "                #G_c_loss = -torch.mean(sw3)\n",
        "\n",
        "                dsc_loss = torch.stack([G_g_loss,G_s_loss,G_l_loss])\n",
        "                #dfs[1] = 1.0\n",
        "                #tdfs = torch.Tensor(dfs).to(device)\n",
        "\n",
        "                #G_loss = adf(dsc_loss) / torch.std(dsc_loss) # torch.dot(tdfs,dsc_loss)\n",
        "                #G_loss = torch.dot(tdfs,dsc_loss)\n",
        "\n",
        "                G_loss = torch.dot(dfs,dsc_loss) + torch.std(dsc_loss)*2\n",
        "\n",
        "                #print(dsc_loss)\n",
        "                #print(G_loss)\n",
        "                \n",
        "                #G_loss =  G_g_loss #+ G_c_loss + G_s_loss*1.5\n",
        "\n",
        "                ##Gs = dsc_loss.cpu().detach().numpy()\n",
        "                ##total_loss = np.dot(dfs,Gs)\n",
        "\n",
        "                self.generator.zero_grad()\n",
        "                G_loss.backward()\n",
        "                #print('backward:')\n",
        "                G_opt.step()\n",
        "                scheduler.step()\n",
        "                #self.generator.eval()\n",
        "\n",
        "                ########################################\n",
        "                \n",
        "                #d_objective = np.mean(Gs) - Gs \n",
        "                #dfs = dfs - d_objective\n",
        "                #dfs = [0.01 if i<0.0 else i for i in dfs]\n",
        "                \n",
        "                ########################################\n",
        "\n",
        "            #print('step:')\n",
        "            gen_gmr_loss_history.append(G_g_loss.cpu().detach().numpy())\n",
        "            #gen_com_loss_history.append(G_c_loss.cpu().detach().numpy())\n",
        "            gen_sim_loss_history.append(G_s_loss.cpu().detach().numpy())\n",
        "            #dis_loss_history.append(D_loss.cpu().detach().numpy())\n",
        "            gen_len_loss_history.append(G_l_loss.cpu().detach().numpy())\n",
        "\n",
        "            #pb.printProgress(+1,f'{i+1}/{epochs} epochs, beta:{dfs} Generator / grammar loss:{G_g_loss}  similarity loss:{G_s_loss}') #,   Discriminator grammar_loss:{D_loss}        ')\n",
        "            pb.printProgress(+1,'{}/{} epochs, grammar loss:{:.4f}  similarity loss:{:.4f} length loss:{:.4f}'.format(i+1,epochs,G_g_loss,G_s_loss,G_l_loss)) #,   Discriminator grammar_loss:{D_loss}        ')\n",
        "            \n",
        "            total_loss_history.append(torch.sum(dsc_loss).item())\n",
        "            losses_std_history.append(torch.std(dsc_loss).item())\n",
        "\n",
        "            if np.mean(gen_gmr_loss_history) > 0.1:\n",
        "                break\n",
        "            \n",
        "        self.generator.eval()\n",
        "        #self.g_discriminator.discriminator.eval()\n",
        "        if display:\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            xs = np.arange(self.source.org_source_length)\n",
        "            plt.bar(xs+0.0,sw0[0].cpu().detach().numpy(),label='before activation weights',width=0.2)\n",
        "            plt.bar(xs+0.2,sw[0].cpu().detach().numpy(),label='after activation weights',width=0.2)\n",
        "            plt.bar(xs+0.4,bias[0].cpu().detach().numpy(),label='bias weights',width=0.2)         \n",
        "            plt.legend()        \n",
        "            plt.show()\n",
        "\n",
        "        return  {'gen_g_loss':gen_gmr_loss_history,'gen_s_loss':gen_sim_loss_history,'gen_l_loss':gen_len_loss_history,'total loss':total_loss_history,'losses std':losses_std_history} #,'dis_loss':dis_loss_history }\n",
        "\n",
        "    def get_summary(self, count):\n",
        "        #texts = []\n",
        "        self.generator.cpu()\n",
        "        self.generator.eval()\n",
        "        #gen_length = len(self.source.story_peaks) + int(len(self.source.story_peaks)*self.frame_expansion_ratio)\n",
        "        noise = torch.randn(count,self.source.org_source_length)\n",
        "        bias = torch.zeros_like(noise)\n",
        "        #bias = torch.randn(1,self.source.org_source_length)\n",
        "        #bias[:,self.source.story_peaks] += self.bias_w #self.last_bias_max.cpu().detach().numpy()\n",
        "        #bias = 0\n",
        "        #bias[:,self.source.seps] = -1.0 #+= self.bias_w  '~~~~.'에 해당하는 token은 제외 되도록 -1의 bias를 삽입\n",
        "        #bias[:,self.source.seps[len(self.source.seps)-1]] = 1.0 # 마지막  '~~~~.'에 해당하는 token은 반드시 포함 하도록 +1의  bias를 삽입\n",
        "\n",
        "        with torch.no_grad():\n",
        "            sw,sw0 = self.generator(noise,bias)\n",
        "            #sw,sw0 = self.generator(noise)\n",
        "\n",
        "        max_score = 0\n",
        "        max_sim = 0\n",
        "        comp_rate = 0\n",
        "        best_text = \"\"\n",
        "\n",
        "        for p_txt in sw:\n",
        "            gtext = self.__text_gen3(p_txt)\n",
        "            text = ' '.join(gtext)\n",
        "            #print('>>',text)\n",
        "            sim_score = self.s_discriminator.similarity(text,self.source.org_text_emb)\n",
        "            if sim_score > max_sim:\n",
        "                best_text = text.strip()\n",
        "                loss, out=self.g_discriminator.transfer_learning([text],train_for = False)\n",
        "                max_score = out[0,1].item()\n",
        "                comp_rate = 1 - len(best_text.split(' '))/self.source.org_source_length\n",
        "                max_sim = sim_score\n",
        "            #texts.append([text.strip(),out,sim_score])\n",
        "        return best_text, max_score, max_sim, comp_rate\n",
        "\n",
        "    def get_samples(self,count):\n",
        "        self.generator.cpu()\n",
        "        self.generator.eval()\n",
        "        noise = torch.randn(count,self.source.org_source_length)\n",
        "        bias = torch.zeros_like(noise)\n",
        "        with torch.no_grad():\n",
        "            sw,sw0 = self.generator(noise,bias)\n",
        "        samples = []\n",
        "        max_g = 0\n",
        "        for p_txt in sw:\n",
        "            gtext = self.__text_gen3(p_txt)\n",
        "            text = ' '.join(gtext).strip()\n",
        "            #print(text)\n",
        "            loss, out=self.g_discriminator.transfer_learning([text],train_for = False)\n",
        "            sim_score = self.s_discriminator.similarity(text,self.source.org_text_emb)    \n",
        "            comp_rate = 1 - len(text.split(' '))/self.source.org_source_length\n",
        "            samples.append((text,out[0,1].item(),sim_score,comp_rate))\n",
        "            if max_g < out[0,1].item():\n",
        "                max_g = out[0,1].item()\n",
        "        return samples, max_g\n"
      ],
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCdfO9iuLH6D"
      },
      "source": [
        "#5. Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_eAwIPLb4aj"
      },
      "source": [
        "## 비교 대상 요약 알고리즘 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7Ty6_5gb_zR",
        "trusted": true
      },
      "source": [
        "\n",
        "def similarity(query_text, org_text):\n",
        "    sentences = nltk.sent_tokenize(org_text)\n",
        "    #print(\"Num sentences:\", len(sentences))\n",
        "    querys = nltk.sent_tokenize(query_text)\n",
        "    #print(\"Num querys:\", len(querys))\n",
        "\n",
        "    #Compute the sentence embeddings\n",
        "    org_embeddings = s_discriminator._embedder.encode(sentences,show_progress_bar=False)\n",
        "    query_embeddings = s_discriminator._embedder.encode(querys,show_progress_bar=False)\n",
        "\n",
        "    #Compute the pair-wise cosine similarities\n",
        "    cos_scores = scipy.spatial.distance.cdist(query_embeddings, org_embeddings, \"cosine\")\n",
        "    similarity_score = 1.0 - np.mean(np.min(cos_scores,axis=0))\n",
        "\n",
        "    return similarity_score\n",
        "\n",
        "def grammarity(text):\n",
        "    \n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    sentences = np.asarray(nltk.sent_tokenize(text))\n",
        "    # For every sentence...\n",
        "    for sent in sentences:\n",
        "        # `encode_plus` will:\n",
        "        #   (1) Tokenize the sentence.\n",
        "        #   (2) Prepend the `[CLS]` token to the start.\n",
        "        #   (3) Append the `[SEP]` token to the end.\n",
        "        #   (4) Map tokens to their IDs.\n",
        "        #   (5) Pad or truncate the sentence to `max_length`\n",
        "        #   (6) Create attention masks for [PAD] tokens.\n",
        "        encoded_dict = g_discriminator.tokenizer.encode_plus(\n",
        "                            sent,                      # Sentence to encode.\n",
        "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                            max_length = 64,           # Pad & truncate all sentences.\n",
        "                            pad_to_max_length = True,\n",
        "                            return_attention_mask = True,   # Construct attn. masks.\n",
        "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                            truncation = True,\n",
        "                       )\n",
        "        # Add the encoded sentence to the list.    \n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "        # And its attention mask (simply differentiates padding from non-padding).\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    # Convert the lists into tensors.\n",
        "    input_ids = torch.cat(input_ids, dim=0).to(device)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0).to(device)\n",
        "    g_discriminator.discriminator.to(device)\n",
        "    #if str(discriminator1.device) == 'cpu':\n",
        "    #    pass\n",
        "    #else:\n",
        "    #    input_ids = input_ids.to(device)\n",
        "    #    attention_masks = attention_masks.to(device)        \n",
        "\n",
        "    with torch.no_grad():        \n",
        "        outputs = g_discriminator.discriminator(input_ids, \n",
        "                               token_type_ids=None, \n",
        "                               attention_mask=attention_masks)\n",
        "    #return torch.sigmoid(outputs[0][:,1])\n",
        "    return torch.mean(outputs[0][:,1]).detach().cpu().numpy()\n",
        "    #return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLbWuwKXcMyk",
        "trusted": true
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def evaluate(method_name, text, g_summ, org_text_1,org_text_2,org_text_3):\n",
        "    result = {}\n",
        "    result['method'] = [method_name]\n",
        "    org_text = org_text_1 + ' ' + org_text_2 + ' ' + org_text_3\n",
        "    result['comp ratio'] = [len(text)/len(org_text)]\n",
        "    result['intro'] = [similarity(text,org_text_1)]\n",
        "    result['body'] = [similarity(text,org_text_2)]\n",
        "    result['ending'] = [similarity(text,org_text_3)]\n",
        "    result['var'] = [np.var([result['intro'][0],result['body'][0],result['ending'][0]])]\n",
        "    result['total'] = [similarity(text,org_text)]\n",
        "    result['grammar'] = [np.tanh(float(grammarity(text)))]\n",
        "    #scores = scorer.score(g_summ,text)\n",
        "    #result['R1'] = [scores['rouge1'].fmeasure]\n",
        "    #result['R2'] = [scores['rouge2'].fmeasure]\n",
        "    #result['RL'] = [scores['rougeL'].fmeasure]\n",
        "    return pd.DataFrame(result),result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7BT0KdG8EXW"
      },
      "source": [
        "def sam_wgan2(g_summ,text, display = False):\n",
        "    source = Source(text)\n",
        "    source.set_key_rate(s_discriminator)\n",
        "    summarizer = SAM_Summarizer(g_discriminator,s_discriminator)\n",
        "    summarizer.ready(source)\n",
        "    hist = summarizer.summarize(epochs=100,batch_size=2,learning_rate=5e-3,display=display)\n",
        "    summary_text, score, sim, cr = summarizer.get_summary(100)\n",
        "    #print('-'*50)\n",
        "    #print('gold summary:')\n",
        "    #print(g_summ)    \n",
        "    print('-'*50)\n",
        "    #print('sam_wgan summary:')\n",
        "    #for txt in summary_text:\n",
        "    print(summary_text,score,sim,cr)\n",
        "    #print('-'*50)\n",
        "    if score < 6.0 or sim < 0.56:\n",
        "        return sam_wgan2(g_summ,text,display)\n",
        "    #df,arr = evaluate('SAM+WGAN',summary_text,g_summ,org_text[0],org_text[1],org_text[2])\n",
        "    return hist, summary_text"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ez_heDyrP800"
      },
      "source": [
        "def sam_wgan3(text,create_count=200):\n",
        "    text_list = []\n",
        "    source = Source(text)\n",
        "    source.set_key_rate(s_discriminator)\n",
        "    max_score = 0\n",
        "    max_sim = 0\n",
        "    best_text = \"\"\n",
        "    idx = 0\n",
        "    while idx < create_count:\n",
        "        text,h = source.get_random_text()\n",
        "        if h in text_list:\n",
        "            pass\n",
        "        else:\n",
        "            #print(text)\n",
        "            idx +=1\n",
        "            loss, out= g_discriminator.transfer_learning([text],train_for = False)\n",
        "            sim_score = s_discriminator.similarity(text,source.org_text_emb)\n",
        "            if sim_score > max_sim and out[0,1].item() > 6.0:\n",
        "                best_text = text.strip()\n",
        "                max_score = out[0,1].item()\n",
        "                max_sim = sim_score\n",
        "            text_list.append(h)\n",
        "    return best_text, max_score, max_sim\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhcoXuPMGy09"
      },
      "source": [
        "def sam_wgan4(text, epochs=100, batch_size=32):\n",
        "    source = Source(text)\n",
        "    source.set_key_rate(s_discriminator)\n",
        "    summarizer = SAM_Summarizer(g_discriminator,s_discriminator)\n",
        "    summarizer.ready(source)\n",
        "    hist = summarizer.summarize(epochs,batch_size=2,learning_rate=5e-3,display=False)\n",
        "    samples, max_g = summarizer.get_samples(batch_size)\n",
        "    #print(samples)\n",
        "    if max_g < 6.0:\n",
        "        return sam_wgan4(text, epochs+10, batch_size)\n",
        "    return samples"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FstAHWGQ8KR",
        "outputId": "14aec8d8-d24f-4dd5-8860-be70af5d5d4f"
      },
      "source": [
        "txt = \"\"\"\n",
        "옛날 어느 집에 귀여운 여자 아기가 태어났어요.\n",
        "아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요.\n",
        "\"\"\"\n",
        "sam_wgan4(txt)"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   100/100 epochs, grammar loss:-0.4958  similarity loss:-0.4302 length loss:-0.0487\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('옛날 어느 집에 귀여운 여자 아기가 태어났고 아기는 자라서 예쁘고 마음씨 고운 소녀가 되었어요.',\n",
              "  6.23342227935791,\n",
              "  0.8720249203343073,\n",
              "  0.06666666666666665),\n",
              " ('옛날 어느 집에 귀여운 여자 아기가 아기는 자라서 예쁘고 마음씨 고운 소녀가 되었어요.',\n",
              "  6.256294250488281,\n",
              "  0.8708088184217355,\n",
              "  0.1333333333333333),\n",
              " ('옛날 어느 집에 귀여운 여자 아기가 아기는 자라서 예쁘고 마음씨 고운 소녀가 되었어요.',\n",
              "  6.256294250488281,\n",
              "  0.8708088184217355,\n",
              "  0.1333333333333333),\n",
              " ('어느 집에 귀여운 여자 아기가 태어났고 아기는 자라서 예쁘고 마음씨 고운 소녀가 되었어요.',\n",
              "  6.231296539306641,\n",
              "  0.8627629119532105,\n",
              "  0.1333333333333333),\n",
              " ('옛날 어느 집에 귀여운 여자 아기가 아기는 자라서 예쁘고 마음씨 고운 소녀가 되었어요.',\n",
              "  6.256294250488281,\n",
              "  0.8708088184217355,\n",
              "  0.1333333333333333),\n",
              " ('옛날 어느 집에 귀여운 여자 아기가 태어났고 아기는 자라서 예쁘고 마음씨 고운 소녀가 되었어요.',\n",
              "  6.23342227935791,\n",
              "  0.8720249203343073,\n",
              "  0.06666666666666665),\n",
              " ('옛날 어느 집에 귀여운 여자 아기가 태어났고 아기는 자라서 예쁘고 마음씨 고운 소녀가 되었어요.',\n",
              "  6.23342227935791,\n",
              "  0.8720249203343073,\n",
              "  0.06666666666666665),\n",
              " ('옛날 어느 집에 귀여운 여자 아기가 태어났고 아기는 자라서 예쁘고 마음씨 고운 소녀가 되었어요.',\n",
              "  6.23342227935791,\n",
              "  0.8720249203343073,\n",
              "  0.06666666666666665),\n",
              " ('옛날 어느 집에 귀여운 여자 아기가 아기는 자라서 예쁘고 마음씨 고운 소녀가 되었어요.',\n",
              "  6.256294250488281,\n",
              "  0.8708088184217355,\n",
              "  0.1333333333333333),\n",
              " ('옛날 어느 집에 귀여운 여자 아기가 태어났고 아기는 자라서 예쁘고 마음씨 고운 소녀가 되었어요.',\n",
              "  6.23342227935791,\n",
              "  0.8720249203343073,\n",
              "  0.06666666666666665),\n",
              " ('어느 집에 귀여운 여자 아기가 태어났고 아기는 자라서 예쁘고 마음씨 고운 소녀가 되었어요.',\n",
              "  6.231296539306641,\n",
              "  0.8627629119532105,\n",
              "  0.1333333333333333),\n",
              " ('옛날 어느 집에 귀여운 여자 아기가 태어났고 아기는 자라서 예쁘고 마음씨 고운 소녀가 되었어요.',\n",
              "  6.23342227935791,\n",
              "  0.8720249203343073,\n",
              "  0.06666666666666665),\n",
              " ('옛날 어느 집에 귀여운 여자 아기가 태어났고 아기는 자라서 예쁘고 마음씨 고운 소녀가 되었어요.',\n",
              "  6.23342227935791,\n",
              "  0.8720249203343073,\n",
              "  0.06666666666666665),\n",
              " ('옛날 어느 집에 귀여운 여자 아기가 아기는 자라서 예쁘고 마음씨 고운 소녀가 되었어요.',\n",
              "  6.256294250488281,\n",
              "  0.8708088184217355,\n",
              "  0.1333333333333333),\n",
              " ('옛날 어느 집에 귀여운 여자 아기가 태어났고 아기는 자라서 예쁘고 마음씨 고운 소녀가 되었어요.',\n",
              "  6.23342227935791,\n",
              "  0.8720249203343073,\n",
              "  0.06666666666666665),\n",
              " ('어느 집에 귀여운 여자 아기가 태어났고 아기는 자라서 예쁘고 마음씨 고운 소녀가 되었어요.',\n",
              "  6.231296539306641,\n",
              "  0.8627629119532105,\n",
              "  0.1333333333333333),\n",
              " ('어느 집에 귀여운 여자 아기가 태어났고 아기는 자라서 예쁘고 마음씨 고운 소녀가 되었어요.',\n",
              "  6.231296539306641,\n",
              "  0.8627629119532105,\n",
              "  0.1333333333333333),\n",
              " ('옛날 어느 집에 귀여운 여자 아기가 아기는 자라서 예쁘고 마음씨 고운 소녀가 되었어요.',\n",
              "  6.256294250488281,\n",
              "  0.8708088184217355,\n",
              "  0.1333333333333333),\n",
              " ('어느 집에 귀여운 여자 아기가 태어났고 아기는 자라서 예쁘고 마음씨 고운 소녀가 되었어요.',\n",
              "  6.231296539306641,\n",
              "  0.8627629119532105,\n",
              "  0.1333333333333333),\n",
              " ('옛날 어느 집에 귀여운 여자 아기가 아기는 자라서 예쁘고 마음씨 고운 소녀가 되었어요.',\n",
              "  6.256294250488281,\n",
              "  0.8708088184217355,\n",
              "  0.1333333333333333),\n",
              " ('옛날 어느 집에 귀여운 여자 아기가 태어났고 아기는 자라서 예쁘고 마음씨 고운 소녀가 되었어요.',\n",
              "  6.23342227935791,\n",
              "  0.8720249203343073,\n",
              "  0.06666666666666665),\n",
              " ('옛날 어느 집에 귀여운 여자 아기가 태어났고 아기는 자라서 예쁘고 마음씨 고운 소녀가 되었어요.',\n",
              "  6.23342227935791,\n",
              "  0.8720249203343073,\n",
              "  0.06666666666666665),\n",
              " ('옛날 어느 집에 귀여운 여자 아기가 태어났고 아기는 자라서 예쁘고 마음씨 고운 소녀가 되었어요.',\n",
              "  6.23342227935791,\n",
              "  0.8720249203343073,\n",
              "  0.06666666666666665),\n",
              " ('옛날 어느 집에 귀여운 여자 아기가 태어났고 아기는 자라서 예쁘고 마음씨 고운 소녀가 되었어요.',\n",
              "  6.23342227935791,\n",
              "  0.8720249203343073,\n",
              "  0.06666666666666665),\n",
              " ('옛날 어느 집에 귀여운 여자 아기가 태어났고 아기는 자라서 예쁘고 마음씨 고운 소녀가 되었어요.',\n",
              "  6.23342227935791,\n",
              "  0.8720249203343073,\n",
              "  0.06666666666666665),\n",
              " ('어느 집에 귀여운 여자 아기가 태어났고 아기는 자라서 예쁘고 마음씨 고운 소녀가 되었어요.',\n",
              "  6.231296539306641,\n",
              "  0.8627629119532105,\n",
              "  0.1333333333333333),\n",
              " ('옛날 어느 집에 귀여운 여자 아기가 아기는 자라서 예쁘고 마음씨 고운 소녀가 되었어요.',\n",
              "  6.256294250488281,\n",
              "  0.8708088184217355,\n",
              "  0.1333333333333333),\n",
              " ('옛날 어느 집에 귀여운 여자 아기가 태어났고 아기는 자라서 예쁘고 마음씨 고운 소녀가 되었어요.',\n",
              "  6.23342227935791,\n",
              "  0.8720249203343073,\n",
              "  0.06666666666666665),\n",
              " ('옛날 어느 집에 귀여운 여자 아기가 태어났고 아기는 자라서 예쁘고 마음씨 고운 소녀가 되었어요.',\n",
              "  6.23342227935791,\n",
              "  0.8720249203343073,\n",
              "  0.06666666666666665),\n",
              " ('옛날 어느 집에 귀여운 여자 아기가 아기는 자라서 예쁘고 마음씨 고운 소녀가 되었어요.',\n",
              "  6.256294250488281,\n",
              "  0.8708088184217355,\n",
              "  0.1333333333333333),\n",
              " ('옛날 어느 집에 귀여운 여자 아기가 태어났고 아기는 자라서 예쁘고 마음씨 고운 소녀가 되었어요.',\n",
              "  6.23342227935791,\n",
              "  0.8720249203343073,\n",
              "  0.06666666666666665),\n",
              " ('옛날 어느 집에 귀여운 여자 아기가 태어났고 아기는 자라서 예쁘고 마음씨 고운 소녀가 되었어요.',\n",
              "  6.23342227935791,\n",
              "  0.8720249203343073,\n",
              "  0.06666666666666665)]"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2k1mLxSQwB7G"
      },
      "source": [
        "def summary(text):\n",
        "    org_sentences = np.array(nltk.sent_tokenize(text.strip()))\n",
        "    summary_text = []\n",
        "    for i in range(0,len(org_sentences),2):\n",
        "        txt = org_sentences[i]\n",
        "        if i < len(org_sentences)-1:\n",
        "            txt +=  ' ' + org_sentences[i+1]\n",
        "        hist,st = sam_wgan2('',txt.strip(),comp_rate=0.4,method=2, display= False)\n",
        "        summary_text.append(st)\n",
        "\n",
        "    return ' '.join(summary_text).strip()\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBDAk_cGVPSu"
      },
      "source": [
        "def summary2(text):\n",
        "    org_sentences = np.array(nltk.sent_tokenize(text.strip()))\n",
        "    summary_text = []\n",
        "    for i in range(0,len(org_sentences),2):\n",
        "        txt = org_sentences[i]\n",
        "        if i < len(org_sentences)-1:\n",
        "            txt +=  ' ' + org_sentences[i+1]\n",
        "        t,g,s = sam_wgan3(txt.strip())\n",
        "        print(t,g,s)\n",
        "        summary_text.append(t)\n",
        "\n",
        "    return ' '.join(summary_text).strip()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "0S301yeelEvG"
      },
      "source": [
        "full_text = \"\"\"\n",
        "옛날 어느 집에 귀여운 여자 아기가 태어났어요.\n",
        "아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요.\n",
        "그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요.\n",
        "소녀의 아버지는 홀로 남은 소녀가 걱정되었어요.\n",
        "그래서 얼마 지나서 새어머니를 맞이했어요.\n",
        "새어머니는 소녀보다 나이가 위인 두명의 딸을 데리고 왔어요.\n",
        "그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요.\n",
        "새어머니는 소녀가 자기 딸들보다 예쁘고 착한게 못마땅했어요.\n",
        "그런데 이번에는 아버지마저 돌아가셨어요.\n",
        "소녀는 쓸고, 닦고, 하녀처럼 하루 종일 집안일을 도맡아 했어요.\n",
        "집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시 쉬곤 했어요.\n",
        "그러던 어느날, 왕궁에서 무도회가 열렸어요.\n",
        "신데렐라의 집에도 무도회 초대장이 왔어요.\n",
        "새어머니는 언니들을 데리고 무도회장으로 떠났어요.\n",
        "신데렐라도 무도회에 가고 싶었어요.\n",
        "혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요.\n",
        "그때 어디선가 마법사 할머니가 나타났어요.\n",
        "신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요.\n",
        "할머니는 소녀를 무도회에 보내줄테니 호박 한개와 생쥐 두마리, 도마뱀을 가지고 오라 했어요.\n",
        "마법사 할머니가 이것들을 보면서 주문을 외웠어요.\n",
        "그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금마차로 변했어요.\n",
        "이번에는 생쥐와 도마뱀을 건드렸어요.\n",
        "그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했어요.\n",
        "신데렐라의 낡은 옷은 구슬 장식이 반짝이는 예쁜 드레스로 바뀌었어요.\n",
        "할머니는 신데렐라에게 반짝반짝 빛나는 유리구두를 신겨 주었어요.\n",
        "그리고 밤 열두시가 되면 모든게 처음대로 돌아간다고 알려주었어요.\n",
        "황금마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼어요. \n",
        "그러니까 반드시 밤 열두시가 되기 전에 돌아와야 해요.\n",
        "신데렐라는 황금마차를 타고 왕궁 무도회장으로 가서 멋진 왕자님을 만났어요.\n",
        "왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요.\n",
        "왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고 신데렐라하고만 춤을 추었어요.\n",
        "신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요.\n",
        "어느덧 시간이 흘러 열두시가 되었어요. \n",
        "벽시계의 열두시를 알리는 종소리에 신데렐라는 화들짝 놀랐어요.\n",
        "신데렐라가 허둥지둥 왕궁을 빠져나가는데, 유리구두 한짝이 벗겨졌어요.\n",
        "하지만 구두를 주울 시간이 없었어요.\n",
        "신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리구두 한짝을 주웠어요.\n",
        "왕자님은 유리구두를 가지고 임금님께 가서 말했어요.\n",
        "이 유리구두의 주인과 결혼하겠어요.\n",
        "그래서 신하들은 유리구두의 주인을 찾아 온 나라를 돌아다녔어요.\n",
        "드디어 신데렐라의 집에까지 신하들이 도착했어요.\n",
        "언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리구두는 너무 작았어요.\n",
        "그때 신데렐라가 조용히 다가와 자기도 한번 신어보게 해달라고 부탁했어요.\n",
        "신데렐라는 신하에게서 받은 유리구두를 신었어요.\n",
        "유리구두는 신데렐라의 발에 꼭 맞았어요.\n",
        "신하들은 신데렐라를 왕궁으로 데리고 갔어요.\n",
        "그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\n",
        "\"\"\""
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "h7mVuIMzsEdN",
        "outputId": "011c8771-42e3-47ec-f620-398ed73ff64e"
      },
      "source": [
        "txt = \"\"\"\n",
        "옛날 어느 집에 귀여운 여자 아기가 태어났어요.\n",
        "아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요.\n",
        "\"\"\"\n",
        "hist,st = sam_wgan2('',txt.strip(),display= True)\n"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   100/100 epochs, grammar loss:-0.0000  similarity loss:-0.0000 length loss:-0.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "자라서 -6.18624210357666 0.43741294084810445 0.06666666666666667\n",
            "--------------------------------------------------\n",
            "옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   100/100 epochs, grammar loss:-0.1825  similarity loss:-0.2158 length loss:-0.0261\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "집에 아기가 태어났고 아기는 자라서 소녀가 되었어요. 6.319525718688965 0.7198371348188417 0.4666666666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjLQHlXbz4El"
      },
      "source": [
        "if False:\n",
        "    org_text = summary(full_text)\n",
        "    print('-'*50)\n",
        "    for txt in np.array(nltk.sent_tokenize(org_text.strip())):\n",
        "        print(txt)\n",
        "\n",
        "    org_text = summary(org_text)\n",
        "    print('-'*50)\n",
        "    for txt in np.array(nltk.sent_tokenize(org_text.strip())):\n",
        "        print(txt)\n",
        "\n",
        "    org_text = summary(org_text)\n",
        "    print('-'*50)\n",
        "    for txt in np.array(nltk.sent_tokenize(org_text.strip())):\n",
        "        print(txt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJpBfwxbVoJd"
      },
      "source": [
        "if False:\n",
        "    org_text = summary2(full_text)\n",
        "    print('-'*50)\n",
        "    for txt in np.array(nltk.sent_tokenize(org_text.strip())):\n",
        "        print(txt)\n",
        "\n",
        "    org_text = summary2(org_text)\n",
        "    print('-'*50)\n",
        "    for txt in np.array(nltk.sent_tokenize(org_text.strip())):\n",
        "        print(txt)\n",
        "\n",
        "    org_text = summary2(org_text)\n",
        "    print('-'*50)\n",
        "    for txt in np.array(nltk.sent_tokenize(org_text.strip())):\n",
        "        print(txt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiByPXpKSMam"
      },
      "source": [
        "if False:\n",
        "    for i in range(3):\n",
        "        org_text = summary(full_text,full_text)\n",
        "        print('-'*50)\n",
        "        for txt in np.array(nltk.sent_tokenize(org_text.strip())):\n",
        "            print(txt)\n",
        "\n",
        "        org_text = summary(full_text,org_text)\n",
        "        print('-'*50)\n",
        "        for txt in np.array(nltk.sent_tokenize(org_text.strip())):\n",
        "            print(txt)\n",
        "\n",
        "        org_text = summary(full_text,org_text)\n",
        "        print('-'*50)\n",
        "        for txt in np.array(nltk.sent_tokenize(org_text.strip())):\n",
        "            print(txt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKZ6xV0W7cEw"
      },
      "source": [
        "# EncoderDecoderModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1LCjLMi7bAh"
      },
      "source": [
        "pre_trained_kobert_model_name='kykim/bert-kor-base'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_Ue51tr7rwm"
      },
      "source": [
        "from transformers import EncoderDecoderModel, BertTokenizerFast\n",
        "import torch\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained(pre_trained_kobert_model_name)"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERrGJOdd_zSM",
        "outputId": "e0977072-936a-4a4e-b7b8-4301462e817d"
      },
      "source": [
        "op = tokenizer('옛날 어느 집에 귀여운 여자 아기가 태어났어요.[SEP]아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.', return_tensors=\"pt\",padding=\"max_length\", truncation=True, max_length=64)\n",
        "print(op)\n",
        "print(\"Tokens (str)      : {}\".format([tokenizer.convert_ids_to_tokens(s) for s in op['input_ids'].tolist()[0]]))\n",
        "print(\"Tokens (int)      : {}\".format(op['input_ids'].tolist()[0]))\n",
        "print(\"Tokens (attn_mask): {}\\n\".format(op['attention_mask'].tolist()[0]))"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[    2, 17463, 14385, 14662, 15886, 14891, 17818, 33791, 13972,  2016,\n",
            "             3, 35244,  4215,  8669,  8035,  8669, 19206,  8044, 17364, 14125,\n",
            "          8472, 26268, 18857,  8048, 17292,  2016,     3,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
            "Tokens (str)      : ['[CLS]', '옛날', '어느', '집에', '귀여운', '여자', '아기가', '태어났', '##어요', '.', '[SEP]', '아기는', '무', '##럭', '##무', '##럭', '자라', '##서', '예쁘고', '마음', '##씨', '고운', '소녀', '##가', '되었어요', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
            "Tokens (int)      : [2, 17463, 14385, 14662, 15886, 14891, 17818, 33791, 13972, 2016, 3, 35244, 4215, 8669, 8035, 8669, 19206, 8044, 17364, 14125, 8472, 26268, 18857, 8048, 17292, 2016, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Tokens (attn_mask): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abW0juJN7zmE"
      },
      "source": [
        "## dataset 만들기...???"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RRUZz027yPs",
        "outputId": "a0b4af22-149c-44d2-ccea-f68f0cbb9a59"
      },
      "source": [
        "\n",
        "encoder_max_length = 64\n",
        "decoder_max_length = 64\n",
        "batch_size = 16 #4 # 64, 128\n",
        "dataset_iterator = []\n",
        "batch_counter = 0\n",
        "\n",
        "\n",
        "org_sentences = np.array(nltk.sent_tokenize(full_text.strip()))\n",
        "summary_text = []\n",
        "for i in range(0,len(org_sentences),2):\n",
        "    txt = org_sentences[i]\n",
        "    txt2 = txt\n",
        "    if i < len(org_sentences)-1:\n",
        "        txt = txt +'[SEP]'+ org_sentences[i+1]\n",
        "        txt2 += org_sentences[i+1]\n",
        "\n",
        "    #text,g,s = sam_wgan3(txt2)\n",
        "    #print(text,g,s)\n",
        "    source = Source(txt2)\n",
        "    source.set_key_rate(s_discriminator)\n",
        "\n",
        "    batch = {}\n",
        "    batch['input_ids'] = []\n",
        "    batch['attention_mask'] = []\n",
        "    batch['decoder_input_ids'] = []\n",
        "    batch['decoder_attention_mask'] = []\n",
        "    batch['labels'] = []\n",
        "    batch['reward'] = []\n",
        "    batch['combine_text'] = []\n",
        "\n",
        "    percent = (\"{0:.2f}\").format(100 * ((i+2) / float(len(org_sentences))))\n",
        "    print(f'{percent}% {i+2}/{str(len(org_sentences))}')\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        text,h = source.get_random_text(0.5)\n",
        "        loss, out= g_discriminator.transfer_learning([text],train_for = False)\n",
        "        g = out[0,1].item()\n",
        "        s = s_discriminator.similarity(text,source.org_text_emb)\n",
        "\n",
        "        inputs = g_discriminator.tokenizer(txt, padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n",
        "        outputs = g_discriminator.tokenizer(text, padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n",
        "\n",
        "        rwd = (g + (s-0.5)*10 + 5)/10 * 5 - 3\n",
        "\n",
        "        batch[\"input_ids\"].append(inputs.input_ids)\n",
        "        batch[\"attention_mask\"].append(inputs.attention_mask)\n",
        "        batch[\"decoder_input_ids\"].append(outputs.input_ids)\n",
        "        batch[\"decoder_attention_mask\"].append(outputs.attention_mask)\n",
        "        batch[\"labels\"].append(outputs.input_ids.copy())\n",
        "        batch[\"reward\"].append(rwd)\n",
        "        batch['combine_text'].append(text)\n",
        "\n",
        "    dataset_iterator.append(batch)\n",
        "\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "--------------------------------------------------\n",
            "4.26% 2/47\n",
            "--------------------------------------------------\n",
            "그러던 어느날 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요. 소녀의 아버지는 홀로 남은 소녀가 걱정되었어요.\n",
            "--------------------------------------------------\n",
            "8.51% 4/47\n",
            "--------------------------------------------------\n",
            "그래서 얼마 지나서 새어머니를 맞이했어요. 새어머니는 소녀보다 나이가 위인 두명의 딸을 데리고 왔어요.\n",
            "--------------------------------------------------\n",
            "12.77% 6/47\n",
            "--------------------------------------------------\n",
            "그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요. 새어머니는 소녀가 자기 딸들보다 예쁘고 착한게 못마땅했어요.\n",
            "--------------------------------------------------\n",
            "17.02% 8/47\n",
            "--------------------------------------------------\n",
            "그런데 이번에는 아버지마저 돌아가셨어요. 소녀는 쓸고 닦고 하녀처럼 하루 종일 집안일을 도맡아 했어요.\n",
            "--------------------------------------------------\n",
            "21.28% 10/47\n",
            "--------------------------------------------------\n",
            "집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시 쉬곤 했어요. 그러던 어느날 왕궁에서 무도회가 열렸어요.\n",
            "--------------------------------------------------\n",
            "25.53% 12/47\n",
            "--------------------------------------------------\n",
            "신데렐라의 집에도 무도회 초대장이 왔어요. 새어머니는 언니들을 데리고 무도회장으로 떠났어요.\n",
            "--------------------------------------------------\n",
            "29.79% 14/47\n",
            "--------------------------------------------------\n",
            "신데렐라도 무도회에 가고 싶었어요. 혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요.\n",
            "--------------------------------------------------\n",
            "34.04% 16/47\n",
            "--------------------------------------------------\n",
            "그때 어디선가 마법사 할머니가 나타났어요. 신데렐라가 고개를 들어보니 마법사 할머니가 빙그레 웃고 있었어요.\n",
            "--------------------------------------------------\n",
            "38.30% 18/47\n",
            "--------------------------------------------------\n",
            "할머니는 소녀를 무도회에 보내줄테니 호박 한개와 생쥐 두마리 도마뱀을 가지고 오라 했어요. 마법사 할머니가 이것들을 보면서 주문을 외웠어요.\n",
            "--------------------------------------------------\n",
            "42.55% 20/47\n",
            "--------------------------------------------------\n",
            "그리고 지팡이로 호박을 건드리자 호박이 화려한 황금마차로 변했어요. 이번에는 생쥐와 도마뱀을 건드렸어요.\n",
            "--------------------------------------------------\n",
            "46.81% 22/47\n",
            "--------------------------------------------------\n",
            "그랬더니 생쥐는 흰말로 도마뱀은 멋진 마부로 변했어요. 신데렐라의 낡은 옷은 구슬 장식이 반짝이는 예쁜 드레스로 바뀌었어요.\n",
            "--------------------------------------------------\n",
            "51.06% 24/47\n",
            "--------------------------------------------------\n",
            "할머니는 신데렐라에게 반짝반짝 빛나는 유리구두를 신겨 주었어요. 그리고 밤 열두시가 되면 모든게 처음대로 돌아간다고 알려주었어요.\n",
            "--------------------------------------------------\n",
            "55.32% 26/47\n",
            "--------------------------------------------------\n",
            "황금마차는 호박으로 흰말은 생쥐로 마부는 도마뱀으로 변하게 돼어요. 그러니까 반드시 밤 열두시가 되기 전에 돌아와야 해요.\n",
            "--------------------------------------------------\n",
            "59.57% 28/47\n",
            "--------------------------------------------------\n",
            "신데렐라는 황금마차를 타고 왕궁 무도회장으로 가서 멋진 왕자님을 만났어요. 왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요.\n",
            "--------------------------------------------------\n",
            "63.83% 30/47\n",
            "--------------------------------------------------\n",
            "왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고 신데렐라하고만 춤을 추었어요. 신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요.\n",
            "--------------------------------------------------\n",
            "68.09% 32/47\n",
            "--------------------------------------------------\n",
            "어느덧 시간이 흘러 열두시가 되었어요. 벽시계의 열두시를 알리는 종소리에 신데렐라는 화들짝 놀랐어요.\n",
            "--------------------------------------------------\n",
            "72.34% 34/47\n",
            "--------------------------------------------------\n",
            "신데렐라가 허둥지둥 왕궁을 빠져나가는데 유리구두 한짝이 벗겨졌어요. 하지만 구두를 주울 시간이 없었어요.\n",
            "--------------------------------------------------\n",
            "76.60% 36/47\n",
            "--------------------------------------------------\n",
            "신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리구두 한짝을 주웠어요. 왕자님은 유리구두를 가지고 임금님께 가서 말했어요.\n",
            "--------------------------------------------------\n",
            "80.85% 38/47\n",
            "--------------------------------------------------\n",
            "이 유리구두의 주인과 결혼하겠어요. 그래서 신하들은 유리구두의 주인을 찾아 온 나라를 돌아다녔어요.\n",
            "--------------------------------------------------\n",
            "85.11% 40/47\n",
            "--------------------------------------------------\n",
            "드디어 신데렐라의 집에까지 신하들이 도착했어요. 언니들은 발을 오므려도 보고 구두를 늘려도 보았지만 한눈에 보기에도 유리구두는 너무 작았어요.\n",
            "--------------------------------------------------\n",
            "89.36% 42/47\n",
            "--------------------------------------------------\n",
            "그때 신데렐라가 조용히 다가와 자기도 한번 신어보게 해달라고 부탁했어요. 신데렐라는 신하에게서 받은 유리구두를 신었어요.\n",
            "--------------------------------------------------\n",
            "93.62% 44/47\n",
            "--------------------------------------------------\n",
            "유리구두는 신데렐라의 발에 꼭 맞았어요. 신하들은 신데렐라를 왕궁으로 데리고 갔어요.\n",
            "--------------------------------------------------\n",
            "97.87% 46/47\n",
            "--------------------------------------------------\n",
            "그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\n",
            "--------------------------------------------------\n",
            "102.13% 48/47\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHHWnCjA-YXI",
        "outputId": "67d06bc3-5073-4e63-906c-50062002b1cb"
      },
      "source": [
        "dataset_iterator[0]['reward']"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3.727023940988065,\n",
              " 4.203595166929454,\n",
              " 4.194410149343762,\n",
              " -1.2215865697216575,\n",
              " -1.2436142234429353,\n",
              " -2.0386367916579022,\n",
              " 3.9898473610676994,\n",
              " 4.2888718187825,\n",
              " 1.5582744831105773,\n",
              " 3.4998481907892867,\n",
              " -1.5350159906038896,\n",
              " 4.380420468458437,\n",
              " -2.136522739547487,\n",
              " -1.9939691043696763,\n",
              " 4.35444715337602,\n",
              " 4.442490392008105]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cL0pg5TF-gVC",
        "outputId": "658d1994-41fb-4f4d-f3be-636354cff5f7"
      },
      "source": [
        "from transformers import EncoderDecoderModel, BertTokenizerFast\n",
        "\n",
        "try:\n",
        "    del model\n",
        "    print('delete model')\n",
        "except Exception as ex:\n",
        "    pass\n",
        "\n",
        "\n",
        "model = EncoderDecoderModel.from_encoder_decoder_pretrained(pre_trained_kobert_model_name, pre_trained_kobert_model_name) # initialize Bert2Bert from pre-trained checkpoints\n",
        "\n",
        "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
        "model.config.eos_token_id = tokenizer.sep_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "model.config.vocab_size = model.config.encoder.vocab_size\n",
        "\n",
        "model.config.max_length = 142\n",
        "model.config.min_length = 56\n",
        "model.config.no_repeat_ngram_size = 3\n",
        "model.config.early_stopping = True\n",
        "model.config.length_penalty = 2.0\n",
        "model.config.num_beams = 4\n",
        "\n",
        "N_EPOCHS = 100"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "delete model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at kykim/bert-kor-base were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at kykim/bert-kor-base were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertLMHeadModel were not initialized from the model checkpoint at kykim/bert-kor-base and are newly initialized: ['bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PU0yl7rY_Fut"
      },
      "source": [
        "from transformers import BertTokenizer, AutoTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
        "\n",
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(dataset_iterator) * N_EPOCHS\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n",
        "        \n",
        "criterion = nn.CrossEntropyLoss(ignore_index = tokenizer.pad_token_id)"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "domg8yWn_LAj"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "\n",
        "        \n",
        "        \n",
        "        ##print(src.shape)\n",
        "        ##print(trg.shape)\n",
        "        input_ids = torch.tensor(batch[\"input_ids\"]).to(device)\n",
        "        attention_mask = torch.tensor(batch[\"attention_mask\"]).to(device)\n",
        "        decoder_input_ids= torch.tensor(batch[\"decoder_input_ids\"]).to(device)\n",
        "        decoder_attention_mask= torch.tensor(batch[\"decoder_attention_mask\"]).to(device)\n",
        "        labels= torch.tensor(batch[\"labels\"]).to(device)\n",
        "        rewards = torch.tensor(batch[\"reward\"]).to(device)\n",
        "\n",
        "        for (b_input_ids,b_attention_mask,b_decoder_input_ids,\n",
        "             b_decoder_attention_mask,b_labels,b_rewards) in zip(input_ids,\n",
        "                                                                 attention_mask,\n",
        "                                                                 decoder_input_ids,\n",
        "                                                                 decoder_attention_mask,\n",
        "                                                                 labels,rewards):\n",
        "            b_input_ids = torch.stack([b_input_ids])\n",
        "            b_attention_mask = torch.stack([b_attention_mask])\n",
        "            b_decoder_input_ids = torch.stack([b_decoder_input_ids])\n",
        "            b_decoder_attention_mask = torch.stack([b_decoder_attention_mask])\n",
        "            b_labels = torch.stack([b_labels])\n",
        "            #b_input_ids = torch.stack([b_input_ids])\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids=b_input_ids, attention_mask=b_attention_mask,\n",
        "                            decoder_input_ids=b_decoder_input_ids, \n",
        "                            decoder_attention_mask=b_decoder_attention_mask,\n",
        "                            labels=b_labels)\n",
        "\n",
        "            loss = outputs.loss #* b_rewards\n",
        "            loss.backward()\n",
        "            ##print('loss',loss)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        ##print('epoch_loss',epoch_loss)\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xUyXy2FE6X4"
      },
      "source": [
        "def train2(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    optimizer.zero_grad()\n",
        "    for i, batch in enumerate(iterator):\n",
        "\n",
        "        input_ids = torch.tensor(batch[\"input_ids\"]).to(device)\n",
        "        attention_mask = torch.tensor(batch[\"attention_mask\"]).to(device)\n",
        "        decoder_input_ids= torch.tensor(batch[\"decoder_input_ids\"]).to(device)\n",
        "        decoder_attention_mask= torch.tensor(batch[\"decoder_attention_mask\"]).to(device)\n",
        "        labels= torch.tensor(batch[\"labels\"]).to(device)\n",
        "\n",
        "            \n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask,\n",
        "                        decoder_input_ids=decoder_input_ids, \n",
        "                        decoder_attention_mask=decoder_attention_mask,\n",
        "                        labels=labels)\n",
        "\n",
        "        loss = outputs.loss #* b_rewards\n",
        "        loss.backward()\n",
        "        ##print('loss',loss)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        ##print('epoch_loss',epoch_loss)\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhNcMCKA_Tel"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZAmw1Lf_VtA"
      },
      "source": [
        "def generate_summary(text):\n",
        "    # cut off at BERT max length 512\n",
        "    inputs = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "    input_ids = inputs.input_ids.to(device)\n",
        "    attention_mask = inputs.attention_mask.to(device)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    outputs = model.generate(input_ids, attention_mask=attention_mask).cpu().detach().numpy()[0]\n",
        "    o=[]\n",
        "    for token in outputs:\n",
        "        if token == tokenizer.pad_token_id:\n",
        "            break\n",
        "        o.append(token)\n",
        "    output_str = tokenizer.batch_decode([o], skip_special_tokens=True)[0]\n",
        "\n",
        "    return output_str"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYZtPeMY_aFW",
        "outputId": "25c67208-5eb7-42b4-932e-e68f07928768"
      },
      "source": [
        "full_text = \"\"\"\n",
        "옛날 어느 집에 귀여운 여자 아기가 태어났어요.\n",
        "아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요.\n",
        "그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요.\n",
        "소녀의 아버지는 홀로 남은 소녀가 걱정되었어요.\n",
        "\"\"\"\n",
        "\n",
        "documents = []\n",
        "documents.append(full_text)\n",
        "\n",
        "encoder_max_length = 64\n",
        "decoder_max_length = 64\n",
        "batch_size = 32 #4 # 64, 128\n",
        "\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "model.to(device)\n",
        "\n",
        "for doc in documents:\n",
        "    dataset_iterator = []\n",
        "    org_sentences = np.array(nltk.sent_tokenize(doc.strip()))\n",
        "    for i in range(len(org_sentences)):\n",
        "        txt = org_sentences[i]\n",
        "        txt2 = txt\n",
        "        if i < len(org_sentences)-1:\n",
        "            txt +=  '[SEP]' + org_sentences[i+1]\n",
        "            txt2 += org_sentences[i+1]\n",
        "\n",
        "        batch = {}\n",
        "        batch['input_ids'] = []\n",
        "        batch['attention_mask'] = []\n",
        "        batch['decoder_input_ids'] = []\n",
        "        batch['decoder_attention_mask'] = []\n",
        "        batch['labels'] = []\n",
        "        samples = sam_wgan4(txt2,epochs=100,batch_size=batch_size)\n",
        "        for (text,g,s,l) in samples:\n",
        "            inputs = g_discriminator.tokenizer(txt, padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n",
        "            outputs = g_discriminator.tokenizer(text, padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n",
        "            batch[\"input_ids\"].append(inputs.input_ids)\n",
        "            batch[\"attention_mask\"].append(inputs.attention_mask)\n",
        "            batch[\"decoder_input_ids\"].append(outputs.input_ids)\n",
        "            batch[\"decoder_attention_mask\"].append(outputs.attention_mask)\n",
        "            batch[\"labels\"].append(outputs.input_ids.copy())\n",
        "        dataset_iterator.append(batch)\n",
        "\n",
        "    for epoch in range(N_EPOCHS):\n",
        "        \n",
        "        start_time = time.time()\n",
        "        train_loss = train2(model, dataset_iterator, optimizer, criterion, CLIP)\n",
        "        end_time = time.time()\n",
        "        \n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "        \n",
        "        #if valid_loss < best_valid_loss:\n",
        "        #    best_valid_loss = valid_loss\n",
        "        #    torch.save(model.state_dict(), 'tut6-model.pt')\n",
        "        \n",
        "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "        #print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "        print(generate_summary('옛날 어느 집에 귀여운 여자 아기가 태어났어요.[SEP]아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.'))"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   100/100 epochs, grammar loss:-0.3114  similarity loss:-0.2599 length loss:-0.0735\n",
            "--------------------------------------------------\n",
            "아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요. 그러던 어느날 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   100/100 epochs, grammar loss:-0.0000  similarity loss:-0.0000 length loss:-0.0000\n",
            "--------------------------------------------------\n",
            "아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요. 그러던 어느날 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   110/110 epochs, grammar loss:-0.4694  similarity loss:-0.3327 length loss:-0.0713\n",
            "--------------------------------------------------\n",
            "그러던 어느날 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요. 소녀의 아버지는 홀로 남은 소녀가 걱정되었어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   100/100 epochs, grammar loss:-0.4523  similarity loss:-0.3685 length loss:-0.0646\n",
            "--------------------------------------------------\n",
            "소녀의 아버지는 홀로 남은 소녀가 걱정되었어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   100/100 epochs, grammar loss:-0.0000  similarity loss:-0.0000 length loss:-0.0000\n",
            "--------------------------------------------------\n",
            "소녀의 아버지는 홀로 남은 소녀가 걱정되었어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   110/110 epochs, grammar loss:-0.0460  similarity loss:-0.0186 length loss:-0.0384\n",
            "--------------------------------------------------\n",
            "소녀의 아버지는 홀로 남은 소녀가 걱정되었어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   120/120 epochs, grammar loss:-0.0000  similarity loss:-0.0000 length loss:-0.0000\n",
            "--------------------------------------------------\n",
            "소녀의 아버지는 홀로 남은 소녀가 걱정되었어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   130/130 epochs, grammar loss:-0.0000  similarity loss:-0.0000 length loss:-0.0000\n",
            "--------------------------------------------------\n",
            "소녀의 아버지는 홀로 남은 소녀가 걱정되었어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   140/140 epochs, grammar loss:-0.0000  similarity loss:-0.0000 length loss:-0.0000\n",
            "--------------------------------------------------\n",
            "소녀의 아버지는 홀로 남은 소녀가 걱정되었어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   150/150 epochs, grammar loss:-0.0000  similarity loss:-0.0000 length loss:-0.0000\n",
            "--------------------------------------------------\n",
            "소녀의 아버지는 홀로 남은 소녀가 걱정되었어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   160/160 epochs, grammar loss:-0.0000  similarity loss:-0.0000 length loss:-0.0000\n",
            "--------------------------------------------------\n",
            "소녀의 아버지는 홀로 남은 소녀가 걱정되었어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   170/170 epochs, grammar loss:-0.0000  similarity loss:-0.0000 length loss:-0.0000\n",
            "--------------------------------------------------\n",
            "소녀의 아버지는 홀로 남은 소녀가 걱정되었어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   180/180 epochs, grammar loss:-0.0000  similarity loss:-0.0000 length loss:-0.0000\n",
            "--------------------------------------------------\n",
            "소녀의 아버지는 홀로 남은 소녀가 걱정되었어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   190/190 epochs, grammar loss:-0.5569  similarity loss:-0.5345 length loss:-0.0928\n",
            "--------------------------------------------------\n",
            "소녀의 아버지는 홀로 남은 소녀가 걱정되었어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   200/200 epochs, grammar loss:-0.0000  similarity loss:-0.0000 length loss:-0.0000\n",
            "--------------------------------------------------\n",
            "소녀의 아버지는 홀로 남은 소녀가 걱정되었어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   210/210 epochs, grammar loss:-0.2632  similarity loss:-0.1863 length loss:-0.1316\n",
            "Epoch: 01 | Time: 0m 2s\n",
            "\tTrain Loss: 9.681 | Train PPL: 16008.132\n",
            "\n",
            "Epoch: 02 | Time: 0m 2s\n",
            "\tTrain Loss: 4.175 | Train PPL:  65.009\n",
            ",,?,,,도,,이,,하,,여,,가,, (,,히,,.,,시,,한,,씬,, 여자,, 젤,,들,,\n",
            "Epoch: 03 | Time: 0m 2s\n",
            "\tTrain Loss: 3.949 | Train PPL:  51.895\n",
            "\n",
            "Epoch: 04 | Time: 0m 2s\n",
            "\tTrain Loss: 2.620 | Train PPL:  13.734\n",
            "##럭럭럭무럭럭\n",
            "Epoch: 05 | Time: 0m 2s\n",
            "\tTrain Loss: 2.323 | Train PPL:  10.210\n",
            "\n",
            "Epoch: 06 | Time: 0m 2s\n",
            "\tTrain Loss: 2.025 | Train PPL:   7.573\n",
            "\n",
            "Epoch: 07 | Time: 0m 2s\n",
            "\tTrain Loss: 1.715 | Train PPL:   5.559\n",
            "소녀 소녀 소녀 예쁘고 예쁘고 소녀 예쁘고 소녀 소녀\n",
            "Epoch: 08 | Time: 0m 2s\n",
            "\tTrain Loss: 1.437 | Train PPL:   4.208\n",
            "자라 자라 자라\n",
            "Epoch: 09 | Time: 0m 2s\n",
            "\tTrain Loss: 1.408 | Train PPL:   4.087\n",
            "자라 자라 자라럭럭럭무럭럭 자라무럭 자라럭 자라 자라무 자라럭무 자라 자라 무럭럭 무 자라럭 무럭 자라 무 자라 자라 그만럭럭.럭럭\n",
            "Epoch: 10 | Time: 0m 2s\n",
            "\tTrain Loss: 1.086 | Train PPL:   2.961\n",
            "자라 자라 자라 무 무 무무 무무무무 무 무 자라서서 마음 마음 마음 고운 고운 고운 마음 고운 마음 마음\n",
            "Epoch: 11 | Time: 0m 2s\n",
            "\tTrain Loss: 0.961 | Train PPL:   2.614\n",
            "소녀 소녀 소녀 여자 소녀 소녀 자라 자라 자라서서 마음 마음 마음씨씨 고운 고운 마음 마음 고운 고운씨씨씨 마음씨 마음 고운씨서씨씨가씨씨 되었어요씨씨씨가씨씨 되었고\n",
            "Epoch: 12 | Time: 0m 2s\n",
            "\tTrain Loss: 0.744 | Train PPL:   2.104\n",
            "##럭럭럭무럭럭 무럭럭 자라럭럭서 예쁘고 소녀 소녀 소녀가 되었어요...\n",
            "Epoch: 13 | Time: 0m 2s\n",
            "\tTrain Loss: 0.709 | Train PPL:   2.033\n",
            "귀여운 무무무무럭 자라서 예쁘고 예쁘고 예쁘고 마음 고운 고운 고운 예쁘고 예쁘고서 예쁘고 마음 예쁘고 고운 고운 소녀가 되었어요..\n",
            "Epoch: 14 | Time: 0m 2s\n",
            "\tTrain Loss: 0.529 | Train PPL:   1.698\n",
            "옛날 옛날 귀여운 옛날 옛날 옛날 소녀 소녀 소녀 태어났 태어났고 아기는 아기는 무 무럭럭 자라서 예쁘고 예쁘고 예쁘고 마음씨씨 고운 고운 소녀 소녀가 되었어요.\n",
            "Epoch: 15 | Time: 0m 2s\n",
            "\tTrain Loss: 0.363 | Train PPL:   1.438\n",
            "옛날 여자 여자 여자 태어났고고 무럭무럭럭럭서 예쁘고 마음씨씨씨 고운 고운 고운 마음씨 고운 마음 고운 고운씨씨 마음 고운씨 고운가씨씨가 되었어요가.\n",
            "Epoch: 16 | Time: 0m 2s\n",
            "\tTrain Loss: 0.280 | Train PPL:   1.323\n",
            "옛날 귀여운 여자 태어났고 무럭 자라서 예쁘고 마음 고운 고운 소녀가 되었어요.\n",
            "Epoch: 17 | Time: 0m 2s\n",
            "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
            "옛날 귀여운 귀여운 귀여운 여자 태어났고 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 18 | Time: 0m 2s\n",
            "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
            "옛날 귀여운 귀여운 여자 태어났고 무럭 자라서 예쁘고 마음씨 고운 소녀가 되었고 어느날 어머니가 병이들어 그만 그만 그만 세상을 말 말았어요.\n",
            "Epoch: 19 | Time: 0m 2s\n",
            "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
            "옛날 귀여운 여자 태어났고 무럭 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었고 어느날 어머니가 병이들어 그만 그만 세상을 말았어요.\n",
            "Epoch: 20 | Time: 0m 2s\n",
            "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
            "옛날 귀여운 여자 태어났고 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 21 | Time: 0m 2s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
            "옛날 어느 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가가 되었어요.\n",
            "Epoch: 22 | Time: 0m 2s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
            "옛날 귀여운 여자 태어났고 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 23 | Time: 0m 2s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "옛날 귀여운 여자 태어났고 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 24 | Time: 0m 2s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "옛날 귀여운 여자 태어났고 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 25 | Time: 0m 2s\n",
            "\tTrain Loss: 0.035 | Train PPL:   1.035\n",
            "옛날 귀여운 여자 태어났고 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었고 어느날 어머니가 병이들어 그만 세상을 세상을 말았어요\n",
            "Epoch: 26 | Time: 0m 2s\n",
            "\tTrain Loss: 0.043 | Train PPL:   1.043\n",
            "옛날 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었고 어느날 어머니가 병이들어 그만 세상을 떠나고 떠나고 떠나고 말았고 소녀의 아버지는 아버지는 아버지는 홀로 남은 소녀가 되었어요 어느날\n",
            "Epoch: 27 | Time: 0m 2s\n",
            "\tTrain Loss: 0.041 | Train PPL:   1.042\n",
            "옛날 어느 귀여운 여자 태어났고 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 28 | Time: 0m 2s\n",
            "\tTrain Loss: 0.032 | Train PPL:   1.032\n",
            "옛날 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 29 | Time: 0m 2s\n",
            "\tTrain Loss: 0.029 | Train PPL:   1.030\n",
            "옛날 어느 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 30 | Time: 0m 2s\n",
            "\tTrain Loss: 0.044 | Train PPL:   1.045\n",
            "옛날 어느 어느 어느 귀여운 여자 태어났고 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요 ) ) ) ( ( ( ) ( )\n",
            "Epoch: 31 | Time: 0m 2s\n",
            "\tTrain Loss: 0.030 | Train PPL:   1.030\n",
            "아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었고 어느날 어머니가 병이들어 그만 세상을 말았어요\n",
            "Epoch: 32 | Time: 0m 2s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "옛날 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 33 | Time: 0m 2s\n",
            "\tTrain Loss: 0.036 | Train PPL:   1.037\n",
            "옛날 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 34 | Time: 0m 2s\n",
            "\tTrain Loss: 0.040 | Train PPL:   1.041\n",
            "옛날 어느 어느 어느 여자 태어났고 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 35 | Time: 0m 2s\n",
            "\tTrain Loss: 0.041 | Train PPL:   1.042\n",
            "옛날 어느 어느 어느 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 36 | Time: 0m 2s\n",
            "\tTrain Loss: 0.027 | Train PPL:   1.027\n",
            "옛날 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 37 | Time: 0m 2s\n",
            "\tTrain Loss: 0.036 | Train PPL:   1.036\n",
            "옛날 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 38 | Time: 0m 2s\n",
            "\tTrain Loss: 0.029 | Train PPL:   1.030\n",
            "아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었고 어느날 어머니가 병이들어 그만 세상을 말았어요.\n",
            "Epoch: 39 | Time: 0m 2s\n",
            "\tTrain Loss: 0.032 | Train PPL:   1.033\n",
            "옛날 어느 어느 어느 귀여운 여자 태어났고 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요 어느날 병이들어 그만 세상을 말았어요.\n",
            "Epoch: 40 | Time: 0m 2s\n",
            "\tTrain Loss: 0.050 | Train PPL:   1.052\n",
            "옛날 어느 어느 어느 귀여운 여자 태어났고 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요 어느날 병이들어 그만 세상을 말았어요.\n",
            "Epoch: 41 | Time: 0m 2s\n",
            "\tTrain Loss: 0.031 | Train PPL:   1.031\n",
            "옛날 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 42 | Time: 0m 2s\n",
            "\tTrain Loss: 0.025 | Train PPL:   1.025\n",
            "옛날 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 43 | Time: 0m 2s\n",
            "\tTrain Loss: 0.023 | Train PPL:   1.023\n",
            "옛날 어느 어느 어느 귀여운 여자 태어났고 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요 어느날 어머니가 병이들어 그만 세상을 말았어요 ) ) ) ( ( ( ) )\n",
            "Epoch: 44 | Time: 0m 2s\n",
            "\tTrain Loss: 0.028 | Train PPL:   1.028\n",
            "옛날 어느 어느 어느 귀여운 여자 태어났고 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요 어느날 어머니가 병이들어 그만 세상을 말았어요 ) ) ) (\n",
            "Epoch: 45 | Time: 0m 2s\n",
            "\tTrain Loss: 0.029 | Train PPL:   1.030\n",
            "옛날 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 46 | Time: 0m 2s\n",
            "\tTrain Loss: 0.024 | Train PPL:   1.025\n",
            "옛날 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 47 | Time: 0m 2s\n",
            "\tTrain Loss: 0.024 | Train PPL:   1.025\n",
            "옛날 귀여운 여자 태어났고 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 48 | Time: 0m 2s\n",
            "\tTrain Loss: 0.023 | Train PPL:   1.023\n",
            "옛날 어느 어느 귀여운 여자 태어났고 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 49 | Time: 0m 2s\n",
            "\tTrain Loss: 0.025 | Train PPL:   1.026\n",
            "옛날 어느 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 50 | Time: 0m 2s\n",
            "\tTrain Loss: 0.023 | Train PPL:   1.023\n",
            "옛날 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 51 | Time: 0m 2s\n",
            "\tTrain Loss: 0.022 | Train PPL:   1.022\n",
            "옛날 귀여운 여자 아기가 태어났고 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 52 | Time: 0m 2s\n",
            "\tTrain Loss: 0.024 | Train PPL:   1.025\n",
            "옛날 귀여운 여자 태어났고 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 53 | Time: 0m 2s\n",
            "\tTrain Loss: 0.021 | Train PPL:   1.021\n",
            "옛날 어느 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 54 | Time: 0m 2s\n",
            "\tTrain Loss: 0.019 | Train PPL:   1.019\n",
            "옛날 어느 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 55 | Time: 0m 2s\n",
            "\tTrain Loss: 0.019 | Train PPL:   1.019\n",
            "옛날 어느 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 56 | Time: 0m 2s\n",
            "\tTrain Loss: 0.020 | Train PPL:   1.020\n",
            "옛날 귀여운 여자 태어났고 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 57 | Time: 0m 2s\n",
            "\tTrain Loss: 0.021 | Train PPL:   1.021\n",
            "옛날 귀여운 여자 아기가 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 58 | Time: 0m 2s\n",
            "\tTrain Loss: 0.019 | Train PPL:   1.019\n",
            "옛날 어느 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 59 | Time: 0m 2s\n",
            "\tTrain Loss: 0.019 | Train PPL:   1.019\n",
            "옛날 어느 귀여운 여자 태어났고 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요 어느날 어머니가 병이들어 그만 세상을 말았어요.\n",
            "Epoch: 60 | Time: 0m 2s\n",
            "\tTrain Loss: 0.018 | Train PPL:   1.018\n",
            "옛날 어느 귀여운 여자 태어났고 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요 어느날 어머니가 병이들어 그만 세상을 떠나고 말았어요.\n",
            "Epoch: 61 | Time: 0m 2s\n",
            "\tTrain Loss: 0.021 | Train PPL:   1.022\n",
            "옛날 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 62 | Time: 0m 2s\n",
            "\tTrain Loss: 0.019 | Train PPL:   1.019\n",
            "옛날 귀여운 여자 아기가 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요 어느날 어머니가 어머니가 병이들어 그만 세상을 말았어요.\n",
            "Epoch: 63 | Time: 0m 2s\n",
            "\tTrain Loss: 0.022 | Train PPL:   1.022\n",
            "옛날 어느 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요 어느날 어머니가 병이들어 그만 세상을 말았어요.\n",
            "Epoch: 64 | Time: 0m 2s\n",
            "\tTrain Loss: 0.019 | Train PPL:   1.019\n",
            "옛날 어느 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요 어느날 어머니가 어머니가 병이들어 그만 세상을 말았어요.\n",
            "Epoch: 65 | Time: 0m 2s\n",
            "\tTrain Loss: 0.020 | Train PPL:   1.020\n",
            "옛날 귀여운 여자 태어났고 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 66 | Time: 0m 2s\n",
            "\tTrain Loss: 0.020 | Train PPL:   1.020\n",
            "옛날 귀여운 여자 태어났고 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 67 | Time: 0m 2s\n",
            "\tTrain Loss: 0.020 | Train PPL:   1.020\n",
            "옛날 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 68 | Time: 0m 2s\n",
            "\tTrain Loss: 0.016 | Train PPL:   1.017\n",
            "옛날 어느 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 69 | Time: 0m 2s\n",
            "\tTrain Loss: 0.019 | Train PPL:   1.019\n",
            "옛날 어느 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 70 | Time: 0m 2s\n",
            "\tTrain Loss: 0.019 | Train PPL:   1.019\n",
            "옛날 어느 귀여운 여자 태어났고 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 71 | Time: 0m 2s\n",
            "\tTrain Loss: 0.016 | Train PPL:   1.017\n",
            "옛날 귀여운 여자 태어났고 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 72 | Time: 0m 2s\n",
            "\tTrain Loss: 0.035 | Train PPL:   1.035\n",
            "옛날 귀여운 여자 태어났고 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 73 | Time: 0m 2s\n",
            "\tTrain Loss: 0.018 | Train PPL:   1.018\n",
            "옛날 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요 어느날 어머니가 병이들어 그만 세상을 말았어요.\n",
            "Epoch: 74 | Time: 0m 2s\n",
            "\tTrain Loss: 0.020 | Train PPL:   1.020\n",
            "옛날 어느 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요 어느날 어머니가 병이들어 그만 세상을 말았어요.\n",
            "Epoch: 75 | Time: 0m 2s\n",
            "\tTrain Loss: 0.020 | Train PPL:   1.020\n",
            "옛날 어느 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요 어느날 어머니가 병이들어 그만 세상을 떠나고 말았어요.\n",
            "Epoch: 76 | Time: 0m 2s\n",
            "\tTrain Loss: 0.018 | Train PPL:   1.018\n",
            "옛날 어느 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요 어느날 어머니가 병이들어 그만 세상을 말았어요.\n",
            "Epoch: 77 | Time: 0m 2s\n",
            "\tTrain Loss: 0.022 | Train PPL:   1.022\n",
            "옛날 어느 귀여운 여자 태어났고 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요 어느날 어머니가 병이들어 그만 세상을 말았어요?\n",
            "Epoch: 78 | Time: 0m 2s\n",
            "\tTrain Loss: 0.018 | Train PPL:   1.019\n",
            "옛날 귀여운 여자 태어났고 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 79 | Time: 0m 2s\n",
            "\tTrain Loss: 0.017 | Train PPL:   1.017\n",
            "옛날 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요 어느날 어머니가 병이들어 그만 세상을 말았어요.\n",
            "Epoch: 80 | Time: 0m 2s\n",
            "\tTrain Loss: 0.016 | Train PPL:   1.016\n",
            "옛날 어느 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요 어느날 어머니가 병이들어 그만 세상을 말았어요.\n",
            "Epoch: 81 | Time: 0m 2s\n",
            "\tTrain Loss: 0.016 | Train PPL:   1.016\n",
            "옛날 어느 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 82 | Time: 0m 2s\n",
            "\tTrain Loss: 0.017 | Train PPL:   1.017\n",
            "옛날 귀여운 여자 태어났고 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 83 | Time: 0m 2s\n",
            "\tTrain Loss: 0.016 | Train PPL:   1.016\n",
            "옛날 귀여운 여자 태어났고 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 84 | Time: 0m 2s\n",
            "\tTrain Loss: 0.016 | Train PPL:   1.016\n",
            "옛날 귀여운 여자 태어났고 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 85 | Time: 0m 2s\n",
            "\tTrain Loss: 0.016 | Train PPL:   1.016\n",
            "옛날 어느 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 86 | Time: 0m 2s\n",
            "\tTrain Loss: 0.016 | Train PPL:   1.016\n",
            "옛날 어느 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요 어느날 어머니가 병이들어 그만 세상을 말았어요.\n",
            "Epoch: 87 | Time: 0m 2s\n",
            "\tTrain Loss: 0.017 | Train PPL:   1.017\n",
            "옛날 어느 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요 어느날 어머니가 병이들어 그만 세상을 말았어요.\n",
            "Epoch: 88 | Time: 0m 2s\n",
            "\tTrain Loss: 0.016 | Train PPL:   1.016\n",
            "옛날 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 89 | Time: 0m 2s\n",
            "\tTrain Loss: 0.016 | Train PPL:   1.016\n",
            "옛날 귀여운 여자 태어났고 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 90 | Time: 0m 2s\n",
            "\tTrain Loss: 0.016 | Train PPL:   1.016\n",
            "옛날 귀여운 여자 아기가 태어났고 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요 어느날 어머니가 병이들어 그만 세상을 말았어요.\n",
            "Epoch: 91 | Time: 0m 2s\n",
            "\tTrain Loss: 0.016 | Train PPL:   1.016\n",
            "옛날 어느 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요 어느날 어머니가 병이들어 그만 세상을 말았어요.\n",
            "Epoch: 92 | Time: 0m 2s\n",
            "\tTrain Loss: 0.014 | Train PPL:   1.014\n",
            "옛날 어느 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요 어느날 어머니가 병이들어 그만 세상을 말았어요.\n",
            "Epoch: 93 | Time: 0m 2s\n",
            "\tTrain Loss: 0.016 | Train PPL:   1.016\n",
            "옛날 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 94 | Time: 0m 2s\n",
            "\tTrain Loss: 0.015 | Train PPL:   1.015\n",
            "옛날 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 95 | Time: 0m 2s\n",
            "\tTrain Loss: 0.014 | Train PPL:   1.014\n",
            "옛날 귀여운 여자 태어났고 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 96 | Time: 0m 2s\n",
            "\tTrain Loss: 0.014 | Train PPL:   1.015\n",
            "옛날 귀여운 여자 아기가 태어났고 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 97 | Time: 0m 2s\n",
            "\tTrain Loss: 0.015 | Train PPL:   1.015\n",
            "옛날 어느 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 98 | Time: 0m 2s\n",
            "\tTrain Loss: 0.015 | Train PPL:   1.015\n",
            "옛날 어느 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 99 | Time: 0m 2s\n",
            "\tTrain Loss: 0.015 | Train PPL:   1.015\n",
            "옛날 어느 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "Epoch: 100 | Time: 0m 2s\n",
            "\tTrain Loss: 0.016 | Train PPL:   1.016\n",
            "옛날 귀여운 여자 태어났고 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTSvu8EnFD3_",
        "outputId": "4b4df802-889a-4d5a-d7c3-e5521e0f6cb5"
      },
      "source": [
        "print(generate_summary('아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요.[SEP]그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요.'))"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었고 어느날 어머니가 병이들어 그만 세상을 말았어요.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}