{"cells":[{"metadata":{"id":"GkQCxNatSIOk"},"cell_type":"markdown","source":"# GAN based Korean summarizer using semi abstractive method\n"},{"metadata":{"id":"K87VNBbeRLFF"},"cell_type":"markdown","source":"#4. Implementation\n"},{"metadata":{"id":"dQZeBAf8NxAR"},"cell_type":"markdown","source":"## 4.1 기본 설정..."},{"metadata":{"id":"tAdXzWGuKSBT","trusted":true,"outputId":"3e27805f-3ef6-4682-9820-46e374d40aba"},"cell_type":"code","source":"if False:\n    from google.colab import drive\n    drive.mount('/content/drive')","execution_count":1,"outputs":[]},{"metadata":{"id":"newO0mBXKVnE","trusted":true,"outputId":"352895fb-e597-4f2a-a090-81a624d89d7a"},"cell_type":"code","source":"#!pip install keybert\n!pip install sentence-transformers==0.3.0\n!pip install transformers==3.0.2\n\n#!pip install sentence-transformers","execution_count":2,"outputs":[{"output_type":"stream","text":"Collecting sentence-transformers==0.3.0\n  Downloading sentence-transformers-0.3.0.tar.gz (61 kB)\n\u001b[K     |████████████████████████████████| 61 kB 298 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: transformers>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers==0.3.0) (4.2.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from sentence-transformers==0.3.0) (4.55.1)\nRequirement already satisfied: torch>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers==0.3.0) (1.7.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers==0.3.0) (1.19.5)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from sentence-transformers==0.3.0) (0.24.1)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers==0.3.0) (1.5.4)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from sentence-transformers==0.3.0) (3.2.4)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.0.1->sentence-transformers==0.3.0) (0.18.2)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.0.1->sentence-transformers==0.3.0) (3.7.4.3)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch>=1.0.1->sentence-transformers==0.3.0) (0.6)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (3.3.0)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (0.0.43)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (3.0.12)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (20.8)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (2020.11.13)\nRequirement already satisfied: tokenizers==0.9.4 in /opt/conda/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (0.9.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (2.25.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers>=3.0.2->sentence-transformers==0.3.0) (3.4.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk->sentence-transformers==0.3.0) (1.15.0)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers>=3.0.2->sentence-transformers==0.3.0) (2.4.7)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (2020.12.5)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (1.26.2)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (2.10)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (3.0.4)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers>=3.0.2->sentence-transformers==0.3.0) (1.0.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers>=3.0.2->sentence-transformers==0.3.0) (7.1.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence-transformers==0.3.0) (2.1.0)\nBuilding wheels for collected packages: sentence-transformers\n  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-0.3.0-py3-none-any.whl size=86754 sha256=5d5552906132ae69b1a4e016debffc4671891571160d1c02f8a7d7c1c762e635\n  Stored in directory: /root/.cache/pip/wheels/3e/15/94/49bc84289d2c77b5059bca513f840c6006d4e2cc7f10275d49\nSuccessfully built sentence-transformers\nInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-0.3.0\nCollecting transformers==3.0.2\n  Downloading transformers-3.0.2-py3-none-any.whl (769 kB)\n\u001b[K     |████████████████████████████████| 769 kB 867 kB/s eta 0:00:01\n\u001b[?25hCollecting tokenizers==0.8.1.rc1\n  Downloading tokenizers-0.8.1rc1-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n\u001b[K     |████████████████████████████████| 3.0 MB 7.0 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (0.0.43)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (3.0.12)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (20.8)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (2020.11.13)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (2.25.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (1.19.5)\nRequirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (0.1.95)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (4.55.1)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==3.0.2) (2.4.7)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3.0.2) (2.10)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3.0.2) (1.26.2)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3.0.2) (2020.12.5)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3.0.2) (3.0.4)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==3.0.2) (1.0.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==3.0.2) (7.1.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==3.0.2) (1.15.0)\nInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.9.4\n    Uninstalling tokenizers-0.9.4:\n      Successfully uninstalled tokenizers-0.9.4\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.2.2\n    Uninstalling transformers-4.2.2:\n      Successfully uninstalled transformers-4.2.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nallennlp 2.0.1 requires transformers<4.3,>=4.1, but you have transformers 3.0.2 which is incompatible.\u001b[0m\nSuccessfully installed tokenizers-0.8.1rc1 transformers-3.0.2\n","name":"stdout"}]},{"metadata":{"id":"cmIxp0FnKXif","trusted":true,"outputId":"47389dc4-94c5-4177-c9a9-735f856c70b4"},"cell_type":"code","source":"import matplotlib.pyplot as plt\n# set seeds for reproducability\nfrom numpy.random import seed\nseed(1)\n\nimport pandas as pd\nimport numpy as np\nimport string, os \n\nimport urllib.request\nimport nltk\nnltk.download('punkt')\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(action='ignore', category=FutureWarning)","execution_count":3,"outputs":[{"output_type":"stream","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","name":"stdout"}]},{"metadata":{"id":"e1yL4NtUKaRn","trusted":true,"outputId":"b935e317-ae25-44f4-eeac-3aa454947b7b"},"cell_type":"code","source":"import tensorflow as tf\n# Get the GPU device name.\ndevice_name = tf.test.gpu_device_name()\n\n# The device name should look like the following:\nif device_name == '/device:GPU:0':\n    print('Found GPU at: {}'.format(device_name))\nelse:\n    print('GPU device not found')","execution_count":4,"outputs":[{"output_type":"stream","text":"Found GPU at: /device:GPU:0\n","name":"stdout"}]},{"metadata":{"id":"v3J0n_lhKcgm","trusted":true,"outputId":"16f02c64-1936-407c-bd9d-6884f5382093"},"cell_type":"code","source":"import torch\n\n# If there's a GPU available...\nif torch.cuda.is_available():    \n\n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","execution_count":5,"outputs":[{"output_type":"stream","text":"There are 1 GPU(s) available.\nWe will use the GPU: Tesla P100-PCIE-16GB\n","name":"stdout"}]},{"metadata":{"id":"6MJy2UYyLAoO","trusted":true},"cell_type":"code","source":"import random\n\nSEED = 1\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","execution_count":6,"outputs":[]},{"metadata":{"id":"Ue_4ZfdRKfdX","trusted":true},"cell_type":"code","source":"# Print iterations progress\nclass ProgressBar:\n\n    def __init__(self,total=20, prefix = '', suffix = '', decimals = 1, length = 20, fill = '|', printEnd = \"\\r\"):\n        self.total = total\n        self.prefix = prefix\n        self.suffix = suffix\n        self.decimals = decimals\n        self.length = length\n        self.fill = fill\n        self.printEnd = printEnd\n        self.ite = 0\n        self.back_filledLength = 0\n\n    def printProgress(self,iteration, text):\n        self.ite += iteration\n        percent = (\"{0:.\" + str(self.decimals) + \"f}\").format(100 * (self.ite / float(self.total)))\n        filledLength = int(self.length * self.ite // self.total)\n        bar = self.fill * filledLength + '.' * (self.length - filledLength)\n        if filledLength > self.back_filledLength or percent == 100:\n            print(f'\\r{self.prefix} |{bar}| {percent}% {self.suffix}  {text}', end=\"\", flush=True)\n            # Print New Line on Complete\n            if self.ite == self.total: \n                print()\n        self.back_filledLength = filledLength    ","execution_count":7,"outputs":[]},{"metadata":{"id":"BNHI0G6JKc5h","trusted":true},"cell_type":"code","source":"import torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision import transforms","execution_count":8,"outputs":[]},{"metadata":{"id":"6Zsv-LVkKmfL"},"cell_type":"markdown","source":"##4.2 Grammar Discriminator Class"},{"metadata":{"trusted":true},"cell_type":"code","source":"# coding=utf-8\n# Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team and Jangwon Park\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" Tokenization classes for KoBert model.\"\"\"\n\n\nimport logging\nimport os\nimport unicodedata\nfrom shutil import copyfile\n\nfrom transformers import PreTrainedTokenizer\n\n\nlogger = logging.getLogger(__name__)\n\nVOCAB_FILES_NAMES = {\"vocab_file\": \"tokenizer_78b3253a26.model\",\n                     \"vocab_txt\": \"vocab.txt\"}\n\nPRETRAINED_VOCAB_FILES_MAP = {\n    \"vocab_file\": {\n        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/tokenizer_78b3253a26.model\",\n        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/tokenizer_78b3253a26.model\",\n        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/tokenizer_78b3253a26.model\"\n    },\n    \"vocab_txt\": {\n        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/vocab.txt\",\n        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/vocab.txt\",\n        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/vocab.txt\"\n    }\n}\n\nPRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n    \"monologg/kobert\": 512,\n    \"monologg/kobert-lm\": 512,\n    \"monologg/distilkobert\": 512\n}\n\nPRETRAINED_INIT_CONFIGURATION = {\n    \"monologg/kobert\": {\"do_lower_case\": False},\n    \"monologg/kobert-lm\": {\"do_lower_case\": False},\n    \"monologg/distilkobert\": {\"do_lower_case\": False}\n}\n\nSPIECE_UNDERLINE = u'▁'\n\n\nclass KoBertTokenizer(PreTrainedTokenizer):\n    \"\"\"\n        SentencePiece based tokenizer. Peculiarities:\n            - requires `SentencePiece <https://github.com/google/sentencepiece>`_\n    \"\"\"\n    vocab_files_names = VOCAB_FILES_NAMES\n    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n\n    def __init__(\n            self,\n            vocab_file,\n            vocab_txt,\n            do_lower_case=False,\n            remove_space=True,\n            keep_accents=False,\n            unk_token=\"[UNK]\",\n            sep_token=\"[SEP]\",\n            pad_token=\"[PAD]\",\n            cls_token=\"[CLS]\",\n            mask_token=\"[MASK]\",\n            **kwargs):\n        super().__init__(\n            unk_token=unk_token,\n            sep_token=sep_token,\n            pad_token=pad_token,\n            cls_token=cls_token,\n            mask_token=mask_token,\n            **kwargs\n        )\n\n        # Build vocab\n        self.token2idx = dict()\n        self.idx2token = []\n        with open(vocab_txt, 'r', encoding='utf-8') as f:\n            for idx, token in enumerate(f):\n                token = token.strip()\n                self.token2idx[token] = idx\n                self.idx2token.append(token)\n\n        try:\n            import sentencepiece as spm\n        except ImportError:\n            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n                           \"pip install sentencepiece\")\n\n        self.do_lower_case = do_lower_case\n        self.remove_space = remove_space\n        self.keep_accents = keep_accents\n        self.vocab_file = vocab_file\n        self.vocab_txt = vocab_txt\n\n        self.sp_model = spm.SentencePieceProcessor()\n        self.sp_model.Load(vocab_file)\n\n    @property\n    def vocab_size(self):\n        return len(self.idx2token)\n\n    def get_vocab(self):\n        return dict(self.token2idx, **self.added_tokens_encoder)\n\n    def __getstate__(self):\n        state = self.__dict__.copy()\n        state[\"sp_model\"] = None\n        return state\n\n    def __setstate__(self, d):\n        self.__dict__ = d\n        try:\n            import sentencepiece as spm\n        except ImportError:\n            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n                           \"pip install sentencepiece\")\n        self.sp_model = spm.SentencePieceProcessor()\n        self.sp_model.Load(self.vocab_file)\n\n    def preprocess_text(self, inputs):\n        if self.remove_space:\n            outputs = \" \".join(inputs.strip().split())\n        else:\n            outputs = inputs\n        outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')\n\n        if not self.keep_accents:\n            outputs = unicodedata.normalize('NFKD', outputs)\n            outputs = \"\".join([c for c in outputs if not unicodedata.combining(c)])\n        if self.do_lower_case:\n            outputs = outputs.lower()\n\n        return outputs\n\n    def _tokenize(self, text, return_unicode=True, sample=False):\n        \"\"\" Tokenize a string. \"\"\"\n        text = self.preprocess_text(text)\n\n        if not sample:\n            pieces = self.sp_model.EncodeAsPieces(text)\n        else:\n            pieces = self.sp_model.SampleEncodeAsPieces(text, 64, 0.1)\n        new_pieces = []\n        for piece in pieces:\n            if len(piece) > 1 and piece[-1] == str(\",\") and piece[-2].isdigit():\n                cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))\n                if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n                    if len(cur_pieces[0]) == 1:\n                        cur_pieces = cur_pieces[1:]\n                    else:\n                        cur_pieces[0] = cur_pieces[0][1:]\n                cur_pieces.append(piece[-1])\n                new_pieces.extend(cur_pieces)\n            else:\n                new_pieces.append(piece)\n\n        return new_pieces\n\n    def _convert_token_to_id(self, token):\n        \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\n        return self.token2idx.get(token, self.token2idx[self.unk_token])\n\n    def _convert_id_to_token(self, index, return_unicode=True):\n        \"\"\"Converts an index (integer) in a token (string/unicode) using the vocab.\"\"\"\n        return self.idx2token[index]\n\n    def convert_tokens_to_string(self, tokens):\n        \"\"\"Converts a sequence of tokens (strings for sub-words) in a single string.\"\"\"\n        out_string = \"\".join(tokens).replace(SPIECE_UNDERLINE, \" \").strip()\n        return out_string\n\n    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n        \"\"\"\n        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n        by concatenating and adding special tokens.\n        A KoBERT sequence has the following format:\n            single sequence: [CLS] X [SEP]\n            pair of sequences: [CLS] A [SEP] B [SEP]\n        \"\"\"\n        if token_ids_1 is None:\n            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n        cls = [self.cls_token_id]\n        sep = [self.sep_token_id]\n        return cls + token_ids_0 + sep + token_ids_1 + sep\n\n    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n        \"\"\"\n        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n        Args:\n            token_ids_0: list of ids (must not contain special tokens)\n            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n                for sequence pairs\n            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n                special tokens for the model\n        Returns:\n            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\n        \"\"\"\n\n        if already_has_special_tokens:\n            if token_ids_1 is not None:\n                raise ValueError(\n                    \"You should not supply a second sequence if the provided sequence of \"\n                    \"ids is already formated with special tokens for the model.\"\n                )\n            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n\n        if token_ids_1 is not None:\n            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n        return [1] + ([0] * len(token_ids_0)) + [1]\n\n    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n        \"\"\"\n        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n        A KoBERT sequence pair mask has the following format:\n        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n        | first sequence    | second sequence\n        if token_ids_1 is None, only returns the first portion of the mask (0's).\n        \"\"\"\n        sep = [self.sep_token_id]\n        cls = [self.cls_token_id]\n        if token_ids_1 is None:\n            return len(cls + token_ids_0 + sep) * [0]\n        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n\n    def save_vocabulary(self, save_directory):\n        \"\"\" Save the sentencepiece vocabulary (copy original file) and special tokens file\n            to a directory.\n        \"\"\"\n        if not os.path.isdir(save_directory):\n            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\n            return\n\n        # 1. Save sentencepiece model\n        out_vocab_model = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_file\"])\n\n        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_model):\n            copyfile(self.vocab_file, out_vocab_model)\n\n        # 2. Save vocab.txt\n        index = 0\n        out_vocab_txt = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_txt\"])\n        with open(out_vocab_txt, \"w\", encoding=\"utf-8\") as writer:\n            for token, token_index in sorted(self.token2idx.items(), key=lambda kv: kv[1]):\n                if index != token_index:\n                    logger.warning(\n                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n                        \" Please check that the vocabulary is not corrupted!\".format(out_vocab_txt)\n                    )\n                    index = token_index\n                writer.write(token + \"\\n\")\n                index += 1\n\n        return out_vocab_model, out_vocab_txt","execution_count":9,"outputs":[{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n","name":"stderr"}]},{"metadata":{"id":"-VQdGLciKc_y","trusted":true},"cell_type":"code","source":"from transformers import BertTokenizer, AutoTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset, random_split\n\nimport time\nimport random\nimport datetime\n\n# 간단한 전처리\ndef clean_text(txt):\n    txt = txt.replace('\\n',' ')\n    txt = txt.replace('\\r',' ')    \n    txt = txt.replace('=','')\n    txt = txt.replace('\\\"','')   \n    txt = txt.replace('\\'','')\n    #txt = txt.replace(',','')\n    txt = txt.replace('..','')\n    txt = txt.replace('...','')\n    txt = txt.replace(' .','.')\n    txt = txt.replace('.','. ')\n    txt = txt.replace('  ',' ')\n    txt = txt.replace('  ',' ')    \n    txt = txt.replace('  ',' ')   \n    txt = txt.replace('  ',' ')           \n    txt = txt.replace('  ',' ')\n    txt = txt.replace('  ',' ')    \n    txt = txt.replace('  ',' ')   \n    txt = txt.replace('  ',' ')             \n    return txt.strip()\n\ndef shuffling(txt):\n    txt_list = txt.split(' ')\n    random.shuffle(txt_list)\n    return ' '.join(txt_list)\n\ndef collect_training_dataset_for_grammar_discriminator(sentences_dataset):\n\n    sentences = []\n    labels = []\n\n    for txtss in sentences_dataset:\n        txtss = clean_text(txtss)\n        txts = txtss.strip().split('.')\n        for txt in txts:  \n            txt = txt.strip()\n            if len(txt) > 40:\n                #ko_grammar_dataset.append([txt,1])\n                txt = txt.replace('.','')\n                tf = random.choice([True,False])\n                # 정상 또는 비정상 둘중에 하나만 데이터셋에 추가\n                if (tf):\n                    sentences.append(txt) # '.'의 위치를 보고 True, False를 판단 하기 땜에...\n                    labels.append(1)\n                else:\n                    sentences.append(shuffling(txt))\n                    labels.append(0)\n\n    return sentences,labels\n\n# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n\ndef format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    \n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))\n\nclass Grammar_Discriminator:\n\n\n    def __init__(self, pretraoned_kobert_model_name='monologg/kobert', input_dir=None):\n\n        if input_dir is None:\n            self.tokenizer = KoBertTokenizer.from_pretrained(pretraoned_kobert_model_name)\n            self.discriminator = BertForSequenceClassification.from_pretrained(\n                                    pretraoned_kobert_model_name, # Use the 12-layer BERT model, with an uncased vocab.\n                                    num_labels = 2, # The number of output labels--2 for binary classification.\n                                                    # You can increase this for multi-class tasks.   \n                                    output_attentions = False, # Whether the model returns attentions weights.\n                                    output_hidden_states = False, # Whether the model returns all hidden-states.\n                                )            \n        else:\n            self.__load_model(input_dir)\n\n\n\n\n    def set_dataset(self, sentences,labels):\n        # Print the original sentence.\n        print(' Original: ', sentences[0])\n\n        # Print the sentence split into tokens.\n        print('Tokenized: ', self.tokenizer.tokenize(sentences[0]))\n\n        # Print the sentence mapped to token ids.\n        print('Token IDs: ', self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(sentences[0])))   \n\n        # Tokenize all of the sentences and map the tokens to thier word IDs.\n        input_ids = []\n        attention_masks = []\n\n        # For every sentence...\n        for sent in sentences:\n            # `encode_plus` will:\n            #   (1) Tokenize the sentence.\n            #   (2) Prepend the `[CLS]` token to the start.\n            #   (3) Append the `[SEP]` token to the end.\n            #   (4) Map tokens to their IDs.\n            #   (5) Pad or truncate the sentence to `max_length`\n            #   (6) Create attention masks for [PAD] tokens.\n            encoded_dict = self.tokenizer.encode_plus(\n                                sent,                      # Sentence to encode.\n                                add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                                max_length = 64,           # Pad & truncate all sentences.\n                                pad_to_max_length = True,\n                                return_attention_mask = True,   # Construct attn. masks.\n                                return_tensors = 'pt',     # Return pytorch tensors.\n                                truncation = True,\n                        )\n            \n            # Add the encoded sentence to the list.    \n            input_ids.append(encoded_dict['input_ids'])\n            \n            # And its attention mask (simply differentiates padding from non-padding).\n            attention_masks.append(encoded_dict['attention_mask'])\n\n        # Convert the lists into tensors.\n        input_ids = torch.cat(input_ids, dim=0)\n        attention_masks = torch.cat(attention_masks, dim=0)\n        labels = torch.tensor(labels)\n\n        # Print sentence 0, now as a list of IDs.\n        print('Original: ', sentences[0])\n        print('Token IDs:', input_ids[0])\n\n        # Training & Validation Split\n        # Divide up our training set to use 90% for training and 10% for validation.\n\n        # Combine the training inputs into a TensorDataset.\n        dataset = TensorDataset(input_ids, attention_masks, labels)\n\n        # Create a 90-10 train-validation split.\n\n        # Calculate the number of samples to include in each set.\n        train_size = int(0.9 * len(dataset))\n        val_size = len(dataset) - train_size\n\n        # Divide the dataset by randomly selecting samples.\n        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\n        print('{:>5,} training samples'.format(train_size))\n        print('{:>5,} validation samples'.format(val_size))\n\n        # The DataLoader needs to know our batch size for training, so we specify it \n        # here. For fine-tuning BERT on a specific task, the authors recommend a batch \n        # size of 16 or 32.\n        self.batch_size = 32\n\n        # Create the DataLoaders for our training and validation sets.\n        # We'll take training samples in random order. \n        self.train_dataloader = DataLoader(\n                    train_dataset,  # The training samples.\n                    sampler = RandomSampler(train_dataset), # Select batches randomly\n                    batch_size = self.batch_size # Trains with this batch size.\n                )\n\n        # For validation the order doesn't matter, so we'll just read them sequentially.\n        self.validation_dataloader = DataLoader(\n                    val_dataset, # The validation samples.\n                    sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n                    batch_size = self.batch_size # Evaluate with this batch size.\n                )        \n\n\n\n    def train(self,epochs=4):\n        # Tell pytorch to run this model on the GPU.\n        self.discriminator.cuda()\n\n        # Get all of the model's parameters as a list of tuples.\n        params = list(self.discriminator.named_parameters())\n\n        print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n\n        print('==== Embedding Layer ====\\n')\n\n        for p in params[0:5]:\n            print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n\n        print('\\n==== First Transformer ====\\n')\n\n        for p in params[5:21]:\n            print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n\n        print('\\n==== Output Layer ====\\n')\n\n        for p in params[-4:]:\n            print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))  \n\n        # Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n        # I believe the 'W' stands for 'Weight Decay fix\"\n        self.optimizer = AdamW(self.discriminator.parameters(),\n                        lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n                        eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n                        )\n\n        # Number of training epochs. The BERT authors recommend between 2 and 4. \n        # We chose to run for 4, but we'll see later that this may be over-fitting the\n        # training data.\n        #epochs = 2\n\n        # Total number of training steps is [number of batches] x [number of epochs]. \n        # (Note that this is not the same as the number of training samples).\n        total_steps = len(self.train_dataloader) * epochs\n\n        # Create the learning rate scheduler.\n        scheduler = get_linear_schedule_with_warmup(self.optimizer, \n                                                    num_warmup_steps = 0, # Default value in run_glue.py\n                                                    num_training_steps = total_steps)\n            \n        # This training code is based on the `run_glue.py` script here:\n        # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n\n        # Set the seed value all over the place to make this reproducible.\n        seed_val = 42\n\n        random.seed(seed_val)\n        np.random.seed(seed_val)\n        torch.manual_seed(seed_val)\n        torch.cuda.manual_seed_all(seed_val)\n\n        # We'll store a number of quantities such as training and validation loss, \n        # validation accuracy, and timings.\n        training_stats = []\n\n        # Measure the total training time for the whole run.\n        total_t0 = time.time()\n\n        # For each epoch...\n        for epoch_i in range(0, epochs):\n            \n            # ========================================\n            #               Training\n            # ========================================\n            \n            # Perform one full pass over the training set.\n\n            print(\"\")\n            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n            print('Training...')\n\n            # Measure how long the training epoch takes.\n            t0 = time.time()\n\n            # Reset the total loss for this epoch.\n            total_train_loss = 0\n\n            # Put the model into training mode. Don't be mislead--the call to \n            # `train` just changes the *mode*, it doesn't *perform* the training.\n            # `dropout` and `batchnorm` layers behave differently during training\n            # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n            self.discriminator.train()\n\n            # For each batch of training data...\n            for step, batch in enumerate(self.train_dataloader):\n\n                # Progress update every 40 batches.\n                if step % 40 == 0 and not step == 0:\n                    # Calculate elapsed time in minutes.\n                    elapsed = format_time(time.time() - t0)\n                    \n                    # Report progress.\n                    print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(self.train_dataloader), elapsed))\n\n                # Unpack this training batch from our dataloader. \n                #\n                # As we unpack the batch, we'll also copy each tensor to the GPU using the \n                # `to` method.\n                #\n                # `batch` contains three pytorch tensors:\n                #   [0]: input ids \n                #   [1]: attention masks\n                #   [2]: labels \n                b_input_ids = batch[0].to(device)\n                b_input_mask = batch[1].to(device)\n                b_labels = batch[2].to(device)\n\n                # Always clear any previously calculated gradients before performing a\n                # backward pass. PyTorch doesn't do this automatically because \n                # accumulating the gradients is \"convenient while training RNNs\". \n                # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n                self.discriminator.zero_grad()        \n\n                # Perform a forward pass (evaluate the model on this training batch).\n                # The documentation for this `model` function is here: \n                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n                # It returns different numbers of parameters depending on what arguments\n                # arge given and what flags are set. For our useage here, it returns\n                # the loss (because we provided labels) and the \"logits\"--the model\n                # outputs prior to activation.\n                loss, logits = self.discriminator(b_input_ids, \n                                    token_type_ids=None, \n                                    attention_mask=b_input_mask, \n                                    labels=b_labels)\n\n                # Accumulate the training loss over all of the batches so that we can\n                # calculate the average loss at the end. `loss` is a Tensor containing a\n                # single value; the `.item()` function just returns the Python value \n                # from the tensor.\n                total_train_loss += loss.item()\n\n                # Perform a backward pass to calculate the gradients.\n                loss.backward()\n\n                # Clip the norm of the gradients to 1.0.\n                # This is to help prevent the \"exploding gradients\" problem.\n                torch.nn.utils.clip_grad_norm_(self.discriminator.parameters(), 1.0)\n\n                # Update parameters and take a step using the computed gradient.\n                # The optimizer dictates the \"update rule\"--how the parameters are\n                # modified based on their gradients, the learning rate, etc.\n                self.optimizer.step()\n\n                # Update the learning rate.\n                scheduler.step()\n\n            # Calculate the average loss over all of the batches.\n            avg_train_loss = total_train_loss / len(self.train_dataloader)            \n            \n            # Measure how long this epoch took.\n            training_time = format_time(time.time() - t0)\n\n            print(\"\")\n            print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n            print(\"  Training epcoh took: {:}\".format(training_time))\n                \n            # ========================================\n            #               Validation\n            # ========================================\n            # After the completion of each training epoch, measure our performance on\n            # our validation set.\n\n            print(\"\")\n            print(\"Running Validation...\")\n\n            t0 = time.time()\n\n            # Put the model in evaluation mode--the dropout layers behave differently\n            # during evaluation.\n            self.discriminator.eval()\n\n            # Tracking variables \n            total_eval_accuracy = 0\n            total_eval_loss = 0\n            nb_eval_steps = 0\n\n            # Evaluate data for one epoch\n            for batch in self.validation_dataloader:\n                \n                # Unpack this training batch from our dataloader. \n                #\n                # As we unpack the batch, we'll also copy each tensor to the GPU using \n                # the `to` method.\n                #\n                # `batch` contains three pytorch tensors:\n                #   [0]: input ids \n                #   [1]: attention masks\n                #   [2]: labels \n                b_input_ids = batch[0].to(device)\n                b_input_mask = batch[1].to(device)\n                b_labels = batch[2].to(device)\n                \n                # Tell pytorch not to bother with constructing the compute graph during\n                # the forward pass, since this is only needed for backprop (training).\n                with torch.no_grad():        \n\n                    # Forward pass, calculate logit predictions.\n                    # token_type_ids is the same as the \"segment ids\", which \n                    # differentiates sentence 1 and 2 in 2-sentence tasks.\n                    # The documentation for this `model` function is here: \n                    # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n                    # Get the \"logits\" output by the model. The \"logits\" are the output\n                    # values prior to applying an activation function like the softmax.\n                    (loss, logits) = self.discriminator(b_input_ids, \n                                        token_type_ids=None, \n                                        attention_mask=b_input_mask,\n                                        labels=b_labels)\n                    \n                # Accumulate the validation loss.\n                total_eval_loss += loss.item()\n\n                # Move logits and labels to CPU\n                logits = logits.detach().cpu().numpy()\n                label_ids = b_labels.to('cpu').numpy()\n\n                # Calculate the accuracy for this batch of test sentences, and\n                # accumulate it over all batches.\n                total_eval_accuracy += flat_accuracy(logits, label_ids)\n                \n\n            # Report the final accuracy for this validation run.\n            avg_val_accuracy = total_eval_accuracy / len(self.validation_dataloader)\n            print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n\n            # Calculate the average loss over all of the batches.\n            avg_val_loss = total_eval_loss / len(self.validation_dataloader)\n            \n            # Measure how long the validation run took.\n            validation_time = format_time(time.time() - t0)\n            \n            print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n            print(\"  Validation took: {:}\".format(validation_time))\n\n            # Record all statistics from this epoch.\n            training_stats.append(\n                {\n                    'epoch': epoch_i + 1,\n                    'Training Loss': avg_train_loss,\n                    'Valid. Loss': avg_val_loss,\n                    'Valid. Accur.': avg_val_accuracy,\n                    'Training Time': training_time,\n                    'Validation Time': validation_time\n                }\n            )\n\n        print(\"\")\n        print(\"Training complete!\")\n\n        print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n            \n\n        return training_stats\n\n    def save_model(self, output_dir = './model_save/'):\n        # Create output directory if needed\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n\n        print(\"Saving model to %s\" % output_dir)\n\n        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n        # They can then be reloaded using `from_pretrained()`\n        model_to_save = self.discriminator.module if hasattr(self.discriminator, 'module') else self.discriminator  # Take care of distributed/parallel training\n        model_to_save.save_pretrained(output_dir)\n        self.tokenizer.save_pretrained(output_dir)\n\n        # Good practice: save your training arguments together with the trained model\n        # torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n\n    def __load_model(self, input_dir = './drive/MyDrive/Colab Notebooks/summary/en_grammar_check_model'):\n        print('Loading BERT tokenizer...')\n        self.tokenizer = BertTokenizer.from_pretrained(input_dir)\n        self.discriminator = BertForSequenceClassification.from_pretrained(input_dir)\n\n    def transfer_learning(self, sentences, train_for = True):\n        \n        input_ids = []\n        attention_masks = []\n\n        # For every sentence...\n        for sent in sentences:\n            # `encode_plus` will:\n            #   (1) Tokenize the sentence.\n            #   (2) Prepend the `[CLS]` token to the start.\n            #   (3) Append the `[SEP]` token to the end.\n            #   (4) Map tokens to their IDs.\n            #   (5) Pad or truncate the sentence to `max_length`\n            #   (6) Create attention masks for [PAD] tokens.\n            encoded_dict = self.tokenizer.encode_plus(\n                                sent,                      # Sentence to encode.\n                                add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                                max_length = 64,           # Pad & truncate all sentences.\n                                pad_to_max_length = True,\n                                return_attention_mask = True,   # Construct attn. masks.\n                                return_tensors = 'pt',     # Return pytorch tensors.\n                                truncation = True,\n                        )\n            # Add the encoded sentence to the list.    \n            input_ids.append(encoded_dict['input_ids'])\n\n            # And its attention mask (simply differentiates padding from non-padding).\n            attention_masks.append(encoded_dict['attention_mask'])\n        \n        if train_for:\n            b_labels = torch.ones(len(sentences),dtype=torch.long).to(device)\n        else:\n            b_labels = torch.zeros(len(sentences),dtype=torch.long).to(device)\n        #print(b_labels)\n        # Convert the lists into tensors.\n        input_ids = torch.cat(input_ids, dim=0).to(device)\n        attention_masks = torch.cat(attention_masks, dim=0).to(device)    \n        #if str(discriminator1.device) == 'cpu':\n        #    pass\n        #else:\n        #    input_ids = input_ids.to(device)\n        #    attention_masks = attention_masks.to(device)        \n\n        loss, logits = self.discriminator(input_ids, \n                            token_type_ids=None, \n                            attention_mask=attention_masks, \n                                labels=b_labels)\n        #return torch.sigmoid(outputs[0][:,1])\n        #return outputs[0][:,1]\n        return loss, logits\n","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"urls = ['https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-ABC%20%EC%82%B4%EC%9D%B8%EC%82%AC%EA%B1%B4.txt',\n        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EA%B7%B8%EB%A6%AC%EA%B3%A0%20%EC%95%84%EB%AC%B4%EB%8F%84%20%EC%97%86%EC%97%88%EB%8B%A4.txt',\n        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EB%82%98%EC%9D%BC%EA%B0%95%EC%9D%98%20%EC%A3%BD%EC%9D%8C.txt',\n        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EB%A7%8C%EC%B0%AC%ED%9A%8C%EC%9D%98%2013%EC%9D%B8.txt',\n        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EB%A9%94%EC%86%8C%ED%8F%AC%ED%83%80%EB%AF%B8%EC%95%84%EC%9D%98%20%EC%A3%BD%EC%9D%8C.txt',\n        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EB%AA%A9%EC%82%AC%EA%B4%80%EC%82%B4%EC%9D%B8.txt',\n        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EB%B2%99%EC%96%B4%EB%A6%AC%20%EB%AA%A9%EA%B2%A9%EC%9E%90.txt',\n        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EB%B9%84%EB%B0%80%20%EC%84%9C%EB%A5%98%EB%A5%BC%20%EB%85%B8%EB%A0%A4%EB%9D%BC.txt',\n        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EC%8A%A4%ED%8E%98%EC%9D%B8%EA%B6%A4%EC%A7%9D%EC%9D%98%20%EB%B9%84%EB%B0%80.txt',\n        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EC%95%84%ED%8C%8C%ED%8A%B8%EC%97%90%20%EB%82%98%ED%83%80%EB%82%9C%20%EC%9A%94%EC%A0%95.txt',\n        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EC%95%A0%ED%81%AC%EB%A1%9C%EC%9D%B4%EB%93%9C%20%EC%82%B4%EC%9D%B8%20%EC%82%AC%EA%B1%B4.txt',\n        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EC%98%88%EA%B3%A0%20%EC%82%B4%EC%9D%B8.txt',\n        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EC%A5%90%EB%8D%AB.txt',\n        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EC%BB%A4%ED%8A%BC.txt',\n        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%ED%81%AC%EB%A6%AC%EC%8A%A4%EB%A7%88%EC%8A%A4%20%ED%91%B8%EB%94%A9%EC%9D%98%20%EB%AA%A8%ED%97%98.txt',\n        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%ED%91%B8%EB%A5%B8%EC%97%B4%EC%B0%A8%EC%9D%98%EC%A3%BD%EC%9D%8C.txt',\n        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%ED%99%94%EC%9A%94%EC%9D%BC%20%ED%81%B4%EB%9F%BD%EC%9D%98%20%EC%82%B4%EC%9D%B8.txt']\n\nko_sentences_dataset = []\nfor url in urls:\n    raw_text = urllib.request.urlopen(url).read().decode('utf-8')\n    ko_sentences_dataset += nltk.sent_tokenize(clean_text(raw_text))\n    ","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(ko_sentences_dataset)","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"95889"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/kosentences/korean_sentences.csv')","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ko_sentences_dataset += list(df['sentence'])","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(ko_sentences_dataset)","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"350047"},"metadata":{}}]},{"metadata":{"id":"l7Zf2oRMMXmH","trusted":true,"outputId":"fb55ef25-c1c5-4d46-c7b6-ed10c52fd31f"},"cell_type":"code","source":"use_pretrained_model = False\n\nif use_pretrained_model:\n    #g_discriminator = Grammar_Discriminator(input_dir = '/content/drive/MyDrive/Colab Notebooks/summary/model_save')\n    g_discriminator = Grammar_Discriminator(input_dir = '../input/grammarcheckmodel2')\nelse:\n    sentences,labels = collect_training_dataset_for_grammar_discriminator(ko_sentences_dataset)\n    print(len(sentences))\n    g_discriminator = Grammar_Discriminator()\n    g_discriminator.set_dataset(sentences,labels)\n    g_discriminator.train(epochs=1)\n    g_discriminator.save_model()","execution_count":16,"outputs":[{"output_type":"stream","text":"264130\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/371k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"607bf66b0f0342829c7d4635d3e482c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/77.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff48296601ec43dfb9aba3ee19f2be06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/426 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b33f36b4a0cc417e9596265065aa98b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/369M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"878a82b66fe448c89a4a7fe0cf150942"}},"metadata":{}},{"output_type":"stream","text":" Original:  이 이야기에서는 내가 직접 입회한 사건이나 장면만을 이야기하는 전의 내 방법을 바꿔 보았다\nTokenized:  ['▁이', '▁이야기', '에서는', '▁내가', '▁직접', '▁입', '회', '한', '▁사건', '이나', '▁장면', '만', '을', '▁이야기', '하는', '▁전', '의', '▁내', '▁방법', '을', '▁바꿔', '▁보', '았다']\nToken IDs:  [3647, 3714, 6904, 1435, 4358, 3836, 7953, 7828, 2574, 7098, 3960, 6150, 7088, 3714, 7794, 4012, 7095, 1434, 2270, 7088, 2188, 2355, 6828]\nOriginal:  이 이야기에서는 내가 직접 입회한 사건이나 장면만을 이야기하는 전의 내 방법을 바꿔 보았다\nToken IDs: tensor([   2, 3647, 3714, 6904, 1435, 4358, 3836, 7953, 7828, 2574, 7098, 3960,\n        6150, 7088, 3714, 7794, 4012, 7095, 1434, 2270, 7088, 2188, 2355, 6828,\n           3,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1])\n237,717 training samples\n26,413 validation samples\nThe BERT model has 201 different named parameters.\n\n==== Embedding Layer ====\n\nbert.embeddings.word_embeddings.weight                   (8002, 768)\nbert.embeddings.position_embeddings.weight                (512, 768)\nbert.embeddings.token_type_embeddings.weight                (2, 768)\nbert.embeddings.LayerNorm.weight                              (768,)\nbert.embeddings.LayerNorm.bias                                (768,)\n\n==== First Transformer ====\n\nbert.encoder.layer.0.attention.self.query.weight          (768, 768)\nbert.encoder.layer.0.attention.self.query.bias                (768,)\nbert.encoder.layer.0.attention.self.key.weight            (768, 768)\nbert.encoder.layer.0.attention.self.key.bias                  (768,)\nbert.encoder.layer.0.attention.self.value.weight          (768, 768)\nbert.encoder.layer.0.attention.self.value.bias                (768,)\nbert.encoder.layer.0.attention.output.dense.weight        (768, 768)\nbert.encoder.layer.0.attention.output.dense.bias              (768,)\nbert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\nbert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\nbert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\nbert.encoder.layer.0.intermediate.dense.bias                 (3072,)\nbert.encoder.layer.0.output.dense.weight                 (768, 3072)\nbert.encoder.layer.0.output.dense.bias                        (768,)\nbert.encoder.layer.0.output.LayerNorm.weight                  (768,)\nbert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n\n==== Output Layer ====\n\nbert.pooler.dense.weight                                  (768, 768)\nbert.pooler.dense.bias                                        (768,)\nclassifier.weight                                           (2, 768)\nclassifier.bias                                                 (2,)\n\n======== Epoch 1 / 1 ========\nTraining...\n  Batch    40  of  7,429.    Elapsed: 0:00:10.\n  Batch    80  of  7,429.    Elapsed: 0:00:19.\n  Batch   120  of  7,429.    Elapsed: 0:00:27.\n  Batch   160  of  7,429.    Elapsed: 0:00:36.\n  Batch   200  of  7,429.    Elapsed: 0:00:45.\n  Batch   240  of  7,429.    Elapsed: 0:00:54.\n  Batch   280  of  7,429.    Elapsed: 0:01:03.\n  Batch   320  of  7,429.    Elapsed: 0:01:12.\n  Batch   360  of  7,429.    Elapsed: 0:01:21.\n  Batch   400  of  7,429.    Elapsed: 0:01:30.\n  Batch   440  of  7,429.    Elapsed: 0:01:39.\n  Batch   480  of  7,429.    Elapsed: 0:01:47.\n  Batch   520  of  7,429.    Elapsed: 0:01:56.\n  Batch   560  of  7,429.    Elapsed: 0:02:05.\n  Batch   600  of  7,429.    Elapsed: 0:02:14.\n  Batch   640  of  7,429.    Elapsed: 0:02:23.\n  Batch   680  of  7,429.    Elapsed: 0:02:32.\n  Batch   720  of  7,429.    Elapsed: 0:02:40.\n  Batch   760  of  7,429.    Elapsed: 0:02:49.\n  Batch   800  of  7,429.    Elapsed: 0:02:58.\n  Batch   840  of  7,429.    Elapsed: 0:03:07.\n  Batch   880  of  7,429.    Elapsed: 0:03:16.\n  Batch   920  of  7,429.    Elapsed: 0:03:25.\n  Batch   960  of  7,429.    Elapsed: 0:03:34.\n  Batch 1,000  of  7,429.    Elapsed: 0:03:42.\n  Batch 1,040  of  7,429.    Elapsed: 0:03:51.\n  Batch 1,080  of  7,429.    Elapsed: 0:04:00.\n  Batch 1,120  of  7,429.    Elapsed: 0:04:09.\n  Batch 1,160  of  7,429.    Elapsed: 0:04:18.\n  Batch 1,200  of  7,429.    Elapsed: 0:04:27.\n  Batch 1,240  of  7,429.    Elapsed: 0:04:35.\n  Batch 1,280  of  7,429.    Elapsed: 0:04:44.\n  Batch 1,320  of  7,429.    Elapsed: 0:04:53.\n  Batch 1,360  of  7,429.    Elapsed: 0:05:02.\n  Batch 1,400  of  7,429.    Elapsed: 0:05:11.\n  Batch 1,440  of  7,429.    Elapsed: 0:05:20.\n  Batch 1,480  of  7,429.    Elapsed: 0:05:28.\n  Batch 1,520  of  7,429.    Elapsed: 0:05:37.\n  Batch 1,560  of  7,429.    Elapsed: 0:05:46.\n  Batch 1,600  of  7,429.    Elapsed: 0:05:55.\n  Batch 1,640  of  7,429.    Elapsed: 0:06:04.\n  Batch 1,680  of  7,429.    Elapsed: 0:06:13.\n  Batch 1,720  of  7,429.    Elapsed: 0:06:21.\n  Batch 1,760  of  7,429.    Elapsed: 0:06:30.\n  Batch 1,800  of  7,429.    Elapsed: 0:06:39.\n  Batch 1,840  of  7,429.    Elapsed: 0:06:48.\n  Batch 1,880  of  7,429.    Elapsed: 0:06:57.\n  Batch 1,920  of  7,429.    Elapsed: 0:07:06.\n  Batch 1,960  of  7,429.    Elapsed: 0:07:14.\n  Batch 2,000  of  7,429.    Elapsed: 0:07:23.\n  Batch 2,040  of  7,429.    Elapsed: 0:07:32.\n  Batch 2,080  of  7,429.    Elapsed: 0:07:41.\n  Batch 2,120  of  7,429.    Elapsed: 0:07:50.\n  Batch 2,160  of  7,429.    Elapsed: 0:07:59.\n  Batch 2,200  of  7,429.    Elapsed: 0:08:07.\n  Batch 2,240  of  7,429.    Elapsed: 0:08:16.\n  Batch 2,280  of  7,429.    Elapsed: 0:08:25.\n  Batch 2,320  of  7,429.    Elapsed: 0:08:34.\n  Batch 2,360  of  7,429.    Elapsed: 0:08:43.\n  Batch 2,400  of  7,429.    Elapsed: 0:08:52.\n  Batch 2,440  of  7,429.    Elapsed: 0:09:00.\n  Batch 2,480  of  7,429.    Elapsed: 0:09:09.\n  Batch 2,520  of  7,429.    Elapsed: 0:09:18.\n  Batch 2,560  of  7,429.    Elapsed: 0:09:27.\n  Batch 2,600  of  7,429.    Elapsed: 0:09:36.\n  Batch 2,640  of  7,429.    Elapsed: 0:09:45.\n  Batch 2,680  of  7,429.    Elapsed: 0:09:53.\n  Batch 2,720  of  7,429.    Elapsed: 0:10:02.\n  Batch 2,760  of  7,429.    Elapsed: 0:10:11.\n  Batch 2,800  of  7,429.    Elapsed: 0:10:20.\n  Batch 2,840  of  7,429.    Elapsed: 0:10:29.\n  Batch 2,880  of  7,429.    Elapsed: 0:10:38.\n  Batch 2,920  of  7,429.    Elapsed: 0:10:46.\n  Batch 2,960  of  7,429.    Elapsed: 0:10:55.\n  Batch 3,000  of  7,429.    Elapsed: 0:11:04.\n  Batch 3,040  of  7,429.    Elapsed: 0:11:13.\n  Batch 3,080  of  7,429.    Elapsed: 0:11:22.\n  Batch 3,120  of  7,429.    Elapsed: 0:11:31.\n  Batch 3,160  of  7,429.    Elapsed: 0:11:39.\n  Batch 3,200  of  7,429.    Elapsed: 0:11:48.\n  Batch 3,240  of  7,429.    Elapsed: 0:11:57.\n  Batch 3,280  of  7,429.    Elapsed: 0:12:06.\n  Batch 3,320  of  7,429.    Elapsed: 0:12:15.\n  Batch 3,360  of  7,429.    Elapsed: 0:12:24.\n  Batch 3,400  of  7,429.    Elapsed: 0:12:32.\n  Batch 3,440  of  7,429.    Elapsed: 0:12:41.\n  Batch 3,480  of  7,429.    Elapsed: 0:12:50.\n  Batch 3,520  of  7,429.    Elapsed: 0:12:59.\n  Batch 3,560  of  7,429.    Elapsed: 0:13:08.\n  Batch 3,600  of  7,429.    Elapsed: 0:13:16.\n  Batch 3,640  of  7,429.    Elapsed: 0:13:25.\n  Batch 3,680  of  7,429.    Elapsed: 0:13:34.\n  Batch 3,720  of  7,429.    Elapsed: 0:13:43.\n  Batch 3,760  of  7,429.    Elapsed: 0:13:51.\n  Batch 3,800  of  7,429.    Elapsed: 0:14:00.\n  Batch 3,840  of  7,429.    Elapsed: 0:14:09.\n  Batch 3,880  of  7,429.    Elapsed: 0:14:18.\n  Batch 3,920  of  7,429.    Elapsed: 0:14:27.\n  Batch 3,960  of  7,429.    Elapsed: 0:14:36.\n  Batch 4,000  of  7,429.    Elapsed: 0:14:44.\n  Batch 4,040  of  7,429.    Elapsed: 0:14:53.\n  Batch 4,080  of  7,429.    Elapsed: 0:15:02.\n  Batch 4,120  of  7,429.    Elapsed: 0:15:11.\n  Batch 4,160  of  7,429.    Elapsed: 0:15:20.\n  Batch 4,200  of  7,429.    Elapsed: 0:15:28.\n  Batch 4,240  of  7,429.    Elapsed: 0:15:37.\n  Batch 4,280  of  7,429.    Elapsed: 0:15:46.\n  Batch 4,320  of  7,429.    Elapsed: 0:15:55.\n  Batch 4,360  of  7,429.    Elapsed: 0:16:03.\n  Batch 4,400  of  7,429.    Elapsed: 0:16:12.\n  Batch 4,440  of  7,429.    Elapsed: 0:16:21.\n  Batch 4,480  of  7,429.    Elapsed: 0:16:30.\n  Batch 4,520  of  7,429.    Elapsed: 0:16:39.\n  Batch 4,560  of  7,429.    Elapsed: 0:16:48.\n  Batch 4,600  of  7,429.    Elapsed: 0:16:57.\n","name":"stdout"},{"output_type":"stream","text":"  Batch 4,640  of  7,429.    Elapsed: 0:17:05.\n  Batch 4,680  of  7,429.    Elapsed: 0:17:14.\n  Batch 4,720  of  7,429.    Elapsed: 0:17:23.\n  Batch 4,760  of  7,429.    Elapsed: 0:17:32.\n  Batch 4,800  of  7,429.    Elapsed: 0:17:41.\n  Batch 4,840  of  7,429.    Elapsed: 0:17:50.\n  Batch 4,880  of  7,429.    Elapsed: 0:17:59.\n  Batch 4,920  of  7,429.    Elapsed: 0:18:08.\n  Batch 4,960  of  7,429.    Elapsed: 0:18:16.\n  Batch 5,000  of  7,429.    Elapsed: 0:18:25.\n  Batch 5,040  of  7,429.    Elapsed: 0:18:34.\n  Batch 5,080  of  7,429.    Elapsed: 0:18:43.\n  Batch 5,120  of  7,429.    Elapsed: 0:18:52.\n  Batch 5,160  of  7,429.    Elapsed: 0:19:01.\n  Batch 5,200  of  7,429.    Elapsed: 0:19:09.\n  Batch 5,240  of  7,429.    Elapsed: 0:19:18.\n  Batch 5,280  of  7,429.    Elapsed: 0:19:27.\n  Batch 5,320  of  7,429.    Elapsed: 0:19:36.\n  Batch 5,360  of  7,429.    Elapsed: 0:19:45.\n  Batch 5,400  of  7,429.    Elapsed: 0:19:53.\n  Batch 5,440  of  7,429.    Elapsed: 0:20:02.\n  Batch 5,480  of  7,429.    Elapsed: 0:20:11.\n  Batch 5,520  of  7,429.    Elapsed: 0:20:20.\n  Batch 5,560  of  7,429.    Elapsed: 0:20:29.\n  Batch 5,600  of  7,429.    Elapsed: 0:20:37.\n  Batch 5,640  of  7,429.    Elapsed: 0:20:46.\n  Batch 5,680  of  7,429.    Elapsed: 0:20:55.\n  Batch 5,720  of  7,429.    Elapsed: 0:21:04.\n  Batch 5,760  of  7,429.    Elapsed: 0:21:13.\n  Batch 5,800  of  7,429.    Elapsed: 0:21:21.\n  Batch 5,840  of  7,429.    Elapsed: 0:21:30.\n  Batch 5,880  of  7,429.    Elapsed: 0:21:39.\n  Batch 5,920  of  7,429.    Elapsed: 0:21:48.\n  Batch 5,960  of  7,429.    Elapsed: 0:21:57.\n  Batch 6,000  of  7,429.    Elapsed: 0:22:05.\n  Batch 6,040  of  7,429.    Elapsed: 0:22:14.\n  Batch 6,080  of  7,429.    Elapsed: 0:22:23.\n  Batch 6,120  of  7,429.    Elapsed: 0:22:32.\n  Batch 6,160  of  7,429.    Elapsed: 0:22:41.\n  Batch 6,200  of  7,429.    Elapsed: 0:22:50.\n  Batch 6,240  of  7,429.    Elapsed: 0:22:58.\n  Batch 6,280  of  7,429.    Elapsed: 0:23:07.\n  Batch 6,320  of  7,429.    Elapsed: 0:23:16.\n  Batch 6,360  of  7,429.    Elapsed: 0:23:25.\n  Batch 6,400  of  7,429.    Elapsed: 0:23:34.\n  Batch 6,440  of  7,429.    Elapsed: 0:23:43.\n  Batch 6,480  of  7,429.    Elapsed: 0:23:51.\n  Batch 6,520  of  7,429.    Elapsed: 0:24:00.\n  Batch 6,560  of  7,429.    Elapsed: 0:24:09.\n  Batch 6,600  of  7,429.    Elapsed: 0:24:18.\n  Batch 6,640  of  7,429.    Elapsed: 0:24:27.\n  Batch 6,680  of  7,429.    Elapsed: 0:24:35.\n  Batch 6,720  of  7,429.    Elapsed: 0:24:44.\n  Batch 6,760  of  7,429.    Elapsed: 0:24:53.\n  Batch 6,800  of  7,429.    Elapsed: 0:25:02.\n  Batch 6,840  of  7,429.    Elapsed: 0:25:11.\n  Batch 6,880  of  7,429.    Elapsed: 0:25:19.\n  Batch 6,920  of  7,429.    Elapsed: 0:25:28.\n  Batch 6,960  of  7,429.    Elapsed: 0:25:37.\n  Batch 7,000  of  7,429.    Elapsed: 0:25:46.\n  Batch 7,040  of  7,429.    Elapsed: 0:25:55.\n  Batch 7,080  of  7,429.    Elapsed: 0:26:03.\n  Batch 7,120  of  7,429.    Elapsed: 0:26:12.\n  Batch 7,160  of  7,429.    Elapsed: 0:26:21.\n  Batch 7,200  of  7,429.    Elapsed: 0:26:30.\n  Batch 7,240  of  7,429.    Elapsed: 0:26:39.\n  Batch 7,280  of  7,429.    Elapsed: 0:26:47.\n  Batch 7,320  of  7,429.    Elapsed: 0:26:56.\n  Batch 7,360  of  7,429.    Elapsed: 0:27:05.\n  Batch 7,400  of  7,429.    Elapsed: 0:27:14.\n\n  Average training loss: 0.02\n  Training epcoh took: 0:27:20\n\nRunning Validation...\n  Accuracy: 1.00\n  Validation Loss: 0.01\n  Validation took: 0:00:51\n\nTraining complete!\nTotal training took 0:28:11 (h:mm:ss)\nSaving model to ./model_save/\n","name":"stdout"}]},{"metadata":{"id":"cbGhj6JGuFab","trusted":true},"cell_type":"code","source":"if False: ## 추가적인 fine-tuning\n    #sentences,labels = collect_training_dataset_for_grammar_discriminator(ko_sentences_dataset)\n    #print(len(sentences))\n    #g_discriminator = Grammar_Discriminator()\n    #g_discriminator.set_dataset(sentences,labels)\n    g_discriminator.train(epochs=1)\n    g_discriminator.save_model()","execution_count":17,"outputs":[]},{"metadata":{"id":"d96kaCAHKuUc"},"cell_type":"markdown","source":"##4.3 Static similarity discriminator class"},{"metadata":{"id":"xZDpXe7XKxeg","trusted":true},"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nfrom transformers import BertTokenizer\nfrom scipy.signal import find_peaks\nimport matplotlib.pyplot as plt\nfrom scipy.misc import electrocardiogram\nimport scipy\n\n\nclass Similarity_Discriminator:\n    '''\n    _instance = None\n    _embedder = None\n    def __new__(cls,pre_trained_model_name='stsb-roberta-large'):\n        if cls._instance is None:\n            print('Creating Similarity_Discriminator object')\n            cls._instance = super(Similarity_Discriminator, cls).__new__(cls)\n            # Put any initialization here.\n            cls._embedder = SentenceTransformer(pre_trained_model_name)\n        return cls._instance\n\n    '''\n\n    def __init__(self,pre_trained_model_name='xlm-r-large-en-ko-nli-ststb'):\n        print('Creating Similarity_Discriminator object')\n        # Put any initialization here.\n        self._embedder = SentenceTransformer(pre_trained_model_name)  \n        #self.cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n\n    def encode(self,texts):\n        return self._embedder.encode(texts,show_progress_bar=False)\n\n    def similarity(self, query_text, org_text_emb):\n        queries = nltk.sent_tokenize(query_text)\n        query_embeddings = self._embedder.encode(queries,show_progress_bar=False)\n        #query_embeddings = self._embedder.encode(queries,show_progress_bar=False)\n        #print(queries)\n        #print(org_text_emb)\n        \n        if len(query_embeddings) == 0:\n            return 0.0\n\n        cos_scores = scipy.spatial.distance.cdist(query_embeddings, org_text_emb, \"cosine\")\n        similarity_score = 1.0 - np.mean(np.min(cos_scores,axis=0))\n        '''\n        for query, query_embedding in zip(queries, query_embeddings):\n            distances = scipy.spatial.distance.cdist([query_embedding], [org_text_emb], \"cosine\")[0]\n            results = zip(range(len(distances)), distances)\n            for idx, distance in results:\n                scores.append(1-distance)\n        '''\n        return similarity_score  \n ","execution_count":18,"outputs":[]},{"metadata":{"id":"2sQZ36GuMumP"},"cell_type":"markdown","source":"###4.3.1 영어 문장 유사도 pre-trained model 적용"},{"metadata":{"id":"B9Miao14Muww","trusted":true,"outputId":"b51d668c-c75c-4796-ba6c-413af9eefac7"},"cell_type":"code","source":"s_discriminator = Similarity_Discriminator()\n#s_discriminator = Similarity_Discriminator()","execution_count":19,"outputs":[{"output_type":"stream","text":"Creating Similarity_Discriminator object\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 1.80G/1.80G [00:40<00:00, 44.7MB/s]\n","name":"stderr"}]},{"metadata":{"id":"Xnk9GsQ0K1t1"},"cell_type":"markdown","source":"##4.4 Document source class"},{"metadata":{"id":"-_ztc0q3M_4F"},"cell_type":"markdown","source":"###4.4.1 keyBERT를 위한 pre-trained model의 적재"},{"metadata":{"trusted":true,"id":"dEc9R82hi1gO"},"cell_type":"code","source":"#!pip install keybert","execution_count":20,"outputs":[]},{"metadata":{"id":"kJFTjWlwK3Uz","trusted":true},"cell_type":"code","source":"#from keybert import KeyBERT\n#key_model = KeyBERT('distilbert-base-nli-mean-tokens')","execution_count":21,"outputs":[]},{"metadata":{"id":"SnBm6RCvNIWG"},"cell_type":"markdown","source":"###4.4.2 frame term 추출을 위한 source class 구현"},{"metadata":{"id":"PsJKbtc2K4xN","trusted":true},"cell_type":"code","source":"\n\nclass Source:\n\n    def __init__(self,org_text):\n        self.org_text = org_text\n\n    def __crean_text(self, txt):\n        txt = txt.replace('\\n',' ')\n        txt = txt.replace('\\r',' ')    \n        txt = txt.replace('=','')\n        txt = txt.replace('\\\"','')   \n        txt = txt.replace('\\'','')\n        #txt = txt.replace(',','')\n        txt = txt.replace('..','')\n        txt = txt.replace('...','')\n        txt = txt.replace(' .','.')\n        txt = txt.replace('.','. ')\n        txt = txt.replace('  ',' ')\n        txt = txt.replace('  ',' ')    \n        txt = txt.replace('  ',' ')   \n        txt = txt.replace('  ',' ')           \n        txt = txt.replace('  ',' ')\n        txt = txt.replace('  ',' ')    \n        txt = txt.replace('  ',' ')   \n        txt = txt.replace('  ',' ')           \n        return txt.strip()\n\n\n    def extract_keywords(self,s_discriminator,key_model,comp_rate=0.2):\n        self.org_text = self.__crean_text(self.org_text.strip())\n        print('------------------------------------------------------------------')\n        print(self.org_text)\n        print('------------------------------------------------------------------')\n        self.org_sentences = np.array(nltk.sent_tokenize(self.org_text))\n        self.org_term_set = (' ' + self.org_text + ' ').split(' ')\n        self.org_source_length = len(self.org_term_set)\n        self.term_table = {}\n        #morp_table = {}\n        index_table = {}\n        for index, word in zip(range(len(self.org_term_set)),self.org_term_set):\n            self.term_table[index] = word\n        '''\n        print('Token table of origin text')\n        print('---------------------------------------------')\n        print(' Code     Token     ')\n        for k in self.term_table.keys():\n            print( f'  {str(k).ljust(5)}     {self.term_table[k]}')\n        print('---------------------------------------------')\n        '''\n        self.s_discriminator = s_discriminator\n        # 원문의 embedding...\n        self.org_text_emb = self.s_discriminator.encode(self.org_sentences)\n        '''\n        # weight 들의 초기화\n        terms = np.array(list(self.term_table.values()))\n\n        word_filters=np.array([[0]])\n\n        story_weights = np.zeros(self.org_source_length,)\n        word_weights = np.zeros(self.org_source_length,)\n\n        #terms = np.array(list(self.term_table.values()))\n\n        # story에 지배적인 word를 찾는다.\n        # 먼저 word의 강세 분석\n        for filter in word_filters:\n            #print(filter)\n            last_idx = len(terms)-(max(filter)+1)\n            pb = ProgressBar(last_idx,prefix='Frame token scan:')\n            for conv in range(last_idx,0,-1):\n                pb.printProgress(+1,f'filer:{filter} {conv}/{last_idx}       ')\n                t = np.array(filter) + conv\n                part_sen = ' '.join(terms[t]) \n                #print('\\n part_sen:',part_sen)\n                score = self.s_discriminator.similarity(part_sen.strip(),self.org_text_emb)\n                word_weights[t] += score \n\n        # story의 강세 분석\n        for filter in story_filters:\n            #print(filter)\n            last_idx = len(terms)-(max(filter)+1)\n            pb = ProgressBar(last_idx,prefix='Frame token scan:')\n            for conv in range(last_idx,0,-1):\n                pb.printProgress(+1,f'filer:{filter} {conv}/{last_idx}       ')\n                t = np.array(filter) + conv\n                part_sen = ' '.join(terms[t]) \n                score = self.s_discriminator.similarity(part_sen.strip(),self.org_text_emb)\n                story_weights[t] += score\n\n        #각각의 peak를 산출\n        word_peaks, _ = find_peaks(word_weights, height=0)\n        story_peaks, _ = find_peaks(story_weights, height=0)\n\n        #두개의 peak가 겹치는 word에 대해 한개 word가 유사도에 미치는 영향이 큰것으로 간주\n        #해당 word를 유사도 판단 필터에서 제외하고 다시 필터링...\n        #이를 통해 story에 대한 word를 최대한 추출 한다.\n\n        dup_order = []\n        for i in range(self.org_source_length):\n            #lst = \"\"\n            if (i in word_peaks) and (i in story_peaks):\n                if terms[i].endswith('.'):\n                    pass\n                else:\n                    dup_order.append(i)\n                    \n        # Story에 대한 weight을 추출하기 위해, word에 유독 강세가 있는 term을 제외 시킨다.\n        print('Negative tokens:',terms[dup_order])\n        '''\n\n        top_n = int(len(self.term_table) * comp_rate)\n\n        self.story_peaks = []\n        keywords = key_model.extract_keywords(self.org_text,top_n=top_n)\n        #print('keywords len',len(keywords))\n        #print('keywords',keywords)\n        for keyword,p in keywords:\n            for k in self.term_table.keys():\n                if self.term_table[k] == keyword: # and k not in dup_order:\n                    self.story_peaks.append(k)\n\n        self.story_peaks.append(len(self.term_table)-2)\n        self.story_peaks = np.sort(np.asarray(self.story_peaks))\n        print('story_peaks:',self.story_peaks)\n        print('Peak count:',len(self.story_peaks))          \n\n\n        # story skeleton 추출\n        self.frame_text = \"\"\n        for k in self.story_peaks:\n            #print(k,term_weight[k],word_table[k])\n            self.frame_text += self.term_table[k]+' '  \n\n        print('Frame tokens:',self.frame_text)\n        print('')\n        print(f'Similarity : {self.s_discriminator.similarity(self.frame_text.strip(),self.org_text_emb)}')    \n\n    def set_key_rate(self,s_discriminator,comp_rate=0.2):\n        self.org_text = self.__crean_text(self.org_text.strip())\n        print('------------------------------------------------------------------')\n        print(self.org_text)\n        print('------------------------------------------------------------------')\n        self.org_sentences = np.array(nltk.sent_tokenize(self.org_text))\n        self.org_term_set = (' ' + self.org_text + ' ').split(' ')\n        self.org_source_length = len(self.org_term_set)\n        self.term_table = {}\n        #morp_table = {}\n\n        for index, word in zip(range(len(self.org_term_set)),self.org_term_set):\n            self.term_table[index] = word\n\n        self.s_discriminator = s_discriminator\n        # 원문의 embedding...\n        self.org_text_emb = self.s_discriminator.encode(self.org_sentences)\n        top_n = int(len(self.term_table) * comp_rate)\n        #print('top_n',top_n)\n        self.story_peaks = [i+1 for i in range(top_n)]\n\n    def analysis_frame_terms(self,s_discriminator,story_filters=np.array([[0,1],[0,1,2],[0,1,2,3]]),peak_base_line = 0.0,comp_rate=0.2,except_key=True,display=False):\n\n        self.org_text = self.__crean_text(self.org_text.strip())\n        print('------------------------------------------------------------------')\n        print(self.org_text)\n        print('------------------------------------------------------------------')\n        self.org_sentences = np.array(nltk.sent_tokenize(self.org_text))\n        self.org_term_set = (' ' + self.org_text + ' ').split(' ')\n        self.org_source_length = len(self.org_term_set)\n        self.term_table = {}\n        #morp_table = {}\n\n        for index, word in zip(range(len(self.org_term_set)),self.org_term_set):\n            self.term_table[index] = word\n        '''\n        print('Token table of origin text')\n        print('---------------------------------------------')\n        print(' Code     Token     ')\n        for k in self.term_table.keys():\n            print( f'  {str(k).ljust(5)}     {self.term_table[k]}')\n        print('---------------------------------------------')\n        '''\n\n        self.s_discriminator = s_discriminator\n        # 원문의 embedding...\n        self.org_text_emb = self.s_discriminator.encode(self.org_sentences)\n\n        # weight 들의 초기화\n        terms = np.array(list(self.term_table.values()))\n\n        word_filters=np.array([[0]])\n\n        story_weights = np.zeros(self.org_source_length,)\n        word_weights = np.zeros(self.org_source_length,)\n\n        #terms = np.array(list(self.term_table.values()))\n\n        if except_key:\n            # story에 지배적인 word를 찾는다.\n            # 먼저 word의 강세 분석\n            for filter in word_filters:\n                #print(filter)\n                last_idx = len(terms)-(max(filter)+1)\n                pb = ProgressBar(last_idx,prefix='Frame token scan:')\n                for conv in range(last_idx,0,-1):\n                    pb.printProgress(+1,f'filer:{filter} {conv}/{last_idx}       ')\n                    t = np.array(filter) + conv\n                    part_sen = ' '.join(terms[t]) \n                    score = self.s_discriminator.similarity(part_sen.strip(),self.org_text_emb)\n                    word_weights[t] += score \n\n            # story의 강세 분석\n            for filter in story_filters:\n                #print(filter)\n                last_idx = len(terms)-(max(filter)+1)\n                pb = ProgressBar(last_idx,prefix='Frame token scan:')\n                for conv in range(last_idx,0,-1):\n                    pb.printProgress(+1,f'filer:{filter} {conv}/{last_idx}       ')\n                    t = np.array(filter) + conv\n                    part_sen = ' '.join(terms[t]) \n                    score = self.s_discriminator.similarity(part_sen.strip(),self.org_text_emb)\n                    story_weights[t] += score\n\n            #각각의 peak를 산출\n            word_peaks, _ = find_peaks(word_weights, height=0)\n            story_peaks, _ = find_peaks(story_weights, height=0)\n\n            #두개의 peak가 겹치는 word에 대해 한개 word가 유사도에 미치는 영향이 큰것으로 간주\n            #해당 word를 유사도 판단 필터에서 제외하고 다시 필터링...\n            #이를 통해 story에 대한 word를 최대한 추출 한다.\n\n            dup_order = []\n            for i in range(self.org_source_length):\n                #lst = \"\"\n                if (i in word_peaks) and (i in story_peaks):\n                    if terms[i].endswith('.'):\n                        pass\n                    else:\n                        dup_order.append(i)\n                        \n            # Story에 대한 weight을 추출하기 위해, word에 유독 강세가 있는 term을 제외 시킨다.\n            print('Negative tokens:',terms[dup_order])\n            if except_key:\n                terms[dup_order] = '---'\n        '''\n        print('Token table of origin text')\n        print('---------------------------------------------')\n        print(' Code         Token      ')\n        print('')\n        for index, word in zip(range(len(terms)),terms):\n            print( f'  {str(index).ljust(8)}    {word}')\n        print('---------------------------------------------')\n        '''\n        self.story_weights = np.zeros(self.org_source_length,)\n        # 그리고 다시 story 분석 스캔\n        for filter in story_filters:\n            #print(filter)\n            last_idx = len(terms)-(max(filter)+1)\n            pb = ProgressBar(last_idx,prefix='Frame token scan:')\n            for conv in range(last_idx):\n                pb.printProgress(+1,f'filer:{filter} {conv}/{last_idx}       ')\n                t = np.array(filter) + conv\n                part_sen = ' '.join(terms[t]) \n                #part_sen = part_sen.replace('소녀','---')\n                score = self.s_discriminator.similarity(part_sen.strip(),self.org_text_emb)\n                self.story_weights[t] += score        \n\n\n        # base line\n        base_line = peak_base_line\n        # 다시 peak 추출\n        story_peaks, _ = find_peaks(self.story_weights, height=base_line)\n\n        top_n = int(len(self.term_table) * comp_rate)\n\n        if len(story_peaks) > top_n:\n            peak_dict = {}\n            for i,peak in zip(range(len(story_peaks)),story_peaks):\n                peak_dict[peak] = self.story_weights[peak]\n            #print(peak_dict)\n            peaks = sorted(peak_dict, key=peak_dict.get, reverse=True)\n            #print(peaks)\n            peaks = peaks[:top_n]\n            #print(peaks)\n            peaks.sort()\n            story_peaks = peaks\n            #print(story_peaks)\n\n        #print('top_n:',top_n,'story_peaks:',len(story_peaks))\n        #print(story_peaks)\n        \n        self.story_peaks = np.append(story_peaks,len(story_weights)-2)\n        #print(self.story_peaks)\n        # story density 표출\n        if display:\n            plt.figure(figsize=(12, 6))\n            plt.plot(self.story_weights)\n            plt.plot(self.story_peaks, self.story_weights[self.story_peaks], \"x\")\n            plt.plot(np.zeros_like(self.story_weights)+base_line, \"--\", color=\"gray\")\n            plt.show() \n        print('Peak count:',len(self.story_peaks))          \n\n\n        # story skeleton 추출\n        self.frame_text = \"\"\n        for k in self.story_peaks:\n            #print(k,term_weight[k],word_table[k])\n            self.frame_text += self.term_table[k]+' '  \n\n        print('Frame tokens:',self.frame_text)\n        print('')\n        print(f'Similarity : {self.s_discriminator.similarity(self.frame_text.strip(),self.org_text_emb)}')      \n        ''' \n        for index, word in zip(range(len(self.org_term_set)),self.org_term_set):\n            self.term_table[index] = word\n   \n        print('Token table of origin text')\n        print('---------------------------------------------')\n        print(' Code     Score        Token              ')\n        print('')\n        for k in self.term_table.keys(): \n            print( f'  {str(k).ljust(5)}   {str(round(self.story_weights[k],4)).ljust(8)}  {self.term_table[k]}')\n\n        print('---------------------------------------------') \n        '''\n    def get_org_sample(self, num):\n        return self.org_sentences[np.random.choice(len(self.org_sentences), num)]\n\n    def get_source_embedded_code(self):\n        return self.org_text_emb","execution_count":22,"outputs":[]},{"metadata":{"id":"7XY59mdNK8ub"},"cell_type":"markdown","source":"##4.5 Generator class"},{"metadata":{"id":"M5CLF3WcK6lp","trusted":true},"cell_type":"code","source":"from functools import reduce\n\nclass Generator(nn.Module):\n    \"\"\"\n        Simple Generator w/ MLP\n    \"\"\"\n    def __init__(self, input_size=1024):\n        super(Generator, self).__init__()\n        self.layer = nn.Sequential(\n            nn.Linear(input_size, input_size*2),\n            nn.LeakyReLU(0.2),\n            nn.Linear(input_size*2, input_size*3),\n            nn.LeakyReLU(0.2),\n            nn.Linear(input_size*3, input_size*3),\n            nn.LeakyReLU(0.2),            \n            nn.Linear(input_size*3, input_size*2),\n            nn.LeakyReLU(0.2),\n            nn.Linear(input_size*2, input_size),\n            #nn.BatchNorm1d(term_length*4),\n            nn.Tanh() # -1 ~ 1\n        )\n    '''\n    def forward(self, x, story_peaks, bias):\n        #biased_noise = torch.randn(N,_NOISE_DIM)\n        # stroy peak에 해당하는 term에게 평균값에 해당하는 bias를 추가 한다.\n                 \n        y_ = self.layer(x)\n        y_[:,story_peaks] += bias\n        y_ = nn.Sigmoid()(y_)\n        #reduce(torch.add, [y_,bias]) / 2\n        return y_\n    '''\n\n    \n    def forward(self, x, bias):\n        #biased_noise = torch.randn(N,_NOISE_DIM)\n        # stroy peak에 해당하는 term에게 평균값에 해당하는 bias를 추가 한다.\n                 \n        y_ = self.layer(x)\n        y = torch.add(y_,bias)\n        #y = nn.Sigmoid()(y)\n\n        return y, y_\n\n    '''    \n    def forward(self, x):\n        #biased_noise = torch.randn(N,_NOISE_DIM)\n        # stroy peak에 해당하는 term에게 평균값에 해당하는 bias를 추가 한다.\n                 \n        y_ = self.layer(x)\n        #y = torch.add(y_,bias)\n        y = nn.Sigmoid()(y_)\n\n        return y, y_    \n    '''    ","execution_count":23,"outputs":[]},{"metadata":{"id":"GLVVmQdxLBHZ"},"cell_type":"markdown","source":"##4.6 Summarizer class (GAN training)"},{"metadata":{"id":"Nd8GTS7HKz1H","trusted":true},"cell_type":"code","source":"import random\nimport math\nimport numpy as np\nfrom scipy.special import expit\n\nclass SAM_Summarizer:\n\n    def __init__(self,g_discriminator,s_discriminator):\n        self.g_discriminator = g_discriminator\n        self.s_discriminator = s_discriminator\n        self.m = nn.Sigmoid()\n\n    def ready(self,source):\n        self.source = source  \n        #self.source.analysis_frame_terms(self.s_discriminator)\n        self.generator = Generator(input_size=self.source.org_source_length)\n\n        return self\n\n    def summarize(self,epochs=10,batch_size=2,frame_expansion_ratio = 0.8,init_bias = 1.0,learning_rate=2e-4, display = False):\n        self.frame_expansion_ratio = frame_expansion_ratio\n        history = self.__train(epochs,batch_size,init_bias,learning_rate,display)\n        if display:\n            plt.figure(figsize=(12, 6))\n            plt.plot(history['gen_g_loss'],label='generator grammar loss')\n            plt.plot(history['gen_s_loss'],label='generator similarity loss')\n            #if 'dis_loss' in history:\n            #    plt.plot(history['dis_loss'],label='discriminator grammar loss')\n            plt.legend()\n            plt.show()\n\n        return self\n\n    # text의 생성 for torch\n    def __text_gen2(self, noise, gen_length):\n        gtext = []\n        sorted_noise, i = torch.sort(noise, descending=True)\n        order, i = torch.sort(i[:gen_length], descending=False)\n        #print(len(order))\n        assert len(order) == gen_length\n        order = order.cpu().detach().numpy()\n        for k in order:\n            gtext.append((self.source.term_table[k],k))\n        return gtext\n\n    def __discrete_gradient(self,weights,gen_length,beta,use_gpu=False, verbose=0):\n        fake_gen_out = torch.zeros(weights.shape).to(device)\n        fake_sim_out = torch.zeros(weights.shape).to(device)\n\n        real_text = self.source.get_org_sample(weights.shape[0])\n        fake_outs = []\n        real_outs = []\n        apply_order = []\n        for i, noise in enumerate(weights):\n            gtext = self.__text_gen2(noise,gen_length)\n            tw = \"\"\n            tk = []\n            fake_scores = []\n            for (w,k) in gtext:\n                tw += w + ' '\n                tk.append(k)\n                if w.endswith('.'):\n                    fake_outs.append(tw.strip())\n                    real_outs.append(real_text[i])\n                    apply_order.append((i,tk))\n                    tw = \"\"\n                    tk = []\n                    \n            if len(tk) > 0:\n                fake_outs.append(tw.strip())\n                real_outs.append(real_text[i])\n                apply_order.append((i,tk))\n\n        D_z_loss, fake_gmr_out=self.g_discriminator.transfer_learning(fake_outs,train_for = False)\n        D_x_loss, real_gmr_out=self.g_discriminator.transfer_learning(real_outs,train_for = True)   # not use of 'real_gmr_out'\n\n        f_sim_out = []\n        for fake_text in fake_outs:\n            f_sim_out.append(self.s_discriminator.similarity(fake_text,self.source.org_text_emb))\n\n        #if use_gpu:\n        #    apply_order = torch.FloatTensor(apply_order).to(device)  \n        \n        #print(fake_dis_out)\n        \n        for j, (i,tk) in enumerate(apply_order):\n            #fake_gen_out[i,tk] += fake_dis_out[j].numpy() --> 이거는 tf 용...\n            #fake_gen_out[i,tk] += fake_dis_out[j] #.cpu().detach().numpy()\n            # \n            try:\n                #print('fake_gmr_out:',fake_gmr_out[j,1])\n                #print('real_gmr_out:',real_gmr_out[j,1])\n                #fake_gen_out[i,tk] += torch.sigmoid(fake_gmr_out[j,1])\n\n                fake_gen_out[i,tk] += torch.tanh( fake_gmr_out[j,1])\n                fake_sim_out[i,tk] += f_sim_out[j] * beta\n                \n            except Exception as ex:\n                print(j,i,tk)\n                print(fake_gmr_out)\n                raise ex\n\n        return fake_gen_out, fake_sim_out, D_z_loss, D_x_loss\n\n\n    def __train(self, epochs=10,batch_size=10,init_bias = 1.0,learning_rate=2e-4, display = False):\n        # In the Deepmind paper they use RMSProp however then Adam optimizer\n        # improves training time\n        #generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n        # This method returns a helper function to compute cross entropy loss\n        #cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\n        # Set the seed value all over the place to make this reproducible.\n        seed_val = 10\n\n        random.seed(seed_val)\n        np.random.seed(seed_val)\n        torch.manual_seed(seed_val)\n        torch.cuda.manual_seed_all(seed_val)\n        \n        criterion = nn.BCELoss()\n        #D_opt = torch.optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n        G_opt = torch.optim.Adam(self.generator.parameters(), lr=learning_rate)\n        D1_opt = AdamW(self.g_discriminator.discriminator.parameters(),\n                        lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n                        eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n                        )\n\n        \n        gen_length = len(self.source.story_peaks) + int(len(self.source.story_peaks)*self.frame_expansion_ratio)\n        pb = ProgressBar(epochs,prefix='Train...')\n        gen_gmr_loss_history = []\n        gen_sim_loss_history = []\n        dis_loss_history = []    \n\n        #model 들은 cuda로 보낸다.\n        self.g_discriminator.discriminator.to(device)\n        self.g_discriminator.discriminator.eval() # 학습하지 않는다...\n\n        self.generator.to(device)       \n        self.generator.train()\n\n        self.bias_w = init_bias\n        initial_bias = 0\n        G_s_loss = torch.tensor(0)\n        G_g_loss = torch.tensor(0)\n\n        beta = 1\n\n        for i in range(epochs):\n            '''\n            noise = torch.randn(batch_size,self.source.org_source_length).to(device)\n            bias = torch.zeros_like(noise).to(device)\n            bias[:,self.source.story_peaks] += self.bias_w \n            with torch.no_grad():        \n                sw, sw0 = self.generator(noise,bias)\n\n            self.g_discriminator.discriminator.train()          #discriminator는 evaluation 모드로 전환\n            fake_gmr_out, fake_sim_out, D_z_loss, D_x_loss = self.__discrete_gradient(sw,gen_length)\n            \n            D_loss = D_x_loss + D_z_loss      \n\n            self.g_discriminator.discriminator.zero_grad()\n            D_loss.backward()\n            D1_opt.step()\n            self.g_discriminator.discriminator.eval()\n            '''\n            if True:\n                noise = torch.randn(batch_size,self.source.org_source_length).to(device)\n                bias = torch.zeros_like(noise).to(device)\n                bias[:,self.source.story_peaks] += self.bias_w\n\n                sw, sw0 = self.generator(noise,bias)\n\n                with torch.no_grad():                \n                    fake_gmr_out, fake_sim_out, D_z_loss, D_x_loss = self.__discrete_gradient(sw,gen_length,beta)\n                \n                '''\n                if int(i/10)%2 == 0:  # grammar와 similarity를 각각 한번씩 교대로 학습한다?\n                    sw1 = sw * fake_sim_out\n                    G_s_loss = -torch.mean(sw1)\n                    G_loss = G_s_loss    \n                else: #if i%2 == 1:\n                    sw1 = sw * fake_gmr_out\n                    G_g_loss = -torch.mean(sw1)\n                    G_loss = G_g_loss\n                '''\n                sw1 = sw * fake_sim_out\n                G_s_loss = -torch.mean(sw1)\n                sw2 = sw * fake_gmr_out\n                G_g_loss = -torch.mean(sw2)\n\n                G_loss =  G_g_loss + G_s_loss\n                \n                self.generator.zero_grad()\n                G_loss.backward()\n                #print('backward:')\n                G_opt.step()\n                #self.generator.eval()\n            #print('step:')\n            gen_gmr_loss_history.append(G_g_loss.cpu().detach().numpy())\n            gen_sim_loss_history.append(G_s_loss.cpu().detach().numpy())\n            #dis_loss_history.append(D_loss.cpu().detach().numpy())\n\n            beta = self.m(-(G_g_loss-G_s_loss)*10) * 4\n\n            if math.isnan(beta) or beta > 5:\n                beta = 1\n\n            pb.printProgress(+1,f'{i+1}/{epochs} epochs, beta:{beta} Generator / grammar loss:{G_g_loss}   similarity loss:{G_s_loss}') #,   Discriminator grammar_loss:{D_loss}        ')\n            \n            \n        self.generator.eval()\n        self.g_discriminator.discriminator.eval()\n        if display:\n            plt.figure(figsize=(12, 6))\n            plt.plot(sw0[0].cpu().detach().numpy(),label='before activation weights')\n            plt.plot(sw[0].cpu().detach().numpy(),label='after activation weights')\n            plt.plot(bias[0].cpu().detach().numpy(),label='bias weights')\n            plt.legend()        \n            plt.show()\n\n        return  {'gen_g_loss':gen_gmr_loss_history,'gen_s_loss':gen_sim_loss_history} #,'dis_loss':dis_loss_history }\n\n    def get_summary(self, count):\n        texts = []\n        self.generator.cpu()\n        self.generator.eval()\n        gen_length = len(self.source.story_peaks) + int(len(self.source.story_peaks)*self.frame_expansion_ratio)\n        noise = torch.randn(count,self.source.org_source_length)\n        bias = torch.zeros_like(noise)\n        #bias = torch.randn(1,self.source.org_source_length)\n        bias[:,self.source.story_peaks] += self.bias_w #self.last_bias_max.cpu().detach().numpy()\n        #bias = 0\n        with torch.no_grad():\n            sw,sw0 = self.generator(noise,bias)\n            #sw,sw0 = self.generator(noise)\n\n        for noise in sw:\n            gtext = self.__text_gen2(noise,gen_length)\n            text = ' '.join([w for (w,k) in gtext])\n            #print(text)\n            texts.append(text)\n        return texts","execution_count":24,"outputs":[]},{"metadata":{"id":"oCdfO9iuLH6D"},"cell_type":"markdown","source":"#5. Experiment"},{"metadata":{"id":"7_eAwIPLb4aj"},"cell_type":"markdown","source":"## 비교 대상 요약 알고리즘 준비"},{"metadata":{"id":"_7Ty6_5gb_zR","trusted":true},"cell_type":"code","source":"def similarity(query_text, org_text):\n    sentences = nltk.sent_tokenize(org_text)\n    #print(\"Num sentences:\", len(sentences))\n    querys = nltk.sent_tokenize(query_text)\n    #print(\"Num querys:\", len(querys))\n\n    #Compute the sentence embeddings\n    org_embeddings = s_discriminator._embedder.encode(sentences,show_progress_bar=False)\n    query_embeddings = s_discriminator._embedder.encode(querys,show_progress_bar=False)\n\n    #Compute the pair-wise cosine similarities\n    cos_scores = scipy.spatial.distance.cdist(query_embeddings, org_embeddings, \"cosine\")\n    similarity_score = 1.0 - np.mean(np.min(cos_scores,axis=0))\n\n    return similarity_score\n\ndef grammarity(text):\n    \n    input_ids = []\n    attention_masks = []\n\n    sentences = np.asarray(nltk.sent_tokenize(text))\n    # For every sentence...\n    for sent in sentences:\n        # `encode_plus` will:\n        #   (1) Tokenize the sentence.\n        #   (2) Prepend the `[CLS]` token to the start.\n        #   (3) Append the `[SEP]` token to the end.\n        #   (4) Map tokens to their IDs.\n        #   (5) Pad or truncate the sentence to `max_length`\n        #   (6) Create attention masks for [PAD] tokens.\n        encoded_dict = g_discriminator.tokenizer.encode_plus(\n                            sent,                      # Sentence to encode.\n                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                            max_length = 64,           # Pad & truncate all sentences.\n                            pad_to_max_length = True,\n                            return_attention_mask = True,   # Construct attn. masks.\n                            return_tensors = 'pt',     # Return pytorch tensors.\n                            truncation = True,\n                       )\n        # Add the encoded sentence to the list.    \n        input_ids.append(encoded_dict['input_ids'])\n\n        # And its attention mask (simply differentiates padding from non-padding).\n        attention_masks.append(encoded_dict['attention_mask'])\n\n    # Convert the lists into tensors.\n    input_ids = torch.cat(input_ids, dim=0).to(device)\n    attention_masks = torch.cat(attention_masks, dim=0).to(device)\n    g_discriminator.discriminator.to(device)\n    #if str(discriminator1.device) == 'cpu':\n    #    pass\n    #else:\n    #    input_ids = input_ids.to(device)\n    #    attention_masks = attention_masks.to(device)        \n\n    with torch.no_grad():        \n        outputs = g_discriminator.discriminator(input_ids, \n                               token_type_ids=None, \n                               attention_mask=attention_masks)\n    #return torch.sigmoid(outputs[0][:,1])\n    return torch.mean(outputs[0][:,1]).detach().cpu().numpy()\n    #return outputs","execution_count":25,"outputs":[]},{"metadata":{"id":"oLbWuwKXcMyk","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ndef evaluate(method_name, text, g_summ, org_text_1,org_text_2,org_text_3):\n    result = {}\n    result['method'] = [method_name]\n    org_text = org_text_1 + ' ' + org_text_2 + ' ' + org_text_3\n    result['comp ratio'] = [len(text)/len(org_text)]\n    result['intro'] = [similarity(text,org_text_1)]\n    result['body'] = [similarity(text,org_text_2)]\n    result['ending'] = [similarity(text,org_text_3)]\n    result['var'] = [np.var([result['intro'][0],result['body'][0],result['ending'][0]])]\n    result['total'] = [similarity(text,org_text)]\n    result['grammar'] = [np.tanh(float(grammarity(text)))]\n    #scores = scorer.score(g_summ,text)\n    #result['R1'] = [scores['rouge1'].fmeasure]\n    #result['R2'] = [scores['rouge2'].fmeasure]\n    #result['RL'] = [scores['rougeL'].fmeasure]\n    return pd.DataFrame(result),result","execution_count":26,"outputs":[]},{"metadata":{"id":"utotZ2vLcSSO","trusted":true},"cell_type":"code","source":"\"\"\"\nLexRank implementation\nSource: https://github.com/crabcamp/lexrank/tree/dev\n\"\"\"\n\nimport numpy as np\nfrom scipy.sparse.csgraph import connected_components\n\ndef degree_centrality_scores(\n    similarity_matrix,\n    threshold=None,\n    increase_power=True,\n):\n    if not (\n        threshold is None\n        or isinstance(threshold, float)\n        and 0 <= threshold < 1\n    ):\n        raise ValueError(\n            '\\'threshold\\' should be a floating-point number '\n            'from the interval [0, 1) or None',\n        )\n\n    if threshold is None:\n        markov_matrix = create_markov_matrix(similarity_matrix)\n\n    else:\n        markov_matrix = create_markov_matrix_discrete(\n            similarity_matrix,\n            threshold,\n        )\n\n    scores = stationary_distribution(\n        markov_matrix,\n        increase_power=increase_power,\n        normalized=False,\n    )\n\n    return scores\n\n\ndef _power_method(transition_matrix, increase_power=True):\n    eigenvector = np.ones(len(transition_matrix))\n\n    if len(eigenvector) == 1:\n        return eigenvector\n\n    transition = transition_matrix.transpose()\n\n    while True:\n        eigenvector_next = np.dot(transition, eigenvector)\n\n        if np.allclose(eigenvector_next, eigenvector):\n            return eigenvector_next\n\n        eigenvector = eigenvector_next\n\n        if increase_power:\n            transition = np.dot(transition, transition)\n\n\ndef connected_nodes(matrix):\n    _, labels = connected_components(matrix)\n\n    groups = []\n\n    for tag in np.unique(labels):\n        group = np.where(labels == tag)[0]\n        groups.append(group)\n\n    return groups\n\n\ndef create_markov_matrix(weights_matrix):\n    n_1, n_2 = weights_matrix.shape\n    if n_1 != n_2:\n        raise ValueError('\\'weights_matrix\\' should be square')\n\n    row_sum = weights_matrix.sum(axis=1, keepdims=True)\n\n    return weights_matrix / row_sum\n\n\ndef create_markov_matrix_discrete(weights_matrix, threshold):\n    discrete_weights_matrix = np.zeros(weights_matrix.shape)\n    ixs = np.where(weights_matrix >= threshold)\n    discrete_weights_matrix[ixs] = 1\n\n    return create_markov_matrix(discrete_weights_matrix)\n\n\ndef graph_nodes_clusters(transition_matrix, increase_power=True):\n    clusters = connected_nodes(transition_matrix)\n    clusters.sort(key=len, reverse=True)\n\n    centroid_scores = []\n\n    for group in clusters:\n        t_matrix = transition_matrix[np.ix_(group, group)]\n        eigenvector = _power_method(t_matrix, increase_power=increase_power)\n        centroid_scores.append(eigenvector / len(group))\n\n    return clusters, centroid_scores\n\n\ndef stationary_distribution(\n    transition_matrix,\n    increase_power=True,\n    normalized=True,\n):\n    n_1, n_2 = transition_matrix.shape\n    if n_1 != n_2:\n        raise ValueError('\\'transition_matrix\\' should be square')\n\n    distribution = np.zeros(n_1)\n\n    grouped_indices = connected_nodes(transition_matrix)\n\n    for group in grouped_indices:\n        t_matrix = transition_matrix[np.ix_(group, group)]\n        eigenvector = _power_method(t_matrix, increase_power=increase_power)\n        distribution[group] = eigenvector\n\n    if normalized:\n        distribution /= n_1\n\n    return distribution","execution_count":27,"outputs":[]},{"metadata":{"id":"M25NP9gOeX15"},"cell_type":"markdown","source":"\n* Hands-on Guide To Extractive Text Summarization With BERTSum<br>\nhttps://analyticsindiamag.com/hands-on-guide-to-extractive-text-summarization-with-bertsum/ <br>\nhttps://pypi.org/project/bert-extractive-summarizer/"},{"metadata":{"id":"c9oEW5wyeI9C","trusted":true,"outputId":"15dbda76-747d-44de-b135-d2d83e95dfdd"},"cell_type":"code","source":"!pip install bert-extractive-summarizer","execution_count":28,"outputs":[{"output_type":"stream","text":"Collecting bert-extractive-summarizer\n  Downloading bert_extractive_summarizer-0.7.1-py3-none-any.whl (18 kB)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from bert-extractive-summarizer) (0.24.1)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (from bert-extractive-summarizer) (3.0.2)\nRequirement already satisfied: spacy in /opt/conda/lib/python3.7/site-packages (from bert-extractive-summarizer) (2.3.5)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->bert-extractive-summarizer) (2.1.0)\nRequirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->bert-extractive-summarizer) (1.5.4)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->bert-extractive-summarizer) (1.0.0)\nRequirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->bert-extractive-summarizer) (1.19.5)\nRequirement already satisfied: thinc<7.5.0,>=7.4.1 in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (7.4.5)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (4.55.1)\nRequirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (1.1.3)\nRequirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (0.8.2)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (3.0.5)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (2.25.1)\nRequirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (1.0.5)\nRequirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (1.0.0)\nRequirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (0.7.4)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (1.0.5)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (2.0.5)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (49.6.0.post20201009)\nRequirement already satisfied: importlib-metadata>=0.20 in /opt/conda/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy->bert-extractive-summarizer) (3.3.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy->bert-extractive-summarizer) (3.4.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy->bert-extractive-summarizer) (3.7.4.3)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (3.0.4)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2.10)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (1.26.2)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2020.12.5)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers->bert-extractive-summarizer) (20.8)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers->bert-extractive-summarizer) (2020.11.13)\nRequirement already satisfied: tokenizers==0.8.1.rc1 in /opt/conda/lib/python3.7/site-packages (from transformers->bert-extractive-summarizer) (0.8.1rc1)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers->bert-extractive-summarizer) (0.0.43)\nRequirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.7/site-packages (from transformers->bert-extractive-summarizer) (0.1.95)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers->bert-extractive-summarizer) (3.0.12)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers->bert-extractive-summarizer) (2.4.7)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers->bert-extractive-summarizer) (1.15.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers->bert-extractive-summarizer) (7.1.2)\nInstalling collected packages: bert-extractive-summarizer\nSuccessfully installed bert-extractive-summarizer-0.7.1\n","name":"stdout"}]},{"metadata":{"id":"MJb2hLPRPp7Q","trusted":true},"cell_type":"code","source":"def bert_lexrank_sum(g_summ,org_text,n_top=8):\n    input_text = org_text[0] + org_text[1] + org_text[2]\n    #Split the document into sentences\n    sentences = nltk.sent_tokenize(input_text)\n    #print(\"Num sentences:\", len(sentences))\n\n    #Compute the sentence embeddings\n    embeddings = s_discriminator._embedder.encode(sentences,show_progress_bar=False)\n\n    #Compute the pair-wise cosine similarities\n    cos_scores = scipy.spatial.distance.cdist(embeddings, embeddings, \"cosine\")\n    #util.pytorch_cos_sim(embeddings, embeddings).numpy()\n    #print(cos_scores)\n    #Compute the centrality for each sentence\n    centrality_scores = degree_centrality_scores(cos_scores, threshold=None)\n\n    #We argsort so that the first element is the sentence with the highest score\n    most_central_sentence_indices = np.argsort(-centrality_scores)\n\n    #Print the 5 sentences with the highest scores\n    summary_text = \"\"\n    for idx in most_central_sentence_indices[0:n_top]:\n        summary_text += sentences[idx].strip()\n    print('bert_lexrank summary:')\n    print(summary_text)\n    print('-'*50)\n    df,arr = evaluate('BERT+LexRank',summary_text,g_summ,org_text[0],org_text[1],org_text[2])\n    return df,arr\n","execution_count":29,"outputs":[]},{"metadata":{"id":"QcDVXR4XQZAh","trusted":true,"outputId":"c51de7d3-e17d-4379-804f-37bfcbebd9c3"},"cell_type":"code","source":"\nfrom summarizer import Summarizer\n\n\nmodel1 = Summarizer()\n\n\ndef besm(g_summ,org_text):\n    result = model1(org_text[0] + org_text[1] + org_text[2], num_sentences=5)\n    summary_text = \"\".join(result)\n    print('besm summary:')\n    print(summary_text)\n    print('-'*50)    \n    df,arr = evaluate('BESM',summary_text,g_summ,org_text[0],org_text[1],org_text[2])\n    return df,arr","execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/434 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb63847c82774fe2bdbc368632db9559"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.34G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3473f6b144ca44cc970587fa0837ebb8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb815ca5ed024941bcf67a017d4621ee"}},"metadata":{}}]},{"metadata":{"trusted":true,"id":"T24x_-DPi1gV"},"cell_type":"code","source":"from transformers import AutoConfig,AutoTokenizer,AutoModel\n\nSQUAD_MODEL = \"monologg/kobert\"\n\n#SQUAD_MODEL = \"bert-large-uncased\"\n# Load model, model config and tokenizer via Transformers\ncustom_config = AutoConfig.from_pretrained(SQUAD_MODEL)\ncustom_config.output_hidden_states=True\ncustom_tokenizer = AutoTokenizer.from_pretrained(SQUAD_MODEL)\ncustom_model = AutoModel.from_pretrained(SQUAD_MODEL, config=custom_config)\n\nmodel2 = Summarizer(custom_model=custom_model, custom_tokenizer=custom_tokenizer)\n\ndef besm_bert(g_summ,org_text):\n    result = model2(org_text[0].lower() + org_text[1].lower() + org_text[2].lower(), num_sentences=5)\n    summary_text = \"\".join(result)\n    print('besm_bert summary:')\n    print(summary_text)\n    print('-'*50)      \n    df,arr = evaluate('BESM+kobert',summary_text,g_summ,org_text[0],org_text[1],org_text[2])\n    return df,arr","execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/51.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b52f5fa9a4bb4e7dbecf478f645e0bd3"}},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"org_text_1 = \"\"\"\n옛날 어느 집에 귀여운 여자 아기가 태어났어요.\n아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요.\n그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요.\n소녀의 아버지는 홀로 남은 소녀가 걱정되었어요.\n그래서 얼마 후 새어머니를 맞이했어요.\n새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요.\n그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요.\n새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요.\n그런데 이번에는 아버지마저 돌아가셨어요.\n소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요.\n해도 해도 끝이 없는 집안일이 힘들어 지칠때면\n난롯가에 앉아서 잠시 쉬곤 했지요.\n\"\"\"\n\norg_text_2 = \"\"\"\n어느 날, 왕궁에서 무도회가 열렸어요.\n신데렐라의 집에도 초대장이 왔어요.\n새어머니는 언니들을 데리고 무도회장으로 떠났어요.\n신데렐라도 무도회에 가고 싶었어요.\n혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요.\n신데렐라, 너도 무도회에 가고 싶니?\n신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요.\n내가 너를 무도회에 보내주마 호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴.\n마법사 할머니가 주문을 외웠어요.\n그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요.\n이번에는 생쥐와 도마뱀을 건드렸어요.\n그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다.\n신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요.\n신데렐라, 발을 내밀어 보거라.\n할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요.\n신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼. 그러니까 반드시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지?\n왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요.\n왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,신데렐라하고만 춤을 추었어요.\n신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요.\n땡, 땡, 땡...... 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요.\n신데렐라가 허둥지둥 왕궁을 빠져나가는데,\n유리 구두 한 짝이 벗겨졌어요.\n하지만 구두를 주울 틈이 없었어요.\n신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요.\n왕자님은 유리 구두를 가지고 임금님께 가서 말했어요.\n이 유리 구두의 주인과 결혼하겠어요.\n\"\"\"\n\norg_text_3 = \"\"\"\n그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요.\n언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리 구두는 너무 작았어요.\n그때, 신데렐라가 조용히 다가와 말했어요.\n저도 한번 신어 볼 수 있나요?\n신데렐라는 신하게 건넨 유리 구두를 신었어요,\n유리 구두는 신데렐라의 발에 꼭 맞았어요.\n신하들은 신데렐라를 왕궁으로 데리고 갔어요.\n그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\n\"\"\"","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"urls = ['https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-ABC%20%EC%82%B4%EC%9D%B8%EC%82%AC%EA%B1%B4.txt',\n        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EA%B7%B8%EB%A6%AC%EA%B3%A0%20%EC%95%84%EB%AC%B4%EB%8F%84%20%EC%97%86%EC%97%88%EB%8B%A4.txt',\n        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EB%82%98%EC%9D%BC%EA%B0%95%EC%9D%98%20%EC%A3%BD%EC%9D%8C.txt',\n        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EB%A7%8C%EC%B0%AC%ED%9A%8C%EC%9D%98%2013%EC%9D%B8.txt',\n        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EB%A9%94%EC%86%8C%ED%8F%AC%ED%83%80%EB%AF%B8%EC%95%84%EC%9D%98%20%EC%A3%BD%EC%9D%8C.txt',\n        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EB%AA%A9%EC%82%AC%EA%B4%80%EC%82%B4%EC%9D%B8.txt',\n        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EB%B2%99%EC%96%B4%EB%A6%AC%20%EB%AA%A9%EA%B2%A9%EC%9E%90.txt',\n        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EB%B9%84%EB%B0%80%20%EC%84%9C%EB%A5%98%EB%A5%BC%20%EB%85%B8%EB%A0%A4%EB%9D%BC.txt',\n        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EC%8A%A4%ED%8E%98%EC%9D%B8%EA%B6%A4%EC%A7%9D%EC%9D%98%20%EB%B9%84%EB%B0%80.txt',\n        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EC%95%84%ED%8C%8C%ED%8A%B8%EC%97%90%20%EB%82%98%ED%83%80%EB%82%9C%20%EC%9A%94%EC%A0%95.txt',\n        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EC%95%A0%ED%81%AC%EB%A1%9C%EC%9D%B4%EB%93%9C%20%EC%82%B4%EC%9D%B8%20%EC%82%AC%EA%B1%B4.txt',\n        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EC%98%88%EA%B3%A0%20%EC%82%B4%EC%9D%B8.txt',\n        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EC%A5%90%EB%8D%AB.txt',\n        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EC%BB%A4%ED%8A%BC.txt',\n        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%ED%81%AC%EB%A6%AC%EC%8A%A4%EB%A7%88%EC%8A%A4%20%ED%91%B8%EB%94%A9%EC%9D%98%20%EB%AA%A8%ED%97%98.txt',\n        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%ED%91%B8%EB%A5%B8%EC%97%B4%EC%B0%A8%EC%9D%98%EC%A3%BD%EC%9D%8C.txt',\n        'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%ED%99%94%EC%9A%94%EC%9D%BC%20%ED%81%B4%EB%9F%BD%EC%9D%98%20%EC%82%B4%EC%9D%B8.txt']\n\nko_sentences_dataset2 = []\nfor url in urls:\n    raw_text = urllib.request.urlopen(url).read().decode('utf-8')\n    ko_sentences_dataset2 += nltk.sent_tokenize(clean_text(raw_text))","execution_count":33,"outputs":[]},{"metadata":{"id":"3run5EVvMSiB","trusted":true,"outputId":"7f214733-a7da-41e4-f33a-6c75502629fd"},"cell_type":"code","source":"document = []\noffset = 0\ndocument += [[org_text_1,org_text_2,org_text_3]]\nwhile (offset < len(ko_sentences_dataset2)):\n    intro_cnt = random.choice([5,8,10,13])\n    body_cnt = random.choice([10,15,18,20,25])\n    conclu_cnt = random.choice([5,8,10,13])\n    intro = ' '.join(ko_sentences_dataset[offset:offset+intro_cnt])\n    body = ' '.join(ko_sentences_dataset[offset+intro_cnt:offset+intro_cnt+body_cnt])\n    conclu = ' '.join(ko_sentences_dataset[offset+intro_cnt+body_cnt:offset+intro_cnt+body_cnt+conclu_cnt])\n    offset = offset+intro_cnt+body_cnt+conclu_cnt\n    document.append([intro,body,conclu])\n\nprint(len(document))\n\ndocument[0]","execution_count":34,"outputs":[{"output_type":"stream","text":"2681\n","name":"stdout"},{"output_type":"execute_result","execution_count":34,"data":{"text/plain":"['\\n옛날 어느 집에 귀여운 여자 아기가 태어났어요.\\n아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요.\\n그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요.\\n소녀의 아버지는 홀로 남은 소녀가 걱정되었어요.\\n그래서 얼마 후 새어머니를 맞이했어요.\\n새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요.\\n그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요.\\n새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요.\\n그런데 이번에는 아버지마저 돌아가셨어요.\\n소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요.\\n해도 해도 끝이 없는 집안일이 힘들어 지칠때면\\n난롯가에 앉아서 잠시 쉬곤 했지요.\\n',\n '\\n어느 날, 왕궁에서 무도회가 열렸어요.\\n신데렐라의 집에도 초대장이 왔어요.\\n새어머니는 언니들을 데리고 무도회장으로 떠났어요.\\n신데렐라도 무도회에 가고 싶었어요.\\n혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요.\\n신데렐라, 너도 무도회에 가고 싶니?\\n신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요.\\n내가 너를 무도회에 보내주마 호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴.\\n마법사 할머니가 주문을 외웠어요.\\n그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요.\\n이번에는 생쥐와 도마뱀을 건드렸어요.\\n그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다.\\n신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요.\\n신데렐라, 발을 내밀어 보거라.\\n할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요.\\n신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼. 그러니까 반드시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지?\\n왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요.\\n왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,신데렐라하고만 춤을 추었어요.\\n신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요.\\n땡, 땡, 땡...... 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요.\\n신데렐라가 허둥지둥 왕궁을 빠져나가는데,\\n유리 구두 한 짝이 벗겨졌어요.\\n하지만 구두를 주울 틈이 없었어요.\\n신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요.\\n왕자님은 유리 구두를 가지고 임금님께 가서 말했어요.\\n이 유리 구두의 주인과 결혼하겠어요.\\n',\n '\\n그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요.\\n언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리 구두는 너무 작았어요.\\n그때, 신데렐라가 조용히 다가와 말했어요.\\n저도 한번 신어 볼 수 있나요?\\n신데렐라는 신하게 건넨 유리 구두를 신었어요,\\n유리 구두는 신데렐라의 발에 꼭 맞았어요.\\n신하들은 신데렐라를 왕궁으로 데리고 갔어요.\\n그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\\n']"},"metadata":{}}]},{"metadata":{"trusted":true,"id":"rcvSSH_Ri1gZ"},"cell_type":"code","source":"def sam_wgan(g_summ,org_text,init_bias=0.0, display = False):\n    source = Source(org_text[0] + org_text[1] + org_text[2])\n    if init_bias > 0:\n        source.analysis_frame_terms(s_discriminator,comp_rate=0.08,except_key=True,display=display)\n    else:\n        #source.extract_keywords(s_discriminator,key_model, comp_rate=0.1)\n        source.set_key_rate(s_discriminator,comp_rate=0.08)\n    summarizer = SAM_Summarizer(g_discriminator,s_discriminator)\n    summarizer.ready(source)\n    summarizer.summarize(epochs=500,batch_size=1,frame_expansion_ratio = 1.5, init_bias=init_bias,learning_rate=5e-5,display=display)\n    summary_text = summarizer.get_summary(3)[0]\n    print('-'*50)\n    print('gold summary:')\n    print(g_summ)    \n    print('-'*50)\n    print('sam_wgan summary:')\n    print(summary_text)\n    print('-'*50)\n    df,arr = evaluate('SAM+WGAN',summary_text,g_summ,org_text[0],org_text[1],org_text[2])\n    return df,arr","execution_count":37,"outputs":[]},{"metadata":{"id":"cCxKl8YVgF6F"},"cell_type":"markdown","source":"Test용 Data 준비"},{"metadata":{"trusted":true},"cell_type":"code","source":"def seeding(seed):\n\n    SEED = seed\n\n    random.seed(SEED)\n    np.random.seed(SEED)\n    torch.manual_seed(SEED)\n    torch.cuda.manual_seed(SEED)\n    torch.backends.cudnn.deterministic = True","execution_count":40,"outputs":[]},{"metadata":{"id":"vvigjmChLVPb","trusted":true,"outputId":"e69aafdd-0034-4cef-c918-5d5c0ed66d53"},"cell_type":"code","source":"pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n\n\n\nnum = 0\n\nfor i in range(11):\n    for j in range(3):\n        seeding(i+j)\n        df1,_ = sam_wgan('',document[num],init_bias=1.0-0.1*i,display= False)\n        #df2,_ = bert_lexrank_sum('',[org_text_1,org_text_2,org_text_3])\n        #df3,_ = besm('',[org_text_1,org_text_2,org_text_3])\n        #df4,_ = besm_bert('',[org_text_1,org_text_2,org_text_3])\n        #df5,_ = abstract_method_1(g_summ,[org_text_1,org_text_2,org_text_3])\n        #df6,_ = abstract_method_2(g_summ,[org_text_1,org_text_2,org_text_3])\n        #result = pd.concat([df1, df2, df3, df4], ignore_index=True)\n        #result\n        print(df1)\n","execution_count":39,"outputs":[{"output_type":"stream","text":"------------------------------------------------------------------\n옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요. 그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요. 소녀의 아버지는 홀로 남은 소녀가 걱정되었어요. 그래서 얼마 후 새어머니를 맞이했어요. 새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요. 그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요. 새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요. 그런데 이번에는 아버지마저 돌아가셨어요. 소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시 쉬곤 했지요. 어느 날, 왕궁에서 무도회가 열렸어요. 신데렐라의 집에도 초대장이 왔어요. 새어머니는 언니들을 데리고 무도회장으로 떠났어요. 신데렐라도 무도회에 가고 싶었어요. 혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요. 신데렐라, 너도 무도회에 가고 싶니? 신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요. 내가 너를 무도회에 보내주마 호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴. 마법사 할머니가 주문을 외웠어요. 그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요. 이번에는 생쥐와 도마뱀을 건드렸어요. 그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다. 신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요. 신데렐라, 발을 내밀어 보거라. 할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요. 신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼. 그러니까 반드시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지? 왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요. 왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,신데렐라하고만 춤을 추었어요. 신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요. 땡, 땡, 땡 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요. 신데렐라가 허둥지둥 왕궁을 빠져나가는데, 유리 구두 한 짝이 벗겨졌어요. 하지만 구두를 주울 틈이 없었어요. 신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요. 왕자님은 유리 구두를 가지고 임금님께 가서 말했어요. 이 유리 구두의 주인과 결혼하겠어요. 그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요. 언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리 구두는 너무 작았어요. 그때, 신데렐라가 조용히 다가와 말했어요. 저도 한번 신어 볼 수 있나요? 신데렐라는 신하게 건넨 유리 구두를 신었어요, 유리 구두는 신데렐라의 발에 꼭 맞았어요. 신하들은 신데렐라를 왕궁으로 데리고 갔어요. 그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\n------------------------------------------------------------------\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/327        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/326        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/325        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/324        \nNegative tokens: ['여자' '소녀가' '소녀의' '소녀가' '딸을' '새어머니와' '새어머니는' '집안일이' '신데렐라의' '신데렐라도'\n '신데렐라는' '신데렐라,' '신데렐라가' '할머니가' '무도회에' '할머니가' '마차로' '신데렐라의' '신데렐라,'\n '신데렐라에게' '신데렐라,' '흰말은' '신데렐라에게' '신데렐라는' '신데렐라는' '신데렐라가' '구두' '구두를'\n '신데렐라를' '구두' '왕자님은' '구두를' '구두의' '언니들은' '구두를' '신데렐라가' '신데렐라는' '신데렐라의'\n '신데렐라를' '신데렐라는']\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 325/326       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 324/325       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 323/324       \nPeak count: 27\nFrame tokens: 소녀의 맞이했어요. 언니들은 딸들보다 소녀는 왕궁에서 언니들을 무도회에 무도회에 마부로 드레스로 할머니는 구두를 왕자님도 무도회장에 않고,신데렐라하고만 춤을 왕궁을 왕자님은 결혼하겠어요. 발을 구두는 구두를 구두는 왕궁으로 결혼하여 살았대요. \n\nSimilarity : 0.48041270657702473\nTrain... |||||||||||||||||||||| 100.0%   500/500 epochs, beta:2.36873459815979 Generator / grammar loss:-0.2190435379743576   similarity loss:-0.1817435771226883664\n--------------------------------------------------\ngold summary:\n\n--------------------------------------------------\nsam_wgan summary:\n 옛날 태어났어요. 맞이했어요. 소녀보다 언니들은 심술쟁이들이었어요. 딸들보다 예쁘고 착한 소녀는 쓸고, 집안일을 해도 해도 난롯가에 앉아서 쉬곤 했지요. 어느 왕궁에서 열렸어요. 초대장이 언니들을 데리고 떠났어요. 신데렐라도 무도회에 가고 무도회에 가고 웃고 호박 한개와 도마뱀을 건드리자, 화려한 변했어요. 생쥐와 도마뱀을 마부로 신데렐라의 장식이 드레스로 내밀어 보거라. 할머니는 반짝반짝 유리 구두를 주었어요. 돼. 해. 모인 왕자님은 가서 결혼하겠어요. 발을 유리 구두는 수 구두를 구두는 왕궁으로 데리고 결혼하여 살았대요.\n--------------------------------------------------\n     method  comp ratio    intro      body    ending       var     total  \\\n0  SAM+WGAN    0.205801  0.56299  0.586069  0.581326  0.000099  0.579828   \n\n    grammar  \n0  0.971201  \n------------------------------------------------------------------\n옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요. 그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요. 소녀의 아버지는 홀로 남은 소녀가 걱정되었어요. 그래서 얼마 후 새어머니를 맞이했어요. 새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요. 그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요. 새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요. 그런데 이번에는 아버지마저 돌아가셨어요. 소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시 쉬곤 했지요. 어느 날, 왕궁에서 무도회가 열렸어요. 신데렐라의 집에도 초대장이 왔어요. 새어머니는 언니들을 데리고 무도회장으로 떠났어요. 신데렐라도 무도회에 가고 싶었어요. 혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요. 신데렐라, 너도 무도회에 가고 싶니? 신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요. 내가 너를 무도회에 보내주마 호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴. 마법사 할머니가 주문을 외웠어요. 그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요. 이번에는 생쥐와 도마뱀을 건드렸어요. 그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다. 신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요. 신데렐라, 발을 내밀어 보거라. 할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요. 신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼. 그러니까 반드시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지? 왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요. 왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,신데렐라하고만 춤을 추었어요. 신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요. 땡, 땡, 땡 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요. 신데렐라가 허둥지둥 왕궁을 빠져나가는데, 유리 구두 한 짝이 벗겨졌어요. 하지만 구두를 주울 틈이 없었어요. 신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요. 왕자님은 유리 구두를 가지고 임금님께 가서 말했어요. 이 유리 구두의 주인과 결혼하겠어요. 그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요. 언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리 구두는 너무 작았어요. 그때, 신데렐라가 조용히 다가와 말했어요. 저도 한번 신어 볼 수 있나요? 신데렐라는 신하게 건넨 유리 구두를 신었어요, 유리 구두는 신데렐라의 발에 꼭 맞았어요. 신하들은 신데렐라를 왕궁으로 데리고 갔어요. 그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\n------------------------------------------------------------------\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/327        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/326        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/325        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/324        \nNegative tokens: ['여자' '소녀가' '소녀의' '소녀가' '딸을' '새어머니와' '새어머니는' '집안일이' '신데렐라의' '신데렐라도'\n '신데렐라는' '신데렐라,' '신데렐라가' '할머니가' '무도회에' '할머니가' '마차로' '신데렐라의' '신데렐라,'\n '신데렐라에게' '신데렐라,' '흰말은' '신데렐라에게' '신데렐라는' '신데렐라는' '신데렐라가' '구두' '구두를'\n '신데렐라를' '구두' '왕자님은' '구두를' '구두의' '언니들은' '구두를' '신데렐라가' '신데렐라는' '신데렐라의'\n '신데렐라를' '신데렐라는']\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 325/326       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 324/325       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 323/324       \nPeak count: 27\nFrame tokens: 소녀의 맞이했어요. 언니들은 딸들보다 소녀는 왕궁에서 언니들을 무도회에 무도회에 마부로 드레스로 할머니는 구두를 왕자님도 무도회장에 않고,신데렐라하고만 춤을 왕궁을 왕자님은 결혼하겠어요. 발을 구두는 구두를 구두는 왕궁으로 결혼하여 살았대요. \n\nSimilarity : 0.48041270657702473\nTrain... |||||||||||||||||||||| 100.0%   500/500 epochs, beta:2.36873459815979 Generator / grammar loss:-0.2190435379743576   similarity loss:-0.1817435771226883664\n--------------------------------------------------\ngold summary:\n\n--------------------------------------------------\nsam_wgan summary:\n 옛날 태어났어요. 맞이했어요. 소녀보다 언니들은 심술쟁이들이었어요. 딸들보다 예쁘고 착한 소녀는 쓸고, 집안일을 해도 해도 난롯가에 앉아서 쉬곤 했지요. 어느 왕궁에서 열렸어요. 초대장이 언니들을 데리고 떠났어요. 신데렐라도 무도회에 가고 무도회에 가고 웃고 호박 한개와 도마뱀을 건드리자, 화려한 변했어요. 생쥐와 도마뱀을 마부로 신데렐라의 장식이 드레스로 내밀어 보거라. 할머니는 반짝반짝 유리 구두를 주었어요. 돼. 해. 모인 왕자님은 가서 결혼하겠어요. 발을 유리 구두는 수 구두를 구두는 왕궁으로 데리고 결혼하여 살았대요.\n--------------------------------------------------\n     method  comp ratio    intro      body    ending       var     total  \\\n0  SAM+WGAN    0.205801  0.56299  0.586069  0.581326  0.000099  0.579828   \n\n    grammar  \n0  0.971201  \n------------------------------------------------------------------\n옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요. 그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요. 소녀의 아버지는 홀로 남은 소녀가 걱정되었어요. 그래서 얼마 후 새어머니를 맞이했어요. 새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요. 그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요. 새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요. 그런데 이번에는 아버지마저 돌아가셨어요. 소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시 쉬곤 했지요. 어느 날, 왕궁에서 무도회가 열렸어요. 신데렐라의 집에도 초대장이 왔어요. 새어머니는 언니들을 데리고 무도회장으로 떠났어요. 신데렐라도 무도회에 가고 싶었어요. 혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요. 신데렐라, 너도 무도회에 가고 싶니? 신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요. 내가 너를 무도회에 보내주마 호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴. 마법사 할머니가 주문을 외웠어요. 그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요. 이번에는 생쥐와 도마뱀을 건드렸어요. 그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다. 신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요. 신데렐라, 발을 내밀어 보거라. 할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요. 신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼. 그러니까 반드시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지? 왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요. 왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,신데렐라하고만 춤을 추었어요. 신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요. 땡, 땡, 땡 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요. 신데렐라가 허둥지둥 왕궁을 빠져나가는데, 유리 구두 한 짝이 벗겨졌어요. 하지만 구두를 주울 틈이 없었어요. 신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요. 왕자님은 유리 구두를 가지고 임금님께 가서 말했어요. 이 유리 구두의 주인과 결혼하겠어요. 그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요. 언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리 구두는 너무 작았어요. 그때, 신데렐라가 조용히 다가와 말했어요. 저도 한번 신어 볼 수 있나요? 신데렐라는 신하게 건넨 유리 구두를 신었어요, 유리 구두는 신데렐라의 발에 꼭 맞았어요. 신하들은 신데렐라를 왕궁으로 데리고 갔어요. 그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\n------------------------------------------------------------------\n","name":"stdout"},{"output_type":"stream","text":"Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/327        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/326        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/325        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/324        \nNegative tokens: ['여자' '소녀가' '소녀의' '소녀가' '딸을' '새어머니와' '새어머니는' '집안일이' '신데렐라의' '신데렐라도'\n '신데렐라는' '신데렐라,' '신데렐라가' '할머니가' '무도회에' '할머니가' '마차로' '신데렐라의' '신데렐라,'\n '신데렐라에게' '신데렐라,' '흰말은' '신데렐라에게' '신데렐라는' '신데렐라는' '신데렐라가' '구두' '구두를'\n '신데렐라를' '구두' '왕자님은' '구두를' '구두의' '언니들은' '구두를' '신데렐라가' '신데렐라는' '신데렐라의'\n '신데렐라를' '신데렐라는']\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 325/326       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 324/325       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 323/324       \nPeak count: 27\nFrame tokens: 소녀의 맞이했어요. 언니들은 딸들보다 소녀는 왕궁에서 언니들을 무도회에 무도회에 마부로 드레스로 할머니는 구두를 왕자님도 무도회장에 않고,신데렐라하고만 춤을 왕궁을 왕자님은 결혼하겠어요. 발을 구두는 구두를 구두는 왕궁으로 결혼하여 살았대요. \n\nSimilarity : 0.48041270657702473\nTrain... |||||||||||||||||||||| 100.0%   500/500 epochs, beta:2.36873459815979 Generator / grammar loss:-0.2190435379743576   similarity loss:-0.1817435771226883664\n--------------------------------------------------\ngold summary:\n\n--------------------------------------------------\nsam_wgan summary:\n 옛날 태어났어요. 맞이했어요. 소녀보다 언니들은 심술쟁이들이었어요. 딸들보다 예쁘고 착한 소녀는 쓸고, 집안일을 해도 해도 난롯가에 앉아서 쉬곤 했지요. 어느 왕궁에서 열렸어요. 초대장이 언니들을 데리고 떠났어요. 신데렐라도 무도회에 가고 무도회에 가고 웃고 호박 한개와 도마뱀을 건드리자, 화려한 변했어요. 생쥐와 도마뱀을 마부로 신데렐라의 장식이 드레스로 내밀어 보거라. 할머니는 반짝반짝 유리 구두를 주었어요. 돼. 해. 모인 왕자님은 가서 결혼하겠어요. 발을 유리 구두는 수 구두를 구두는 왕궁으로 데리고 결혼하여 살았대요.\n--------------------------------------------------\n     method  comp ratio    intro      body    ending       var     total  \\\n0  SAM+WGAN    0.205801  0.56299  0.586069  0.581326  0.000099  0.579828   \n\n    grammar  \n0  0.971201  \n------------------------------------------------------------------\n옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요. 그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요. 소녀의 아버지는 홀로 남은 소녀가 걱정되었어요. 그래서 얼마 후 새어머니를 맞이했어요. 새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요. 그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요. 새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요. 그런데 이번에는 아버지마저 돌아가셨어요. 소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시 쉬곤 했지요. 어느 날, 왕궁에서 무도회가 열렸어요. 신데렐라의 집에도 초대장이 왔어요. 새어머니는 언니들을 데리고 무도회장으로 떠났어요. 신데렐라도 무도회에 가고 싶었어요. 혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요. 신데렐라, 너도 무도회에 가고 싶니? 신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요. 내가 너를 무도회에 보내주마 호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴. 마법사 할머니가 주문을 외웠어요. 그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요. 이번에는 생쥐와 도마뱀을 건드렸어요. 그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다. 신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요. 신데렐라, 발을 내밀어 보거라. 할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요. 신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼. 그러니까 반드시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지? 왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요. 왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,신데렐라하고만 춤을 추었어요. 신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요. 땡, 땡, 땡 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요. 신데렐라가 허둥지둥 왕궁을 빠져나가는데, 유리 구두 한 짝이 벗겨졌어요. 하지만 구두를 주울 틈이 없었어요. 신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요. 왕자님은 유리 구두를 가지고 임금님께 가서 말했어요. 이 유리 구두의 주인과 결혼하겠어요. 그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요. 언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리 구두는 너무 작았어요. 그때, 신데렐라가 조용히 다가와 말했어요. 저도 한번 신어 볼 수 있나요? 신데렐라는 신하게 건넨 유리 구두를 신었어요, 유리 구두는 신데렐라의 발에 꼭 맞았어요. 신하들은 신데렐라를 왕궁으로 데리고 갔어요. 그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\n------------------------------------------------------------------\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/327        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/326        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/325        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/324        \nNegative tokens: ['여자' '소녀가' '소녀의' '소녀가' '딸을' '새어머니와' '새어머니는' '집안일이' '신데렐라의' '신데렐라도'\n '신데렐라는' '신데렐라,' '신데렐라가' '할머니가' '무도회에' '할머니가' '마차로' '신데렐라의' '신데렐라,'\n '신데렐라에게' '신데렐라,' '흰말은' '신데렐라에게' '신데렐라는' '신데렐라는' '신데렐라가' '구두' '구두를'\n '신데렐라를' '구두' '왕자님은' '구두를' '구두의' '언니들은' '구두를' '신데렐라가' '신데렐라는' '신데렐라의'\n '신데렐라를' '신데렐라는']\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 325/326       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 324/325       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 323/324       \nPeak count: 27\nFrame tokens: 소녀의 맞이했어요. 언니들은 딸들보다 소녀는 왕궁에서 언니들을 무도회에 무도회에 마부로 드레스로 할머니는 구두를 왕자님도 무도회장에 않고,신데렐라하고만 춤을 왕궁을 왕자님은 결혼하겠어요. 발을 구두는 구두를 구두는 왕궁으로 결혼하여 살았대요. \n\nSimilarity : 0.48041270657702473\nTrain... |||||||||||||||||||||| 100.0%   500/500 epochs, beta:2.3683559894561768 Generator / grammar loss:-0.21663734316825867   similarity loss:-0.17937655746936798\n--------------------------------------------------\ngold summary:\n\n--------------------------------------------------\nsam_wgan summary:\n 옛날 태어났어요. 소녀보다 새어머니와 언니들은 심술쟁이들이었어요. 딸들보다 예쁘고 착한 소녀는 쓸고, 집안일을 해도 해도 난롯가에 앉아서 쉬곤 했지요. 어느 왕궁에서 열렸어요. 초대장이 언니들을 데리고 떠났어요. 신데렐라도 무도회에 가고 무도회에 가고 웃고 호박 한개와 도마뱀을 건드리자, 화려한 변했어요. 생쥐와 도마뱀을 마부로 신데렐라의 장식이 드레스로 내밀어 보거라. 할머니는 반짝반짝 유리 구두를 주었어요. 돼. 해. 모인 왕자님은 가서 결혼하겠어요. 발을 유리 구두는 수 구두를 구두는 왕궁으로 데리고 결혼하여 살았대요.\n--------------------------------------------------\n     method  comp ratio     intro      body    ending       var     total  \\\n0  SAM+WGAN     0.20511  0.581429  0.586069  0.581326  0.000005  0.584238   \n\n    grammar  \n0  0.979886  \n------------------------------------------------------------------\n옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요. 그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요. 소녀의 아버지는 홀로 남은 소녀가 걱정되었어요. 그래서 얼마 후 새어머니를 맞이했어요. 새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요. 그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요. 새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요. 그런데 이번에는 아버지마저 돌아가셨어요. 소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시 쉬곤 했지요. 어느 날, 왕궁에서 무도회가 열렸어요. 신데렐라의 집에도 초대장이 왔어요. 새어머니는 언니들을 데리고 무도회장으로 떠났어요. 신데렐라도 무도회에 가고 싶었어요. 혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요. 신데렐라, 너도 무도회에 가고 싶니? 신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요. 내가 너를 무도회에 보내주마 호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴. 마법사 할머니가 주문을 외웠어요. 그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요. 이번에는 생쥐와 도마뱀을 건드렸어요. 그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다. 신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요. 신데렐라, 발을 내밀어 보거라. 할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요. 신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼. 그러니까 반드시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지? 왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요. 왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,신데렐라하고만 춤을 추었어요. 신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요. 땡, 땡, 땡 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요. 신데렐라가 허둥지둥 왕궁을 빠져나가는데, 유리 구두 한 짝이 벗겨졌어요. 하지만 구두를 주울 틈이 없었어요. 신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요. 왕자님은 유리 구두를 가지고 임금님께 가서 말했어요. 이 유리 구두의 주인과 결혼하겠어요. 그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요. 언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리 구두는 너무 작았어요. 그때, 신데렐라가 조용히 다가와 말했어요. 저도 한번 신어 볼 수 있나요? 신데렐라는 신하게 건넨 유리 구두를 신었어요, 유리 구두는 신데렐라의 발에 꼭 맞았어요. 신하들은 신데렐라를 왕궁으로 데리고 갔어요. 그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\n------------------------------------------------------------------\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/327        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/326        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/325        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/324        \nNegative tokens: ['여자' '소녀가' '소녀의' '소녀가' '딸을' '새어머니와' '새어머니는' '집안일이' '신데렐라의' '신데렐라도'\n '신데렐라는' '신데렐라,' '신데렐라가' '할머니가' '무도회에' '할머니가' '마차로' '신데렐라의' '신데렐라,'\n '신데렐라에게' '신데렐라,' '흰말은' '신데렐라에게' '신데렐라는' '신데렐라는' '신데렐라가' '구두' '구두를'\n '신데렐라를' '구두' '왕자님은' '구두를' '구두의' '언니들은' '구두를' '신데렐라가' '신데렐라는' '신데렐라의'\n '신데렐라를' '신데렐라는']\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 325/326       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 324/325       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 323/324       \nPeak count: 27\nFrame tokens: 소녀의 맞이했어요. 언니들은 딸들보다 소녀는 왕궁에서 언니들을 무도회에 무도회에 마부로 드레스로 할머니는 구두를 왕자님도 무도회장에 않고,신데렐라하고만 춤을 왕궁을 왕자님은 결혼하겠어요. 발을 구두는 구두를 구두는 왕궁으로 결혼하여 살았대요. \n\nSimilarity : 0.48041270657702473\nTrain... |||||||||||||||||||||| 100.0%   500/500 epochs, beta:2.3683559894561768 Generator / grammar loss:-0.21663734316825867   similarity loss:-0.17937655746936798\n--------------------------------------------------\ngold summary:\n\n--------------------------------------------------\nsam_wgan summary:\n 옛날 태어났어요. 소녀보다 새어머니와 언니들은 심술쟁이들이었어요. 딸들보다 예쁘고 착한 소녀는 쓸고, 집안일을 해도 해도 난롯가에 앉아서 쉬곤 했지요. 어느 왕궁에서 열렸어요. 초대장이 언니들을 데리고 떠났어요. 신데렐라도 무도회에 가고 무도회에 가고 웃고 호박 한개와 도마뱀을 건드리자, 화려한 변했어요. 생쥐와 도마뱀을 마부로 신데렐라의 장식이 드레스로 내밀어 보거라. 할머니는 반짝반짝 유리 구두를 주었어요. 돼. 해. 모인 왕자님은 가서 결혼하겠어요. 발을 유리 구두는 수 구두를 구두는 왕궁으로 데리고 결혼하여 살았대요.\n--------------------------------------------------\n","name":"stdout"},{"output_type":"stream","text":"     method  comp ratio     intro      body    ending       var     total  \\\n0  SAM+WGAN     0.20511  0.581429  0.586069  0.581326  0.000005  0.584238   \n\n    grammar  \n0  0.979886  \n------------------------------------------------------------------\n옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요. 그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요. 소녀의 아버지는 홀로 남은 소녀가 걱정되었어요. 그래서 얼마 후 새어머니를 맞이했어요. 새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요. 그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요. 새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요. 그런데 이번에는 아버지마저 돌아가셨어요. 소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시 쉬곤 했지요. 어느 날, 왕궁에서 무도회가 열렸어요. 신데렐라의 집에도 초대장이 왔어요. 새어머니는 언니들을 데리고 무도회장으로 떠났어요. 신데렐라도 무도회에 가고 싶었어요. 혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요. 신데렐라, 너도 무도회에 가고 싶니? 신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요. 내가 너를 무도회에 보내주마 호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴. 마법사 할머니가 주문을 외웠어요. 그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요. 이번에는 생쥐와 도마뱀을 건드렸어요. 그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다. 신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요. 신데렐라, 발을 내밀어 보거라. 할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요. 신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼. 그러니까 반드시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지? 왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요. 왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,신데렐라하고만 춤을 추었어요. 신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요. 땡, 땡, 땡 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요. 신데렐라가 허둥지둥 왕궁을 빠져나가는데, 유리 구두 한 짝이 벗겨졌어요. 하지만 구두를 주울 틈이 없었어요. 신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요. 왕자님은 유리 구두를 가지고 임금님께 가서 말했어요. 이 유리 구두의 주인과 결혼하겠어요. 그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요. 언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리 구두는 너무 작았어요. 그때, 신데렐라가 조용히 다가와 말했어요. 저도 한번 신어 볼 수 있나요? 신데렐라는 신하게 건넨 유리 구두를 신었어요, 유리 구두는 신데렐라의 발에 꼭 맞았어요. 신하들은 신데렐라를 왕궁으로 데리고 갔어요. 그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\n------------------------------------------------------------------\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/327        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/326        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/325        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/324        \nNegative tokens: ['여자' '소녀가' '소녀의' '소녀가' '딸을' '새어머니와' '새어머니는' '집안일이' '신데렐라의' '신데렐라도'\n '신데렐라는' '신데렐라,' '신데렐라가' '할머니가' '무도회에' '할머니가' '마차로' '신데렐라의' '신데렐라,'\n '신데렐라에게' '신데렐라,' '흰말은' '신데렐라에게' '신데렐라는' '신데렐라는' '신데렐라가' '구두' '구두를'\n '신데렐라를' '구두' '왕자님은' '구두를' '구두의' '언니들은' '구두를' '신데렐라가' '신데렐라는' '신데렐라의'\n '신데렐라를' '신데렐라는']\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 325/326       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 324/325       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 323/324       \nPeak count: 27\nFrame tokens: 소녀의 맞이했어요. 언니들은 딸들보다 소녀는 왕궁에서 언니들을 무도회에 무도회에 마부로 드레스로 할머니는 구두를 왕자님도 무도회장에 않고,신데렐라하고만 춤을 왕궁을 왕자님은 결혼하겠어요. 발을 구두는 구두를 구두는 왕궁으로 결혼하여 살았대요. \n\nSimilarity : 0.48041270657702473\nTrain... |||||||||||||||||||||| 100.0%   500/500 epochs, beta:2.3683559894561768 Generator / grammar loss:-0.21663734316825867   similarity loss:-0.17937655746936798\n--------------------------------------------------\ngold summary:\n\n--------------------------------------------------\nsam_wgan summary:\n 옛날 태어났어요. 소녀보다 새어머니와 언니들은 심술쟁이들이었어요. 딸들보다 예쁘고 착한 소녀는 쓸고, 집안일을 해도 해도 난롯가에 앉아서 쉬곤 했지요. 어느 왕궁에서 열렸어요. 초대장이 언니들을 데리고 떠났어요. 신데렐라도 무도회에 가고 무도회에 가고 웃고 호박 한개와 도마뱀을 건드리자, 화려한 변했어요. 생쥐와 도마뱀을 마부로 신데렐라의 장식이 드레스로 내밀어 보거라. 할머니는 반짝반짝 유리 구두를 주었어요. 돼. 해. 모인 왕자님은 가서 결혼하겠어요. 발을 유리 구두는 수 구두를 구두는 왕궁으로 데리고 결혼하여 살았대요.\n--------------------------------------------------\n     method  comp ratio     intro      body    ending       var     total  \\\n0  SAM+WGAN     0.20511  0.581429  0.586069  0.581326  0.000005  0.584238   \n\n    grammar  \n0  0.979886  \n------------------------------------------------------------------\n옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요. 그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요. 소녀의 아버지는 홀로 남은 소녀가 걱정되었어요. 그래서 얼마 후 새어머니를 맞이했어요. 새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요. 그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요. 새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요. 그런데 이번에는 아버지마저 돌아가셨어요. 소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시 쉬곤 했지요. 어느 날, 왕궁에서 무도회가 열렸어요. 신데렐라의 집에도 초대장이 왔어요. 새어머니는 언니들을 데리고 무도회장으로 떠났어요. 신데렐라도 무도회에 가고 싶었어요. 혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요. 신데렐라, 너도 무도회에 가고 싶니? 신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요. 내가 너를 무도회에 보내주마 호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴. 마법사 할머니가 주문을 외웠어요. 그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요. 이번에는 생쥐와 도마뱀을 건드렸어요. 그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다. 신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요. 신데렐라, 발을 내밀어 보거라. 할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요. 신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼. 그러니까 반드시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지? 왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요. 왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,신데렐라하고만 춤을 추었어요. 신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요. 땡, 땡, 땡 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요. 신데렐라가 허둥지둥 왕궁을 빠져나가는데, 유리 구두 한 짝이 벗겨졌어요. 하지만 구두를 주울 틈이 없었어요. 신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요. 왕자님은 유리 구두를 가지고 임금님께 가서 말했어요. 이 유리 구두의 주인과 결혼하겠어요. 그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요. 언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리 구두는 너무 작았어요. 그때, 신데렐라가 조용히 다가와 말했어요. 저도 한번 신어 볼 수 있나요? 신데렐라는 신하게 건넨 유리 구두를 신었어요, 유리 구두는 신데렐라의 발에 꼭 맞았어요. 신하들은 신데렐라를 왕궁으로 데리고 갔어요. 그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\n------------------------------------------------------------------\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/327        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/326        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/325        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/324        \nNegative tokens: ['여자' '소녀가' '소녀의' '소녀가' '딸을' '새어머니와' '새어머니는' '집안일이' '신데렐라의' '신데렐라도'\n '신데렐라는' '신데렐라,' '신데렐라가' '할머니가' '무도회에' '할머니가' '마차로' '신데렐라의' '신데렐라,'\n '신데렐라에게' '신데렐라,' '흰말은' '신데렐라에게' '신데렐라는' '신데렐라는' '신데렐라가' '구두' '구두를'\n '신데렐라를' '구두' '왕자님은' '구두를' '구두의' '언니들은' '구두를' '신데렐라가' '신데렐라는' '신데렐라의'\n '신데렐라를' '신데렐라는']\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 325/326       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 324/325       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 323/324       \nPeak count: 27\nFrame tokens: 소녀의 맞이했어요. 언니들은 딸들보다 소녀는 왕궁에서 언니들을 무도회에 무도회에 마부로 드레스로 할머니는 구두를 왕자님도 무도회장에 않고,신데렐라하고만 춤을 왕궁을 왕자님은 결혼하겠어요. 발을 구두는 구두를 구두는 왕궁으로 결혼하여 살았대요. \n\nSimilarity : 0.48041270657702473\nTrain... |||||||||||||||||||||| 100.0%   500/500 epochs, beta:2.368936538696289 Generator / grammar loss:-0.21215254068374634   similarity loss:-0.17483164370059967\n--------------------------------------------------\ngold summary:\n\n--------------------------------------------------\nsam_wgan summary:\n 옛날 태어났어요. 소녀보다 새어머니와 언니들은 심술쟁이들이었어요. 딸들보다 예쁘고 착한 소녀는 쓸고, 집안일을 해도 해도 난롯가에 앉아서 쉬곤 했지요. 어느 왕궁에서 열렸어요. 초대장이 언니들을 데리고 떠났어요. 신데렐라도 무도회에 가고 무도회에 가고 웃고 호박 한개와 도마뱀을 건드리자, 화려한 변했어요. 생쥐와 도마뱀을 마부로 신데렐라의 장식이 드레스로 내밀어 보거라. 할머니는 반짝반짝 유리 구두를 주었어요. 돼. 해. 모인 왕자님은 가서 결혼하겠어요. 발을 유리 구두는 수 구두를 구두는 왕궁으로 데리고 결혼하여 살았대요.\n--------------------------------------------------\n     method  comp ratio     intro      body    ending       var     total  \\\n0  SAM+WGAN     0.20511  0.581429  0.586069  0.581326  0.000005  0.584238   \n\n    grammar  \n0  0.979886  \n------------------------------------------------------------------\n옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요. 그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요. 소녀의 아버지는 홀로 남은 소녀가 걱정되었어요. 그래서 얼마 후 새어머니를 맞이했어요. 새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요. 그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요. 새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요. 그런데 이번에는 아버지마저 돌아가셨어요. 소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시 쉬곤 했지요. 어느 날, 왕궁에서 무도회가 열렸어요. 신데렐라의 집에도 초대장이 왔어요. 새어머니는 언니들을 데리고 무도회장으로 떠났어요. 신데렐라도 무도회에 가고 싶었어요. 혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요. 신데렐라, 너도 무도회에 가고 싶니? 신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요. 내가 너를 무도회에 보내주마 호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴. 마법사 할머니가 주문을 외웠어요. 그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요. 이번에는 생쥐와 도마뱀을 건드렸어요. 그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다. 신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요. 신데렐라, 발을 내밀어 보거라. 할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요. 신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼. 그러니까 반드시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지? 왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요. 왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,신데렐라하고만 춤을 추었어요. 신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요. 땡, 땡, 땡 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요. 신데렐라가 허둥지둥 왕궁을 빠져나가는데, 유리 구두 한 짝이 벗겨졌어요. 하지만 구두를 주울 틈이 없었어요. 신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요. 왕자님은 유리 구두를 가지고 임금님께 가서 말했어요. 이 유리 구두의 주인과 결혼하겠어요. 그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요. 언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리 구두는 너무 작았어요. 그때, 신데렐라가 조용히 다가와 말했어요. 저도 한번 신어 볼 수 있나요? 신데렐라는 신하게 건넨 유리 구두를 신었어요, 유리 구두는 신데렐라의 발에 꼭 맞았어요. 신하들은 신데렐라를 왕궁으로 데리고 갔어요. 그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\n------------------------------------------------------------------\n","name":"stdout"},{"output_type":"stream","text":"Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/327        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/326        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/325        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/324        \nNegative tokens: ['여자' '소녀가' '소녀의' '소녀가' '딸을' '새어머니와' '새어머니는' '집안일이' '신데렐라의' '신데렐라도'\n '신데렐라는' '신데렐라,' '신데렐라가' '할머니가' '무도회에' '할머니가' '마차로' '신데렐라의' '신데렐라,'\n '신데렐라에게' '신데렐라,' '흰말은' '신데렐라에게' '신데렐라는' '신데렐라는' '신데렐라가' '구두' '구두를'\n '신데렐라를' '구두' '왕자님은' '구두를' '구두의' '언니들은' '구두를' '신데렐라가' '신데렐라는' '신데렐라의'\n '신데렐라를' '신데렐라는']\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 325/326       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 324/325       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 323/324       \nPeak count: 27\nFrame tokens: 소녀의 맞이했어요. 언니들은 딸들보다 소녀는 왕궁에서 언니들을 무도회에 무도회에 마부로 드레스로 할머니는 구두를 왕자님도 무도회장에 않고,신데렐라하고만 춤을 왕궁을 왕자님은 결혼하겠어요. 발을 구두는 구두를 구두는 왕궁으로 결혼하여 살았대요. \n\nSimilarity : 0.48041270657702473\nTrain... |||||||||||||||||||||| 100.0%   500/500 epochs, beta:2.368936538696289 Generator / grammar loss:-0.21215254068374634   similarity loss:-0.17483164370059967\n--------------------------------------------------\ngold summary:\n\n--------------------------------------------------\nsam_wgan summary:\n 옛날 태어났어요. 소녀보다 새어머니와 언니들은 심술쟁이들이었어요. 딸들보다 예쁘고 착한 소녀는 쓸고, 집안일을 해도 해도 난롯가에 앉아서 쉬곤 했지요. 어느 왕궁에서 열렸어요. 초대장이 언니들을 데리고 떠났어요. 신데렐라도 무도회에 가고 무도회에 가고 웃고 호박 한개와 도마뱀을 건드리자, 화려한 변했어요. 생쥐와 도마뱀을 마부로 신데렐라의 장식이 드레스로 내밀어 보거라. 할머니는 반짝반짝 유리 구두를 주었어요. 돼. 해. 모인 왕자님은 가서 결혼하겠어요. 발을 유리 구두는 수 구두를 구두는 왕궁으로 데리고 결혼하여 살았대요.\n--------------------------------------------------\n     method  comp ratio     intro      body    ending       var     total  \\\n0  SAM+WGAN     0.20511  0.581429  0.586069  0.581326  0.000005  0.584238   \n\n    grammar  \n0  0.979886  \n------------------------------------------------------------------\n옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요. 그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요. 소녀의 아버지는 홀로 남은 소녀가 걱정되었어요. 그래서 얼마 후 새어머니를 맞이했어요. 새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요. 그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요. 새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요. 그런데 이번에는 아버지마저 돌아가셨어요. 소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시 쉬곤 했지요. 어느 날, 왕궁에서 무도회가 열렸어요. 신데렐라의 집에도 초대장이 왔어요. 새어머니는 언니들을 데리고 무도회장으로 떠났어요. 신데렐라도 무도회에 가고 싶었어요. 혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요. 신데렐라, 너도 무도회에 가고 싶니? 신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요. 내가 너를 무도회에 보내주마 호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴. 마법사 할머니가 주문을 외웠어요. 그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요. 이번에는 생쥐와 도마뱀을 건드렸어요. 그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다. 신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요. 신데렐라, 발을 내밀어 보거라. 할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요. 신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼. 그러니까 반드시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지? 왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요. 왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,신데렐라하고만 춤을 추었어요. 신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요. 땡, 땡, 땡 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요. 신데렐라가 허둥지둥 왕궁을 빠져나가는데, 유리 구두 한 짝이 벗겨졌어요. 하지만 구두를 주울 틈이 없었어요. 신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요. 왕자님은 유리 구두를 가지고 임금님께 가서 말했어요. 이 유리 구두의 주인과 결혼하겠어요. 그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요. 언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리 구두는 너무 작았어요. 그때, 신데렐라가 조용히 다가와 말했어요. 저도 한번 신어 볼 수 있나요? 신데렐라는 신하게 건넨 유리 구두를 신었어요, 유리 구두는 신데렐라의 발에 꼭 맞았어요. 신하들은 신데렐라를 왕궁으로 데리고 갔어요. 그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\n------------------------------------------------------------------\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/327        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/326        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/325        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/324        \nNegative tokens: ['여자' '소녀가' '소녀의' '소녀가' '딸을' '새어머니와' '새어머니는' '집안일이' '신데렐라의' '신데렐라도'\n '신데렐라는' '신데렐라,' '신데렐라가' '할머니가' '무도회에' '할머니가' '마차로' '신데렐라의' '신데렐라,'\n '신데렐라에게' '신데렐라,' '흰말은' '신데렐라에게' '신데렐라는' '신데렐라는' '신데렐라가' '구두' '구두를'\n '신데렐라를' '구두' '왕자님은' '구두를' '구두의' '언니들은' '구두를' '신데렐라가' '신데렐라는' '신데렐라의'\n '신데렐라를' '신데렐라는']\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 325/326       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 324/325       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 323/324       \nPeak count: 27\nFrame tokens: 소녀의 맞이했어요. 언니들은 딸들보다 소녀는 왕궁에서 언니들을 무도회에 무도회에 마부로 드레스로 할머니는 구두를 왕자님도 무도회장에 않고,신데렐라하고만 춤을 왕궁을 왕자님은 결혼하겠어요. 발을 구두는 구두를 구두는 왕궁으로 결혼하여 살았대요. \n\nSimilarity : 0.48041270657702473\nTrain... |||||||||||||||||||||| 100.0%   500/500 epochs, beta:2.368936538696289 Generator / grammar loss:-0.21215254068374634   similarity loss:-0.17483164370059967\n--------------------------------------------------\ngold summary:\n\n--------------------------------------------------\nsam_wgan summary:\n 옛날 태어났어요. 소녀보다 새어머니와 언니들은 심술쟁이들이었어요. 딸들보다 예쁘고 착한 소녀는 쓸고, 집안일을 해도 해도 난롯가에 앉아서 쉬곤 했지요. 어느 왕궁에서 열렸어요. 초대장이 언니들을 데리고 떠났어요. 신데렐라도 무도회에 가고 무도회에 가고 웃고 호박 한개와 도마뱀을 건드리자, 화려한 변했어요. 생쥐와 도마뱀을 마부로 신데렐라의 장식이 드레스로 내밀어 보거라. 할머니는 반짝반짝 유리 구두를 주었어요. 돼. 해. 모인 왕자님은 가서 결혼하겠어요. 발을 유리 구두는 수 구두를 구두는 왕궁으로 데리고 결혼하여 살았대요.\n--------------------------------------------------\n     method  comp ratio     intro      body    ending       var     total  \\\n0  SAM+WGAN     0.20511  0.581429  0.586069  0.581326  0.000005  0.584238   \n\n    grammar  \n0  0.979886  \n------------------------------------------------------------------\n옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요. 그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요. 소녀의 아버지는 홀로 남은 소녀가 걱정되었어요. 그래서 얼마 후 새어머니를 맞이했어요. 새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요. 그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요. 새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요. 그런데 이번에는 아버지마저 돌아가셨어요. 소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시 쉬곤 했지요. 어느 날, 왕궁에서 무도회가 열렸어요. 신데렐라의 집에도 초대장이 왔어요. 새어머니는 언니들을 데리고 무도회장으로 떠났어요. 신데렐라도 무도회에 가고 싶었어요. 혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요. 신데렐라, 너도 무도회에 가고 싶니? 신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요. 내가 너를 무도회에 보내주마 호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴. 마법사 할머니가 주문을 외웠어요. 그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요. 이번에는 생쥐와 도마뱀을 건드렸어요. 그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다. 신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요. 신데렐라, 발을 내밀어 보거라. 할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요. 신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼. 그러니까 반드시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지? 왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요. 왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,신데렐라하고만 춤을 추었어요. 신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요. 땡, 땡, 땡 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요. 신데렐라가 허둥지둥 왕궁을 빠져나가는데, 유리 구두 한 짝이 벗겨졌어요. 하지만 구두를 주울 틈이 없었어요. 신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요. 왕자님은 유리 구두를 가지고 임금님께 가서 말했어요. 이 유리 구두의 주인과 결혼하겠어요. 그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요. 언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리 구두는 너무 작았어요. 그때, 신데렐라가 조용히 다가와 말했어요. 저도 한번 신어 볼 수 있나요? 신데렐라는 신하게 건넨 유리 구두를 신었어요, 유리 구두는 신데렐라의 발에 꼭 맞았어요. 신하들은 신데렐라를 왕궁으로 데리고 갔어요. 그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\n------------------------------------------------------------------\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/327        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/326        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/325        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/324        \nNegative tokens: ['여자' '소녀가' '소녀의' '소녀가' '딸을' '새어머니와' '새어머니는' '집안일이' '신데렐라의' '신데렐라도'\n '신데렐라는' '신데렐라,' '신데렐라가' '할머니가' '무도회에' '할머니가' '마차로' '신데렐라의' '신데렐라,'\n '신데렐라에게' '신데렐라,' '흰말은' '신데렐라에게' '신데렐라는' '신데렐라는' '신데렐라가' '구두' '구두를'\n '신데렐라를' '구두' '왕자님은' '구두를' '구두의' '언니들은' '구두를' '신데렐라가' '신데렐라는' '신데렐라의'\n '신데렐라를' '신데렐라는']\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 325/326       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 324/325       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 323/324       \nPeak count: 27\nFrame tokens: 소녀의 맞이했어요. 언니들은 딸들보다 소녀는 왕궁에서 언니들을 무도회에 무도회에 마부로 드레스로 할머니는 구두를 왕자님도 무도회장에 않고,신데렐라하고만 춤을 왕궁을 왕자님은 결혼하겠어요. 발을 구두는 구두를 구두는 왕궁으로 결혼하여 살았대요. \n\nSimilarity : 0.48041270657702473\nTrain... |||||||||||||||||||||| 100.0%   500/500 epochs, beta:2.3635292053222656 Generator / grammar loss:-0.20558679103851318   similarity loss:-0.1688254028558731\n--------------------------------------------------\ngold summary:\n\n--------------------------------------------------\nsam_wgan summary:\n 옛날 태어났어요. 맞이했어요. 소녀보다 새어머니와 언니들은 심술쟁이들이었어요. 딸들보다 예쁘고 착한 소녀는 쓸고, 집안일을 해도 해도 난롯가에 앉아서 쉬곤 했지요. 어느 왕궁에서 열렸어요. 초대장이 언니들을 데리고 떠났어요. 신데렐라도 무도회에 가고 무도회에 가고 웃고 호박 한개와 도마뱀을 건드리자, 화려한 변했어요. 생쥐와 도마뱀을 마부로 신데렐라의 장식이 드레스로 내밀어 보거라. 할머니는 반짝반짝 유리 구두를 주었어요. 돼. 해. 왕자님은 가서 결혼하겠어요. 발을 유리 구두는 수 구두를 구두는 왕궁으로 데리고 결혼하여 살았대요.\n--------------------------------------------------\n","name":"stdout"},{"output_type":"stream","text":"     method  comp ratio     intro      body    ending       var     total  \\\n0  SAM+WGAN    0.207873  0.581429  0.586069  0.581593  0.000005  0.584278   \n\n    grammar  \n0  0.973843  \n------------------------------------------------------------------\n옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요. 그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요. 소녀의 아버지는 홀로 남은 소녀가 걱정되었어요. 그래서 얼마 후 새어머니를 맞이했어요. 새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요. 그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요. 새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요. 그런데 이번에는 아버지마저 돌아가셨어요. 소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시 쉬곤 했지요. 어느 날, 왕궁에서 무도회가 열렸어요. 신데렐라의 집에도 초대장이 왔어요. 새어머니는 언니들을 데리고 무도회장으로 떠났어요. 신데렐라도 무도회에 가고 싶었어요. 혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요. 신데렐라, 너도 무도회에 가고 싶니? 신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요. 내가 너를 무도회에 보내주마 호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴. 마법사 할머니가 주문을 외웠어요. 그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요. 이번에는 생쥐와 도마뱀을 건드렸어요. 그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다. 신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요. 신데렐라, 발을 내밀어 보거라. 할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요. 신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼. 그러니까 반드시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지? 왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요. 왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,신데렐라하고만 춤을 추었어요. 신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요. 땡, 땡, 땡 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요. 신데렐라가 허둥지둥 왕궁을 빠져나가는데, 유리 구두 한 짝이 벗겨졌어요. 하지만 구두를 주울 틈이 없었어요. 신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요. 왕자님은 유리 구두를 가지고 임금님께 가서 말했어요. 이 유리 구두의 주인과 결혼하겠어요. 그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요. 언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리 구두는 너무 작았어요. 그때, 신데렐라가 조용히 다가와 말했어요. 저도 한번 신어 볼 수 있나요? 신데렐라는 신하게 건넨 유리 구두를 신었어요, 유리 구두는 신데렐라의 발에 꼭 맞았어요. 신하들은 신데렐라를 왕궁으로 데리고 갔어요. 그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\n------------------------------------------------------------------\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/327        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/326        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/325        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/324        \nNegative tokens: ['여자' '소녀가' '소녀의' '소녀가' '딸을' '새어머니와' '새어머니는' '집안일이' '신데렐라의' '신데렐라도'\n '신데렐라는' '신데렐라,' '신데렐라가' '할머니가' '무도회에' '할머니가' '마차로' '신데렐라의' '신데렐라,'\n '신데렐라에게' '신데렐라,' '흰말은' '신데렐라에게' '신데렐라는' '신데렐라는' '신데렐라가' '구두' '구두를'\n '신데렐라를' '구두' '왕자님은' '구두를' '구두의' '언니들은' '구두를' '신데렐라가' '신데렐라는' '신데렐라의'\n '신데렐라를' '신데렐라는']\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 325/326       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 324/325       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 323/324       \nPeak count: 27\nFrame tokens: 소녀의 맞이했어요. 언니들은 딸들보다 소녀는 왕궁에서 언니들을 무도회에 무도회에 마부로 드레스로 할머니는 구두를 왕자님도 무도회장에 않고,신데렐라하고만 춤을 왕궁을 왕자님은 결혼하겠어요. 발을 구두는 구두를 구두는 왕궁으로 결혼하여 살았대요. \n\nSimilarity : 0.48041270657702473\nTrain... |||||||||||||||||||||| 100.0%   500/500 epochs, beta:2.3635292053222656 Generator / grammar loss:-0.20558679103851318   similarity loss:-0.1688254028558731\n--------------------------------------------------\ngold summary:\n\n--------------------------------------------------\nsam_wgan summary:\n 옛날 태어났어요. 맞이했어요. 소녀보다 새어머니와 언니들은 심술쟁이들이었어요. 딸들보다 예쁘고 착한 소녀는 쓸고, 집안일을 해도 해도 난롯가에 앉아서 쉬곤 했지요. 어느 왕궁에서 열렸어요. 초대장이 언니들을 데리고 떠났어요. 신데렐라도 무도회에 가고 무도회에 가고 웃고 호박 한개와 도마뱀을 건드리자, 화려한 변했어요. 생쥐와 도마뱀을 마부로 신데렐라의 장식이 드레스로 내밀어 보거라. 할머니는 반짝반짝 유리 구두를 주었어요. 돼. 해. 왕자님은 가서 결혼하겠어요. 발을 유리 구두는 수 구두를 구두는 왕궁으로 데리고 결혼하여 살았대요.\n--------------------------------------------------\n     method  comp ratio     intro      body    ending       var     total  \\\n0  SAM+WGAN    0.207873  0.581429  0.586069  0.581593  0.000005  0.584278   \n\n    grammar  \n0  0.973843  \n------------------------------------------------------------------\n옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요. 그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요. 소녀의 아버지는 홀로 남은 소녀가 걱정되었어요. 그래서 얼마 후 새어머니를 맞이했어요. 새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요. 그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요. 새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요. 그런데 이번에는 아버지마저 돌아가셨어요. 소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시 쉬곤 했지요. 어느 날, 왕궁에서 무도회가 열렸어요. 신데렐라의 집에도 초대장이 왔어요. 새어머니는 언니들을 데리고 무도회장으로 떠났어요. 신데렐라도 무도회에 가고 싶었어요. 혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요. 신데렐라, 너도 무도회에 가고 싶니? 신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요. 내가 너를 무도회에 보내주마 호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴. 마법사 할머니가 주문을 외웠어요. 그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요. 이번에는 생쥐와 도마뱀을 건드렸어요. 그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다. 신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요. 신데렐라, 발을 내밀어 보거라. 할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요. 신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼. 그러니까 반드시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지? 왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요. 왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,신데렐라하고만 춤을 추었어요. 신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요. 땡, 땡, 땡 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요. 신데렐라가 허둥지둥 왕궁을 빠져나가는데, 유리 구두 한 짝이 벗겨졌어요. 하지만 구두를 주울 틈이 없었어요. 신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요. 왕자님은 유리 구두를 가지고 임금님께 가서 말했어요. 이 유리 구두의 주인과 결혼하겠어요. 그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요. 언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리 구두는 너무 작았어요. 그때, 신데렐라가 조용히 다가와 말했어요. 저도 한번 신어 볼 수 있나요? 신데렐라는 신하게 건넨 유리 구두를 신었어요, 유리 구두는 신데렐라의 발에 꼭 맞았어요. 신하들은 신데렐라를 왕궁으로 데리고 갔어요. 그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\n------------------------------------------------------------------\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/327        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/326        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/325        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/324        \nNegative tokens: ['여자' '소녀가' '소녀의' '소녀가' '딸을' '새어머니와' '새어머니는' '집안일이' '신데렐라의' '신데렐라도'\n '신데렐라는' '신데렐라,' '신데렐라가' '할머니가' '무도회에' '할머니가' '마차로' '신데렐라의' '신데렐라,'\n '신데렐라에게' '신데렐라,' '흰말은' '신데렐라에게' '신데렐라는' '신데렐라는' '신데렐라가' '구두' '구두를'\n '신데렐라를' '구두' '왕자님은' '구두를' '구두의' '언니들은' '구두를' '신데렐라가' '신데렐라는' '신데렐라의'\n '신데렐라를' '신데렐라는']\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 325/326       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 324/325       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 323/324       \nPeak count: 27\nFrame tokens: 소녀의 맞이했어요. 언니들은 딸들보다 소녀는 왕궁에서 언니들을 무도회에 무도회에 마부로 드레스로 할머니는 구두를 왕자님도 무도회장에 않고,신데렐라하고만 춤을 왕궁을 왕자님은 결혼하겠어요. 발을 구두는 구두를 구두는 왕궁으로 결혼하여 살았대요. \n\nSimilarity : 0.48041270657702473\nTrain... |||||||||||||||||||||| 100.0%   500/500 epochs, beta:2.3635292053222656 Generator / grammar loss:-0.20558679103851318   similarity loss:-0.1688254028558731\n--------------------------------------------------\ngold summary:\n\n--------------------------------------------------\nsam_wgan summary:\n 옛날 태어났어요. 맞이했어요. 소녀보다 새어머니와 언니들은 심술쟁이들이었어요. 딸들보다 예쁘고 착한 소녀는 쓸고, 집안일을 해도 해도 난롯가에 앉아서 쉬곤 했지요. 어느 왕궁에서 열렸어요. 초대장이 언니들을 데리고 떠났어요. 신데렐라도 무도회에 가고 무도회에 가고 웃고 호박 한개와 도마뱀을 건드리자, 화려한 변했어요. 생쥐와 도마뱀을 마부로 신데렐라의 장식이 드레스로 내밀어 보거라. 할머니는 반짝반짝 유리 구두를 주었어요. 돼. 해. 왕자님은 가서 결혼하겠어요. 발을 유리 구두는 수 구두를 구두는 왕궁으로 데리고 결혼하여 살았대요.\n--------------------------------------------------\n     method  comp ratio     intro      body    ending       var     total  \\\n0  SAM+WGAN    0.207873  0.581429  0.586069  0.581593  0.000005  0.584278   \n\n    grammar  \n0  0.973843  \n------------------------------------------------------------------\n옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요. 그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요. 소녀의 아버지는 홀로 남은 소녀가 걱정되었어요. 그래서 얼마 후 새어머니를 맞이했어요. 새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요. 그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요. 새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요. 그런데 이번에는 아버지마저 돌아가셨어요. 소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시 쉬곤 했지요. 어느 날, 왕궁에서 무도회가 열렸어요. 신데렐라의 집에도 초대장이 왔어요. 새어머니는 언니들을 데리고 무도회장으로 떠났어요. 신데렐라도 무도회에 가고 싶었어요. 혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요. 신데렐라, 너도 무도회에 가고 싶니? 신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요. 내가 너를 무도회에 보내주마 호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴. 마법사 할머니가 주문을 외웠어요. 그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요. 이번에는 생쥐와 도마뱀을 건드렸어요. 그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다. 신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요. 신데렐라, 발을 내밀어 보거라. 할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요. 신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼. 그러니까 반드시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지? 왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요. 왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,신데렐라하고만 춤을 추었어요. 신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요. 땡, 땡, 땡 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요. 신데렐라가 허둥지둥 왕궁을 빠져나가는데, 유리 구두 한 짝이 벗겨졌어요. 하지만 구두를 주울 틈이 없었어요. 신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요. 왕자님은 유리 구두를 가지고 임금님께 가서 말했어요. 이 유리 구두의 주인과 결혼하겠어요. 그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요. 언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리 구두는 너무 작았어요. 그때, 신데렐라가 조용히 다가와 말했어요. 저도 한번 신어 볼 수 있나요? 신데렐라는 신하게 건넨 유리 구두를 신었어요, 유리 구두는 신데렐라의 발에 꼭 맞았어요. 신하들은 신데렐라를 왕궁으로 데리고 갔어요. 그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\n------------------------------------------------------------------\n","name":"stdout"},{"output_type":"stream","text":"Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/327        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/326        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/325        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/324        \nNegative tokens: ['여자' '소녀가' '소녀의' '소녀가' '딸을' '새어머니와' '새어머니는' '집안일이' '신데렐라의' '신데렐라도'\n '신데렐라는' '신데렐라,' '신데렐라가' '할머니가' '무도회에' '할머니가' '마차로' '신데렐라의' '신데렐라,'\n '신데렐라에게' '신데렐라,' '흰말은' '신데렐라에게' '신데렐라는' '신데렐라는' '신데렐라가' '구두' '구두를'\n '신데렐라를' '구두' '왕자님은' '구두를' '구두의' '언니들은' '구두를' '신데렐라가' '신데렐라는' '신데렐라의'\n '신데렐라를' '신데렐라는']\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 325/326       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 324/325       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 323/324       \nPeak count: 27\nFrame tokens: 소녀의 맞이했어요. 언니들은 딸들보다 소녀는 왕궁에서 언니들을 무도회에 무도회에 마부로 드레스로 할머니는 구두를 왕자님도 무도회장에 않고,신데렐라하고만 춤을 왕궁을 왕자님은 결혼하겠어요. 발을 구두는 구두를 구두는 왕궁으로 결혼하여 살았대요. \n\nSimilarity : 0.48041270657702473\nTrain... |||||||||||||||||||||| 100.0%   500/500 epochs, beta:2.3563272953033447 Generator / grammar loss:-0.19958990812301636   similarity loss:-0.16357281804084778\n--------------------------------------------------\ngold summary:\n\n--------------------------------------------------\nsam_wgan summary:\n 옛날 태어났어요. 병이들어 맞이했어요. 소녀보다 새어머니와 언니들은 심술쟁이들이었어요. 딸들보다 예쁘고 착한 소녀는 쓸고, 집안일을 해도 해도 난롯가에 앉아서 쉬곤 했지요. 어느 왕궁에서 열렸어요. 초대장이 언니들을 데리고 떠났어요. 신데렐라도 무도회에 가고 무도회에 웃고 호박 한개와 도마뱀을 건드리자, 화려한 변했어요. 생쥐와 도마뱀을 마부로 신데렐라의 장식이 드레스로 내밀어 보거라. 할머니는 반짝반짝 유리 구두를 주었어요. 돼. 해. 왕자님은 가서 결혼하겠어요. 발을 유리 구두는 수 구두를 구두는 왕궁으로 데리고 결혼하여 살았대요.\n--------------------------------------------------\n     method  comp ratio     intro      body    ending       var     total  \\\n0  SAM+WGAN    0.209254  0.581429  0.587949  0.581593  0.000009  0.585423   \n\n    grammar  \n0  0.973544  \n------------------------------------------------------------------\n옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요. 그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요. 소녀의 아버지는 홀로 남은 소녀가 걱정되었어요. 그래서 얼마 후 새어머니를 맞이했어요. 새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요. 그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요. 새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요. 그런데 이번에는 아버지마저 돌아가셨어요. 소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시 쉬곤 했지요. 어느 날, 왕궁에서 무도회가 열렸어요. 신데렐라의 집에도 초대장이 왔어요. 새어머니는 언니들을 데리고 무도회장으로 떠났어요. 신데렐라도 무도회에 가고 싶었어요. 혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요. 신데렐라, 너도 무도회에 가고 싶니? 신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요. 내가 너를 무도회에 보내주마 호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴. 마법사 할머니가 주문을 외웠어요. 그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요. 이번에는 생쥐와 도마뱀을 건드렸어요. 그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다. 신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요. 신데렐라, 발을 내밀어 보거라. 할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요. 신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼. 그러니까 반드시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지? 왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요. 왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,신데렐라하고만 춤을 추었어요. 신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요. 땡, 땡, 땡 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요. 신데렐라가 허둥지둥 왕궁을 빠져나가는데, 유리 구두 한 짝이 벗겨졌어요. 하지만 구두를 주울 틈이 없었어요. 신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요. 왕자님은 유리 구두를 가지고 임금님께 가서 말했어요. 이 유리 구두의 주인과 결혼하겠어요. 그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요. 언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리 구두는 너무 작았어요. 그때, 신데렐라가 조용히 다가와 말했어요. 저도 한번 신어 볼 수 있나요? 신데렐라는 신하게 건넨 유리 구두를 신었어요, 유리 구두는 신데렐라의 발에 꼭 맞았어요. 신하들은 신데렐라를 왕궁으로 데리고 갔어요. 그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\n------------------------------------------------------------------\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/327        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/326        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/325        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/324        \nNegative tokens: ['여자' '소녀가' '소녀의' '소녀가' '딸을' '새어머니와' '새어머니는' '집안일이' '신데렐라의' '신데렐라도'\n '신데렐라는' '신데렐라,' '신데렐라가' '할머니가' '무도회에' '할머니가' '마차로' '신데렐라의' '신데렐라,'\n '신데렐라에게' '신데렐라,' '흰말은' '신데렐라에게' '신데렐라는' '신데렐라는' '신데렐라가' '구두' '구두를'\n '신데렐라를' '구두' '왕자님은' '구두를' '구두의' '언니들은' '구두를' '신데렐라가' '신데렐라는' '신데렐라의'\n '신데렐라를' '신데렐라는']\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 325/326       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 324/325       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 323/324       \nPeak count: 27\nFrame tokens: 소녀의 맞이했어요. 언니들은 딸들보다 소녀는 왕궁에서 언니들을 무도회에 무도회에 마부로 드레스로 할머니는 구두를 왕자님도 무도회장에 않고,신데렐라하고만 춤을 왕궁을 왕자님은 결혼하겠어요. 발을 구두는 구두를 구두는 왕궁으로 결혼하여 살았대요. \n\nSimilarity : 0.48041270657702473\nTrain... |||||||||||||||||||||| 100.0%   500/500 epochs, beta:2.3563272953033447 Generator / grammar loss:-0.19958990812301636   similarity loss:-0.16357281804084778\n--------------------------------------------------\ngold summary:\n\n--------------------------------------------------\nsam_wgan summary:\n 옛날 태어났어요. 병이들어 맞이했어요. 소녀보다 새어머니와 언니들은 심술쟁이들이었어요. 딸들보다 예쁘고 착한 소녀는 쓸고, 집안일을 해도 해도 난롯가에 앉아서 쉬곤 했지요. 어느 왕궁에서 열렸어요. 초대장이 언니들을 데리고 떠났어요. 신데렐라도 무도회에 가고 무도회에 웃고 호박 한개와 도마뱀을 건드리자, 화려한 변했어요. 생쥐와 도마뱀을 마부로 신데렐라의 장식이 드레스로 내밀어 보거라. 할머니는 반짝반짝 유리 구두를 주었어요. 돼. 해. 왕자님은 가서 결혼하겠어요. 발을 유리 구두는 수 구두를 구두는 왕궁으로 데리고 결혼하여 살았대요.\n--------------------------------------------------\n     method  comp ratio     intro      body    ending       var     total  \\\n0  SAM+WGAN    0.209254  0.581429  0.587949  0.581593  0.000009  0.585423   \n\n    grammar  \n0  0.973544  \n------------------------------------------------------------------\n옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요. 그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요. 소녀의 아버지는 홀로 남은 소녀가 걱정되었어요. 그래서 얼마 후 새어머니를 맞이했어요. 새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요. 그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요. 새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요. 그런데 이번에는 아버지마저 돌아가셨어요. 소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시 쉬곤 했지요. 어느 날, 왕궁에서 무도회가 열렸어요. 신데렐라의 집에도 초대장이 왔어요. 새어머니는 언니들을 데리고 무도회장으로 떠났어요. 신데렐라도 무도회에 가고 싶었어요. 혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요. 신데렐라, 너도 무도회에 가고 싶니? 신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요. 내가 너를 무도회에 보내주마 호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴. 마법사 할머니가 주문을 외웠어요. 그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요. 이번에는 생쥐와 도마뱀을 건드렸어요. 그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다. 신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요. 신데렐라, 발을 내밀어 보거라. 할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요. 신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼. 그러니까 반드시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지? 왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요. 왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,신데렐라하고만 춤을 추었어요. 신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요. 땡, 땡, 땡 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요. 신데렐라가 허둥지둥 왕궁을 빠져나가는데, 유리 구두 한 짝이 벗겨졌어요. 하지만 구두를 주울 틈이 없었어요. 신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요. 왕자님은 유리 구두를 가지고 임금님께 가서 말했어요. 이 유리 구두의 주인과 결혼하겠어요. 그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요. 언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리 구두는 너무 작았어요. 그때, 신데렐라가 조용히 다가와 말했어요. 저도 한번 신어 볼 수 있나요? 신데렐라는 신하게 건넨 유리 구두를 신었어요, 유리 구두는 신데렐라의 발에 꼭 맞았어요. 신하들은 신데렐라를 왕궁으로 데리고 갔어요. 그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\n------------------------------------------------------------------\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/327        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/326        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/325        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/324        \nNegative tokens: ['여자' '소녀가' '소녀의' '소녀가' '딸을' '새어머니와' '새어머니는' '집안일이' '신데렐라의' '신데렐라도'\n '신데렐라는' '신데렐라,' '신데렐라가' '할머니가' '무도회에' '할머니가' '마차로' '신데렐라의' '신데렐라,'\n '신데렐라에게' '신데렐라,' '흰말은' '신데렐라에게' '신데렐라는' '신데렐라는' '신데렐라가' '구두' '구두를'\n '신데렐라를' '구두' '왕자님은' '구두를' '구두의' '언니들은' '구두를' '신데렐라가' '신데렐라는' '신데렐라의'\n '신데렐라를' '신데렐라는']\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 325/326       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 324/325       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 323/324       \nPeak count: 27\nFrame tokens: 소녀의 맞이했어요. 언니들은 딸들보다 소녀는 왕궁에서 언니들을 무도회에 무도회에 마부로 드레스로 할머니는 구두를 왕자님도 무도회장에 않고,신데렐라하고만 춤을 왕궁을 왕자님은 결혼하겠어요. 발을 구두는 구두를 구두는 왕궁으로 결혼하여 살았대요. \n\nSimilarity : 0.48041270657702473\nTrain... |||||||||||||||||||||| 100.0%   500/500 epochs, beta:2.3563272953033447 Generator / grammar loss:-0.19958990812301636   similarity loss:-0.16357281804084778\n--------------------------------------------------\ngold summary:\n\n--------------------------------------------------\nsam_wgan summary:\n 옛날 태어났어요. 병이들어 맞이했어요. 소녀보다 새어머니와 언니들은 심술쟁이들이었어요. 딸들보다 예쁘고 착한 소녀는 쓸고, 집안일을 해도 해도 난롯가에 앉아서 쉬곤 했지요. 어느 왕궁에서 열렸어요. 초대장이 언니들을 데리고 떠났어요. 신데렐라도 무도회에 가고 무도회에 웃고 호박 한개와 도마뱀을 건드리자, 화려한 변했어요. 생쥐와 도마뱀을 마부로 신데렐라의 장식이 드레스로 내밀어 보거라. 할머니는 반짝반짝 유리 구두를 주었어요. 돼. 해. 왕자님은 가서 결혼하겠어요. 발을 유리 구두는 수 구두를 구두는 왕궁으로 데리고 결혼하여 살았대요.\n--------------------------------------------------\n","name":"stdout"},{"output_type":"stream","text":"     method  comp ratio     intro      body    ending       var     total  \\\n0  SAM+WGAN    0.209254  0.581429  0.587949  0.581593  0.000009  0.585423   \n\n    grammar  \n0  0.973544  \n------------------------------------------------------------------\n옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요. 그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요. 소녀의 아버지는 홀로 남은 소녀가 걱정되었어요. 그래서 얼마 후 새어머니를 맞이했어요. 새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요. 그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요. 새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요. 그런데 이번에는 아버지마저 돌아가셨어요. 소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시 쉬곤 했지요. 어느 날, 왕궁에서 무도회가 열렸어요. 신데렐라의 집에도 초대장이 왔어요. 새어머니는 언니들을 데리고 무도회장으로 떠났어요. 신데렐라도 무도회에 가고 싶었어요. 혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요. 신데렐라, 너도 무도회에 가고 싶니? 신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요. 내가 너를 무도회에 보내주마 호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴. 마법사 할머니가 주문을 외웠어요. 그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요. 이번에는 생쥐와 도마뱀을 건드렸어요. 그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다. 신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요. 신데렐라, 발을 내밀어 보거라. 할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요. 신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼. 그러니까 반드시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지? 왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요. 왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,신데렐라하고만 춤을 추었어요. 신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요. 땡, 땡, 땡 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요. 신데렐라가 허둥지둥 왕궁을 빠져나가는데, 유리 구두 한 짝이 벗겨졌어요. 하지만 구두를 주울 틈이 없었어요. 신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요. 왕자님은 유리 구두를 가지고 임금님께 가서 말했어요. 이 유리 구두의 주인과 결혼하겠어요. 그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요. 언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리 구두는 너무 작았어요. 그때, 신데렐라가 조용히 다가와 말했어요. 저도 한번 신어 볼 수 있나요? 신데렐라는 신하게 건넨 유리 구두를 신었어요, 유리 구두는 신데렐라의 발에 꼭 맞았어요. 신하들은 신데렐라를 왕궁으로 데리고 갔어요. 그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\n------------------------------------------------------------------\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/327        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/326        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/325        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/324        \nNegative tokens: ['여자' '소녀가' '소녀의' '소녀가' '딸을' '새어머니와' '새어머니는' '집안일이' '신데렐라의' '신데렐라도'\n '신데렐라는' '신데렐라,' '신데렐라가' '할머니가' '무도회에' '할머니가' '마차로' '신데렐라의' '신데렐라,'\n '신데렐라에게' '신데렐라,' '흰말은' '신데렐라에게' '신데렐라는' '신데렐라는' '신데렐라가' '구두' '구두를'\n '신데렐라를' '구두' '왕자님은' '구두를' '구두의' '언니들은' '구두를' '신데렐라가' '신데렐라는' '신데렐라의'\n '신데렐라를' '신데렐라는']\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 325/326       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 324/325       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 323/324       \nPeak count: 27\nFrame tokens: 소녀의 맞이했어요. 언니들은 딸들보다 소녀는 왕궁에서 언니들을 무도회에 무도회에 마부로 드레스로 할머니는 구두를 왕자님도 무도회장에 않고,신데렐라하고만 춤을 왕궁을 왕자님은 결혼하겠어요. 발을 구두는 구두를 구두는 왕궁으로 결혼하여 살았대요. \n\nSimilarity : 0.48041270657702473\nTrain... |||||||||||||||||||||| 100.0%   500/500 epochs, beta:2.3150506019592285 Generator / grammar loss:-0.1912313997745514   similarity loss:-0.15946181118488312\n--------------------------------------------------\ngold summary:\n\n--------------------------------------------------\nsam_wgan summary:\n 옛날 태어났어요. 병이들어 떠나고 맞이했어요. 소녀보다 새어머니와 언니들은 심술쟁이들이었어요. 딸들보다 예쁘고 착한 소녀는 쓸고, 집안일을 해도 해도 난롯가에 앉아서 쉬곤 했지요. 어느 왕궁에서 초대장이 언니들을 데리고 떠났어요. 신데렐라도 무도회에 가고 무도회에 가고 웃고 호박 한개와 도마뱀을 건드리자, 화려한 변했어요. 도마뱀을 마부로 신데렐라의 장식이 드레스로 내밀어 보거라. 할머니는 반짝반짝 유리 구두를 주었어요. 돼. 해. 왕자님은 가서 결혼하겠어요. 발을 유리 구두는 수 구두를 구두는 왕궁으로 데리고 결혼하여 살았대요.\n--------------------------------------------------\n     method  comp ratio     intro      body    ending       var     total  \\\n0  SAM+WGAN    0.207182  0.577428  0.577143  0.593614  0.000059  0.579717   \n\n    grammar  \n0  0.970985  \n------------------------------------------------------------------\n옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요. 그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요. 소녀의 아버지는 홀로 남은 소녀가 걱정되었어요. 그래서 얼마 후 새어머니를 맞이했어요. 새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요. 그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요. 새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요. 그런데 이번에는 아버지마저 돌아가셨어요. 소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시 쉬곤 했지요. 어느 날, 왕궁에서 무도회가 열렸어요. 신데렐라의 집에도 초대장이 왔어요. 새어머니는 언니들을 데리고 무도회장으로 떠났어요. 신데렐라도 무도회에 가고 싶었어요. 혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요. 신데렐라, 너도 무도회에 가고 싶니? 신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요. 내가 너를 무도회에 보내주마 호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴. 마법사 할머니가 주문을 외웠어요. 그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요. 이번에는 생쥐와 도마뱀을 건드렸어요. 그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다. 신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요. 신데렐라, 발을 내밀어 보거라. 할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요. 신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼. 그러니까 반드시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지? 왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요. 왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,신데렐라하고만 춤을 추었어요. 신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요. 땡, 땡, 땡 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요. 신데렐라가 허둥지둥 왕궁을 빠져나가는데, 유리 구두 한 짝이 벗겨졌어요. 하지만 구두를 주울 틈이 없었어요. 신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요. 왕자님은 유리 구두를 가지고 임금님께 가서 말했어요. 이 유리 구두의 주인과 결혼하겠어요. 그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요. 언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리 구두는 너무 작았어요. 그때, 신데렐라가 조용히 다가와 말했어요. 저도 한번 신어 볼 수 있나요? 신데렐라는 신하게 건넨 유리 구두를 신었어요, 유리 구두는 신데렐라의 발에 꼭 맞았어요. 신하들은 신데렐라를 왕궁으로 데리고 갔어요. 그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\n------------------------------------------------------------------\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/327        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/326        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/325        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/324        \nNegative tokens: ['여자' '소녀가' '소녀의' '소녀가' '딸을' '새어머니와' '새어머니는' '집안일이' '신데렐라의' '신데렐라도'\n '신데렐라는' '신데렐라,' '신데렐라가' '할머니가' '무도회에' '할머니가' '마차로' '신데렐라의' '신데렐라,'\n '신데렐라에게' '신데렐라,' '흰말은' '신데렐라에게' '신데렐라는' '신데렐라는' '신데렐라가' '구두' '구두를'\n '신데렐라를' '구두' '왕자님은' '구두를' '구두의' '언니들은' '구두를' '신데렐라가' '신데렐라는' '신데렐라의'\n '신데렐라를' '신데렐라는']\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 325/326       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 324/325       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 323/324       \nPeak count: 27\nFrame tokens: 소녀의 맞이했어요. 언니들은 딸들보다 소녀는 왕궁에서 언니들을 무도회에 무도회에 마부로 드레스로 할머니는 구두를 왕자님도 무도회장에 않고,신데렐라하고만 춤을 왕궁을 왕자님은 결혼하겠어요. 발을 구두는 구두를 구두는 왕궁으로 결혼하여 살았대요. \n\nSimilarity : 0.48041270657702473\nTrain... |||||||||||||||||||||| 100.0%   500/500 epochs, beta:2.3150506019592285 Generator / grammar loss:-0.1912313997745514   similarity loss:-0.15946181118488312\n--------------------------------------------------\ngold summary:\n\n--------------------------------------------------\nsam_wgan summary:\n 옛날 태어났어요. 병이들어 떠나고 맞이했어요. 소녀보다 새어머니와 언니들은 심술쟁이들이었어요. 딸들보다 예쁘고 착한 소녀는 쓸고, 집안일을 해도 해도 난롯가에 앉아서 쉬곤 했지요. 어느 왕궁에서 초대장이 언니들을 데리고 떠났어요. 신데렐라도 무도회에 가고 무도회에 가고 웃고 호박 한개와 도마뱀을 건드리자, 화려한 변했어요. 도마뱀을 마부로 신데렐라의 장식이 드레스로 내밀어 보거라. 할머니는 반짝반짝 유리 구두를 주었어요. 돼. 해. 왕자님은 가서 결혼하겠어요. 발을 유리 구두는 수 구두를 구두는 왕궁으로 데리고 결혼하여 살았대요.\n--------------------------------------------------\n     method  comp ratio     intro      body    ending       var     total  \\\n0  SAM+WGAN    0.207182  0.577428  0.577143  0.593614  0.000059  0.579717   \n\n    grammar  \n0  0.970985  \n------------------------------------------------------------------\n옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요. 그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요. 소녀의 아버지는 홀로 남은 소녀가 걱정되었어요. 그래서 얼마 후 새어머니를 맞이했어요. 새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요. 그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요. 새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요. 그런데 이번에는 아버지마저 돌아가셨어요. 소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시 쉬곤 했지요. 어느 날, 왕궁에서 무도회가 열렸어요. 신데렐라의 집에도 초대장이 왔어요. 새어머니는 언니들을 데리고 무도회장으로 떠났어요. 신데렐라도 무도회에 가고 싶었어요. 혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요. 신데렐라, 너도 무도회에 가고 싶니? 신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요. 내가 너를 무도회에 보내주마 호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴. 마법사 할머니가 주문을 외웠어요. 그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요. 이번에는 생쥐와 도마뱀을 건드렸어요. 그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다. 신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요. 신데렐라, 발을 내밀어 보거라. 할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요. 신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼. 그러니까 반드시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지? 왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요. 왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,신데렐라하고만 춤을 추었어요. 신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요. 땡, 땡, 땡 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요. 신데렐라가 허둥지둥 왕궁을 빠져나가는데, 유리 구두 한 짝이 벗겨졌어요. 하지만 구두를 주울 틈이 없었어요. 신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요. 왕자님은 유리 구두를 가지고 임금님께 가서 말했어요. 이 유리 구두의 주인과 결혼하겠어요. 그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요. 언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리 구두는 너무 작았어요. 그때, 신데렐라가 조용히 다가와 말했어요. 저도 한번 신어 볼 수 있나요? 신데렐라는 신하게 건넨 유리 구두를 신었어요, 유리 구두는 신데렐라의 발에 꼭 맞았어요. 신하들은 신데렐라를 왕궁으로 데리고 갔어요. 그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\n------------------------------------------------------------------\n","name":"stdout"},{"output_type":"stream","text":"Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/327        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/326        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/325        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/324        \nNegative tokens: ['여자' '소녀가' '소녀의' '소녀가' '딸을' '새어머니와' '새어머니는' '집안일이' '신데렐라의' '신데렐라도'\n '신데렐라는' '신데렐라,' '신데렐라가' '할머니가' '무도회에' '할머니가' '마차로' '신데렐라의' '신데렐라,'\n '신데렐라에게' '신데렐라,' '흰말은' '신데렐라에게' '신데렐라는' '신데렐라는' '신데렐라가' '구두' '구두를'\n '신데렐라를' '구두' '왕자님은' '구두를' '구두의' '언니들은' '구두를' '신데렐라가' '신데렐라는' '신데렐라의'\n '신데렐라를' '신데렐라는']\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 325/326       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 324/325       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 323/324       \nPeak count: 27\nFrame tokens: 소녀의 맞이했어요. 언니들은 딸들보다 소녀는 왕궁에서 언니들을 무도회에 무도회에 마부로 드레스로 할머니는 구두를 왕자님도 무도회장에 않고,신데렐라하고만 춤을 왕궁을 왕자님은 결혼하겠어요. 발을 구두는 구두를 구두는 왕궁으로 결혼하여 살았대요. \n\nSimilarity : 0.48041270657702473\nTrain... |||||||||||||||||||||| 100.0%   500/500 epochs, beta:2.3150506019592285 Generator / grammar loss:-0.1912313997745514   similarity loss:-0.15946181118488312\n--------------------------------------------------\ngold summary:\n\n--------------------------------------------------\nsam_wgan summary:\n 옛날 태어났어요. 병이들어 떠나고 맞이했어요. 소녀보다 새어머니와 언니들은 심술쟁이들이었어요. 딸들보다 예쁘고 착한 소녀는 쓸고, 집안일을 해도 해도 난롯가에 앉아서 쉬곤 했지요. 어느 왕궁에서 초대장이 언니들을 데리고 떠났어요. 신데렐라도 무도회에 가고 무도회에 가고 웃고 호박 한개와 도마뱀을 건드리자, 화려한 변했어요. 도마뱀을 마부로 신데렐라의 장식이 드레스로 내밀어 보거라. 할머니는 반짝반짝 유리 구두를 주었어요. 돼. 해. 왕자님은 가서 결혼하겠어요. 발을 유리 구두는 수 구두를 구두는 왕궁으로 데리고 결혼하여 살았대요.\n--------------------------------------------------\n     method  comp ratio     intro      body    ending       var     total  \\\n0  SAM+WGAN    0.207182  0.577428  0.577143  0.593614  0.000059  0.579717   \n\n    grammar  \n0  0.970985  \n------------------------------------------------------------------\n옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요. 그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요. 소녀의 아버지는 홀로 남은 소녀가 걱정되었어요. 그래서 얼마 후 새어머니를 맞이했어요. 새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요. 그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요. 새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요. 그런데 이번에는 아버지마저 돌아가셨어요. 소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시 쉬곤 했지요. 어느 날, 왕궁에서 무도회가 열렸어요. 신데렐라의 집에도 초대장이 왔어요. 새어머니는 언니들을 데리고 무도회장으로 떠났어요. 신데렐라도 무도회에 가고 싶었어요. 혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요. 신데렐라, 너도 무도회에 가고 싶니? 신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요. 내가 너를 무도회에 보내주마 호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴. 마법사 할머니가 주문을 외웠어요. 그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요. 이번에는 생쥐와 도마뱀을 건드렸어요. 그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다. 신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요. 신데렐라, 발을 내밀어 보거라. 할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요. 신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼. 그러니까 반드시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지? 왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요. 왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,신데렐라하고만 춤을 추었어요. 신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요. 땡, 땡, 땡 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요. 신데렐라가 허둥지둥 왕궁을 빠져나가는데, 유리 구두 한 짝이 벗겨졌어요. 하지만 구두를 주울 틈이 없었어요. 신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요. 왕자님은 유리 구두를 가지고 임금님께 가서 말했어요. 이 유리 구두의 주인과 결혼하겠어요. 그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요. 언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리 구두는 너무 작았어요. 그때, 신데렐라가 조용히 다가와 말했어요. 저도 한번 신어 볼 수 있나요? 신데렐라는 신하게 건넨 유리 구두를 신었어요, 유리 구두는 신데렐라의 발에 꼭 맞았어요. 신하들은 신데렐라를 왕궁으로 데리고 갔어요. 그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\n------------------------------------------------------------------\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/327        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/326        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/325        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/324        \nNegative tokens: ['여자' '소녀가' '소녀의' '소녀가' '딸을' '새어머니와' '새어머니는' '집안일이' '신데렐라의' '신데렐라도'\n '신데렐라는' '신데렐라,' '신데렐라가' '할머니가' '무도회에' '할머니가' '마차로' '신데렐라의' '신데렐라,'\n '신데렐라에게' '신데렐라,' '흰말은' '신데렐라에게' '신데렐라는' '신데렐라는' '신데렐라가' '구두' '구두를'\n '신데렐라를' '구두' '왕자님은' '구두를' '구두의' '언니들은' '구두를' '신데렐라가' '신데렐라는' '신데렐라의'\n '신데렐라를' '신데렐라는']\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 325/326       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 324/325       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 323/324       \nPeak count: 27\nFrame tokens: 소녀의 맞이했어요. 언니들은 딸들보다 소녀는 왕궁에서 언니들을 무도회에 무도회에 마부로 드레스로 할머니는 구두를 왕자님도 무도회장에 않고,신데렐라하고만 춤을 왕궁을 왕자님은 결혼하겠어요. 발을 구두는 구두를 구두는 왕궁으로 결혼하여 살았대요. \n\nSimilarity : 0.48041270657702473\nTrain... |||||||||||||||||||||| 100.0%   500/500 epochs, beta:2.4784276485443115 Generator / grammar loss:-0.2138051688671112   similarity loss:-0.16501717269420624\n--------------------------------------------------\ngold summary:\n\n--------------------------------------------------\nsam_wgan summary:\n 옛날 태어났어요. 병이들어 떠나고 맞이했어요. 소녀보다 새어머니와 언니들은 심술쟁이들이었어요. 딸들보다 예쁘고 착한 소녀는 쓸고, 집안일을 해도 해도 난롯가에 앉아서 쉬곤 했지요. 어느 왕궁에서 초대장이 언니들을 데리고 떠났어요. 신데렐라도 무도회에 가고 무도회에 가고 웃고 호박 한개와 도마뱀을 건드리자, 화려한 변했어요. 생쥐와 도마뱀을 마부로 신데렐라의 장식이 드레스로 내밀어 보거라. 할머니는 반짝반짝 유리 구두를 주었어요. 돼. 해. 왕자님은 가서 결혼하겠어요. 발을 구두는 수 구두를 구두는 왕궁으로 데리고 결혼하여 살았대요.\n--------------------------------------------------\n     method  comp ratio     intro     body    ending       var     total  \\\n0  SAM+WGAN    0.207873  0.577428  0.57945  0.590106  0.000031  0.580588   \n\n    grammar  \n0  0.977109  \n------------------------------------------------------------------\n옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요. 그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요. 소녀의 아버지는 홀로 남은 소녀가 걱정되었어요. 그래서 얼마 후 새어머니를 맞이했어요. 새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요. 그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요. 새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요. 그런데 이번에는 아버지마저 돌아가셨어요. 소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시 쉬곤 했지요. 어느 날, 왕궁에서 무도회가 열렸어요. 신데렐라의 집에도 초대장이 왔어요. 새어머니는 언니들을 데리고 무도회장으로 떠났어요. 신데렐라도 무도회에 가고 싶었어요. 혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요. 신데렐라, 너도 무도회에 가고 싶니? 신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요. 내가 너를 무도회에 보내주마 호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴. 마법사 할머니가 주문을 외웠어요. 그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요. 이번에는 생쥐와 도마뱀을 건드렸어요. 그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다. 신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요. 신데렐라, 발을 내밀어 보거라. 할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요. 신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼. 그러니까 반드시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지? 왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요. 왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,신데렐라하고만 춤을 추었어요. 신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요. 땡, 땡, 땡 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요. 신데렐라가 허둥지둥 왕궁을 빠져나가는데, 유리 구두 한 짝이 벗겨졌어요. 하지만 구두를 주울 틈이 없었어요. 신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요. 왕자님은 유리 구두를 가지고 임금님께 가서 말했어요. 이 유리 구두의 주인과 결혼하겠어요. 그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요. 언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리 구두는 너무 작았어요. 그때, 신데렐라가 조용히 다가와 말했어요. 저도 한번 신어 볼 수 있나요? 신데렐라는 신하게 건넨 유리 구두를 신었어요, 유리 구두는 신데렐라의 발에 꼭 맞았어요. 신하들은 신데렐라를 왕궁으로 데리고 갔어요. 그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\n------------------------------------------------------------------\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/327        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/326        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/325        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/324        \nNegative tokens: ['여자' '소녀가' '소녀의' '소녀가' '딸을' '새어머니와' '새어머니는' '집안일이' '신데렐라의' '신데렐라도'\n '신데렐라는' '신데렐라,' '신데렐라가' '할머니가' '무도회에' '할머니가' '마차로' '신데렐라의' '신데렐라,'\n '신데렐라에게' '신데렐라,' '흰말은' '신데렐라에게' '신데렐라는' '신데렐라는' '신데렐라가' '구두' '구두를'\n '신데렐라를' '구두' '왕자님은' '구두를' '구두의' '언니들은' '구두를' '신데렐라가' '신데렐라는' '신데렐라의'\n '신데렐라를' '신데렐라는']\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 325/326       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 324/325       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 323/324       \nPeak count: 27\nFrame tokens: 소녀의 맞이했어요. 언니들은 딸들보다 소녀는 왕궁에서 언니들을 무도회에 무도회에 마부로 드레스로 할머니는 구두를 왕자님도 무도회장에 않고,신데렐라하고만 춤을 왕궁을 왕자님은 결혼하겠어요. 발을 구두는 구두를 구두는 왕궁으로 결혼하여 살았대요. \n\nSimilarity : 0.48041270657702473\nTrain... |||||||||||||||||||||| 100.0%   500/500 epochs, beta:2.4784276485443115 Generator / grammar loss:-0.2138051688671112   similarity loss:-0.16501717269420624\n--------------------------------------------------\ngold summary:\n\n--------------------------------------------------\nsam_wgan summary:\n 옛날 태어났어요. 병이들어 떠나고 맞이했어요. 소녀보다 새어머니와 언니들은 심술쟁이들이었어요. 딸들보다 예쁘고 착한 소녀는 쓸고, 집안일을 해도 해도 난롯가에 앉아서 쉬곤 했지요. 어느 왕궁에서 초대장이 언니들을 데리고 떠났어요. 신데렐라도 무도회에 가고 무도회에 가고 웃고 호박 한개와 도마뱀을 건드리자, 화려한 변했어요. 생쥐와 도마뱀을 마부로 신데렐라의 장식이 드레스로 내밀어 보거라. 할머니는 반짝반짝 유리 구두를 주었어요. 돼. 해. 왕자님은 가서 결혼하겠어요. 발을 구두는 수 구두를 구두는 왕궁으로 데리고 결혼하여 살았대요.\n--------------------------------------------------\n","name":"stdout"},{"output_type":"stream","text":"     method  comp ratio     intro     body    ending       var     total  \\\n0  SAM+WGAN    0.207873  0.577428  0.57945  0.590106  0.000031  0.580588   \n\n    grammar  \n0  0.977109  \n------------------------------------------------------------------\n옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요. 그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요. 소녀의 아버지는 홀로 남은 소녀가 걱정되었어요. 그래서 얼마 후 새어머니를 맞이했어요. 새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요. 그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요. 새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요. 그런데 이번에는 아버지마저 돌아가셨어요. 소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시 쉬곤 했지요. 어느 날, 왕궁에서 무도회가 열렸어요. 신데렐라의 집에도 초대장이 왔어요. 새어머니는 언니들을 데리고 무도회장으로 떠났어요. 신데렐라도 무도회에 가고 싶었어요. 혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요. 신데렐라, 너도 무도회에 가고 싶니? 신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요. 내가 너를 무도회에 보내주마 호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴. 마법사 할머니가 주문을 외웠어요. 그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요. 이번에는 생쥐와 도마뱀을 건드렸어요. 그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다. 신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요. 신데렐라, 발을 내밀어 보거라. 할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요. 신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼. 그러니까 반드시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지? 왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요. 왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,신데렐라하고만 춤을 추었어요. 신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요. 땡, 땡, 땡 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요. 신데렐라가 허둥지둥 왕궁을 빠져나가는데, 유리 구두 한 짝이 벗겨졌어요. 하지만 구두를 주울 틈이 없었어요. 신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요. 왕자님은 유리 구두를 가지고 임금님께 가서 말했어요. 이 유리 구두의 주인과 결혼하겠어요. 그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요. 언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리 구두는 너무 작았어요. 그때, 신데렐라가 조용히 다가와 말했어요. 저도 한번 신어 볼 수 있나요? 신데렐라는 신하게 건넨 유리 구두를 신었어요, 유리 구두는 신데렐라의 발에 꼭 맞았어요. 신하들은 신데렐라를 왕궁으로 데리고 갔어요. 그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\n------------------------------------------------------------------\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/327        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/326        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/325        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/324        \nNegative tokens: ['여자' '소녀가' '소녀의' '소녀가' '딸을' '새어머니와' '새어머니는' '집안일이' '신데렐라의' '신데렐라도'\n '신데렐라는' '신데렐라,' '신데렐라가' '할머니가' '무도회에' '할머니가' '마차로' '신데렐라의' '신데렐라,'\n '신데렐라에게' '신데렐라,' '흰말은' '신데렐라에게' '신데렐라는' '신데렐라는' '신데렐라가' '구두' '구두를'\n '신데렐라를' '구두' '왕자님은' '구두를' '구두의' '언니들은' '구두를' '신데렐라가' '신데렐라는' '신데렐라의'\n '신데렐라를' '신데렐라는']\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 325/326       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 324/325       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 323/324       \nPeak count: 27\nFrame tokens: 소녀의 맞이했어요. 언니들은 딸들보다 소녀는 왕궁에서 언니들을 무도회에 무도회에 마부로 드레스로 할머니는 구두를 왕자님도 무도회장에 않고,신데렐라하고만 춤을 왕궁을 왕자님은 결혼하겠어요. 발을 구두는 구두를 구두는 왕궁으로 결혼하여 살았대요. \n\nSimilarity : 0.48041270657702473\nTrain... |||||||||||||||||||||| 100.0%   500/500 epochs, beta:2.4784276485443115 Generator / grammar loss:-0.2138051688671112   similarity loss:-0.16501717269420624\n--------------------------------------------------\ngold summary:\n\n--------------------------------------------------\nsam_wgan summary:\n 옛날 태어났어요. 병이들어 떠나고 맞이했어요. 소녀보다 새어머니와 언니들은 심술쟁이들이었어요. 딸들보다 예쁘고 착한 소녀는 쓸고, 집안일을 해도 해도 난롯가에 앉아서 쉬곤 했지요. 어느 왕궁에서 초대장이 언니들을 데리고 떠났어요. 신데렐라도 무도회에 가고 무도회에 가고 웃고 호박 한개와 도마뱀을 건드리자, 화려한 변했어요. 생쥐와 도마뱀을 마부로 신데렐라의 장식이 드레스로 내밀어 보거라. 할머니는 반짝반짝 유리 구두를 주었어요. 돼. 해. 왕자님은 가서 결혼하겠어요. 발을 구두는 수 구두를 구두는 왕궁으로 데리고 결혼하여 살았대요.\n--------------------------------------------------\n     method  comp ratio     intro     body    ending       var     total  \\\n0  SAM+WGAN    0.207873  0.577428  0.57945  0.590106  0.000031  0.580588   \n\n    grammar  \n0  0.977109  \n------------------------------------------------------------------\n옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요. 그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요. 소녀의 아버지는 홀로 남은 소녀가 걱정되었어요. 그래서 얼마 후 새어머니를 맞이했어요. 새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요. 그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요. 새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요. 그런데 이번에는 아버지마저 돌아가셨어요. 소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시 쉬곤 했지요. 어느 날, 왕궁에서 무도회가 열렸어요. 신데렐라의 집에도 초대장이 왔어요. 새어머니는 언니들을 데리고 무도회장으로 떠났어요. 신데렐라도 무도회에 가고 싶었어요. 혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요. 신데렐라, 너도 무도회에 가고 싶니? 신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요. 내가 너를 무도회에 보내주마 호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴. 마법사 할머니가 주문을 외웠어요. 그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요. 이번에는 생쥐와 도마뱀을 건드렸어요. 그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다. 신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요. 신데렐라, 발을 내밀어 보거라. 할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요. 신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼. 그러니까 반드시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지? 왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요. 왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,신데렐라하고만 춤을 추었어요. 신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요. 땡, 땡, 땡 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요. 신데렐라가 허둥지둥 왕궁을 빠져나가는데, 유리 구두 한 짝이 벗겨졌어요. 하지만 구두를 주울 틈이 없었어요. 신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요. 왕자님은 유리 구두를 가지고 임금님께 가서 말했어요. 이 유리 구두의 주인과 결혼하겠어요. 그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요. 언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리 구두는 너무 작았어요. 그때, 신데렐라가 조용히 다가와 말했어요. 저도 한번 신어 볼 수 있나요? 신데렐라는 신하게 건넨 유리 구두를 신었어요, 유리 구두는 신데렐라의 발에 꼭 맞았어요. 신하들은 신데렐라를 왕궁으로 데리고 갔어요. 그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\n------------------------------------------------------------------\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/327        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/326        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/325        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/324        \nNegative tokens: ['여자' '소녀가' '소녀의' '소녀가' '딸을' '새어머니와' '새어머니는' '집안일이' '신데렐라의' '신데렐라도'\n '신데렐라는' '신데렐라,' '신데렐라가' '할머니가' '무도회에' '할머니가' '마차로' '신데렐라의' '신데렐라,'\n '신데렐라에게' '신데렐라,' '흰말은' '신데렐라에게' '신데렐라는' '신데렐라는' '신데렐라가' '구두' '구두를'\n '신데렐라를' '구두' '왕자님은' '구두를' '구두의' '언니들은' '구두를' '신데렐라가' '신데렐라는' '신데렐라의'\n '신데렐라를' '신데렐라는']\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 325/326       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 324/325       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 323/324       \nPeak count: 27\nFrame tokens: 소녀의 맞이했어요. 언니들은 딸들보다 소녀는 왕궁에서 언니들을 무도회에 무도회에 마부로 드레스로 할머니는 구두를 왕자님도 무도회장에 않고,신데렐라하고만 춤을 왕궁을 왕자님은 결혼하겠어요. 발을 구두는 구두를 구두는 왕궁으로 결혼하여 살았대요. \n\nSimilarity : 0.48041270657702473\nTrain... |||||||||||||||||||||| 100.0%   500/500 epochs, beta:2.358942985534668 Generator / grammar loss:-0.1862739622592926   similarity loss:-0.149986654520034843\n--------------------------------------------------\ngold summary:\n\n--------------------------------------------------\nsam_wgan summary:\n 옛날 태어났어요. 병이들어 떠나고 맞이했어요. 소녀보다 새어머니와 언니들은 심술쟁이들이었어요. 딸들보다 예쁘고 착한 소녀는 쓸고, 집안일을 해도 해도 난롯가에 앉아서 쉬곤 했지요. 어느 왕궁에서 초대장이 언니들을 데리고 떠났어요. 신데렐라도 무도회에 가고 무도회에 가고 웃고 호박 한개와 도마뱀을 건드리자, 화려한 변했어요. 도마뱀을 마부로 신데렐라의 장식이 드레스로 내밀어 보거라. 할머니는 반짝반짝 유리 구두를 주었어요. 돼. 해. 왕자님은 가서 결혼하겠어요. 나라를 발을 구두는 수 구두를 구두는 왕궁으로 데리고 결혼하여 살았대요.\n--------------------------------------------------\n     method  comp ratio     intro      body    ending       var     total  \\\n0  SAM+WGAN    0.207873  0.577428  0.565339  0.578573  0.000036  0.570244   \n\n    grammar  \n0  0.971597  \n------------------------------------------------------------------\n옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요. 그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요. 소녀의 아버지는 홀로 남은 소녀가 걱정되었어요. 그래서 얼마 후 새어머니를 맞이했어요. 새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요. 그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요. 새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요. 그런데 이번에는 아버지마저 돌아가셨어요. 소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시 쉬곤 했지요. 어느 날, 왕궁에서 무도회가 열렸어요. 신데렐라의 집에도 초대장이 왔어요. 새어머니는 언니들을 데리고 무도회장으로 떠났어요. 신데렐라도 무도회에 가고 싶었어요. 혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요. 신데렐라, 너도 무도회에 가고 싶니? 신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요. 내가 너를 무도회에 보내주마 호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴. 마법사 할머니가 주문을 외웠어요. 그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요. 이번에는 생쥐와 도마뱀을 건드렸어요. 그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다. 신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요. 신데렐라, 발을 내밀어 보거라. 할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요. 신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼. 그러니까 반드시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지? 왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요. 왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,신데렐라하고만 춤을 추었어요. 신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요. 땡, 땡, 땡 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요. 신데렐라가 허둥지둥 왕궁을 빠져나가는데, 유리 구두 한 짝이 벗겨졌어요. 하지만 구두를 주울 틈이 없었어요. 신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요. 왕자님은 유리 구두를 가지고 임금님께 가서 말했어요. 이 유리 구두의 주인과 결혼하겠어요. 그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요. 언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리 구두는 너무 작았어요. 그때, 신데렐라가 조용히 다가와 말했어요. 저도 한번 신어 볼 수 있나요? 신데렐라는 신하게 건넨 유리 구두를 신었어요, 유리 구두는 신데렐라의 발에 꼭 맞았어요. 신하들은 신데렐라를 왕궁으로 데리고 갔어요. 그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\n------------------------------------------------------------------\n","name":"stdout"},{"output_type":"stream","text":"Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/327        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/326        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/325        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/324        \nNegative tokens: ['여자' '소녀가' '소녀의' '소녀가' '딸을' '새어머니와' '새어머니는' '집안일이' '신데렐라의' '신데렐라도'\n '신데렐라는' '신데렐라,' '신데렐라가' '할머니가' '무도회에' '할머니가' '마차로' '신데렐라의' '신데렐라,'\n '신데렐라에게' '신데렐라,' '흰말은' '신데렐라에게' '신데렐라는' '신데렐라는' '신데렐라가' '구두' '구두를'\n '신데렐라를' '구두' '왕자님은' '구두를' '구두의' '언니들은' '구두를' '신데렐라가' '신데렐라는' '신데렐라의'\n '신데렐라를' '신데렐라는']\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 325/326       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 324/325       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 323/324       \nPeak count: 27\nFrame tokens: 소녀의 맞이했어요. 언니들은 딸들보다 소녀는 왕궁에서 언니들을 무도회에 무도회에 마부로 드레스로 할머니는 구두를 왕자님도 무도회장에 않고,신데렐라하고만 춤을 왕궁을 왕자님은 결혼하겠어요. 발을 구두는 구두를 구두는 왕궁으로 결혼하여 살았대요. \n\nSimilarity : 0.48041270657702473\nTrain... |||||||||||||||||||||| 100.0%   500/500 epochs, beta:2.358942985534668 Generator / grammar loss:-0.1862739622592926   similarity loss:-0.149986654520034843\n--------------------------------------------------\ngold summary:\n\n--------------------------------------------------\nsam_wgan summary:\n 옛날 태어났어요. 병이들어 떠나고 맞이했어요. 소녀보다 새어머니와 언니들은 심술쟁이들이었어요. 딸들보다 예쁘고 착한 소녀는 쓸고, 집안일을 해도 해도 난롯가에 앉아서 쉬곤 했지요. 어느 왕궁에서 초대장이 언니들을 데리고 떠났어요. 신데렐라도 무도회에 가고 무도회에 가고 웃고 호박 한개와 도마뱀을 건드리자, 화려한 변했어요. 도마뱀을 마부로 신데렐라의 장식이 드레스로 내밀어 보거라. 할머니는 반짝반짝 유리 구두를 주었어요. 돼. 해. 왕자님은 가서 결혼하겠어요. 나라를 발을 구두는 수 구두를 구두는 왕궁으로 데리고 결혼하여 살았대요.\n--------------------------------------------------\n     method  comp ratio     intro      body    ending       var     total  \\\n0  SAM+WGAN    0.207873  0.577428  0.565339  0.578573  0.000036  0.570244   \n\n    grammar  \n0  0.971597  \n------------------------------------------------------------------\n옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요. 그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요. 소녀의 아버지는 홀로 남은 소녀가 걱정되었어요. 그래서 얼마 후 새어머니를 맞이했어요. 새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요. 그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요. 새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요. 그런데 이번에는 아버지마저 돌아가셨어요. 소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시 쉬곤 했지요. 어느 날, 왕궁에서 무도회가 열렸어요. 신데렐라의 집에도 초대장이 왔어요. 새어머니는 언니들을 데리고 무도회장으로 떠났어요. 신데렐라도 무도회에 가고 싶었어요. 혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요. 신데렐라, 너도 무도회에 가고 싶니? 신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요. 내가 너를 무도회에 보내주마 호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴. 마법사 할머니가 주문을 외웠어요. 그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요. 이번에는 생쥐와 도마뱀을 건드렸어요. 그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다. 신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요. 신데렐라, 발을 내밀어 보거라. 할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요. 신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼. 그러니까 반드시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지? 왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요. 왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,신데렐라하고만 춤을 추었어요. 신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요. 땡, 땡, 땡 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요. 신데렐라가 허둥지둥 왕궁을 빠져나가는데, 유리 구두 한 짝이 벗겨졌어요. 하지만 구두를 주울 틈이 없었어요. 신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요. 왕자님은 유리 구두를 가지고 임금님께 가서 말했어요. 이 유리 구두의 주인과 결혼하겠어요. 그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요. 언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리 구두는 너무 작았어요. 그때, 신데렐라가 조용히 다가와 말했어요. 저도 한번 신어 볼 수 있나요? 신데렐라는 신하게 건넨 유리 구두를 신었어요, 유리 구두는 신데렐라의 발에 꼭 맞았어요. 신하들은 신데렐라를 왕궁으로 데리고 갔어요. 그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\n------------------------------------------------------------------\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/327        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/326        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/325        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/324        \nNegative tokens: ['여자' '소녀가' '소녀의' '소녀가' '딸을' '새어머니와' '새어머니는' '집안일이' '신데렐라의' '신데렐라도'\n '신데렐라는' '신데렐라,' '신데렐라가' '할머니가' '무도회에' '할머니가' '마차로' '신데렐라의' '신데렐라,'\n '신데렐라에게' '신데렐라,' '흰말은' '신데렐라에게' '신데렐라는' '신데렐라는' '신데렐라가' '구두' '구두를'\n '신데렐라를' '구두' '왕자님은' '구두를' '구두의' '언니들은' '구두를' '신데렐라가' '신데렐라는' '신데렐라의'\n '신데렐라를' '신데렐라는']\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 325/326       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 324/325       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 323/324       \nPeak count: 27\nFrame tokens: 소녀의 맞이했어요. 언니들은 딸들보다 소녀는 왕궁에서 언니들을 무도회에 무도회에 마부로 드레스로 할머니는 구두를 왕자님도 무도회장에 않고,신데렐라하고만 춤을 왕궁을 왕자님은 결혼하겠어요. 발을 구두는 구두를 구두는 왕궁으로 결혼하여 살았대요. \n\nSimilarity : 0.48041270657702473\nTrain... |||||||||||||||||||||| 100.0%   500/500 epochs, beta:2.358942985534668 Generator / grammar loss:-0.1862739622592926   similarity loss:-0.149986654520034843\n--------------------------------------------------\ngold summary:\n\n--------------------------------------------------\nsam_wgan summary:\n 옛날 태어났어요. 병이들어 떠나고 맞이했어요. 소녀보다 새어머니와 언니들은 심술쟁이들이었어요. 딸들보다 예쁘고 착한 소녀는 쓸고, 집안일을 해도 해도 난롯가에 앉아서 쉬곤 했지요. 어느 왕궁에서 초대장이 언니들을 데리고 떠났어요. 신데렐라도 무도회에 가고 무도회에 가고 웃고 호박 한개와 도마뱀을 건드리자, 화려한 변했어요. 도마뱀을 마부로 신데렐라의 장식이 드레스로 내밀어 보거라. 할머니는 반짝반짝 유리 구두를 주었어요. 돼. 해. 왕자님은 가서 결혼하겠어요. 나라를 발을 구두는 수 구두를 구두는 왕궁으로 데리고 결혼하여 살았대요.\n--------------------------------------------------\n     method  comp ratio     intro      body    ending       var     total  \\\n0  SAM+WGAN    0.207873  0.577428  0.565339  0.578573  0.000036  0.570244   \n\n    grammar  \n0  0.971597  \n------------------------------------------------------------------\n옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요. 그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요. 소녀의 아버지는 홀로 남은 소녀가 걱정되었어요. 그래서 얼마 후 새어머니를 맞이했어요. 새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요. 그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요. 새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요. 그런데 이번에는 아버지마저 돌아가셨어요. 소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시 쉬곤 했지요. 어느 날, 왕궁에서 무도회가 열렸어요. 신데렐라의 집에도 초대장이 왔어요. 새어머니는 언니들을 데리고 무도회장으로 떠났어요. 신데렐라도 무도회에 가고 싶었어요. 혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요. 신데렐라, 너도 무도회에 가고 싶니? 신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요. 내가 너를 무도회에 보내주마 호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴. 마법사 할머니가 주문을 외웠어요. 그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요. 이번에는 생쥐와 도마뱀을 건드렸어요. 그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다. 신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요. 신데렐라, 발을 내밀어 보거라. 할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요. 신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼. 그러니까 반드시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지? 왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요. 왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,신데렐라하고만 춤을 추었어요. 신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요. 땡, 땡, 땡 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요. 신데렐라가 허둥지둥 왕궁을 빠져나가는데, 유리 구두 한 짝이 벗겨졌어요. 하지만 구두를 주울 틈이 없었어요. 신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요. 왕자님은 유리 구두를 가지고 임금님께 가서 말했어요. 이 유리 구두의 주인과 결혼하겠어요. 그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요. 언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리 구두는 너무 작았어요. 그때, 신데렐라가 조용히 다가와 말했어요. 저도 한번 신어 볼 수 있나요? 신데렐라는 신하게 건넨 유리 구두를 신었어요, 유리 구두는 신데렐라의 발에 꼭 맞았어요. 신하들은 신데렐라를 왕궁으로 데리고 갔어요. 그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\n------------------------------------------------------------------\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/327        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/326        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/325        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/324        \nNegative tokens: ['여자' '소녀가' '소녀의' '소녀가' '딸을' '새어머니와' '새어머니는' '집안일이' '신데렐라의' '신데렐라도'\n '신데렐라는' '신데렐라,' '신데렐라가' '할머니가' '무도회에' '할머니가' '마차로' '신데렐라의' '신데렐라,'\n '신데렐라에게' '신데렐라,' '흰말은' '신데렐라에게' '신데렐라는' '신데렐라는' '신데렐라가' '구두' '구두를'\n '신데렐라를' '구두' '왕자님은' '구두를' '구두의' '언니들은' '구두를' '신데렐라가' '신데렐라는' '신데렐라의'\n '신데렐라를' '신데렐라는']\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 325/326       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 324/325       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 323/324       \nPeak count: 27\nFrame tokens: 소녀의 맞이했어요. 언니들은 딸들보다 소녀는 왕궁에서 언니들을 무도회에 무도회에 마부로 드레스로 할머니는 구두를 왕자님도 무도회장에 않고,신데렐라하고만 춤을 왕궁을 왕자님은 결혼하겠어요. 발을 구두는 구두를 구두는 왕궁으로 결혼하여 살았대요. \n\nSimilarity : 0.48041270657702473\nTrain... |||||||||||||||||||||| 100.0%   500/500 epochs, beta:2.35810923576355 Generator / grammar loss:-0.1814960092306137   similarity loss:-0.1452948600053787228\n--------------------------------------------------\ngold summary:\n\n--------------------------------------------------\nsam_wgan summary:\n 옛날 태어났어요. 병이들어 떠나고 맞이했어요. 소녀보다 새어머니와 언니들은 심술쟁이들이었어요. 딸들보다 예쁘고 착한 소녀는 쓸고, 집안일을 해도 해도 난롯가에 앉아서 쉬곤 했지요. 어느 왕궁에서 초대장이 언니들을 데리고 떠났어요. 신데렐라도 무도회에 가고 무도회에 가고 웃고 호박 한개와 도마뱀을 건드리자, 화려한 변했어요. 도마뱀을 마부로 신데렐라의 장식이 드레스로 내밀어 보거라. 할머니는 반짝반짝 유리 구두를 주었어요. 돼. 해. 왕자님은 가서 결혼하겠어요. 나라를 발을 구두는 수 구두를 구두는 왕궁으로 데리고 결혼하여 살았대요.\n--------------------------------------------------\n","name":"stdout"},{"output_type":"stream","text":"     method  comp ratio     intro      body    ending       var     total  \\\n0  SAM+WGAN    0.207873  0.577428  0.565339  0.578573  0.000036  0.570244   \n\n    grammar  \n0  0.971597  \n------------------------------------------------------------------\n옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요. 그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요. 소녀의 아버지는 홀로 남은 소녀가 걱정되었어요. 그래서 얼마 후 새어머니를 맞이했어요. 새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요. 그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요. 새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요. 그런데 이번에는 아버지마저 돌아가셨어요. 소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시 쉬곤 했지요. 어느 날, 왕궁에서 무도회가 열렸어요. 신데렐라의 집에도 초대장이 왔어요. 새어머니는 언니들을 데리고 무도회장으로 떠났어요. 신데렐라도 무도회에 가고 싶었어요. 혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요. 신데렐라, 너도 무도회에 가고 싶니? 신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요. 내가 너를 무도회에 보내주마 호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴. 마법사 할머니가 주문을 외웠어요. 그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요. 이번에는 생쥐와 도마뱀을 건드렸어요. 그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다. 신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요. 신데렐라, 발을 내밀어 보거라. 할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요. 신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼. 그러니까 반드시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지? 왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요. 왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,신데렐라하고만 춤을 추었어요. 신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요. 땡, 땡, 땡 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요. 신데렐라가 허둥지둥 왕궁을 빠져나가는데, 유리 구두 한 짝이 벗겨졌어요. 하지만 구두를 주울 틈이 없었어요. 신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요. 왕자님은 유리 구두를 가지고 임금님께 가서 말했어요. 이 유리 구두의 주인과 결혼하겠어요. 그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요. 언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리 구두는 너무 작았어요. 그때, 신데렐라가 조용히 다가와 말했어요. 저도 한번 신어 볼 수 있나요? 신데렐라는 신하게 건넨 유리 구두를 신었어요, 유리 구두는 신데렐라의 발에 꼭 맞았어요. 신하들은 신데렐라를 왕궁으로 데리고 갔어요. 그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\n------------------------------------------------------------------\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/327        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/326        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/325        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/324        \nNegative tokens: ['여자' '소녀가' '소녀의' '소녀가' '딸을' '새어머니와' '새어머니는' '집안일이' '신데렐라의' '신데렐라도'\n '신데렐라는' '신데렐라,' '신데렐라가' '할머니가' '무도회에' '할머니가' '마차로' '신데렐라의' '신데렐라,'\n '신데렐라에게' '신데렐라,' '흰말은' '신데렐라에게' '신데렐라는' '신데렐라는' '신데렐라가' '구두' '구두를'\n '신데렐라를' '구두' '왕자님은' '구두를' '구두의' '언니들은' '구두를' '신데렐라가' '신데렐라는' '신데렐라의'\n '신데렐라를' '신데렐라는']\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 325/326       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 324/325       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 323/324       \nPeak count: 27\nFrame tokens: 소녀의 맞이했어요. 언니들은 딸들보다 소녀는 왕궁에서 언니들을 무도회에 무도회에 마부로 드레스로 할머니는 구두를 왕자님도 무도회장에 않고,신데렐라하고만 춤을 왕궁을 왕자님은 결혼하겠어요. 발을 구두는 구두를 구두는 왕궁으로 결혼하여 살았대요. \n\nSimilarity : 0.48041270657702473\nTrain... |||||||||||||||||||||| 100.0%   500/500 epochs, beta:2.35810923576355 Generator / grammar loss:-0.1814960092306137   similarity loss:-0.1452948600053787228\n--------------------------------------------------\ngold summary:\n\n--------------------------------------------------\nsam_wgan summary:\n 옛날 태어났어요. 병이들어 떠나고 맞이했어요. 소녀보다 새어머니와 언니들은 심술쟁이들이었어요. 딸들보다 예쁘고 착한 소녀는 쓸고, 집안일을 해도 해도 난롯가에 앉아서 쉬곤 했지요. 어느 왕궁에서 초대장이 언니들을 데리고 떠났어요. 신데렐라도 무도회에 가고 무도회에 가고 웃고 호박 한개와 도마뱀을 건드리자, 화려한 변했어요. 도마뱀을 마부로 신데렐라의 장식이 드레스로 내밀어 보거라. 할머니는 반짝반짝 유리 구두를 주었어요. 돼. 해. 왕자님은 가서 결혼하겠어요. 나라를 발을 구두는 수 구두를 구두는 왕궁으로 데리고 결혼하여 살았대요.\n--------------------------------------------------\n     method  comp ratio     intro      body    ending       var     total  \\\n0  SAM+WGAN    0.207873  0.577428  0.565339  0.578573  0.000036  0.570244   \n\n    grammar  \n0  0.971597  \n------------------------------------------------------------------\n옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요. 그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요. 소녀의 아버지는 홀로 남은 소녀가 걱정되었어요. 그래서 얼마 후 새어머니를 맞이했어요. 새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요. 그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요. 새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요. 그런데 이번에는 아버지마저 돌아가셨어요. 소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시 쉬곤 했지요. 어느 날, 왕궁에서 무도회가 열렸어요. 신데렐라의 집에도 초대장이 왔어요. 새어머니는 언니들을 데리고 무도회장으로 떠났어요. 신데렐라도 무도회에 가고 싶었어요. 혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요. 신데렐라, 너도 무도회에 가고 싶니? 신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요. 내가 너를 무도회에 보내주마 호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴. 마법사 할머니가 주문을 외웠어요. 그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요. 이번에는 생쥐와 도마뱀을 건드렸어요. 그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다. 신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요. 신데렐라, 발을 내밀어 보거라. 할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요. 신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼. 그러니까 반드시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지? 왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요. 왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,신데렐라하고만 춤을 추었어요. 신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요. 땡, 땡, 땡 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요. 신데렐라가 허둥지둥 왕궁을 빠져나가는데, 유리 구두 한 짝이 벗겨졌어요. 하지만 구두를 주울 틈이 없었어요. 신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요. 왕자님은 유리 구두를 가지고 임금님께 가서 말했어요. 이 유리 구두의 주인과 결혼하겠어요. 그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요. 언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리 구두는 너무 작았어요. 그때, 신데렐라가 조용히 다가와 말했어요. 저도 한번 신어 볼 수 있나요? 신데렐라는 신하게 건넨 유리 구두를 신었어요, 유리 구두는 신데렐라의 발에 꼭 맞았어요. 신하들은 신데렐라를 왕궁으로 데리고 갔어요. 그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\n------------------------------------------------------------------\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/327        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/326        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/325        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/324        \nNegative tokens: ['여자' '소녀가' '소녀의' '소녀가' '딸을' '새어머니와' '새어머니는' '집안일이' '신데렐라의' '신데렐라도'\n '신데렐라는' '신데렐라,' '신데렐라가' '할머니가' '무도회에' '할머니가' '마차로' '신데렐라의' '신데렐라,'\n '신데렐라에게' '신데렐라,' '흰말은' '신데렐라에게' '신데렐라는' '신데렐라는' '신데렐라가' '구두' '구두를'\n '신데렐라를' '구두' '왕자님은' '구두를' '구두의' '언니들은' '구두를' '신데렐라가' '신데렐라는' '신데렐라의'\n '신데렐라를' '신데렐라는']\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 325/326       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 324/325       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 323/324       \nPeak count: 27\nFrame tokens: 소녀의 맞이했어요. 언니들은 딸들보다 소녀는 왕궁에서 언니들을 무도회에 무도회에 마부로 드레스로 할머니는 구두를 왕자님도 무도회장에 않고,신데렐라하고만 춤을 왕궁을 왕자님은 결혼하겠어요. 발을 구두는 구두를 구두는 왕궁으로 결혼하여 살았대요. \n\nSimilarity : 0.48041270657702473\nTrain... |||||||||||||||||||||| 100.0%   500/500 epochs, beta:2.35810923576355 Generator / grammar loss:-0.1814960092306137   similarity loss:-0.1452948600053787228\n--------------------------------------------------\ngold summary:\n\n--------------------------------------------------\nsam_wgan summary:\n 옛날 태어났어요. 병이들어 떠나고 맞이했어요. 소녀보다 새어머니와 언니들은 심술쟁이들이었어요. 딸들보다 예쁘고 착한 소녀는 쓸고, 집안일을 해도 해도 난롯가에 앉아서 쉬곤 했지요. 어느 왕궁에서 초대장이 언니들을 데리고 떠났어요. 신데렐라도 무도회에 가고 무도회에 가고 웃고 호박 한개와 도마뱀을 건드리자, 화려한 변했어요. 도마뱀을 마부로 신데렐라의 장식이 드레스로 내밀어 보거라. 할머니는 반짝반짝 유리 구두를 주었어요. 돼. 해. 왕자님은 가서 결혼하겠어요. 나라를 발을 구두는 수 구두를 구두는 왕궁으로 데리고 결혼하여 살았대요.\n--------------------------------------------------\n     method  comp ratio     intro      body    ending       var     total  \\\n0  SAM+WGAN    0.207873  0.577428  0.565339  0.578573  0.000036  0.570244   \n\n    grammar  \n0  0.971597  \n------------------------------------------------------------------\n옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요. 그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요. 소녀의 아버지는 홀로 남은 소녀가 걱정되었어요. 그래서 얼마 후 새어머니를 맞이했어요. 새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요. 그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요. 새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요. 그런데 이번에는 아버지마저 돌아가셨어요. 소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시 쉬곤 했지요. 어느 날, 왕궁에서 무도회가 열렸어요. 신데렐라의 집에도 초대장이 왔어요. 새어머니는 언니들을 데리고 무도회장으로 떠났어요. 신데렐라도 무도회에 가고 싶었어요. 혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요. 신데렐라, 너도 무도회에 가고 싶니? 신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요. 내가 너를 무도회에 보내주마 호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴. 마법사 할머니가 주문을 외웠어요. 그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요. 이번에는 생쥐와 도마뱀을 건드렸어요. 그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다. 신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요. 신데렐라, 발을 내밀어 보거라. 할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요. 신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼. 그러니까 반드시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지? 왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요. 왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,신데렐라하고만 춤을 추었어요. 신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요. 땡, 땡, 땡 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요. 신데렐라가 허둥지둥 왕궁을 빠져나가는데, 유리 구두 한 짝이 벗겨졌어요. 하지만 구두를 주울 틈이 없었어요. 신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요. 왕자님은 유리 구두를 가지고 임금님께 가서 말했어요. 이 유리 구두의 주인과 결혼하겠어요. 그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요. 언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리 구두는 너무 작았어요. 그때, 신데렐라가 조용히 다가와 말했어요. 저도 한번 신어 볼 수 있나요? 신데렐라는 신하게 건넨 유리 구두를 신었어요, 유리 구두는 신데렐라의 발에 꼭 맞았어요. 신하들은 신데렐라를 왕궁으로 데리고 갔어요. 그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\n------------------------------------------------------------------\n","name":"stdout"},{"output_type":"stream","text":"Frame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/327        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/326        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/325        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/324        \nNegative tokens: ['여자' '소녀가' '소녀의' '소녀가' '딸을' '새어머니와' '새어머니는' '집안일이' '신데렐라의' '신데렐라도'\n '신데렐라는' '신데렐라,' '신데렐라가' '할머니가' '무도회에' '할머니가' '마차로' '신데렐라의' '신데렐라,'\n '신데렐라에게' '신데렐라,' '흰말은' '신데렐라에게' '신데렐라는' '신데렐라는' '신데렐라가' '구두' '구두를'\n '신데렐라를' '구두' '왕자님은' '구두를' '구두의' '언니들은' '구두를' '신데렐라가' '신데렐라는' '신데렐라의'\n '신데렐라를' '신데렐라는']\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 325/326       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 324/325       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 323/324       \nPeak count: 27\nFrame tokens: 소녀의 맞이했어요. 언니들은 딸들보다 소녀는 왕궁에서 언니들을 무도회에 무도회에 마부로 드레스로 할머니는 구두를 왕자님도 무도회장에 않고,신데렐라하고만 춤을 왕궁을 왕자님은 결혼하겠어요. 발을 구두는 구두를 구두는 왕궁으로 결혼하여 살았대요. \n\nSimilarity : 0.48041270657702473\nTrain... |||||||||||||||||||||| 100.0%   500/500 epochs, beta:2.449862003326416 Generator / grammar loss:-0.19069623947143555   similarity loss:-0.14492745697498322\n--------------------------------------------------\ngold summary:\n\n--------------------------------------------------\nsam_wgan summary:\n 옛날 태어났어요. 병이들어 떠나고 맞이했어요. 소녀보다 새어머니와 언니들은 심술쟁이들이었어요. 딸들보다 예쁘고 착한 소녀는 쓸고, 집안일을 해도 해도 난롯가에 앉아서 쉬곤 했지요. 어느 왕궁에서 열렸어요. 초대장이 언니들을 데리고 떠났어요. 신데렐라도 무도회에 가고 무도회에 가고 웃고 호박 한개와 도마뱀을 건드리자, 화려한 변했어요. 도마뱀을 마부로 신데렐라의 장식이 드레스로 내밀어 보거라. 할머니는 반짝반짝 유리 구두를 주었어요. 해. 알리는 왕자님은 가서 결혼하겠어요. 발을 구두는 수 구두를 구두는 왕궁으로 데리고 결혼하여 살았대요.\n--------------------------------------------------\n     method  comp ratio     intro      body    ending       var     total  \\\n0  SAM+WGAN    0.209945  0.582162  0.577418  0.578483  0.000004  0.578714   \n\n    grammar  \n0  0.975317  \n------------------------------------------------------------------\n옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요. 그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요. 소녀의 아버지는 홀로 남은 소녀가 걱정되었어요. 그래서 얼마 후 새어머니를 맞이했어요. 새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요. 그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요. 새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요. 그런데 이번에는 아버지마저 돌아가셨어요. 소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시 쉬곤 했지요. 어느 날, 왕궁에서 무도회가 열렸어요. 신데렐라의 집에도 초대장이 왔어요. 새어머니는 언니들을 데리고 무도회장으로 떠났어요. 신데렐라도 무도회에 가고 싶었어요. 혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요. 신데렐라, 너도 무도회에 가고 싶니? 신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요. 내가 너를 무도회에 보내주마 호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴. 마법사 할머니가 주문을 외웠어요. 그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요. 이번에는 생쥐와 도마뱀을 건드렸어요. 그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다. 신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요. 신데렐라, 발을 내밀어 보거라. 할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요. 신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼. 그러니까 반드시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지? 왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요. 왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,신데렐라하고만 춤을 추었어요. 신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요. 땡, 땡, 땡 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요. 신데렐라가 허둥지둥 왕궁을 빠져나가는데, 유리 구두 한 짝이 벗겨졌어요. 하지만 구두를 주울 틈이 없었어요. 신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요. 왕자님은 유리 구두를 가지고 임금님께 가서 말했어요. 이 유리 구두의 주인과 결혼하겠어요. 그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요. 언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리 구두는 너무 작았어요. 그때, 신데렐라가 조용히 다가와 말했어요. 저도 한번 신어 볼 수 있나요? 신데렐라는 신하게 건넨 유리 구두를 신었어요, 유리 구두는 신데렐라의 발에 꼭 맞았어요. 신하들은 신데렐라를 왕궁으로 데리고 갔어요. 그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\n------------------------------------------------------------------\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/327        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/326        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/325        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/324        \nNegative tokens: ['여자' '소녀가' '소녀의' '소녀가' '딸을' '새어머니와' '새어머니는' '집안일이' '신데렐라의' '신데렐라도'\n '신데렐라는' '신데렐라,' '신데렐라가' '할머니가' '무도회에' '할머니가' '마차로' '신데렐라의' '신데렐라,'\n '신데렐라에게' '신데렐라,' '흰말은' '신데렐라에게' '신데렐라는' '신데렐라는' '신데렐라가' '구두' '구두를'\n '신데렐라를' '구두' '왕자님은' '구두를' '구두의' '언니들은' '구두를' '신데렐라가' '신데렐라는' '신데렐라의'\n '신데렐라를' '신데렐라는']\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 325/326       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 324/325       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 323/324       \nPeak count: 27\nFrame tokens: 소녀의 맞이했어요. 언니들은 딸들보다 소녀는 왕궁에서 언니들을 무도회에 무도회에 마부로 드레스로 할머니는 구두를 왕자님도 무도회장에 않고,신데렐라하고만 춤을 왕궁을 왕자님은 결혼하겠어요. 발을 구두는 구두를 구두는 왕궁으로 결혼하여 살았대요. \n\nSimilarity : 0.48041270657702473\nTrain... |||||||||||||||||||||| 100.0%   500/500 epochs, beta:2.449862003326416 Generator / grammar loss:-0.19069623947143555   similarity loss:-0.14492745697498322\n--------------------------------------------------\ngold summary:\n\n--------------------------------------------------\nsam_wgan summary:\n 옛날 태어났어요. 병이들어 떠나고 맞이했어요. 소녀보다 새어머니와 언니들은 심술쟁이들이었어요. 딸들보다 예쁘고 착한 소녀는 쓸고, 집안일을 해도 해도 난롯가에 앉아서 쉬곤 했지요. 어느 왕궁에서 열렸어요. 초대장이 언니들을 데리고 떠났어요. 신데렐라도 무도회에 가고 무도회에 가고 웃고 호박 한개와 도마뱀을 건드리자, 화려한 변했어요. 도마뱀을 마부로 신데렐라의 장식이 드레스로 내밀어 보거라. 할머니는 반짝반짝 유리 구두를 주었어요. 해. 알리는 왕자님은 가서 결혼하겠어요. 발을 구두는 수 구두를 구두는 왕궁으로 데리고 결혼하여 살았대요.\n--------------------------------------------------\n     method  comp ratio     intro      body    ending       var     total  \\\n0  SAM+WGAN    0.209945  0.582162  0.577418  0.578483  0.000004  0.578714   \n\n    grammar  \n0  0.975317  \n------------------------------------------------------------------\n옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요. 그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요. 소녀의 아버지는 홀로 남은 소녀가 걱정되었어요. 그래서 얼마 후 새어머니를 맞이했어요. 새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요. 그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요. 새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요. 그런데 이번에는 아버지마저 돌아가셨어요. 소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시 쉬곤 했지요. 어느 날, 왕궁에서 무도회가 열렸어요. 신데렐라의 집에도 초대장이 왔어요. 새어머니는 언니들을 데리고 무도회장으로 떠났어요. 신데렐라도 무도회에 가고 싶었어요. 혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요. 신데렐라, 너도 무도회에 가고 싶니? 신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요. 내가 너를 무도회에 보내주마 호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴. 마법사 할머니가 주문을 외웠어요. 그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요. 이번에는 생쥐와 도마뱀을 건드렸어요. 그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다. 신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요. 신데렐라, 발을 내밀어 보거라. 할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요. 신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼. 그러니까 반드시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지? 왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요. 왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,신데렐라하고만 춤을 추었어요. 신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요. 땡, 땡, 땡 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요. 신데렐라가 허둥지둥 왕궁을 빠져나가는데, 유리 구두 한 짝이 벗겨졌어요. 하지만 구두를 주울 틈이 없었어요. 신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요. 왕자님은 유리 구두를 가지고 임금님께 가서 말했어요. 이 유리 구두의 주인과 결혼하겠어요. 그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요. 언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리 구두는 너무 작았어요. 그때, 신데렐라가 조용히 다가와 말했어요. 저도 한번 신어 볼 수 있나요? 신데렐라는 신하게 건넨 유리 구두를 신었어요, 유리 구두는 신데렐라의 발에 꼭 맞았어요. 신하들은 신데렐라를 왕궁으로 데리고 갔어요. 그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\n------------------------------------------------------------------\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0] 1/327        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 1/326        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 1/325        \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 1/324        \nNegative tokens: ['여자' '소녀가' '소녀의' '소녀가' '딸을' '새어머니와' '새어머니는' '집안일이' '신데렐라의' '신데렐라도'\n '신데렐라는' '신데렐라,' '신데렐라가' '할머니가' '무도회에' '할머니가' '마차로' '신데렐라의' '신데렐라,'\n '신데렐라에게' '신데렐라,' '흰말은' '신데렐라에게' '신데렐라는' '신데렐라는' '신데렐라가' '구두' '구두를'\n '신데렐라를' '구두' '왕자님은' '구두를' '구두의' '언니들은' '구두를' '신데렐라가' '신데렐라는' '신데렐라의'\n '신데렐라를' '신데렐라는']\nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1] 325/326       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2] 324/325       \nFrame token scan: |||||||||||||||||||||| 100.0%   filer:[0, 1, 2, 3] 323/324       \nPeak count: 27\nFrame tokens: 소녀의 맞이했어요. 언니들은 딸들보다 소녀는 왕궁에서 언니들을 무도회에 무도회에 마부로 드레스로 할머니는 구두를 왕자님도 무도회장에 않고,신데렐라하고만 춤을 왕궁을 왕자님은 결혼하겠어요. 발을 구두는 구두를 구두는 왕궁으로 결혼하여 살았대요. \n\nSimilarity : 0.48041270657702473\nTrain... ||||||||||||||||.....| 75.0%   375/500 epochs, beta:2.463460683822632 Generator / grammar loss:-0.1895204335451126   similarity loss:-0.1423169821500778238","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-f6521136b49f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mdf1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msam_wgan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minit_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;31m#df2,_ = bert_lexrank_sum('',[org_text_1,org_text_2,org_text_3])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#df3,_ = besm('',[org_text_1,org_text_2,org_text_3])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-37-ad9ff6ddaca6>\u001b[0m in \u001b[0;36msam_wgan\u001b[0;34m(g_summ, org_text, init_bias, display)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0msummarizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSAM_Summarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_discriminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms_discriminator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0msummarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0msummarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mframe_expansion_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0msummary_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-24-832cf7329490>\u001b[0m in \u001b[0;36msummarize\u001b[0;34m(self, epochs, batch_size, frame_expansion_ratio, init_bias, learning_rate, display)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mframe_expansion_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minit_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_expansion_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe_expansion_ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minit_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-24-832cf7329490>\u001b[0m in \u001b[0;36m__train\u001b[0;34m(self, epochs, batch_size, init_bias, learning_rate, display)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m                     \u001b[0mfake_gmr_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_sim_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_z_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_x_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__discrete_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgen_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                 '''\n","\u001b[0;32m<ipython-input-24-832cf7329490>\u001b[0m in \u001b[0;36m__discrete_gradient\u001b[0;34m(self, weights, gen_length, beta, use_gpu, verbose)\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0mapply_order\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mD_z_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_gmr_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_discriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransfer_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_outs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_for\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mD_x_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_gmr_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_discriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransfer_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_outs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_for\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# not use of 'real_gmr_out'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-e774c9e7792b>\u001b[0m in \u001b[0;36mtransfer_learning\u001b[0;34m(self, sentences, train_for)\u001b[0m\n\u001b[1;32m    506\u001b[0m                             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m                             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_masks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m                                 labels=b_labels)\n\u001b[0m\u001b[1;32m    509\u001b[0m         \u001b[0;31m#return torch.sigmoid(outputs[0][:,1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;31m#return outputs[0][:,1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m   1265\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1268\u001b[0m         )\n\u001b[1;32m   1269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_extended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m         )\n\u001b[1;32m    764\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    437\u001b[0m                     \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m                 )\n\u001b[1;32m    441\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    369\u001b[0m     ):\n\u001b[1;32m    370\u001b[0m         self_attention_outputs = self.attention(\n\u001b[0;32m--> 371\u001b[0;31m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m         )\n\u001b[1;32m    373\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    313\u001b[0m     ):\n\u001b[1;32m    314\u001b[0m         self_outputs = self.self(\n\u001b[0;32m--> 315\u001b[0;31m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m         )\n\u001b[1;32m    317\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0mattention_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_probs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true,"id":"OhUFVVRki1ga"},"cell_type":"code","source":"import sys\n\ndef get_features(dct1):\n    return [dct1['comp ratio'][0],dct1['intro'][0],dct1['body'][0],dct1['ending'][0],dct1['var'][0],dct1['total'][0],dct1['grammar'][0]]\n\ntest_result = {}\ntest_result['SAM+WGAN']=[]\ntest_result['BERT+LexRank']=[]\ntest_result['BESM']=[]\ntest_result['BESM+kobert']=[]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"R0yLlIo3i1ga"},"cell_type":"code","source":"def get_test_statistics(test_result):\n    df_data = {}\n    df_data['method'] = []\n    df_data['comp rate'] = []\n    df_data['intro'] = []\n    df_data['body'] = []\n    df_data['conclusion'] = []\n    df_data['isthmus'] = []\n    df_data['simlirality'] = []\n    df_data['grammarity'] = []\n\n    for key in test_result:\n        df_data['method'].append(key)\n        data = np.asarray(test_result[key])\n        df_data['comp rate'].append(np.mean(data[:,0]))\n        df_data['intro'].append(np.mean(data[:,1]))\n        df_data['body'].append(np.mean(data[:,2]))\n        df_data['conclusion'].append(np.mean(data[:,3]))\n        df_data['isthmus'].append(np.mean(data[:,4]))\n        df_data['simlirality'].append(np.mean(data[:,5]))\n        df_data['grammarity'].append(np.mean(data[:,6]))\n\n\n    df = pd.DataFrame(df_data)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"8E5m2SVti1gb"},"cell_type":"code","source":"def prepare_data(offset,length):\n    return document[offset:offset+length]\n\n\nko_docs = prepare_data(0,70)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"wOfI-ih2i1gb","outputId":"9733b712-8129-4b15-857a-dee9f7983583"},"cell_type":"code","source":"ko_docs[2]","execution_count":null,"outputs":[]},{"metadata":{"id":"HBEVNQsqBy7w","trusted":true},"cell_type":"code","source":"pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n\nstep = 0\nfor intro,body,end in ko_docs:\n    step += 1\n    print(\"=\" * 50)\n    print(str(step),\"/\",len(ko_docs))\n    print(\"=\" * 50)\n    \n    org_text_1 = intro\n    org_text_2 = body\n    org_text_3 = end\n\n    try:\n        df1,dct1 = sam_wgan('',[org_text_1,org_text_2,org_text_3],init_bias=0.0,display= True)\n        if dct1['grammar'][0] > 0.0:\n            df2,dct2 = bert_lexrank_sum('',[org_text_1,org_text_2,org_text_3])\n            df3,dct3 = besm('',[org_text_1,org_text_2,org_text_3])\n            df4,dct4 = besm_bert('',[org_text_1,org_text_2,org_text_3])\n            #df5,dct5 = abstract_method_1(g_summ,[org_text_1,org_text_2,org_text_3])\n            #df6,dct6 = abstract_method_2(g_summ,[org_text_1,org_text_2,org_text_3])\n\n            test_result['SAM+WGAN'].append(get_features(dct1))\n            test_result['BERT+LexRank'].append(get_features(dct2))\n            test_result['BESM'].append(get_features(dct3))\n            test_result['BESM+kobert'].append(get_features(dct4))\n            #test_result['Transformer'].append(get_features(dct5))\n            #test_result['T5'].append(get_features(dct6))\n            #result = pd.concat([df1, df2, df3, df4, df5, df6 ], ignore_index=True)\n            result = pd.concat([df1, df2, df3, df4 ], ignore_index=True)\n            \n            print(result)\n            \n            print(\"Current result\",\"=\" * 50)\n            print(\"Sample count:\",len(test_result['SAM+WGAN']))\n            print(get_test_statistics(test_result))\n        \n    except KeyboardInterrupt as ki:\n        raise ki\n    except :\n        print(\"Unexpected error:\", sys.exc_info()[0])\n        #raise e\n        pass\n\nget_test_statistics(test_result)","execution_count":null,"outputs":[]},{"metadata":{"id":"VUb6xK7Cigry","trusted":true,"outputId":"9dc54bbb-678c-4a6e-9935-cc4f1ac0f79d"},"cell_type":"code","source":"pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n\ntest_result = {}\ntest_result['SAM+WGAN']=[]\n\nstep = 0\nfor intro,body,end in ko_docs:\n    step += 1\n    print(\"=\" * 50)\n    print(str(step),\"/\",len(ko_docs))\n    print(\"=\" * 50)\n    \n    org_text_1 = intro\n    org_text_2 = body\n    org_text_3 = end\n\n    try:\n        df1,dct1 = sam_wgan('',[org_text_1,org_text_2,org_text_3],init_bias=1.0,display= True)\n        if dct1['grammar'][0] > 0.0:\n\n            test_result['SAM+WGAN'].append(get_features(dct1))\n            #result = pd.concat([df1, df2, df3, df4, df5, df6 ], ignore_index=True)\n            #result = pd.concat([df1, df2, df3, df5, df6 ], ignore_index=True)\n            \n            print(df1)\n            \n            print(\"Current result\",\"=\" * 50)\n            print(\"Sample count:\",len(test_result['SAM+WGAN']))\n            print(get_test_statistics(test_result))\n        \n    except KeyboardInterrupt as ki:\n        raise ki\n    except :\n        print(\"Unexpected error:\", sys.exc_info()[0])\n        #raise e\n        pass\n\nget_test_statistics(test_result)","execution_count":null,"outputs":[]},{"metadata":{"id":"ibzIVByDK7CA"},"cell_type":"markdown","source":"## Reference\n\n[1] Rada Mihalcea and Paul Tarau, (2004). TextRank: Bringing Order into Texts <br>\n[2] G¨une¸s Erkan. (2004). LexRank: Graph-based Lexical Centrality as Salience in Text Summarization <br>\n[3] Susan T. Dumais (2005). \"Latent Semantic Analysis\". Annual Review of Information Science and Technology. 38: 188–230. <br>\n[4] Hans Peter Luhn (1960). Keyword-in-context index for technical literature. American Documentation, 11(4):288–295. ISSN 0002-823 <br>\n[5] Aria Haghighi, (2009). Exploring Content Models for Multi-Document Summarization <br>\n[6] Colin Raffel. (2020). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer <br>\n[7] Mike Lewis, (2019). BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension <br>\n[8] Alec Radford, (2018). Language Models are Unsupervised Multitask Learners <br>\n[9] Guillaume Lample, (2019). Cross-lingual Language Model Pretraining <br>\n[10] Nils Reimers and Iryna Gurevych, (2019). Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks <br>\n[11] Sharma, P., & Li, Y. (2019). Self-Supervised Contextual Keyword and Keyphrase Retrieval with Self-Labelling <br>\n[12] Jacob Devlin, (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding <br>\n[13] Ian J. Goodfellow, (2014). Generative Adversarial Nets <br>\n[14] Yau-ShianWang, (2018). Learning to Encode Text as Human-Readable Summaries using Generative Adversarial Networks <br>\n[15] Martin Arjovsky, (2017). Wasserstein GAN <br>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}