{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "korean_frame_token_0_1.0_gamma_10.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dolmani38/Summary2/blob/main/ko_multi-discriminator%20GAN%200926.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkQCxNatSIOk"
      },
      "source": [
        "# Korean Multi-Discriminator GAN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZjIW9VwyjDf"
      },
      "source": [
        "ABSTRACT\n",
        "\n",
        "----\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K87VNBbeRLFF"
      },
      "source": [
        "#4. Implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQZeBAf8NxAR"
      },
      "source": [
        "## 4.1 기본 설정..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAdXzWGuKSBT",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00210848-ac02-4570-d7d0-01bcbd06a6a5"
      },
      "source": [
        "if True:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "newO0mBXKVnE",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c436635a-c17e-449c-eeb9-14bf4ab08561"
      },
      "source": [
        "#!pip install keybert\n",
        "!pip3 install transformers\n",
        "!pip3 install sentence-transformers\n",
        "\n",
        "#!pip install sentence-transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.10.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.17)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.10.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.9.0+cu102)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.96)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.62.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.10.0+cu102)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.0.17)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.46)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.8.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.10.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (5.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmIxp0FnKXif",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f84db14-dfb4-4c84-b023-ed6ff58fac99"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# set seeds for reproducability\n",
        "from numpy.random import seed\n",
        "#seed(1)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string, os \n",
        "\n",
        "import urllib.request\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3J0n_lhKcgm",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "596749ec-41e2-4315-8cdf-67ccec7e5f36"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "logout = True"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ue_4ZfdRKfdX",
        "trusted": true
      },
      "source": [
        "# Print iterations progress\n",
        "class ProgressBar:\n",
        "\n",
        "    def __init__(self,total=20, prefix = '', suffix = '', decimals = 1, length = 20, fill = '|', printEnd = \"\\r\"):\n",
        "        self.total = total\n",
        "        self.prefix = prefix\n",
        "        self.suffix = suffix\n",
        "        self.decimals = decimals\n",
        "        self.length = length\n",
        "        self.fill = fill\n",
        "        self.printEnd = printEnd\n",
        "        self.ite = 0\n",
        "        self.back_filledLength = 0\n",
        "\n",
        "    def printProgress(self,iteration, text):\n",
        "        self.ite += iteration\n",
        "        percent = (\"{0:.\" + str(self.decimals) + \"f}\").format(100 * (self.ite / float(self.total)))\n",
        "        filledLength = int(self.length * self.ite // self.total)\n",
        "        bar = self.fill * filledLength + '.' * (self.length - filledLength)\n",
        "        if filledLength > self.back_filledLength or percent == 100:\n",
        "            if logout:\n",
        "                print(f'\\r{self.prefix} |{bar}| {percent}% {self.suffix}  {text}', end=\"\", flush=True)\n",
        "            # Print New Line on Complete\n",
        "            if self.ite == self.total: \n",
        "                if logout:\n",
        "                    print()\n",
        "        self.back_filledLength = filledLength    "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNHI0G6JKc5h",
        "trusted": true
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Zsv-LVkKmfL"
      },
      "source": [
        "##4.2 Grammar Discriminator Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VQdGLciKc_y",
        "trusted": true
      },
      "source": [
        "from transformers import BertTokenizer, BertTokenizerFast,AutoTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset, random_split\n",
        "\n",
        "import time\n",
        "import random\n",
        "import datetime\n",
        "import pickle\n",
        "\n",
        "# 간단한 전처리\n",
        "def clean_text(txt):\n",
        "    txt = txt.replace('\\n',' ')\n",
        "    txt = txt.replace('\\r',' ')    \n",
        "    txt = txt.replace('=','')\n",
        "    txt = txt.replace('\\\"','')   \n",
        "    txt = txt.replace('\\'','')\n",
        "    #txt = txt.replace(',','')\n",
        "    txt = txt.replace('..','')\n",
        "    txt = txt.replace('...','')\n",
        "    txt = txt.replace(' .','.')\n",
        "    txt = txt.replace('.','. ')\n",
        "    txt = txt.replace('  ',' ')\n",
        "    txt = txt.replace('  ',' ')    \n",
        "    txt = txt.replace('  ',' ')   \n",
        "    txt = txt.replace('  ',' ')           \n",
        "    txt = txt.replace('  ',' ')\n",
        "    txt = txt.replace('  ',' ')    \n",
        "    txt = txt.replace('  ',' ')   \n",
        "    txt = txt.replace('  ',' ')             \n",
        "    return txt.strip()\n",
        "\n",
        "def shuffling(txt):\n",
        "    txt_list = txt.split(' ')\n",
        "    random.shuffle(txt_list)\n",
        "    return ' '.join(txt_list)\n",
        "\n",
        "def collect_training_dataset_for_grammar_discriminator(sentences_dataset):\n",
        "\n",
        "    sentences = []\n",
        "    labels = []\n",
        "\n",
        "    for txtss in sentences_dataset:\n",
        "        txtss = clean_text(txtss)\n",
        "        txts = txtss.strip().split('.')\n",
        "        for txt in txts:  \n",
        "            txt = txt.strip()\n",
        "            if len(txt) > 10:\n",
        "                #ko_grammar_dataset.append([txt,1])\n",
        "                txt = txt.replace('.','')\n",
        "                tf = random.choice([True,False])\n",
        "                # 정상 또는 비정상 둘중에 하나만 데이터셋에 추가\n",
        "                if (tf):\n",
        "                    sentences.append(txt) # '.'의 위치를 보고 True, False를 판단 하기 땜에...\n",
        "                    labels.append(1)\n",
        "                else:\n",
        "                    sentences.append(shuffling(txt))\n",
        "                    labels.append(0)\n",
        "\n",
        "    return sentences,labels\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "class Grammar_Discriminator:\n",
        "\n",
        "\n",
        "    def __init__(self, pretraoned_kobert_model_name='bert-base-v2', input_dir=None):\n",
        "\n",
        "        if input_dir is None:\n",
        "            self.tokenizer = BertTokenizerFast.from_pretrained(pretraoned_kobert_model_name)\n",
        "            self.discriminator = BertForSequenceClassification.from_pretrained(\n",
        "                                    pretraoned_kobert_model_name, # Use the 12-layer BERT model, with an uncased vocab.\n",
        "                                    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                                                    # You can increase this for multi-class tasks.   \n",
        "                                    output_attentions = False, # Whether the model returns attentions weights.\n",
        "                                    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "                                )            \n",
        "        else:\n",
        "            self.__load_model(input_dir)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def set_dataset(self, sentences,labels):\n",
        "        # Print the original sentence.\n",
        "        print(' Original: ', sentences[0])\n",
        "\n",
        "        # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "        input_ids = []\n",
        "        attention_masks = []\n",
        "\n",
        "        # For every sentence...\n",
        "        for i, sent in enumerate(sentences):\n",
        "            print(f'\\r Tokenize {i+1}/{len(sentences)}', end=\"\", flush=True)            \n",
        "            # `encode_plus` will:\n",
        "            #   (1) Tokenize the sentence.\n",
        "            #   (2) Prepend the `[CLS]` token to the start.\n",
        "            #   (3) Append the `[SEP]` token to the end.\n",
        "            #   (4) Map tokens to their IDs.\n",
        "            #   (5) Pad or truncate the sentence to `max_length`\n",
        "            #   (6) Create attention masks for [PAD] tokens.\n",
        "            encoded_dict = self.tokenizer.encode_plus(\n",
        "                                sent,                      # Sentence to encode.\n",
        "                                add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                                max_length = 64,           # Pad & truncate all sentences.\n",
        "                                pad_to_max_length = True,\n",
        "                                return_attention_mask = True,   # Construct attn. masks.\n",
        "                                return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                                truncation = True,\n",
        "                        )\n",
        "            \n",
        "            # Add the encoded sentence to the list.    \n",
        "            input_ids.append(encoded_dict['input_ids'])\n",
        "            \n",
        "            # And its attention mask (simply differentiates padding from non-padding).\n",
        "            attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "        # Convert the lists into tensors.\n",
        "        input_ids = torch.cat(input_ids, dim=0)\n",
        "        attention_masks = torch.cat(attention_masks, dim=0)\n",
        "        labels = torch.tensor(labels)\n",
        "\n",
        "        # Print sentence 0, now as a list of IDs.\n",
        "        print('Original: ', sentences[0])\n",
        "        print('Token IDs:', input_ids[0])\n",
        "\n",
        "        # Training & Validation Split\n",
        "        # Divide up our training set to use 90% for training and 10% for validation.\n",
        "\n",
        "        # Combine the training inputs into a TensorDataset.\n",
        "        dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "        # Create a 90-10 train-validation split.\n",
        "\n",
        "        # Calculate the number of samples to include in each set.\n",
        "        train_size = int(0.9 * len(dataset))\n",
        "        val_size = len(dataset) - train_size\n",
        "\n",
        "        # Divide the dataset by randomly selecting samples.\n",
        "        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "        print('{:>5,} training samples'.format(train_size))\n",
        "        print('{:>5,} validation samples'.format(val_size))\n",
        "\n",
        "        # The DataLoader needs to know our batch size for training, so we specify it \n",
        "        # here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "        # size of 16 or 32.\n",
        "        self.batch_size = 32\n",
        "\n",
        "        # Create the DataLoaders for our training and validation sets.\n",
        "        # We'll take training samples in random order. \n",
        "        self.train_dataloader = DataLoader(\n",
        "                    train_dataset,  # The training samples.\n",
        "                    sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "                    batch_size = self.batch_size # Trains with this batch size.\n",
        "                )\n",
        "\n",
        "        # For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "        self.validation_dataloader = DataLoader(\n",
        "                    val_dataset, # The validation samples.\n",
        "                    sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "                    batch_size = self.batch_size # Evaluate with this batch size.\n",
        "                )        \n",
        "\n",
        "\n",
        "    def train(self,epochs=4):\n",
        "        # Tell pytorch to run this model on the GPU.\n",
        "        self.discriminator.cuda()\n",
        "\n",
        "        # Get all of the model's parameters as a list of tuples.\n",
        "        params = list(self.discriminator.named_parameters())\n",
        "\n",
        "        print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "        print('==== Embedding Layer ====\\n')\n",
        "\n",
        "        for p in params[0:5]:\n",
        "            print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "        print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "        for p in params[5:21]:\n",
        "            print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "        print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "        for p in params[-4:]:\n",
        "            print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))  \n",
        "\n",
        "        # Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "        # I believe the 'W' stands for 'Weight Decay fix\"\n",
        "        self.optimizer = AdamW(self.discriminator.parameters(),\n",
        "                        lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                        eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                        )\n",
        "\n",
        "        # Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "        # We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "        # training data.\n",
        "        #epochs = 2\n",
        "\n",
        "        # Total number of training steps is [number of batches] x [number of epochs]. \n",
        "        # (Note that this is not the same as the number of training samples).\n",
        "        total_steps = len(self.train_dataloader) * epochs\n",
        "\n",
        "        # Create the learning rate scheduler.\n",
        "        scheduler = get_linear_schedule_with_warmup(self.optimizer, \n",
        "                                                    num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                                    num_training_steps = total_steps)\n",
        "            \n",
        "        # This training code is based on the `run_glue.py` script here:\n",
        "        # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "        # Set the seed value all over the place to make this reproducible.\n",
        "        seed_val = 42\n",
        "\n",
        "        random.seed(seed_val)\n",
        "        np.random.seed(seed_val)\n",
        "        torch.manual_seed(seed_val)\n",
        "        torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "        # We'll store a number of quantities such as training and validation loss, \n",
        "        # validation accuracy, and timings.\n",
        "        training_stats = []\n",
        "\n",
        "        # Measure the total training time for the whole run.\n",
        "        total_t0 = time.time()\n",
        "\n",
        "        # For each epoch...\n",
        "        for epoch_i in range(0, epochs):\n",
        "            \n",
        "            # ========================================\n",
        "            #               Training\n",
        "            # ========================================\n",
        "            \n",
        "            # Perform one full pass over the training set.\n",
        "\n",
        "            print(\"\")\n",
        "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "            print('Training...')\n",
        "\n",
        "            # Measure how long the training epoch takes.\n",
        "            t0 = time.time()\n",
        "\n",
        "            # Reset the total loss for this epoch.\n",
        "            total_train_loss = 0\n",
        "\n",
        "            # Put the model into training mode. Don't be mislead--the call to \n",
        "            # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "            # `dropout` and `batchnorm` layers behave differently during training\n",
        "            # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "            self.discriminator.train()\n",
        "\n",
        "            # For each batch of training data...\n",
        "            for step, batch in enumerate(self.train_dataloader):\n",
        "\n",
        "                # Progress update every 40 batches.\n",
        "                if step % 40 == 0 and not step == 0:\n",
        "                    # Calculate elapsed time in minutes.\n",
        "                    elapsed = format_time(time.time() - t0)\n",
        "                    \n",
        "                    # Report progress.\n",
        "                    print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(self.train_dataloader), elapsed))\n",
        "\n",
        "                # Unpack this training batch from our dataloader. \n",
        "                #\n",
        "                # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "                # `to` method.\n",
        "                #\n",
        "                # `batch` contains three pytorch tensors:\n",
        "                #   [0]: input ids \n",
        "                #   [1]: attention masks\n",
        "                #   [2]: labels \n",
        "                b_input_ids = batch[0].to(device)\n",
        "                b_input_mask = batch[1].to(device)\n",
        "                b_labels = batch[2].to(device)\n",
        "\n",
        "                # Always clear any previously calculated gradients before performing a\n",
        "                # backward pass. PyTorch doesn't do this automatically because \n",
        "                # accumulating the gradients is \"convenient while training RNNs\". \n",
        "                # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "                self.discriminator.zero_grad()        \n",
        "\n",
        "                # Perform a forward pass (evaluate the model on this training batch).\n",
        "                # The documentation for this `model` function is here: \n",
        "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "                # It returns different numbers of parameters depending on what arguments\n",
        "                # arge given and what flags are set. For our useage here, it returns\n",
        "                # the loss (because we provided labels) and the \"logits\"--the model\n",
        "                # outputs prior to activation.\n",
        "                outputs = self.discriminator(b_input_ids, \n",
        "                                    token_type_ids=None, \n",
        "                                    attention_mask=b_input_mask, \n",
        "                                    labels=b_labels)\n",
        "                loss, logits = outputs.loss, outputs.logits\n",
        "                # Accumulate the training loss over all of the batches so that we can\n",
        "                # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "                # single value; the `.item()` function just returns the Python value \n",
        "                # from the tensor.\n",
        "                total_train_loss += loss.item()\n",
        "\n",
        "                # Perform a backward pass to calculate the gradients.\n",
        "                loss.backward()\n",
        "\n",
        "                # Clip the norm of the gradients to 1.0.\n",
        "                # This is to help prevent the \"exploding gradients\" problem.\n",
        "                torch.nn.utils.clip_grad_norm_(self.discriminator.parameters(), 1.0)\n",
        "\n",
        "                # Update parameters and take a step using the computed gradient.\n",
        "                # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "                # modified based on their gradients, the learning rate, etc.\n",
        "                self.optimizer.step()\n",
        "\n",
        "                # Update the learning rate.\n",
        "                scheduler.step()\n",
        "\n",
        "            # Calculate the average loss over all of the batches.\n",
        "            avg_train_loss = total_train_loss / len(self.train_dataloader)            \n",
        "            \n",
        "            # Measure how long this epoch took.\n",
        "            training_time = format_time(time.time() - t0)\n",
        "\n",
        "            print(\"\")\n",
        "            print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "            print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "                \n",
        "            # ========================================\n",
        "            #               Validation\n",
        "            # ========================================\n",
        "            # After the completion of each training epoch, measure our performance on\n",
        "            # our validation set.\n",
        "\n",
        "            print(\"\")\n",
        "            print(\"Running Validation...\")\n",
        "\n",
        "            t0 = time.time()\n",
        "\n",
        "            # Put the model in evaluation mode--the dropout layers behave differently\n",
        "            # during evaluation.\n",
        "            self.discriminator.eval()\n",
        "\n",
        "            # Tracking variables \n",
        "            total_eval_accuracy = 0\n",
        "            total_eval_loss = 0\n",
        "            nb_eval_steps = 0\n",
        "\n",
        "            # Evaluate data for one epoch\n",
        "            for batch in self.validation_dataloader:\n",
        "                \n",
        "                # Unpack this training batch from our dataloader. \n",
        "                #\n",
        "                # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "                # the `to` method.\n",
        "                #\n",
        "                # `batch` contains three pytorch tensors:\n",
        "                #   [0]: input ids \n",
        "                #   [1]: attention masks\n",
        "                #   [2]: labels \n",
        "                b_input_ids = batch[0].to(device)\n",
        "                b_input_mask = batch[1].to(device)\n",
        "                b_labels = batch[2].to(device)\n",
        "                \n",
        "                # Tell pytorch not to bother with constructing the compute graph during\n",
        "                # the forward pass, since this is only needed for backprop (training).\n",
        "                with torch.no_grad():        \n",
        "\n",
        "                    # Forward pass, calculate logit predictions.\n",
        "                    # token_type_ids is the same as the \"segment ids\", which \n",
        "                    # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "                    # The documentation for this `model` function is here: \n",
        "                    # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "                    # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "                    # values prior to applying an activation function like the softmax.\n",
        "                    outputs = self.discriminator(b_input_ids, \n",
        "                                        token_type_ids=None, \n",
        "                                        attention_mask=b_input_mask,\n",
        "                                        labels=b_labels)\n",
        "                loss, logits = outputs.loss, outputs.logits\n",
        "                # Accumulate the validation loss.\n",
        "                total_eval_loss += loss.item()\n",
        "\n",
        "                # Move logits and labels to CPU\n",
        "                logits = logits.detach().cpu().numpy()\n",
        "                label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "                # Calculate the accuracy for this batch of test sentences, and\n",
        "                # accumulate it over all batches.\n",
        "                total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "                \n",
        "\n",
        "            # Report the final accuracy for this validation run.\n",
        "            avg_val_accuracy = total_eval_accuracy / len(self.validation_dataloader)\n",
        "            print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "            # Calculate the average loss over all of the batches.\n",
        "            avg_val_loss = total_eval_loss / len(self.validation_dataloader)\n",
        "            \n",
        "            # Measure how long the validation run took.\n",
        "            validation_time = format_time(time.time() - t0)\n",
        "            \n",
        "            print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "            print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "            # Record all statistics from this epoch.\n",
        "            training_stats.append(\n",
        "                {\n",
        "                    'epoch': epoch_i + 1,\n",
        "                    'Training Loss': avg_train_loss,\n",
        "                    'Valid. Loss': avg_val_loss,\n",
        "                    'Valid. Accur.': avg_val_accuracy,\n",
        "                    'Training Time': training_time,\n",
        "                    'Validation Time': validation_time\n",
        "                }\n",
        "            )\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"Training complete!\")\n",
        "\n",
        "        print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "            \n",
        "\n",
        "        return training_stats\n",
        "\n",
        "    def save_model(self, output_dir = './model_save/'):\n",
        "        # Create output directory if needed\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "\n",
        "        print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "        # They can then be reloaded using `from_pretrained()`\n",
        "        model_to_save = self.discriminator.module if hasattr(self.discriminator, 'module') else self.discriminator  # Take care of distributed/parallel training\n",
        "        model_to_save.save_pretrained(output_dir)\n",
        "        self.tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "        # Good practice: save your training arguments together with the trained model\n",
        "        # torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n",
        "\n",
        "    def __load_model(self, input_dir = './drive/MyDrive/Colab Notebooks/summary/en_grammar_check_model'):\n",
        "        print('Loading BERT tokenizer...')\n",
        "        self.tokenizer = BertTokenizerFast.from_pretrained(input_dir)\n",
        "        self.discriminator = BertForSequenceClassification.from_pretrained(input_dir)\n",
        "\n",
        "    def transfer_learning(self, sentences, train_for = True):\n",
        "        \n",
        "        input_ids = []\n",
        "        attention_masks = []\n",
        "\n",
        "        # For every sentence...\n",
        "        for sent in sentences:\n",
        "            # `encode_plus` will:\n",
        "            #   (1) Tokenize the sentence.\n",
        "            #   (2) Prepend the `[CLS]` token to the start.\n",
        "            #   (3) Append the `[SEP]` token to the end.\n",
        "            #   (4) Map tokens to their IDs.\n",
        "            #   (5) Pad or truncate the sentence to `max_length`\n",
        "            #   (6) Create attention masks for [PAD] tokens.\n",
        "            encoded_dict = self.tokenizer.encode_plus(\n",
        "                                sent,                      # Sentence to encode.\n",
        "                                add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                                max_length = 64,           # Pad & truncate all sentences.\n",
        "                                pad_to_max_length = True,\n",
        "                                return_attention_mask = True,   # Construct attn. masks.\n",
        "                                return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                                truncation = True,\n",
        "                        )\n",
        "            # Add the encoded sentence to the list.    \n",
        "            input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "            # And its attention mask (simply differentiates padding from non-padding).\n",
        "            attention_masks.append(encoded_dict['attention_mask'])\n",
        "        \n",
        "        if train_for:\n",
        "            b_labels = torch.ones(len(sentences),dtype=torch.long).to(device)\n",
        "        else:\n",
        "            b_labels = torch.zeros(len(sentences),dtype=torch.long).to(device)\n",
        "        #print(b_labels)\n",
        "        # Convert the lists into tensors.\n",
        "        input_ids = torch.cat(input_ids, dim=0).to(device)\n",
        "        attention_masks = torch.cat(attention_masks, dim=0).to(device)    \n",
        "        #if str(discriminator1.device) == 'cpu':\n",
        "        #    pass\n",
        "        #else:\n",
        "        #    input_ids = input_ids.to(device)\n",
        "        #    attention_masks = attention_masks.to(device)        \n",
        "\n",
        "        outputs = self.discriminator(input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=attention_masks, \n",
        "                                labels=b_labels)\n",
        "\n",
        "        #print(outputs)\n",
        "        #return torch.sigmoid(outputs[0][:,1])\n",
        "        #return outputs[0][:,1]\n",
        "        return outputs['loss'], outputs['logits']\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4zeEb0NR2QH"
      },
      "source": [
        "# 문법 discriminator 활용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7Zf2oRMMXmH",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74d4896f-f8c2-434d-f5a6-4775a2eea319"
      },
      "source": [
        "g_discriminator = Grammar_Discriminator(input_dir = '/content/drive/MyDrive/Colab Notebooks/summary/ko_grammar_model4')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BERT tokenizer...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEXRsgqlXkpf",
        "outputId": "40e9c760-1f31-4f43-9f8c-d3a1b9195d85"
      },
      "source": [
        "txt = ['최근 날씨가 포근해지면서 산을 찾는 사람들도 늘고 있는데요','서비스를 음원 플랫폼 스포티파이가 국내 론칭한다']\n",
        "g_discriminator.discriminator.to(device)\n",
        "g_discriminator.transfer_learning(txt)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(4.3705, device='cuda:0', grad_fn=<NllLossBackward>),\n",
              " tensor([[-4.8017,  5.1989],\n",
              "         [ 4.3898, -4.3509]], device='cuda:0', grad_fn=<AddmmBackward>))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d96kaCAHKuUc"
      },
      "source": [
        "##4.3 Static similarity discriminator class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZDpXe7XKxeg",
        "trusted": true
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import BertTokenizer\n",
        "from scipy.signal import find_peaks\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.misc import electrocardiogram\n",
        "import scipy\n",
        "\n",
        "\n",
        "class Similarity_Discriminator:\n",
        "    '''\n",
        "    _instance = None\n",
        "    _embedder = None\n",
        "    def __new__(cls,pre_trained_model_name='stsb-roberta-large'):\n",
        "        if cls._instance is None:\n",
        "            print('Creating Similarity_Discriminator object')\n",
        "            cls._instance = super(Similarity_Discriminator, cls).__new__(cls)\n",
        "            # Put any initialization here.\n",
        "            cls._embedder = SentenceTransformer(pre_trained_model_name)\n",
        "        return cls._instance\n",
        "\n",
        "    '''\n",
        "\n",
        "    def __init__(self,pre_trained_model_name='stsb-roberta-large'): #'roberta-large-nli-stsb-mean-tokens'):\n",
        "        print('Creating Similarity_Discriminator object')\n",
        "        # Put any initialization here.\n",
        "        self._embedder = SentenceTransformer(pre_trained_model_name,device=device)  \n",
        "        #self.cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "    def encode(self,texts):\n",
        "        return self._embedder.encode(texts,show_progress_bar=False)\n",
        "\n",
        "    def similarity(self, query_text, org_text_emb):\n",
        "        queries = nltk.sent_tokenize(query_text)\n",
        "        query_embeddings = self._embedder.encode(queries,show_progress_bar=False)\n",
        "        #query_embeddings = self._embedder.encode(queries,show_progress_bar=False)\n",
        "        #print(queries)\n",
        "        #print(org_text_emb)\n",
        "        \n",
        "        if len(query_embeddings) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        cos_scores = scipy.spatial.distance.cdist(query_embeddings, org_text_emb, \"cosine\")\n",
        "        similarity_score = 1.0 - np.min(np.max(cos_scores,axis=1))\n",
        "        '''\n",
        "        for query, query_embedding in zip(queries, query_embeddings):\n",
        "            distances = scipy.spatial.distance.cdist([query_embedding], [org_text_emb], \"cosine\")[0]\n",
        "            results = zip(range(len(distances)), distances)\n",
        "            for idx, distance in results:\n",
        "                scores.append(1-distance)\n",
        "        '''\n",
        "        return similarity_score  \n",
        " "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sQZ36GuMumP"
      },
      "source": [
        "###4.3.1 한국어 문장 유사도 pre-trained model 적용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9Miao14Muww",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "954c25e3-03fc-4a3c-a4cc-f9c23f458c3e"
      },
      "source": [
        "#del s_discriminator\n",
        "\n",
        "s_discriminator = Similarity_Discriminator(pre_trained_model_name='roberta-large-nli-stsb-mean-tokens')\n",
        "#s_discriminator = Similarity_Discriminator()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Similarity_Discriminator object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NGqfC0qL1JU"
      },
      "source": [
        "# ExtactiveSummarizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-NxbjyjT9k-"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import re\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import BertTokenizer, BertModel"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpwkLTqaUApn"
      },
      "source": [
        "\n",
        "class ExtactiveSummarizer:\n",
        "    # 한국어의 경우, 'kykim/bert-kor-base'\n",
        "    def __init__(self,model_name='bert-base-uncased'):\n",
        "\n",
        "        #nltk.download('stopwords')\n",
        "        nltk.download('punkt')\n",
        "\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "        self.model = BertModel.from_pretrained(model_name, return_dict=True, output_attentions=True)\n",
        "        self.cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "        # If there's a GPU available...\n",
        "        if torch.cuda.is_available():    \n",
        "            # Tell PyTorch to use the GPU.    \n",
        "            self.device = torch.device(\"cuda\")\n",
        "        # If not...\n",
        "        else:\n",
        "            self.device = torch.device(\"cpu\")\n",
        "\n",
        "        self.model.to(self.device)\n",
        "        self.cos.to(self.device)\n",
        "        \n",
        "    def read_article(self,text):        \n",
        "        sentences =[]        \n",
        "        sentences = sent_tokenize(text)    \n",
        "        for sentence in sentences:        \n",
        "            sentence.replace(\"[^a-zA-Z0-9]\",\" \")     \n",
        "        return sentences\n",
        "\n",
        "    def sentence_similarity(self,sent1,sent2):\n",
        "        tok_sent1 = self.tokenizer(sent1, return_tensors=\"pt\")\n",
        "        tok_sent2 = self.tokenizer(sent2, return_tensors=\"pt\")\n",
        "        tok_sent1.to(self.device)\n",
        "        tok_sent2.to(self.device)\n",
        "        outputs = self.model(**tok_sent1)\n",
        "        sent_1_pooler_output = outputs.pooler_output\n",
        "\n",
        "        outputs = self.model(**tok_sent2)\n",
        "        sent_2_pooler_output = outputs.pooler_output\n",
        "        return self.cos(sent_1_pooler_output, sent_2_pooler_output).cpu().detach().numpy()\n",
        "\n",
        "    def get_self_attention_weight(self,sentence):\n",
        "        tok_sent = self.tokenizer(sentence, return_tensors=\"pt\")\n",
        "        tok_sent.to(self.device)\n",
        "        outputs = self.model(**tok_sent)\n",
        "        \n",
        "        attentions = torch.stack(outputs.attentions)\n",
        "        #attention = outputs[-1]  # Output includes attention weights when output_attentions=True\n",
        "        last_attentions = attentions[11][0][11]\n",
        "        #print(last_attentions.shape)\n",
        "        tokens = [self.tokenizer.convert_ids_to_tokens(s) for s in tok_sent['input_ids'].tolist()[0]]\n",
        "        #print(tokens)\n",
        "        attention_map = []\n",
        "        for i,token in enumerate(tokens):\n",
        "            attention_map.append((i,token,torch.sum(last_attentions[:,i]).item()))\n",
        "        return attention_map\n",
        "\n",
        "    # Create similarity matrix among all sentences\n",
        "    def build_similarity_matrix(self,sentences):\n",
        "        #create an empty similarity matrix\n",
        "        similarity_matrix = np.zeros((len(sentences),len(sentences)))\n",
        "        \n",
        "        for idx1 in range(len(sentences)):\n",
        "            for idx2 in range(len(sentences)):\n",
        "                if idx1!=idx2:\n",
        "                    similarity_matrix[idx1][idx2] = self.sentence_similarity(sentences[idx1],sentences[idx2])\n",
        "                    \n",
        "        return similarity_matrix\n",
        "\n",
        "    # Generate and return text summary\n",
        "    def generate_summary(self,text,top_n,min_length=30):\n",
        "        \n",
        "        ft = []\n",
        "        org_sentences = np.array(nltk.sent_tokenize(text))\n",
        "        for txt in org_sentences:\n",
        "            if len(txt) > min_length:\n",
        "                ft.append(txt)\n",
        "        text = ' '.join(ft)\n",
        "\n",
        "        #stop_words = stopwords.words('english')\n",
        "        summarize_text = []\n",
        "        \n",
        "        # Step1: read text and tokenize\n",
        "        sentences = self.read_article(text)\n",
        "        \n",
        "        # Steo2: generate similarity matrix across sentences\n",
        "        sentence_similarity_matrix = self.build_similarity_matrix(sentences)\n",
        "        \n",
        "        # Step3: Rank sentences in similarirty matrix\n",
        "        sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_matrix)\n",
        "        scores = nx.pagerank(sentence_similarity_graph)\n",
        "        '''\n",
        "        #Step4: sort the rank and place top sentences\n",
        "        ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)),reverse=True)\n",
        "        \n",
        "        #print(ranked_sentences)\n",
        "        # Step 5: get the top n number of sentences based on rank    \n",
        "        for i in range(top_n if top_n < len(ranked_sentences) else len(ranked_sentences)):\n",
        "            summarize_text.append(ranked_sentences[i][1])\n",
        "        '''\n",
        "        orderd = [(o,s) for o,s in enumerate(scores)]\n",
        "        orderd.sort(key=lambda e: e[1],reverse=True)\n",
        "        top_n = top_n if len(orderd) > top_n else len(orderd)\n",
        "        a = [orderd[i][0] for i in range(0,top_n)]\n",
        "        a.sort()\n",
        "        summ_text = \" \".join([sentences[i] for i in a])\n",
        "        \n",
        "        # Step 6 : outpur the summarized version\n",
        "        return summ_text,len(sentences)   # \" \".join(summarize_text), len(sentences)  #"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVsoAq3hz3mr",
        "outputId": "ddfb6571-5b87-4e17-9082-c213054d5738"
      },
      "source": [
        "es = ExtactiveSummarizer(model_name='kykim/bert-kor-base')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at kykim/bert-kor-base were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVWEePCD0VTr",
        "outputId": "0a4fbecb-6e1f-4dd2-abb2-a390d229044d"
      },
      "source": [
        "es.sentence_similarity('검사 건수가 줄어드는 주말이지만, 전날 3273명에 이어 역대 두 번째로 높은 수치를 기록했습니다.','검사 줄어드는 주말이지만, 전날 3273명에 이어 건수가 역대 두 번째로 높은 수치를 기록했습니다.')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.96269816], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qG4aL4oRz8mu",
        "outputId": "ff26d417-5f24-46c8-e760-6101ac77ff38"
      },
      "source": [
        "es.get_self_attention_weight('검사 건수가 줄어드는 주말이지만, 전날 3273명에 이어 역대 두 번째로 높은 수치를 기록했습니다.')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, '[CLS]', 0.898577868938446),\n",
              " (1, '검사', 0.5013369917869568),\n",
              " (2, '건', 0.5563274025917053),\n",
              " (3, '##수가', 0.8456028699874878),\n",
              " (4, '줄어드는', 0.8770967721939087),\n",
              " (5, '주말', 0.6293196082115173),\n",
              " (6, '##이지만', 1.2746737003326416),\n",
              " (7, ',', 0.7676275372505188),\n",
              " (8, '전날', 0.2254231870174408),\n",
              " (9, '32', 0.03144495189189911),\n",
              " (10, '##73', 0.6037891507148743),\n",
              " (11, '##명에', 0.28671056032180786),\n",
              " (12, '이어', 1.0958600044250488),\n",
              " (13, '역대', 1.278624415397644),\n",
              " (14, '두', 0.5708823800086975),\n",
              " (15, '번째로', 2.155564785003662),\n",
              " (16, '높은', 0.6527961492538452),\n",
              " (17, '수치를', 0.45199745893478394),\n",
              " (18, '기록', 0.771113932132721),\n",
              " (19, '##했습니다', 0.9891840815544128),\n",
              " (20, '.', 5.295563220977783),\n",
              " (21, '[SEP]', 1.2404826879501343)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xnk9GsQ0K1t1"
      },
      "source": [
        "# 4.4 Document source class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYs__02JjKjT"
      },
      "source": [
        "## 한국어 Sample data 수집"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USTpJuw1uxrY"
      },
      "source": [
        "import json  \n",
        "import zipfile  \n",
        "\n",
        "if False:\n",
        "    data_1 = None  \n",
        "    data = None  \n",
        "    with zipfile.ZipFile(\"/content/drive/MyDrive/Colab Notebooks/summary/data/사설잡지_2.train_original.json.zip\", \"r\") as z:\n",
        "        with z.open('train_original.json') as f:  \n",
        "            data = f.read()  \n",
        "            data_1 = json.loads(data.decode(\"utf-8\"))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N97rzoPTuypR"
      },
      "source": [
        "if False:\n",
        "    data_2 = None  \n",
        "    data = None  \n",
        "    with zipfile.ZipFile(\"/content/drive/MyDrive/Colab Notebooks/summary/data/신문기사_2.train_original.json.zip\", \"r\") as z:\n",
        "        with z.open('train_original.json') as f:  \n",
        "            data = f.read()  \n",
        "            data_2 = json.loads(data.decode(\"utf-8\"))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLo_S8UivO2R"
      },
      "source": [
        "if False:\n",
        "    random.shuffle(data_1)\n",
        "    random.shuffle(data_2)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVArlfj-vU38"
      },
      "source": [
        "if False:\n",
        "    sentences_dataset = []\n",
        "    gold_summary = []\n",
        "    for i in range(len(data_2)):\n",
        "        sentences_dataset.append(' '.join(data_2[i]['article_original']))\n",
        "        gold_summary.append(data_2[i]['abstractive'])\n",
        "\n",
        "    for i in range(len(data_1)):\n",
        "        sentences_dataset.append(' '.join(data_1[i]['article_original']))\n",
        "        gold_summary.append(data_1[i]['abstractive'])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rtLd56DwfQ0"
      },
      "source": [
        "import pickle\n",
        "if False:\n",
        "    with open(\"/content/drive/MyDrive/Colab Notebooks/summary/data/ko_ai_hub_sample.bin\", \"wb\") as fp:\n",
        "        pickle.dump([sentences_dataset,gold_summary],fp)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCOWg1jX-OKH"
      },
      "source": [
        "import pickle\n",
        "if True:\n",
        "    with open(\"/content/drive/MyDrive/Colab Notebooks/summary/data/ko_ai_hub_sample.bin\", \"rb\") as fp:\n",
        "        dt = pickle.load(fp)\n",
        "    sentences_dataset = dt[0]\n",
        "    gold_summary = dt[1]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnBm6RCvNIWG"
      },
      "source": [
        "## 4.4.2 source class 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYHIkaar2zb-"
      },
      "source": [
        "# 간단한 전처리\n",
        "def __clean_text(txt):\n",
        "    txt = txt.replace('\\n',' ')\n",
        "    txt = txt.replace('\\r',' ')    \n",
        "    #txt = txt.replace('=','')\n",
        "    #txt = txt.replace('\\\"','')   \n",
        "    #txt = txt.replace('\\'','')\n",
        "    #txt = txt.replace(',','')\n",
        "    #txt = txt.replace('..','')\n",
        "    #txt = txt.replace('...','')\n",
        "    txt = txt.replace('.',' ')\n",
        "    #txt = txt.replace('.','. ')\n",
        "    txt = txt.replace('  ',' ')\n",
        "    txt = txt.replace('  ',' ')    \n",
        "    txt = txt.replace('  ',' ')   \n",
        "    txt = txt.replace('  ',' ')           \n",
        "    txt = txt.replace('  ',' ')\n",
        "    txt = txt.replace('  ',' ')    \n",
        "    txt = txt.replace('  ',' ')   \n",
        "    txt = txt.replace('  ',' ')             \n",
        "    return txt.strip()\n",
        "\n",
        "def get_prepared_doc(txt):\n",
        "    docs = []\n",
        "    sentences = np.array(nltk.sent_tokenize(txt))\n",
        "    for sen in sentences:\n",
        "        docs.append(__clean_text(sen) +'.')\n",
        "    return (' '.join(docs)).strip()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vx70WdqDyEDL"
      },
      "source": [
        "\n",
        "combine_matching_table = {}\n",
        "combine_matching_table['다.'] = '고'\n",
        "conjunction_table = ['그러던','그래서','그러나','그런데','그리고','그랬더니','그러니까','하지만','그래서']\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07FFosxaynL6"
      },
      "source": [
        "def combine_sentence(txt):\n",
        "    for c in combine_matching_table.keys():\n",
        "        if txt.endswith(c):\n",
        "            txt = txt.replace(c,combine_matching_table[c])\n",
        "    return txt"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsJKbtc2K4xN",
        "trusted": true
      },
      "source": [
        "\n",
        "\n",
        "class Source:\n",
        "\n",
        "    def __init__(self,full_text,org_text,delete_ending = False,attendtion_rate=0.3):\n",
        "        self.full_text = full_text\n",
        "        self.org_text = org_text\n",
        "        self.delete_ending = delete_ending\n",
        "        self.attendtion_rate = attendtion_rate\n",
        "\n",
        "    def __crean_text(self, txt):\n",
        "        txt = txt.replace('\\n',' ')\n",
        "        txt = txt.replace('\\r',' ')    \n",
        "        txt = txt.replace('=','')\n",
        "        txt = txt.replace('\\\"','')   \n",
        "        txt = txt.replace('\\'','')\n",
        "        txt = txt.replace(',','')\n",
        "        txt = txt.replace('..','')\n",
        "        txt = txt.replace('...','')\n",
        "        txt = txt.replace(' .','.')\n",
        "        txt = txt.replace('.','. ')\n",
        "        txt = txt.replace('  ',' ')\n",
        "        txt = txt.replace('  ',' ')    \n",
        "        txt = txt.replace('  ',' ')   \n",
        "        txt = txt.replace('  ',' ')           \n",
        "        txt = txt.replace('  ',' ')\n",
        "        txt = txt.replace('  ',' ')    \n",
        "        txt = txt.replace('  ',' ')   \n",
        "        txt = txt.replace('  ',' ')           \n",
        "        return txt.strip()\n",
        "\n",
        "    def set_key_rate(self,s_discriminator):\n",
        "        # full_text에 대한 처리...\n",
        "        self.full_text = self.__crean_text(self.full_text.strip())\n",
        "        self.full_sentences = np.array(nltk.sent_tokenize(self.full_text))\n",
        "        self.s_discriminator = s_discriminator\n",
        "        self.full_text_emb = self.s_discriminator.encode(self.full_sentences)   \n",
        "\n",
        "        # original sentance, 즉 source sentence에 대한 처리\n",
        "        self.org_text = self.__crean_text(self.org_text.strip())\n",
        "        if logout:\n",
        "            print('-'*50)\n",
        "            print(self.org_text)\n",
        "            print('-'*50)  \n",
        "\n",
        "        # 두장 이상의 문장이 있는 경우, '그리고' 등의 연결문을 삭제 한다.\n",
        "        # 그래야 grammar dicsriminator의 성능이 나온다.\n",
        "        self.org_sentences = np.array(nltk.sent_tokenize(self.org_text))\n",
        "        self.org_text_emb = self.s_discriminator.encode(self.org_sentences)\n",
        "        for i,sents in enumerate(self.org_sentences):\n",
        "            for cj in conjunction_table: \n",
        "                if sents.startswith(cj):\n",
        "                    self.org_sentences[i] = sents[len(cj):].strip()\n",
        "\n",
        "        #print(s)\n",
        "        #print(self.org_sentences)\n",
        "        # 하나의 문장을 token 단위로 잘라서 {index:token} dict을 만든다.\n",
        "        # 또한, 각 token의 attention을 설정한다.\n",
        "        self.org_term_set = (' '.join(self.org_sentences)).strip().split(' ')\n",
        "        self.org_source_length = len(self.org_term_set)\n",
        "        self.term_table = {}\n",
        "\n",
        "        self.seps = []\n",
        "        self.bias_table = {}\n",
        "        #morp_table = {}\n",
        "        aw = 0.0\n",
        "        attentions = []\n",
        "        self_attentions = es.get_self_attention_weight(self.org_text)\n",
        "        self_attentions_max = 0\n",
        "        self_attentions_map = {}\n",
        "        for index, word in enumerate(self.org_term_set):\n",
        "            self.term_table[index] = word\n",
        "            #attention = cosine_similarity(self.full_text,word)\n",
        "            #attentions.append(attention)\n",
        "            sa = 0\n",
        "            for u in range(index,len(self_attentions)):\n",
        "                if word.lower().replace('.','') == self_attentions[u][1]:\n",
        "                    sa = self_attentions[u][2]\n",
        "                    if sa > self_attentions_max:\n",
        "                        self_attentions_max = sa\n",
        "\n",
        "            self_attentions_map[index] = sa\n",
        "            #print(f'{word} \\t\\t {attention:.4f} {sa:.4f}')\n",
        "            \n",
        "        #print('self_attentions_max',self_attentions_max)\n",
        "        attentions = list(self_attentions_map.values())\n",
        "        attentions.sort(reverse=True)\n",
        "        #문장 전체 token의 30%에 attention을 준다.\n",
        "        trs = attentions[int(len(attentions)*self.attendtion_rate + 0.5)]\n",
        "\n",
        "        for index, word in enumerate(self.org_term_set):\n",
        "            self.term_table[index] = word\n",
        "            self.bias_table[index] = self_attentions_map[index] - trs #self_attentions_max\n",
        "            #attention = self_attentions_map[index] #cosine_similarity(self.full_text,word)\n",
        "            self.bias_table[index] = (0.0 if self.bias_table[index] > 0 else -1.0)\n",
        "            #self.bias_table[index] = (self.bias_table[index] if self.bias_table[index] > 0 else -1.0)\n",
        "            if len(self.org_term_set) - 1 == index:\n",
        "                    self.bias_table[index] = 0.1\n",
        "            '''\n",
        "            if attention >= trs or index == len(self.org_term_set)-1:\n",
        "                self.bias_table[index] = self_attentions_map[index] - trs #0.0 #attention\n",
        "            else:\n",
        "                self.bias_table[index] = -1.0 #attention #-cosine_similarity(self.full_text,word)\n",
        "            '''\n",
        "            \n",
        "            if word.endswith(('.','?')):\n",
        "                self.seps.append(index)\n",
        "                if self.org_source_length - 1 == index:\n",
        "                    pass\n",
        "                else:\n",
        "                    self.term_table[index] = combine_sentence(word)\n",
        "        #print(self.term_table)\n",
        "        #print(list(self.bias_table.values()))\n",
        "        # 또 다른 token 단위의 {index:token} dict을 만드는데, 이는 generator의 조합이\n",
        "        # 문법적으로 부실할 경우, corrector가 보정할때 '~~고'의 중간 연결문을 \n",
        "        # 부드럽게 만들기 위해 중간 문장의 '~다.'를 삭제한 dict에 해당한다.\n",
        "        self.combination_table = {}\n",
        "        for index, word in enumerate(self.org_term_set):\n",
        "            self.combination_table[index] = word\n",
        "            if index < len(self.org_term_set)-1: #중간 문장의 '~다.'를 삭제한다.\n",
        "                if self.org_term_set[index].endswith('다.'):\n",
        "                    self.combination_table[index] = word[0:len(word)-2]\n",
        "        if logout:\n",
        "            print('Length ------------------------------------|',len(self.term_table))\n",
        "            print(self.bias_table)\n",
        "        #print(self.combination_table)\n",
        "        if len(self.term_table) > 128:\n",
        "            raise Exception(\"Too much sentence length.\")\n",
        "\n",
        "    def get_org_sample(self, num):\n",
        "        return self.org_sentences[np.random.choice(len(self.org_sentences), num)]\n",
        "\n",
        "    def get_source_embedded_code(self):\n",
        "        return self.org_text_emb\n",
        "\n",
        "    def get_random_text(self,rate=0.5):\n",
        "        cnt = int(len(self.term_table) * rate)\n",
        "        a = list(self.term_table.keys())\n",
        "        b = np.random.choice(a, cnt)\n",
        "        c = [fruit for fruit in a if fruit not in b]\n",
        "        txt = []\n",
        "        for i in c:\n",
        "            txt.append(self.term_table[i])\n",
        "        return ' '.join(txt).strip(), hash(tuple(b))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af5b1VF7poE2"
      },
      "source": [
        "## N-Gram Similarity Comparison\n",
        "\n",
        "https://gist.github.com/gaulinmp/da5825de975ed0ea6a24186434c24fe4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAfA5fHxBoGW",
        "outputId": "1c2dd899-e35a-463b-d06f-b976a300811f"
      },
      "source": [
        "# Get Tuple algorithms \n",
        "import re\n",
        "import math\n",
        "import numpy as np\n",
        "from itertools import chain\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.util import ngrams # This is the ngram magic.\n",
        "from textblob import TextBlob\n",
        "\n",
        "NGRAM = 4\n",
        "\n",
        "re_sent_ends_naive = re.compile(r'[.\\n]')\n",
        "re_stripper_alpha = re.compile('[^a-zA-Z]+')\n",
        "re_stripper_naive = re.compile('[^a-zA-Z\\.\\n]')\n",
        "\n",
        "splitter_naive = lambda x: re_sent_ends_naive.split(re_stripper_naive.sub(' ', x))\n",
        "\n",
        "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "def get_tuples_nosentences(txt):\n",
        "    \"\"\"Get tuples that ignores all punctuation (including sentences).\"\"\"\n",
        "    if not txt: return None\n",
        "    #ng = ngrams(re_stripper_alpha.sub(' ', txt).split(), NGRAM)\n",
        "    ng = ngrams(txt, NGRAM)\n",
        "    return list(ng)\n",
        "\n",
        "def get_tuples_manual_sentences(txt):\n",
        "    \"\"\"Naive get tuples that uses periods or newlines to denote sentences.\"\"\"\n",
        "    if not txt: return None\n",
        "    sentences = (x.split() for x in splitter_naive(txt) if x)\n",
        "    ng = (ngrams(x, NGRAM) for x in sentences if len(x) >= NGRAM)\n",
        "    return list(chain(*ng))\n",
        "\n",
        "def get_tuples_nltk_punkt_sentences(txt):\n",
        "    \"\"\"Get tuples that doesn't use textblob.\"\"\"\n",
        "    if not txt: return None\n",
        "    sentences = (re_stripper_alpha.split(x) for x in sent_detector.tokenize(txt) if x)\n",
        "    # Need to filter X because of empty 'words' from punctuation split\n",
        "    ng = (ngrams(filter(None, x), NGRAM) for x in sentences if len(x) >= NGRAM)\n",
        "    return list(chain(*ng))\n",
        "\n",
        "def get_tuples_textblob_sentences(txt):\n",
        "    \"\"\"New get_tuples that does use textblob.\"\"\"\n",
        "    if not txt: return None\n",
        "    tb = TextBlob(txt)\n",
        "    ng = (ngrams(x.words, NGRAM) for x in tb.sentences if len(x.words) > NGRAM)\n",
        "    return [item for sublist in ng for item in sublist]\n",
        "\n",
        "def jaccard_distance(a, b):\n",
        "    \"\"\"Calculate the jaccard distance between sets A and B\"\"\"\n",
        "    a = set(a)\n",
        "    b = set(b)\n",
        "    return 1.0 * len(a&b)/len(a|b)\n",
        "\n",
        "def cosine_similarity_ngrams(a, b):\n",
        "    vec1 = Counter(a)\n",
        "    vec2 = Counter(b)\n",
        "    \n",
        "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
        "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
        "\n",
        "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
        "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
        "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
        "\n",
        "    if not denominator:\n",
        "        return 0.0\n",
        "    return float(numerator) / denominator\n",
        "\n",
        "\n",
        "def test():\n",
        "    paragraph = \"\"\"It was the best of times, it was the worst of times.\n",
        "               It was the age of wisdom? It was the age of foolishness!\n",
        "               I first met Dr. Frankenstein in Munich; his monster was, presumably, at home.\"\"\"\n",
        "    print(paragraph)\n",
        "    _ = get_tuples_nosentences(paragraph);print(\"Number of N-grams (no sentences):\", len(_));_\n",
        "\n",
        "    _ = get_tuples_manual_sentences(paragraph);print(\"Number of N-grams (naive sentences):\", len(_));_\n",
        "\n",
        "    _ = get_tuples_nltk_punkt_sentences(paragraph);print(\"Number of N-grams (nltk sentences):\", len(_));_\n",
        "\n",
        "    _ = get_tuples_textblob_sentences(paragraph);print(\"Number of N-grams (TextBlob sentences):\", len(_));_\n",
        "\n",
        "    a = get_tuples_nosentences(\"It was the best of times.\")\n",
        "    b = get_tuples_nosentences(\"It was the worst of times.\")\n",
        "    print(\"Jaccard: {}   Cosine: {}\".format(jaccard_distance(a,b), cosine_similarity_ngrams(a,b)))\n",
        "\n",
        "    a = get_tuples_nosentences(\"Above is a bad example of four-gram similarity.\")\n",
        "    b = get_tuples_nosentences(\"This is a better example of four-gram similarity.\")\n",
        "    print(\"Jaccard: {}   Cosine: {}\".format(jaccard_distance(a,b), cosine_similarity_ngrams(a,b)))\n",
        "\n",
        "    a = get_tuples_nosentences(\"Jaccard Index ignores repetition repetition repetition repetition repetition.\")\n",
        "    b = get_tuples_nosentences(\"Cosine similarity weighs repetition repetition repetition repetition repetition.\")\n",
        "    print(\"Jaccard: {}   Cosine: {}\".format(jaccard_distance(a,b), cosine_similarity_ngrams(a,b)))\n",
        "test()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It was the best of times, it was the worst of times.\n",
            "               It was the age of wisdom? It was the age of foolishness!\n",
            "               I first met Dr. Frankenstein in Munich; his monster was, presumably, at home.\n",
            "Number of N-grams (no sentences): 214\n",
            "Number of N-grams (naive sentences): 25\n",
            "Number of N-grams (nltk sentences): 25\n",
            "Number of N-grams (TextBlob sentences): 25\n",
            "Jaccard: 0.6071428571428571   Cosine: 0.755742181606458\n",
            "Jaccard: 0.6071428571428571   Cosine: 0.755742181606458\n",
            "Jaccard: 0.23214285714285715   Cosine: 0.9208243668497166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akhPuNZHBx4w"
      },
      "source": [
        "def cosine_similarity(src_txt,trg_txt):\n",
        "    try:\n",
        "        if src_txt == None or src_txt.strip() == '':\n",
        "            return 0.0\n",
        "        if trg_txt == None or trg_txt.strip() == '':\n",
        "            return 0.0\n",
        "\n",
        "        a = get_tuples_nosentences(src_txt)\n",
        "        b = get_tuples_nosentences(trg_txt)\n",
        "        return cosine_similarity_ngrams(a,b)\n",
        "    except Exception as ex:\n",
        "        #print(src_txt,trg_txt)\n",
        "        return 0.0"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UAeFBYMMxKY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5b7e514-c2bb-4696-973c-acf6dc5595a7"
      },
      "source": [
        "txt = \"\"\"\n",
        "The judge agreed with police that he would have been over the limit at the time his red Citroen hit Miss Titley’s blue Daihatsu Cuore on a road near Yarmouth, Isle of Wight, on October 11, 2013.\n",
        "His phone records showed he was also texting around the time of the crash.\n",
        "\"\"\"\n",
        "s = Source(txt,txt)\n",
        "s.set_key_rate(s_discriminator)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "The judge agreed with police that he would have been over the limit at the time his red Citroen hit Miss Titley’s blue Daihatsu Cuore on a road near Yarmouth Isle of Wight on October 11 2013. His phone records showed he was also texting around the time of the crash.\n",
            "--------------------------------------------------\n",
            "Length ------------------------------------| 51\n",
            "{0: -1.0, 1: -1.0, 2: -1.0, 3: 0.0, 4: -1.0, 5: 0.0, 6: -1.0, 7: -1.0, 8: -1.0, 9: -1.0, 10: 0.0, 11: -1.0, 12: -1.0, 13: 0.0, 14: -1.0, 15: 0.0, 16: -1.0, 17: 0.0, 18: -1.0, 19: -1.0, 20: -1.0, 21: -1.0, 22: 0.0, 23: -1.0, 24: -1.0, 25: 0.0, 26: 0.0, 27: -1.0, 28: -1.0, 29: -1.0, 30: -1.0, 31: 0.0, 32: -1.0, 33: 0.0, 34: -1.0, 35: 0.0, 36: 0.0, 37: -1.0, 38: -1.0, 39: -1.0, 40: -1.0, 41: -1.0, 42: -1.0, 43: -1.0, 44: -1.0, 45: -1.0, 46: -1.0, 47: 0.0, 48: 0.0, 49: -1.0, 50: 0.1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lH9NLi9koYvo"
      },
      "source": [
        "def besm(full_text,top_rank=2):\n",
        "    '''\n",
        "    ft = []\n",
        "    org_sentences = np.array(nltk.sent_tokenize(full_text))\n",
        "    for txt in org_sentences:\n",
        "        if len(txt) > 30:\n",
        "            ft.append(txt)\n",
        "    full_text = ' '.join(ft)\n",
        "    '''\n",
        "    queries = nltk.sent_tokenize(full_text)\n",
        "    src_sentences = nltk.sent_tokenize(full_text)\n",
        "    query_embeddings = s_discriminator._embedder.encode(queries,show_progress_bar=False)\n",
        "    full_text_embeddings = s_discriminator._embedder.encode(src_sentences,show_progress_bar=False)\n",
        "    #print(queries)\n",
        "    #print(org_text_emb)\n",
        "    \n",
        "    if len(query_embeddings) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    cos_scores = scipy.spatial.distance.cdist(query_embeddings, full_text_embeddings, \"cosine\")\n",
        "    scores = np.max(cos_scores,axis=1)\n",
        "    orderd = [(o,s) for o,s in enumerate(scores)]\n",
        "    orderd.sort(key=lambda e: e[1],reverse=True)\n",
        "    top_rank = top_rank if len(orderd) > top_rank else len(orderd)\n",
        "    a = [orderd[i][0] for i in range(0,top_rank)]\n",
        "    a.sort()\n",
        "    summ_text = \" \".join([queries[i] for i in a])\n",
        "\n",
        "    return summ_text"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YA0YDeInoh7z",
        "outputId": "a6a61010-21fe-4c24-d7e6-1cf8dc4f5b23"
      },
      "source": [
        "full_text = get_prepared_doc(sentences_dataset[0])\n",
        "org_sentences = np.array(nltk.sent_tokenize(full_text))\n",
        "for txt in org_sentences:\n",
        "    print(txt)\n",
        "print()\n",
        "\n",
        "org_sentences = np.array(nltk.sent_tokenize(besm(full_text,top_rank=6)))\n",
        "for txt in org_sentences:\n",
        "    print(txt)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "구리·남양주시가 타 시·군과 차별화된 미세먼지 저감 대책을 세웠다.\n",
            "7일 구리시에 따르면 2022년까지 미세먼지 농도를 현재 24㎍/㎥에서 19㎍/㎥로 낮추겠다는 목표로 단기 및 중장기 대책을 마련했다.\n",
            "먼저 버스정류장 저감 시스템을 설치한다.\n",
            "시는 한국철도기술연구원과 함께 버스 정류장에 사물인터넷(IoT)을 기반으로 한 미세먼지 집진 모듈을 설치하는 기술을 개발하고 있다.\n",
            "올해부터 3년간 총 10억원의 예산을 들여 관내 버스 중앙 차로 버스정류장에 시스템을 설치할 계획이다.\n",
            "지난해 말부터는 미세먼지 취약계층 이용시설에 휴대용 미세먼지 측정기를 보급하고 있다.\n",
            "올해까지 시립어린이집 12개소와 지역아동센터 15개소에 미세먼지 측정기 설치를 완료할 예정이다.\n",
            "또한 미세먼지 농도에 따라 구리타워의 조명 색상을 파랑(좋음), 녹색(보통), 노랑(나쁨), 빨강(매우 나쁨) 등으로 바꿔 시민들이 대기질을 즉시 알 수 있도록 했다.\n",
            "시 관계자는 \"현재 관내 미세먼지 취약계층이 상주하는 경로당, 어린이집 등 시설에 공기청정기 898개를 보급하고, 미세먼지 저감을 위한 계획과 실행 방법 등을 담은 소통형 스마트폰 앱(App)을 개발하는 등 미세먼지 대응에 총력을 다하고 있다\"고 말했다.\n",
            "남양주시 역시 미세먼지 측정망 운영 등 미세먼지 저감 10대 중점과제 등 맞춤형 미세먼지 종합대책을 마련해 시행 중이다.\n",
            "특히 미세먼지 피해가 큰 어린이·노인 등 민감 계층에게 나쁨 단계부터 미세먼지 예보를 문자로 알리는 맞춤형 예·경보제가 주목을 받고 있다.\n",
            "찾아가는 미세먼지 시민강좌 역시 호평 속에 운영 중이다.\n",
            "연간 300회 실시되는 이 강좌는 10인 이상 신청 단체에게 미세먼지 위해성과 고농도 미세먼지 발생시 행동요령 등을 알려준다.\n",
            "또한 교육시설에는 미세먼지 알림 시스템을 구축해 운영하고 있다.\n",
            "관내 어린이집, 유치원, 초중학교 등 교육시설 700개소에 남양주시 자체 대기오염 측정소에서 측정된 해당 지역의 실시간 미세먼지 정보를 제공하고 있다.\n",
            "시 관계자는 \"미세먼지 저감은 단기적인 계획이나 한두 가지 정책만으로 효과를 볼 수 없다\"며 \"종합적인 대책 마련은 물론이고 시민들의 피부에 와 닿는 정책을 마련하기 위해 애쓰고 있다\"고 말했다.\n",
            "\n",
            "7일 구리시에 따르면 2022년까지 미세먼지 농도를 현재 24㎍/㎥에서 19㎍/㎥로 낮추겠다는 목표로 단기 및 중장기 대책을 마련했다.\n",
            "올해부터 3년간 총 10억원의 예산을 들여 관내 버스 중앙 차로 버스정류장에 시스템을 설치할 계획이다.\n",
            "올해까지 시립어린이집 12개소와 지역아동센터 15개소에 미세먼지 측정기 설치를 완료할 예정이다.\n",
            "시 관계자는 \"현재 관내 미세먼지 취약계층이 상주하는 경로당, 어린이집 등 시설에 공기청정기 898개를 보급하고, 미세먼지 저감을 위한 계획과 실행 방법 등을 담은 소통형 스마트폰 앱(App)을 개발하는 등 미세먼지 대응에 총력을 다하고 있다\"고 말했다.\n",
            "연간 300회 실시되는 이 강좌는 10인 이상 신청 단체에게 미세먼지 위해성과 고농도 미세먼지 발생시 행동요령 등을 알려준다.\n",
            "관내 어린이집, 유치원, 초중학교 등 교육시설 700개소에 남양주시 자체 대기오염 측정소에서 측정된 해당 지역의 실시간 미세먼지 정보를 제공하고 있다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGTFQU_PqaLk",
        "outputId": "5804dc00-c2c0-4529-93ae-55b0ffd40328"
      },
      "source": [
        "sum1 = es.generate_summary(full_text,top_n=9,min_length=0)[0]\n",
        "org_sentences = np.array(nltk.sent_tokenize(sum1))\n",
        "for txt in org_sentences:\n",
        "    print(txt)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "또한 미세먼지 농도에 따라 구리타워의 조명 색상을 파랑(좋음), 녹색(보통), 노랑(나쁨), 빨강(매우 나쁨) 등으로 바꿔 시민들이 대기질을 즉시 알 수 있도록 했다.\n",
            "시 관계자는 \"현재 관내 미세먼지 취약계층이 상주하는 경로당, 어린이집 등 시설에 공기청정기 898개를 보급하고, 미세먼지 저감을 위한 계획과 실행 방법 등을 담은 소통형 스마트폰 앱(App)을 개발하는 등 미세먼지 대응에 총력을 다하고 있다\"고 말했다.\n",
            "남양주시 역시 미세먼지 측정망 운영 등 미세먼지 저감 10대 중점과제 등 맞춤형 미세먼지 종합대책을 마련해 시행 중이다.\n",
            "특히 미세먼지 피해가 큰 어린이·노인 등 민감 계층에게 나쁨 단계부터 미세먼지 예보를 문자로 알리는 맞춤형 예·경보제가 주목을 받고 있다.\n",
            "찾아가는 미세먼지 시민강좌 역시 호평 속에 운영 중이다.\n",
            "연간 300회 실시되는 이 강좌는 10인 이상 신청 단체에게 미세먼지 위해성과 고농도 미세먼지 발생시 행동요령 등을 알려준다.\n",
            "또한 교육시설에는 미세먼지 알림 시스템을 구축해 운영하고 있다.\n",
            "관내 어린이집, 유치원, 초중학교 등 교육시설 700개소에 남양주시 자체 대기오염 측정소에서 측정된 해당 지역의 실시간 미세먼지 정보를 제공하고 있다.\n",
            "시 관계자는 \"미세먼지 저감은 단기적인 계획이나 한두 가지 정책만으로 효과를 볼 수 없다\"며 \"종합적인 대책 마련은 물론이고 시민들의 피부에 와 닿는 정책을 마련하기 위해 애쓰고 있다\"고 말했다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5ng68INoY4D"
      },
      "source": [
        "\n",
        "def besm2(full_text,text,top_rank=2):\n",
        "    scores = []\n",
        "    queries = nltk.sent_tokenize(text)\n",
        "    for sen in queries:\n",
        "        s = cosine_similarity(sen,full_text)\n",
        "        scores.append(s)\n",
        "        #print(s,sen)\n",
        "    orderd = [(o,s) for o,s in enumerate(scores)]\n",
        "    orderd.sort(key=lambda e: e[1],reverse=True)\n",
        "    a = [orderd[i][0] for i in range(0,top_rank)]\n",
        "    a.sort()\n",
        "    summ_text = \" \".join([queries[i] for i in a])\n",
        "\n",
        "    return summ_text"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XY59mdNK8ub"
      },
      "source": [
        "# 4.5 Generator class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5CLF3WcK6lp",
        "trusted": true
      },
      "source": [
        "from functools import reduce\n",
        "\n",
        "\n",
        "# custom weights initialization called on netG and netD\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Linear') != -1:\n",
        "        m.weight.data.normal_(0.02, 0.08)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.05)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "        Simple Generator w/ MLP\n",
        "    \"\"\"\n",
        "    '''\n",
        "    def __init__(self, input_size=1024):\n",
        "        super(Generator, self).__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(input_size, input_size*2),\n",
        "            nn.BatchNorm1d(input_size*2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(input_size*2, input_size*3),\n",
        "            nn.BatchNorm1d(input_size*3),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(input_size*3, input_size*3),\n",
        "            nn.BatchNorm1d(input_size*3),\n",
        "            nn.ReLU(True),            \n",
        "            nn.Linear(input_size*3, input_size*2),\n",
        "            nn.BatchNorm1d(input_size*2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(input_size*2, input_size),\n",
        "            #nn.BatchNorm1d(term_length*4),\n",
        "            nn.Tanh() # -1 ~ 1\n",
        "        )\n",
        "\n",
        "    \n",
        "    def __init__(self, input_size=1024):\n",
        "        super(Generator, self).__init__()\n",
        "        l1 = nn.Linear(input_size, input_size*4)\n",
        "        l1.weight.data.normal_(0.0, 0.01)\n",
        "        bn = nn.BatchNorm1d(input_size*4)\n",
        "        bn.weight.data.normal_(0.0, 0.01)\n",
        "        bn.bias.data.fill_(0)        \n",
        "        l2 = nn.Linear(input_size*4, input_size)\n",
        "        l2.weight.data.normal_(0.05, 0.01)\n",
        "        self.layer = nn.Sequential(\n",
        "            l1,\n",
        "            bn,\n",
        "            nn.ReLU(True), #nn.LeakyReLU(0.2),\n",
        "            l2,\n",
        "            #nn.BatchNorm1d(term_length*4),\n",
        "            nn.Tanh() # -1 ~ 1\n",
        "        )\n",
        "    '''\n",
        "\n",
        "    def __init__(self, input_size=1024):\n",
        "        super(Generator, self).__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(input_size, input_size*4),\n",
        "            nn.BatchNorm1d(input_size*4),\n",
        "            nn.ReLU(True), #nn.LeakyReLU(0.2),\n",
        "            nn.Linear(input_size*4, input_size),\n",
        "            #nn.BatchNorm1d(input_size*4),\n",
        "            #nn.ReLU(True), #nn.LeakyReLU(0.2),            \n",
        "            #nn.Linear(input_size*4, input_size),\n",
        "            #nn.BatchNorm1d(input_size),\n",
        "            #nn.ReLU(True), #nn.LeakyReLU(0.2),\n",
        "            nn.Tanh() # -1 ~ 1\n",
        "        )\n",
        "\n",
        "    def forward(self, x, bias):\n",
        "        #biased_noise = torch.randn(N,_NOISE_DIM)\n",
        "        # stroy peak에 해당하는 term에게 평균값에 해당하는 bias를 추가 한다.\n",
        "                 \n",
        "        y_ = self.layer(x)\n",
        "        y = torch.add(y_,bias)\n",
        "        #y = nn.Sigmoid()(y)\n",
        "\n",
        "        return y, y_"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0RQOPpQgUTE"
      },
      "source": [
        "# SAM_Summarizer 학습기..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nd8GTS7HKz1H",
        "trusted": true
      },
      "source": [
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "from scipy.special import expit\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SAM_Summarizer:\n",
        "\n",
        "    def __init__(self,g_discriminator,s_discriminator):\n",
        "        self.g_discriminator = g_discriminator\n",
        "        #self.c_discriminator = c_discriminator\n",
        "        self.s_discriminator = s_discriminator\n",
        "        self.m = nn.Sigmoid()\n",
        "        self.with_bias = True\n",
        "\n",
        "    def ready(self,source):\n",
        "        self.source = source  \n",
        "        #self.source.analysis_frame_terms(self.s_discriminator)\n",
        "        self.generator = Generator(input_size=self.source.org_source_length)\n",
        "        self.generator.apply(weights_init)\n",
        "        return self\n",
        "\n",
        "    def summarize(self,epochs=10,batch_size=1,learning_rate=2e-4, display = False,comp_rate=1.0):\n",
        "        history = self.__train(epochs,batch_size,learning_rate,display,comp_rate)\n",
        "\n",
        "        if display and history is not None:\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            plt.plot(history['gen_g_loss'],label='grammar loss')\n",
        "            plt.plot(history['gen_l_loss'],label='compression loss')\n",
        "            plt.plot(history['gen_s_loss'],label='n-gram similarity loss')\n",
        "            #plt.plot(history['gen_c_loss'],label='context similarity loss')\n",
        "            #plt.plot(history['total loss'],label='total loss')\n",
        "            plt.plot(history['losses std'],label='standard deviation of losses')\n",
        "            \n",
        "            #if 'dis_loss' in history:\n",
        "            #    plt.plot(history['dis_loss'],label='discriminator grammar loss')\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "        return history\n",
        "\n",
        "    # text의 생성 for torch\n",
        "    def __text_gen2(self, p_txt, gen_length):\n",
        "        gtext = []\n",
        "        sorted_noise, i = torch.sort(p_txt, descending=True)\n",
        "        order, i = torch.sort(i[:gen_length], descending=False)\n",
        "        #print(len(order))\n",
        "        #print(gen_length)\n",
        "        assert len(order) == gen_length\n",
        "        order = order.cpu().detach().numpy()\n",
        "        for k in order:\n",
        "            gtext.append((self.source.term_table[k],k))\n",
        "        return gtext\n",
        "\n",
        "    def __text_gen3(self, p_txt):\n",
        "        gtext = []\n",
        "\n",
        "        for order,p in enumerate(p_txt):\n",
        "            if p > 0.0:\n",
        "                gtext.append(self.source.term_table[order])\n",
        "        return gtext\n",
        "\n",
        "    def __text_gen5(self, p_txt):\n",
        "        gtext = []\n",
        "\n",
        "        for order,p in enumerate(p_txt):\n",
        "            if p > 0.0:\n",
        "                gtext.append(self.source.combination_table[order])\n",
        "        return gtext\n",
        "\n",
        "    def __text_hash(self, p_txt):\n",
        "        b = []\n",
        "        #hash(tuple(b))\n",
        "        for order,p in enumerate(p_txt):\n",
        "            if p > 0.0:\n",
        "                b.append(order)\n",
        "        return hash(tuple(b))\n",
        "\n",
        "    def __text_gen4(self, p_txt):\n",
        "        gtext = \"\"\n",
        "        indexs = []\n",
        "        for order,p in enumerate(p_txt):\n",
        "            if p > 0.0:\n",
        "                gtext += self.source.term_table[order] + ' '\n",
        "                indexs.append(order)\n",
        "        return gtext.strip(),indexs\n",
        "\n",
        "    def __train(self, epochs=10,batch_size=10,learning_rate=2e-4,display = False,comp_rate=1.0):\n",
        "        # In the Deepmind paper they use RMSProp however then Adam optimizer\n",
        "        # improves training time\n",
        "        #generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "        # This method returns a helper function to compute cross entropy loss\n",
        "        #cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "        # Set the seed value all over the place to make this reproducible.\n",
        "        seed_val = int(random.random()*100)\n",
        "\n",
        "        random.seed(seed_val)\n",
        "        np.random.seed(seed_val)\n",
        "        torch.manual_seed(seed_val)\n",
        "        torch.cuda.manual_seed_all(seed_val)\n",
        "        \n",
        "        criterion = nn.MSELoss()\n",
        "        #D_opt = torch.optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "        G_opt = AdamW(self.generator.parameters(),\n",
        "                        lr = 2e-3, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                        eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                        )\n",
        "        # Create the learning rate scheduler.\n",
        "        scheduler = get_linear_schedule_with_warmup(G_opt, \n",
        "                                                    num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                                    num_training_steps = epochs)\n",
        "        \n",
        "        #gen_length = len(self.source.story_peaks) + int(len(self.source.story_peaks)*self.frame_expansion_ratio)\n",
        "        pb = ProgressBar(epochs,prefix='Train...')\n",
        "        gen_gmr_loss_history = []\n",
        "        gen_len_loss_history = []\n",
        "        gen_sim_loss_history = []\n",
        "        #gen_cos_loss_history = []\n",
        "        dis_loss_history = []    \n",
        "        total_loss_history = []\n",
        "        losses_std_history = []\n",
        "\n",
        "        #model 들은 cuda로 보낸다.\n",
        "        self.g_discriminator.discriminator.to(device)\n",
        "        self.g_discriminator.discriminator.eval() # 학습하지 않는다...\n",
        "        #self.c_discriminator.discriminator.to(device)\n",
        "        #self.c_discriminator.discriminator.eval() # 학습하지 않는다...\n",
        "\n",
        "        self.generator.to(device)       \n",
        "        self.generator.train()\n",
        "\n",
        "        #self.bias_w = init_bias\n",
        "        initial_bias = 0\n",
        "        #G_s_loss = torch.tensor(0)\n",
        "        #G_c_loss = torch.tensor(0)\n",
        "        #G_g_loss = torch.tensor(0)\n",
        "\n",
        "        '''\n",
        "        epsilon = 1 # Epsilon-greedy algorithm in initialized at 1 meaning every step is random at the start\n",
        "        max_epsilon = 1 # You can't explore more than 100% of the time\n",
        "        min_epsilon = 0.001 # At a minimum, we'll always explore 1% of the time\n",
        "        decay = 10/epochs\n",
        "        '''\n",
        "\n",
        "        dfs = torch.tensor([ 1.0, similarity, comp_rate], device=device, dtype=torch.float, requires_grad=True)\n",
        "        target = torch.tensor([list(self.source.bias_table.values()) for u in range(batch_size)],dtype=torch.float).to(device)\n",
        "        #print(target)\n",
        "        #noise = torch.randn(batch_size,self.source.org_source_length).to(device) \n",
        "        #a_w = 1.0\n",
        "        for i in range(epochs):\n",
        "   \n",
        "            if True:\n",
        "                noise = torch.randn(batch_size,self.source.org_source_length).to(device) \n",
        "                '''\n",
        "                random_number = np.random.rand()\n",
        "                # 2. Explore using the Epsilon Greedy Exploration Strategy\n",
        "                if random_number <= epsilon:\n",
        "                    # Explore\n",
        "                    bias = torch.randn(batch_size,self.source.org_source_length).to(device) * epsilon\n",
        "                    #b = torch.tensor([list(self.source.bias_table.values()) for u in range(batch_size)]).to(device)\n",
        "                    #bias = torch.add(a,b)\n",
        "                    #noise = torch.randn(batch_size,self.source.org_source_length).to(device) \n",
        "                else:\n",
        "                    #bias = torch.tensor([list(self.source.bias_table.values()) for u in range(batch_size)]).to(device)\n",
        "                    bias = torch.zeros_like(noise).to(device)\n",
        "                '''\n",
        "                bias = torch.zeros_like(noise).to(device)\n",
        "\n",
        "\n",
        "                #if self.with_bias:\n",
        "                #    bias[:,noise.shape[1]-1] = 0.1\n",
        "                #bias[:,noise.shape[1]-1] = 0.5\n",
        "                #if i < epochs/4:\n",
        "                #bias = torch.randn(batch_size,self.source.org_source_length).to(device) / 4                 \n",
        "                #bias = torch.randn(batch_size,self.source.org_source_length).to(device) \n",
        "\n",
        "                sw, sw0 = self.generator(noise,bias)\n",
        "                # sw를 복제하고 gradient 안되도록 detach 시키고... cpu에서 작업\n",
        "                sw_relu = F.relu(sw.clone().detach().cpu())\n",
        "                sw_relu_indexs = (sw_relu > 0).nonzero()\n",
        "                grammar_filter = torch.where(sw > 0.0, 1.0, 0.0)\n",
        "                similar_filter = torch.where(sw > 0.0, 1.0, 0.0)\n",
        "                for j in range(sw.shape[0]): # batch size\n",
        "                    tm1 = [v[1].item() for v in sw_relu_indexs if v[0]==j]\n",
        "                    text = ' '.join([self.source.term_table[x] for x in tm1])\n",
        "                    #print('text',text)\n",
        "                    # 문법성에 대해서...\n",
        "                    loss, out=self.g_discriminator.transfer_learning([text],train_for = False)\n",
        "                    grammar_rate = out[0,1].item()\n",
        "                    #print('grammar_rate',grammar_rate)               \n",
        "                    grammar_filter[j] = grammar_filter[j] * np.tanh(grammar_rate)\n",
        "                    # 유사성에 대해서...\n",
        "                    sim_rate = cosine_similarity(self.source.full_text,text)  \n",
        "                    similar_filter[j] = similar_filter[j] * sim_rate\n",
        "                #print('grammar_filter',grammar_filter)  \n",
        "                G_g_loss = -torch.mean(sw*grammar_filter)\n",
        "                #print('G_g_loss',G_g_loss)  \n",
        "                G_s_loss = -torch.mean(sw*similar_filter)\n",
        "                G_l_loss = (criterion(sw,target) - 1)\n",
        "                '''\n",
        "                #print(sw)\n",
        "                with torch.no_grad():                \n",
        "                    fake_gmr_out, fake_sim_out, fake_cos_out, fake_len_out = self.__discrete_gradient(sw)\n",
        "\n",
        "                #print(fake_len_out)\n",
        "                #print(fake_gmr_out)\n",
        "                sw2 = sw * fake_gmr_out\n",
        "                #print(sw2)\n",
        "                G_g_loss = -torch.mean(sw2)\n",
        "                #print(G_g_loss)\n",
        "                sw1 = sw * fake_sim_out\n",
        "                G_s_loss = -torch.mean(sw1)\n",
        "\n",
        "                sw4 = sw * fake_cos_out\n",
        "                G_c_loss = -torch.mean(sw4) \n",
        "\n",
        "                #sw3 = sw * fake_len_out\n",
        "                #G_l_loss = -torch.mean(sw3)\n",
        "\n",
        "                G_l_loss = (criterion(sw,target) - 1) #* (1-epsilon)\n",
        "                '''\n",
        "\n",
        "                dsc_loss = torch.stack([G_g_loss,G_s_loss,G_l_loss])\n",
        "\n",
        "                G_loss = torch.dot(dfs,dsc_loss) + torch.std(dsc_loss)*std_factor\n",
        "                #G_loss =  G_g_loss  + G_s_loss + G_l_loss * comp_rate\n",
        "                #G_loss = G_l_loss\n",
        "\n",
        "                #print(G_loss)\n",
        "                \n",
        "                self.generator.zero_grad()\n",
        "                G_loss.backward()\n",
        "                #print('backward:')\n",
        "                G_opt.step()\n",
        "                scheduler.step()\n",
        "                '''\n",
        "                learning_rate = 0.02\n",
        "                with torch.no_grad():\n",
        "                    dfs += learning_rate * dfs.grad\n",
        "                    dfs.grad = None                    \n",
        "                    dfs[dfs < 0] = 0.1                \n",
        "                '''\n",
        "                #if G_g_loss == 0:# or (i > 100 and G_g_loss > 0):\n",
        "                #    return None\n",
        "\n",
        "                #if G_g_loss > 0:\n",
        "                #    a_w += 0.4\n",
        "            \n",
        "            gen_gmr_loss_history.append(G_g_loss.item())\n",
        "            #gen_cos_loss_history.append(G_c_loss.cpu().detach().numpy())\n",
        "            gen_sim_loss_history.append(G_s_loss.item())\n",
        "            #dis_loss_history.append(D_loss.cpu().detach().numpy())\n",
        "            gen_len_loss_history.append(G_l_loss.item())\n",
        "\n",
        "            #pb.printProgress(+1,f'{i+1}/{epochs} epochs, beta:{dfs} Generator / grammar loss:{G_g_loss}  similarity loss:{G_s_loss}') #,   Discriminator grammar_loss:{D_loss}        ')\n",
        "            #pb.printProgress(+1,'{}/{} epochs, beta:{}, grammar loss:{:.4f}  similarity loss:{:.4f} length loss:{:.4f}'.format(i+1,epochs,dfs,G_g_loss,G_s_loss,G_l_loss)) #,   Discriminator grammar_loss:{D_loss}        ')\n",
        "            pb.printProgress(+1,'{}/{} epochs, gl:{:.8f}  sl:{:.4f} ll:{:.4f}'.format(i+1,epochs, G_g_loss,G_s_loss,G_l_loss)) #,   Discriminator grammar_loss:{D_loss}        ')\n",
        "            \n",
        "            total_loss_history.append(torch.sum(dsc_loss).item())\n",
        "            losses_std_history.append(torch.std(dsc_loss).item())\n",
        "\n",
        "            #epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay * i)\n",
        "            \n",
        "            del G_g_loss\n",
        "            del G_s_loss\n",
        "            del G_l_loss\n",
        "\n",
        "        self.generator.eval()\n",
        "\n",
        "        if np.min(gen_gmr_loss_history[-10:]) > -0.10:\n",
        "            return None\n",
        "        #self.g_discriminator.discriminator.eval()\n",
        "\n",
        "        if display:\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            xs = np.arange(self.source.org_source_length)\n",
        "            #plt.bar(xs+0.0,sw0[0].cpu().detach().numpy(),label='before activation weights',width=0.2)\n",
        "            plt.bar(xs+0.0,sw[0].cpu().detach().numpy(),label='generated value',width=0.2)\n",
        "            plt.bar(xs+0.2,list(self.source.bias_table.values()),label='-self_attention',width=0.2)         \n",
        "            plt.legend()        \n",
        "            plt.show()\n",
        "\n",
        "        return  {'gen_g_loss':gen_gmr_loss_history,'gen_s_loss':gen_sim_loss_history,'gen_l_loss':gen_len_loss_history,'total loss':total_loss_history,'losses std':losses_std_history} #,'dis_loss':dis_loss_history }\n",
        "    '''\n",
        "    def get_summary(self, count):\n",
        "        #texts = []\n",
        "        self.generator.cpu()\n",
        "        self.generator.eval()\n",
        "        #gen_length = len(self.source.story_peaks) + int(len(self.source.story_peaks)*self.frame_expansion_ratio)\n",
        "        noise = torch.randn(count,self.source.org_source_length)\n",
        "        bias = torch.zeros_like(noise)\n",
        "        if self.with_bias:\n",
        "            bias[:,noise.shape[1]-1] = 1\n",
        "        with torch.no_grad():\n",
        "            sw,sw0 = self.generator(noise,bias)\n",
        "            #sw,sw0 = self.generator(noise)\n",
        "\n",
        "        max_score = 0\n",
        "        max_sim = 0\n",
        "        comp_rate = 0\n",
        "        best_text = \"\"\n",
        "\n",
        "        for p_txt in sw:\n",
        "            gtext = self.__text_gen3(p_txt)\n",
        "            text = ' '.join(gtext)\n",
        "            \n",
        "            #print('>>',text)\n",
        "            sim_score = self.s_discriminator.similarity(text,self.source.full_text_emb)\n",
        "            if sim_score > max_sim:\n",
        "                best_text = text.strip()\n",
        "                loss, out=self.g_discriminator.transfer_learning([text],train_for = False)\n",
        "                max_score = out[0,1].item()\n",
        "                comp_rate = 1 - len(best_text)/len(self.source.org_text)\n",
        "                max_sim = sim_score\n",
        "            #texts.append([text.strip(),out,sim_score])\n",
        "        return best_text, max_score, max_sim, comp_rate\n",
        "    '''\n",
        "    def get_samples(self,count):\n",
        "        self.generator.cpu()\n",
        "        self.generator.eval()\n",
        "        noise = torch.randn(count,self.source.org_source_length)\n",
        "        bias = torch.zeros_like(noise)\n",
        "        #if self.with_bias:\n",
        "        #    bias[:,noise.shape[1]-1] = 1\n",
        "        with torch.no_grad():\n",
        "            sw,sw0 = self.generator(noise,bias)\n",
        "        #samples = []\n",
        "        best_p_txt = None\n",
        "        best_text = \"\"\n",
        "        best_grammar_rate = 0\n",
        "        best_sim_rate = 0\n",
        "        best_comp_rate = 0\n",
        "        max_score = 0\n",
        "       \n",
        "        hash_list = []\n",
        "        for p_txt in sw:\n",
        "            gtext = self.__text_gen3(p_txt)\n",
        "            h = self.__text_hash(p_txt)\n",
        "            if h in hash_list:\n",
        "                pass\n",
        "            else:\n",
        "                hash_list.append(h)\n",
        "                text = (' '.join(gtext).strip())\n",
        "                loss, out=self.g_discriminator.transfer_learning([text],train_for = False)\n",
        "                grammar_rate = out[0,1].item()\n",
        "                #sim_score = self.s_discriminator.similarity(text,self.source.org_text_emb)\n",
        "                sim_rate = es.sentence_similarity(self.source.org_text,text)[0]    \n",
        "                comp_rate = 1 - len(text)/len(self.source.org_text)\n",
        "\n",
        "                #samples.append((text,out[0,1].item(),sim_score,comp_rate))\n",
        "                #score = out[0,1].item() + sim_score + comp_rate*2\n",
        "                score = grammar_rate/6 + sim_rate + (1- np.abs(0.5 - comp_rate))\n",
        "                if logout:\n",
        "                    print('g {:.4f} \\ts {:.4f} \\tc {:.4f}, score {:.4f}, [{}]'.format(grammar_rate,sim_rate,comp_rate,score,text))\n",
        "                if max_score < score and (comp_rate > 0.4 and comp_rate < 0.6):\n",
        "                    best_p_txt = p_txt\n",
        "                    max_score = score\n",
        "                    best_text = text\n",
        "                    best_grammar_rate = grammar_rate\n",
        "                    best_sim_rate = sim_rate\n",
        "                    best_comp_rate = comp_rate\n",
        "\n",
        "        if best_text.endswith('.'):\n",
        "            pass\n",
        "        else:\n",
        "            best_text += '.'\n",
        "            \n",
        "        if logout:\n",
        "            print(f'요약률 {best_comp_rate:.4f} 유사성 {best_sim_rate:.4f} 문법성 {best_grammar_rate:.4f} 요약 [{best_text}]')             \n",
        "        \n",
        "        correct_best_text = sentence_correct(' '.join(self.__text_gen5(best_p_txt)))\n",
        "        #return [best_text for i in range(count)], max_score\n",
        "        '''\n",
        "        correct_best_text = sentence_correct(' '.join(self.__text_gen5(best_p_txt))) #' '.join(self.__text_gen5(best_p_txt))\n",
        "        loss, out=self.g_discriminator.transfer_learning([best_text],train_for = False)\n",
        "        best_grammar_score = out[0,1].item()\n",
        "        loss, out=self.g_discriminator.transfer_learning([correct_best_text],train_for = False)\n",
        "        correct_best_grammar_score = out[0,1].item()\n",
        "        if best_grammar_score < 5.0 and correct_best_grammar_score > best_grammar_score:\n",
        "            if logout:\n",
        "                print('correct_grammar_score:{:.4f} best_grammar_score:{:.4f}'.format(correct_best_grammar_score,best_grammar_score))\n",
        "                print(best_text)\n",
        "                print(correct_best_text)\n",
        "            best_text = correct_best_text\n",
        "            best_grammar_score = correct_best_grammar_score\n",
        "        '''\n",
        "        return (best_text,correct_best_text), max_score, best_grammar_rate\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCdfO9iuLH6D"
      },
      "source": [
        "#5. Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YWDU3PbMo1T",
        "outputId": "05006772-def0-4a17-cfee-e46baf99aef9"
      },
      "source": [
        "txt = \"\"\"\n",
        "금융위는 폐업하는 37곳 중 거래 내역이 확인되는 14곳에 예치된 투자금은 지난 21일 기준 41억8000만원으로 추산한다. 지난 4월 2600억원을 웃돌았던 것과 비교하면 급감했다.\n",
        "\"\"\"\n",
        "\n",
        "source = Source(txt,txt)\n",
        "source.set_key_rate(s_discriminator)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "금융위는 폐업하는 37곳 중 거래 내역이 확인되는 14곳에 예치된 투자금은 지난 21일 기준 41억8000만원으로 추산한다. 지난 4월 2600억원을 웃돌았던 것과 비교하면 급감했다.\n",
            "--------------------------------------------------\n",
            "Length ------------------------------------| 22\n",
            "{0: -1.0, 1: -1.0, 2: -1.0, 3: 0.0, 4: 0.0, 5: -1.0, 6: -1.0, 7: -1.0, 8: -1.0, 9: -1.0, 10: -1.0, 11: 0.0, 12: 0.0, 13: -1.0, 14: -1.0, 15: -1.0, 16: 0.0, 17: -1.0, 18: -1.0, 19: 0.0, 20: 0.0, 21: 0.1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_eAwIPLb4aj"
      },
      "source": [
        "# sam_wgan4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhcoXuPMGy09"
      },
      "source": [
        "def sam_wgan4(full_text,text, epochs=50, batch_size=100,display=False, retry = True, retry_count = 0,comp_rate=1.0):\n",
        "    if retry_count > 30:\n",
        "        raise Exception(\"Can't summarize the text\")\n",
        "    if len(text) < 10:\n",
        "        return None\n",
        "    source = Source(full_text,text,delete_ending = False,attendtion_rate=atten_rate)\n",
        "    source.set_key_rate(s_discriminator)\n",
        "    summarizer = SAM_Summarizer(g_discriminator,s_discriminator)\n",
        "    summarizer.ready(source)\n",
        "    hist = summarizer.summarize(epochs,batch_size=2,learning_rate=5e-3,display=display,comp_rate=comp_rate)\n",
        "    if retry and hist == None and retry_count < 10:\n",
        "        print('\\n')\n",
        "        return sam_wgan4(full_text,text, epochs+10, batch_size,display=display,retry_count=retry_count+1)\n",
        "    (sum_text,corrected_text), max_score, best_grammar_rate = summarizer.get_samples(batch_size)\n",
        "    #print(samples)\n",
        "    \n",
        "    if retry and best_grammar_rate < (3.0 - retry_count*0.1):\n",
        "        if logout:\n",
        "            print('재시도 max score:{} grammar:{} text:{}'.format(max_score,best_grammar_rate,sum_text))\n",
        "        return sam_wgan4(full_text,text, epochs+10, batch_size,display=display,retry_count=retry_count+1)\n",
        "    \n",
        "    return (sum_text,corrected_text), max_score, best_grammar_rate"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6v9sEryZOLa"
      },
      "source": [
        "# Sentence Corrector (EncoderDecoderModel)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUe3ZCSIz8N8"
      },
      "source": [
        "from transformers import EncoderDecoderModel, BertTokenizerFast\n",
        "import torch\n",
        "\n",
        "pre_trained_kobert_model_name='kykim/bert-kor-base'\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained(pre_trained_kobert_model_name)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euuB9E5uZ1j2"
      },
      "source": [
        "from transformers import EncoderDecoderModel, BertTokenizerFast\n",
        "try:\n",
        "    del model\n",
        "    print('delete model')\n",
        "except Exception as ex:\n",
        "    pass\n",
        "model = EncoderDecoderModel.from_pretrained(\"/content/drive/MyDrive/GAN_ENDE/sentence_complete_model\")"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr_6N_CYaKAI"
      },
      "source": [
        "def sentence_correct(text):\n",
        "    text = text.strip().lower()\n",
        "    text = text.replace('!','')\n",
        "    text = text.replace('?','')\n",
        "    w = text.split(' ')\n",
        "    last_token = w[-1]\n",
        "    if last_token.endswith(('.')):\n",
        "        last_token = w[-1][:-1]\n",
        "\n",
        "    last_character = w[len(w)-1][:-1]\n",
        "    inputs = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "    input_ids = inputs.input_ids.to(device)\n",
        "    attention_mask = inputs.attention_mask.to(device)\n",
        "    '''\n",
        "    v = torch.sum(attention_mask[0]).item()\n",
        "    c = random.sample([i for i in range(v)],int(v/2))\n",
        "    print(c)\n",
        "    #input_ids[0][c] = 0\n",
        "    attention_mask[0][c] = 0 #random.random()\n",
        "    attention_mask[0][0] = 1\n",
        "    attention_mask[0][v-1] = 1\n",
        "    \n",
        "    print(input_ids)    \n",
        "    print(attention_mask)\n",
        "    '''\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    outputs = model.generate(input_ids, attention_mask=attention_mask).cpu().detach().numpy()[0]\n",
        "    o=[]\n",
        "    for token in outputs:\n",
        "        if token == tokenizer.pad_token_id:\n",
        "            break\n",
        "        o.append(token)\n",
        "    output_str = tokenizer.batch_decode([o], skip_special_tokens=True)[0]\n",
        "    output_str = output_str.replace(' ’ ',\"'\") #' - '\n",
        "    output_str = output_str.replace(' - ',\"-\") #' - '\n",
        "    #if logout:\n",
        "    #    print('raw:',output_str)\n",
        "    vb = [output_str.find('.'),output_str.find('?'),output_str.find('!')]\n",
        "    for v in range(len(vb)):\n",
        "        vb[v] = vb[v] if vb[v] >= 0 else 1000\n",
        "    eos = np.min(vb)\n",
        "    real_eos =  eos\n",
        "    if len(last_token) >= 3:\n",
        "        eos2 = output_str.find(last_token) \n",
        "        if eos2 > 0 and eos2 < eos:\n",
        "            real_eos = eos2 + len(last_token) \n",
        "            tmp = output_str[0:real_eos] + '.'\n",
        "            # 적어도 5어절 이상은 되어야 인정해 준다.\n",
        "            if len(tmp.split(' ')) > 5:\n",
        "                output_str = tmp              \n",
        "    else:\n",
        "        output_str = output_str[0:real_eos] + '.'\n",
        "\n",
        "    if output_str.endswith('.'):\n",
        "        pass\n",
        "    else:\n",
        "        output_str += '.'\n",
        "    return output_str"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "GkdOT4m6Jlx9",
        "outputId": "7e2a0475-5f0e-47a4-c400-6fe882e2be79"
      },
      "source": [
        "txt = '아기가 태어났 아버지는 홀로 걱정되었 새어머니를 맞이했 새어머니와 언니들은 성질이 아주 심술쟁이들이었 이번에는 아버지마저 돌아가셨 왕궁에서 무도회가 열렸다.'\n",
        "#txt = 'Cynthia human factors researcher Federal Aviation that it tests on how quickly passengers can leave a plane two seats from a seat on the seat behind it is the pitch crowding to more serious issues than fighting space in crashing elbows seat.'\n",
        "#txt = 'spin-off the E!'\n",
        "sentence_correct(txt)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'아기가 태어났고, 아버지는 홀로 살다가 걱정되었던 새어머니를 맞이했는데, 그 때 새어 어머니와 언니들은 성질이 아주 급한 심술쟁이들이었고, 이번에는 아버지마저 돌아가셨고, 결국 왕궁에서 무도회가 열렸다.'"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LKIH0nG31yx"
      },
      "source": [
        "\n",
        "def similarity3(full_text,org_text):\n",
        "    sentences = nltk.sent_tokenize(full_text)\n",
        "    #print(\"Num sentences:\", len(sentences))\n",
        "    querys = nltk.sent_tokenize(org_text)\n",
        "    #print(\"Num querys:\", len(querys))\n",
        "\n",
        "    #Compute the sentence embeddings\n",
        "    org_embeddings = s_discriminator._embedder.encode(sentences,show_progress_bar=False)\n",
        "    query_embeddings = s_discriminator._embedder.encode(querys,show_progress_bar=False)\n",
        "\n",
        "    #Compute the pair-wise cosine similarities\n",
        "    cos_scores = scipy.spatial.distance.cdist(query_embeddings, org_embeddings, \"cosine\")\n",
        "    similarity_score = 1.0 - np.mean(np.mean(cos_scores,axis=0))\n",
        "\n",
        "    return similarity_score"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1L5pLIoV-62"
      },
      "source": [
        "def grammar3(full_text,org_text):\n",
        "    querys = nltk.sent_tokenize(org_text)\n",
        "    g = []\n",
        "    for txt in querys:\n",
        "        loss, out=g_discriminator.transfer_learning([txt],train_for = False)\n",
        "        g.append(out[0,1].item())\n",
        "    return np.tanh(np.mean(g))"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VadbdJnzL8Lq"
      },
      "source": [
        "# 실험 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFJ5v8fia6VD"
      },
      "source": [
        "def summary(ft,text,steps=4,top_rank=2,comp_rate=1.0):\n",
        "    org_sentences = np.array(nltk.sent_tokenize(text.strip()))\n",
        "    summary_text = []\n",
        "    corrected_summary_text = []\n",
        "    g = []\n",
        "    sm = []\n",
        "    for i in range(0,len(org_sentences),steps):\n",
        "        txt = ''\n",
        "        cnt = 0\n",
        "        for s in range(i,i+steps):\n",
        "            if s < len(org_sentences):\n",
        "                txt +=  ' ' + org_sentences[s]\n",
        "                cnt +=1\n",
        "        #print(cnt,top_rank)\n",
        "        txt = txt.strip()\n",
        "        if cnt > top_rank:\n",
        "            txt = besm2(ft,txt,top_rank=top_rank)\n",
        "\n",
        "        (t,ct),score, grammar = sam_wgan4(ft,txt.strip(),epochs=300,display=logout,comp_rate=comp_rate)\n",
        "        if logout:\n",
        "            print('-'*50)\n",
        "            print(t,score,grammar)\n",
        "        #t = sentence_correct(t)\n",
        "        #print(t)\n",
        "        summary_text.append(t)\n",
        "        corrected_summary_text.append(ct)\n",
        "        g.append(grammar)\n",
        "        sm.append(similarity3(ft,t))\n",
        "\n",
        "    return (' '.join(summary_text).strip(),' '.join(corrected_summary_text).strip()),np.tanh(np.mean(g)),np.mean(sm)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irjx-TOnPBL4"
      },
      "source": [
        "## Main 실험"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "lx1wrLCQBSdp",
        "outputId": "afbcf198-298a-42d0-abd2-d7f26808aed8"
      },
      "source": [
        "full_text = get_prepared_doc(sentences_dataset[0])\n",
        "full_text"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'구리·남양주시가 타 시·군과 차별화된 미세먼지 저감 대책을 세웠다. 7일 구리시에 따르면 2022년까지 미세먼지 농도를 현재 24㎍/㎥에서 19㎍/㎥로 낮추겠다는 목표로 단기 및 중장기 대책을 마련했다. 먼저 버스정류장 저감 시스템을 설치한다. 시는 한국철도기술연구원과 함께 버스 정류장에 사물인터넷(IoT)을 기반으로 한 미세먼지 집진 모듈을 설치하는 기술을 개발하고 있다. 올해부터 3년간 총 10억원의 예산을 들여 관내 버스 중앙 차로 버스정류장에 시스템을 설치할 계획이다. 지난해 말부터는 미세먼지 취약계층 이용시설에 휴대용 미세먼지 측정기를 보급하고 있다. 올해까지 시립어린이집 12개소와 지역아동센터 15개소에 미세먼지 측정기 설치를 완료할 예정이다. 또한 미세먼지 농도에 따라 구리타워의 조명 색상을 파랑(좋음), 녹색(보통), 노랑(나쁨), 빨강(매우 나쁨) 등으로 바꿔 시민들이 대기질을 즉시 알 수 있도록 했다. 시 관계자는 \"현재 관내 미세먼지 취약계층이 상주하는 경로당, 어린이집 등 시설에 공기청정기 898개를 보급하고, 미세먼지 저감을 위한 계획과 실행 방법 등을 담은 소통형 스마트폰 앱(App)을 개발하는 등 미세먼지 대응에 총력을 다하고 있다\"고 말했다. 남양주시 역시 미세먼지 측정망 운영 등 미세먼지 저감 10대 중점과제 등 맞춤형 미세먼지 종합대책을 마련해 시행 중이다. 특히 미세먼지 피해가 큰 어린이·노인 등 민감 계층에게 나쁨 단계부터 미세먼지 예보를 문자로 알리는 맞춤형 예·경보제가 주목을 받고 있다. 찾아가는 미세먼지 시민강좌 역시 호평 속에 운영 중이다. 연간 300회 실시되는 이 강좌는 10인 이상 신청 단체에게 미세먼지 위해성과 고농도 미세먼지 발생시 행동요령 등을 알려준다. 또한 교육시설에는 미세먼지 알림 시스템을 구축해 운영하고 있다. 관내 어린이집, 유치원, 초중학교 등 교육시설 700개소에 남양주시 자체 대기오염 측정소에서 측정된 해당 지역의 실시간 미세먼지 정보를 제공하고 있다. 시 관계자는 \"미세먼지 저감은 단기적인 계획이나 한두 가지 정책만으로 효과를 볼 수 없다\"며 \"종합적인 대책 마련은 물론이고 시민들의 피부에 와 닿는 정책을 마련하기 위해 애쓰고 있다\"고 말했다.'"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FstAHWGQ8KR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6a9d96a4-a6e5-4200-b1dc-94093c959e3c"
      },
      "source": [
        "logout = True\n",
        "atten_rate = 0.2\n",
        "similarity = 1.0\n",
        "std_factor = 3.0\n",
        "txt = \"\"\"\n",
        "특히 미세먼지 피해가 큰 어린이·노인 등 민감 계층에게 나쁨 단계부터 미세먼지 예보를 문자로 알리는 맞춤형 예·경보제가 주목을 받고 있다. 찾아가는 미세먼지 시민강좌 역시 호평 속에 운영 중이다. 연간 300회 실시되는 이 강좌는 10인 이상 신청 단체에게 미세먼지 위해성과 고농도 미세먼지 발생시 행동요령 등을 알려준다.\n",
        "\"\"\"\n",
        "#The diocese says he contracted the infection through contaminated food while attending a conference for newly ordained bishops in Italy last month. Symptoms of hepatitis A include fever, tiredness, loss of appetite, nausea and abdominal discomfort. Fargo Catholic Diocese in North Dakota (pictured) is where the bishop is located.\n",
        "#The judge agreed with police that he would have been over the limit at the time his red Citroen hit Miss Titley’s blue Daihatsu Cuore on a road near Yarmouth, Isle of Wight, on October 11, 2013. His phone records showed he was also texting around the time of the crash.\n",
        "\n",
        "sam_wgan4(get_prepared_doc(sentences_dataset[0]),txt,epochs=300,display= True,retry = False,comp_rate= 1.5)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "특히 미세먼지 피해가 큰 어린이·노인 등 민감 계층에게 나쁨 단계부터 미세먼지 예보를 문자로 알리는 맞춤형 예·경보제가 주목을 받고 있다. 찾아가는 미세먼지 시민강좌 역시 호평 속에 운영 중이다. 연간 300회 실시되는 이 강좌는 10인 이상 신청 단체에게 미세먼지 위해성과 고농도 미세먼지 발생시 행동요령 등을 알려준다.\n",
            "--------------------------------------------------\n",
            "Length ------------------------------------| 44\n",
            "{0: -1.0, 1: -1.0, 2: 0.0, 3: 0.0, 4: -1.0, 5: 0.0, 6: -1.0, 7: -1.0, 8: -1.0, 9: -1.0, 10: -1.0, 11: -1.0, 12: -1.0, 13: 0.0, 14: -1.0, 15: -1.0, 16: 0.0, 17: -1.0, 18: -1.0, 19: -1.0, 20: -1.0, 21: -1.0, 22: -1.0, 23: -1.0, 24: 0.0, 25: 0.0, 26: -1.0, 27: -1.0, 28: -1.0, 29: -1.0, 30: -1.0, 31: -1.0, 32: -1.0, 33: 0.0, 34: 0.0, 35: -1.0, 36: -1.0, 37: -1.0, 38: -1.0, 39: -1.0, 40: -1.0, 41: -1.0, 42: -1.0, 43: 0.1}\n",
            "Train... |||||||||||||||||||||| 100.0%   300/300 epochs, gl:-0.29230765  sl:-0.0800 ll:-0.3243\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAssAAAFlCAYAAAAd9qXYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeVyU5fr48c8zCzPsICLuouaCMAiKW+RuLnncso6mlti30/Y72qq2Z2UnKzst51hWZraYS2UuaXXMNJfUDAR3UZQdlXVgYPZ5fn/MMIGAgoKI3e/Xq1cwz3bPgMM113Pd1y3JsowgCIIgCIIgCFUpGnsAgiAIgiAIgnC9EsGyIAiCIAiCINRABMuCIAiCIAiCUAMRLAuCIAiCIAhCDUSwLAiCIAiCIAg1EMGyIAiCIAiCINRA1dgDqEnz5s3l0NDQxh6GIAiCIAiCcIOLj4/Pk2U5uLpt122wHBoayh9//NHYwxAEQRAEQRBucJIkpdW0TZRhCIIgCIIgCEINRLAsCIIgCIIgCDUQwbIgCIIgCIIg1OC6rVmujtVqJTMzE5PJ1NhDEYQ602q1tG3bFrVa3dhDEQRBEAShlppUsJyZmYmvry+hoaFIktTYwxGEWpNlmfz8fDIzM+nYsWNjD0cQBEEQhFpqUmUYJpOJoKAgESgLTY4kSQQFBYm7IoIgCILQxDSpYBkQgbLQZInfXUEQBEFoeppcsCxcvSFDhoge1oIgCIIgCLUgguUGYLPZGnsIbrIs43A4GnsYgiAIgiAITZIIluvolVdeoVu3btxyyy3cddddLF68GHBmax999FFiYmJ499132bRpE/369SM6OpoRI0Zw/vx5ABYsWMDMmTMZOHAgHTp0YN26dcybNw+dTsfo0aOxWq2AcwXDp59+mqioKGJiYkhISGDUqFF07tyZpUuXAmAwGBg+fDi9evVCp9OxYcMGAFJTU+nWrRv33HMPERERZGRk1Ph8Vq1ahU6nIyIigvnz5wNgt9uJi4sjIiICnU7H22+/DcB7771Hjx49iIyMZOrUqQ3zAguCIAiCIFxHmlQ3jIpe2nSUY9nF9XrOHq39eHFceI3bDxw4wLfffktSUhJWq5VevXrRu3dv93aLxeIubygsLGTfvn1IksSyZct44403eOuttwBISUlh+/btHDt2jAEDBvDtt9/yxhtvMGnSJDZv3szEiRMBaN++PYmJiTz22GPExcWxZ88eTCYTERERPPjgg2i1Wr777jv8/PzIy8ujf//+jB8/HoBTp07x2Wef0b9//xqfT3Z2NvPnzyc+Pp7AwEBGjhzJ+vXradeuHVlZWRw5cgSAoqIiABYtWsTZs2fRaDTuxwRBEARBEG5kTTZYbgx79uxhwoQJaLVatFot48aNq7R9ypQp7q8zMzOZMmUKOTk5WCyWSu3CxowZg1qtRqfTYbfbGT16NAA6nY7U1FT3fuWBr06nw2Aw4Ovri6+vrztY9fb25plnnmHnzp0oFAqysrLcGewOHTpcMlAGZ/A/ZMgQgoODAZg+fTo7d+7k+eef58yZM8yePZuxY8cycuRIACIjI5k+fToTJ050B/TCtWc9fx6FtzdKH5/GHoogCIIg3PCabLB8qQxwY/H29nZ/PXv2bB5//HHGjx/Pjh07WLBggXubRqMBQKFQoFar3V0SFApFpXrnivuVf11xv5UrV5Kbm0t8fDxqtZrQ0FB3a7KKY6mrwMBAkpKS+Omnn1i6dClr165l+fLlbN68mZ07d7Jp0yZeffVVDh8+jErVZH+Fmqz0e/8Pn1tiCXn66cYeiiAIgiDc8ETNch3ExsayadMmTCYTBoOB77//vsZ99Xo9bdq0AeCzzz5rkPHo9XpatGiBWq1m+/btpKWl1en4vn378uuvv5KXl4fdbmfVqlUMHjyYvLw8HA4HkydPZuHChSQkJOBwOMjIyGDo0KG8/vrr6PV6DAZDgzwv4dLs+fnY8gsaexiCIAiC8Jcg0oJ10KdPH8aPH09kZCQhISHodDr8/f2r3XfBggXceeedBAYGMmzYMM6ePVvv45k+fTrjxo1Dp9MRExND9+7d63R8q1atWLRoEUOHDkWWZcaOHcuECRNISkpi1qxZ7i4ar732Gna7nRkzZqDX65FlmTlz5hAQEFDvz0m4PNliQbZYGnsYgiAIgvCXIMmy3NhjqFZMTIx8cS/g48ePExYW1kgjcjIYDPj4+FBWVsagQYP46KOP6NWrV6OOSWg66uN3+LguEp/YWNot/aCeRiUIgiAIf22SJMXLshxT3TaRWa6j+++/n2PHjmEymZg5c6YIlIVrSnY4wGoVmWVBEARBuEZEsFxHX331VWMPQfgLKw+SHRZzI49EEARBEP4axAQ/QWhCyoNl2WJt5JEIgiAIwl+DCJYFoQmRzc6MsijDEARBEIRrQwTLgtCEuDPLZlGGIQiCIAjXggiWBaEJcbjLMERmWRAEQRCuBREs/4VkZ2dzxx131Mu5hgwZwsWt/YSGJ4tgWRAEQRCuKREsNxEVl8G+Uq1bt+abb76ph9EIjeXPbhgiWBYEQRCEa0EEy3X0+eefExkZSc+ePbn77rsBSE1NZdiwYURGRjJ8+HDS09MBiIuL46GHHqJ///506tSJHTt2cO+99xIWFkZcXJz7nD4+Pjz22GOEh4czfPhwcnNzAWf29tFHHyUmJoZ3332X+Ph4Bg8eTO/evRk1ahQ5OTkAvPfee/To0YPIyEimTp0KwK+//kpUVBRRUVFER0dTUlJCamoqERERAJhMJmbNmoVOpyM6Oprt27cDsGLFCm6//XZGjx5Nly5dmDdv3mVfk1WrVqHT6YiIiGD+/PkA2O124uLiiIiIQKfT8fbbb9c4VqH2RGZZEARBEK6tpttn+Yen4Nzh+j1nSx2MWVTj5qNHj7Jw4UJ+++03mjdvTkFBAQCzZ89m5syZzJw5k+XLlzNnzhzWr18PQGFhIXv37mXjxo2MHz+ePXv2sGzZMvr06UNiYiJRUVGUlpYSExPD22+/zcsvv8xLL73Ef//7XwAsFgt//PEHVquVwYMHs2HDBoKDg1mzZg3PPvssy5cvZ9GiRZw9exaNRkNRUREAixcvZsmSJcTGxmIwGNBqtZWey5IlS5AkicOHD3PixAlGjhxJcnIyAImJiRw8eBCNRkO3bt2YPXs27dq1q/Y1yc7OZv78+cTHxxMYGMjIkSNZv3497dq1IysriyNHjgC4x1XdWIXaE90wBEEQBOHaqpfMsiRJoyVJOilJ0mlJkp6qZvvjkiQdkyTpkCRJ2yRJ6lAf173WfvnlF+68806aN28OQLNmzQDYu3cv06ZNA+Duu+9m9+7d7mPGjRuHJEnodDpCQkLQ6XQoFArCw8NJTU0FQKFQMGXKFABmzJhR6fjyx0+ePMmRI0e49dZbiYqKYuHChWRmZgIQGRnJ9OnT+fLLL1GpnJ9/YmNjefzxx3nvvfcoKipyP15u9+7dzJgxA4Du3bvToUMHd7A8fPhw/P390Wq19OjRg7S0tBpfkwMHDjBkyBCCg4NRqVRMnz6dnTt30qlTJ86cOcPs2bP58ccf8fPzq3GsQu25yy/sduR6KM0RBEEQBOHSrjpakSRJCSwBbgUygQOSJG2UZflYhd0OAjGyLJdJkvQQ8AYw5aoufIkM8PVEo9EAzoC4/Ovy72uqQ5Ykyf21t7c3ALIsEx4ezt69e6vsv3nzZnbu3MmmTZt49dVXOXz4ME899RRjx45ly5YtxMbG8tNPP1XJLl9uzABKpfKK6qUDAwNJSkrip59+YunSpaxdu5bly5dXO1YRNNdexYyybLEgiddOEARBEBpUfWSW+wKnZVk+I8uyBVgNTKi4gyzL22VZLnN9uw9oWw/XveaGDRvG119/TX5+PoC7DOPmm29m9erVAKxcuZKBAwfW6bwOh8M98e6rr77illtuqbJPt27dyM3NdQfLVquVo0eP4nA4yMjIYOjQobz++uvo9XoMBgMpKSnodDrmz59Pnz59OHHiRKXzDRw4kJUrVwKQnJxMeno63bp1q9O4Afr27cuvv/5KXl4edrudVatWMXjwYPLy8nA4HEyePJmFCxeSkJBQ41iF2qu4cp8oxRAEQRCEhlcfaak2QEaF7zOBfpfY//+AH+rhutdceHg4zz77LIMHD0apVBIdHc2KFSv4z3/+w6xZs3jzzTcJDg7m008/rdN5vb29+f3331m4cCEtWrRgzZo1Vfbx8PDgm2++Yc6cOej1emw2G48++ihdu3ZlxowZ6PV6ZFlmzpw5BAQE8Pzzz7N9+3Z3yceYMWPcEwIBHn74YR566CF0Oh0qlYoVK1ZUyijXVqtWrVi0aBFDhw5FlmXGjh3LhAkTSEpKYtasWTgcDgBee+017HZ7tWMVaq/iYiQOiwVlI45FEARBEP4KJFmWr+4EknQHMFqW5ftc398N9JNl+Z/V7DsD+CcwWJblKkuQSZJ0P3A/QPv27XtfXCt7/PhxwsLCrmq81yMfHx+RYf2LuNrf4cLVqzm34CUAOv+8FY+2VW/S5P53CTgcBM+ZfcXXEQRBEIS/EkmS4mVZjqluW32UYWQBFVsltHU9dvEgRgDPAuOrC5QBZFn+SJblGFmWY4KDg+thaIJwY7m4Zrk6hu3bMezada2GJAiCIAg3tPoIlg8AXSRJ6ihJkgcwFdhYcQdJkqKBD3EGyhfq4Zo3FJFVFmrLUaEMo6Zg2VZYgMNYVu02QRAEQRDq5qprlmVZtkmS9E/gJ0AJLJdl+agkSS8Df8iyvBF4E/ABvnZ1ekiXZXn81V5bEP5qapNZtheK/tWCIAiCUF/qpe+ULMtbgC0XPfZCha9H1Md1BOGvrlI3DHPVaiaH0YhsNCJfwWRNQRAEQRCqEstdC0ITcnE3jIvZCwud28pEGYYgCIIg1AcRLAtCE3K5MgybK1iWLRaxwp8gCIIg1AMRLN/gNm7cyKJFdVvt8LbbbqOoyFn36uPjU+drlh9fVFTE+++/X6djU1NTiYiIqPM1/ypka4Vg2VxdZvnPemWH0XhNxiQIgiAINzIRLDeiK1lGuq7Gjx/PU089VadjtmzZckWLhciyjMPhcB9/JcGycGmVumFYqwuWC/7ct0wEy4IgCIJwtUSwXAepqamEhYXxj3/8g/DwcEaOHImxhuzdK6+8Qrdu3bjlllu46667WLx4MQBDhgzh0UcfJSYmhnfffZdNmzbRr18/oqOjGTFiBOfPnwdgwYIFzJw5k4EDB9KhQwfWrVvHvHnz0Ol0jB49GqvVWuWa7733Hj169CAyMpKpU6cCsGLFCv75T+f6MHFxcTz00EP079+fTp06sWPHDu69917CwsKIi4tznyc0NJS8vLxK5zYYDAwfPpxevXqh0+nYsGGD+zXp1q0b99xzDxEREWRkZLiPf+qpp0hJSSEqKoq5c+dyzz33sH79evc5p0+f7j5PdUwmE7NmzUKn0xEdHc327dsBOHr0KH379iUqKorIyEhOnTpFaWkpY8eOpWfPnkRERFS7CuKNQLZYMSk9XF/XXLMMIIv2cYIgCIJw1eqlG0ZjeP331zlRcKJez9m9WXfm951/yX1OnTrFqlWr+Pjjj/n73//Ot99+y4wZMyrtc+DAAb799luSkpKwWq306tWL3r17u7dbLBb++OMPAAoLC9m3bx+SJLFs2TLeeOMN3nrrLQBSUlLYvn07x44dY8CAAXz77be88cYbTJo0ic2bNzNx4sRK1120aBFnz55Fo9G4yyguVlhYyN69e9m4cSPjx49nz549LFu2jD59+pCYmEhUVFS1x2m1Wr777jv8/PzIy8ujf//+jB8/3v2afPbZZ/Tv37/KeI4cOUJiYiIAv/76K2+//TYTJ05Er9fz22+/8dlnn9X4Wi9ZsgRJkjh8+DAnTpxg5MiRJCcns3TpUh555BGmT5+OxWLBbrezZcsWWrduzebNmwHQ6/U1nrcpky0WytRatHZLpSxzOVuFYFlM8hMEQRCEqycyy3XUsWNHd0DZu3dvUlNTq+yzZ88eJkyYgFarxdfXl3HjxlXaPmXKFPfXmZmZjBo1Cp1Ox5tvvsnRo0fd28aMGYNarUan02G32xk9ejQAOp2u2utGRkYyffp0vvzyS1Sq6j8HjRs3DkmS0Ol0hISEoNPpUCgUhIeHV3vOcrIs88wzzxAZGcmIESPIyspyZ8E7dOhQJVCuzuDBgzl16hS5ubmsWrWKyZMn1zhOgN27d7s/iHTv3p0OHTqQnJzMgAED+Ne//sXrr79OWloanp6e6HQ6tm7dyvz589m1axf+/v6XHU9TJJvNlKmcbeGqzSwXVAiWRc2yIAiCIFy1JptZvlwGuKFoKvSvVSqVGI1GMjIy3AHxgw8+eNlzeHt7u7+ePXs2jz/+OOPHj2fHjh0sWLCgyrUUCgVqtRrXgi4oFIpq6503b97Mzp072bRpE6+++iqHDx+ucfwKhaLSc6npnOVWrlxJbm4u8fHxqNVqQkNDMZlMVZ7P5dxzzz18+eWXrF69mk8//bTWx1U0bdo0+vXrx+bNm7ntttv48MMPGTZsGAkJCWzZsoXnnnuO4cOH88ILL1z+ZE2MbLFQqta6vq5aimMXmWVBEARBqFcis1wP2rVrR2JiIomJiTz44IPExsayadMmTCYTBoOB77//vsZj9Xo9bdq0AbhkScLlOBwOMjIyGDp0KK+//jp6vb5el9HW6/W0aNECtVrN9u3bSUtLu+wxvr6+lJSUVHosLi6Od955B4AePXpc8viBAweycuVKAJKTk0lPT6dbt26cOXOGTp06MWfOHCZMmMChQ4fIzs7Gy8uLGTNmMHfuXBISEq7wmV7fHBYLparyYLn6mmWFK6suJvgJgiAIwtVrspnl61mfPn0YP348kZGR7lKHmsoCFixYwJ133klgYCDDhg3j7NmzV3RNu93OjBkz0Ov1yLLMnDlzrqijRU2mT5/OuHHj0Ol0xMTE0L1798seExQURGxsLBEREYwZM4Y333yTkJAQwsLCqtRbV+fhhx/moYceQqfToVKpWLFiBRqNhrVr1/LFF1+gVqtp2bIlzzzzDAcOHGDu3LnuLPwHH3xQH0/7uiObzVgVKmwKZbUr+NkKC1C3bo1ZrxeZZUEQBEGoB5Isy409hmrFxMTI5ZPgyh0/fpywsLBGGlHdGAwGfHx8KCsrY9CgQXz00Uf06tWrsYfV6MrKytDpdCQkJNywdcWXcrW/wym3jWWbyYe+uSdpNW0qIU9XbguYHHsLntFRGH7eRsgLz9Ns2rSrHbIgCIIg3PAkSYqXZTmmum2iDKOB3H///URFRdGrVy8mT54sAmXg559/JiwsjNmzZ/8lA+X64LBYsSpUWJXqKn2WZYcDe1ERHq6yHllklgVBEAThqokyjAby1VdfNfYQrjsjRoyoVa2zUDPZYsaqVWFTqqq0jnMUF4PdjqplK+f3omZZEAShSZBtNkr37sU7NhZJIfKY1xvxExGEJkS2WLAoncHyxd0wynssq4KaIXl5iZplQRCEJsKwezcZ/7ifoht0Qa2mTgTLgtCEyBaLswxDoarSDcNe6FyIRhkYiMLTU/RZFgRBaCIsrnUOct99D/sNuqhWUyaCZUFoQsozy1aluko3DHuRM7OsDGyGQmSWBUG4zskOB/aL2ov+VVkzs5DUaux6PXnvv9/YwxEuIoJlQWgiZIcDbDZX67hqMssFBQCoAgNcmWURLAvXF/Pp01gyMhp7GMJ1omjtWk4PHeYuIfsrs2Zk4NG5MwF33EHByq+wZGY29pCECkSwXA/eeecdyuoxixcaGkpeXt4VH79ixQr++c9/Nuh1br755ktuLyoq4v0Kn46zs7O54447ruhatbVr1y7Cw8OJiorCeFEJgo+PT4Ne+1ooD46d3TCqBsvlf3CUgYEovLyQxQQ/4TqTNXce5155pbGHIVwnTEeP4jAYKN6ypbGH0ugsWZl4tGtL4N0zwGbDePBgYw9JqEAEy/WgvoPlurLb7df8mr/99tslt18cLLdu3ZpvvvmmQce0cuVKnn76aRITE/H09GzQazWG8rILq0KFRaHCYa1asyxptSi8vFB4eYoyDOG6IssyltRULKIjjuBiSXfeZdCv39DII2lcsixjzcxC3aYtmtBQUCoxn05p7GEJFYhguQ5KS0sZO3YsPXv2JCIigjVr1vDee++RnZ3N0KFDGTp0KAAPPfQQMTExhIeH8+KLL7qPDw0N5cUXX6RXr17odDpOnDgBQH5+PiNHjiQ8PJz77ruPigvFTJw4kd69exMeHs5HH33kftzHx4cnnniCnj17snfvXj799FO6du1K37592bNnT7Xjv9R1vvzyS/r27UtUVBQPPPAAdrudpUuXMnfuXPc+FTPW5Zlag8HA8OHD3c9pwwbnm95TTz1FSkoKUVFRzJ07l9TUVCIiIgAwmUzMmjULnU5HdHQ027dvd5//9ttvZ/To0XTp0oV58+ZV+zy2bdtGdHQ0Op2Oe++9F7PZzLJly1i7di3PP/8806dPr/FnKMsyc+fOJSIiAp1OxxrXzOOcnBwGDRpEVFQUERER7Nq1C7vdTlxcnHvft99+G4CUlBRGjx5N7969GThwoPvn+PXXXxMREUHPnj0ZNGhQjWO4Uu7MstI1wc9ctQxDGRgI4OyGISb4XXOy3U72/PkYDx9p7KFcd+wFBchGI9bsHORG+IAvXJosy6TNjEO/adM1u6YlIx3JwwPT4cOYU67P4LAsIYGyhIbN8trz8pBNJtTt2iJ5eODRvj2WM43/esgOB7LN1tjDuC402T7L5/71L8zHT9TrOTVh3Wn5zDM1bv/xxx9p3bo1mzdvBkCv1+Pv78+///1vtm/fTvPmzQF49dVXadasGXa7neHDh3Po0CEiIyMBaN68OQkJCbz//vssXryYZcuW8dJLL3HLLbfwwgsvsHnzZj755BP3NZcvX06zZs0wGo306dOHyZMnExQURGlpKf369eOtt94iJyeHadOmER8fj7+/P0OHDiU6OrrK+Gu6zvHjx1mzZg179uxBrVbz8MMPs3LlSiZPnsyAAQN48803AVizZg3PPvtspXNqtVq+++47/Pz8yMvLo3///owfP55FixZx5MgREhMTAUh1zfQFWLJkCZIkcfjwYU6cOMHIkSNJTk4GIDExkYMHD6LRaOjWrRuzZ8+mXbt27mNNJhNxcXFs27aNrl27cs899/DBBx/w6KOPsnv3bv72t79dstxj3bp1JCYmkpSURF5eHn369GHQoEF89dVXjBo1imeffRa73U5ZWRmJiYlkZWVx5Igz8CkqcnabuP/++1m6dCldunRh//79PPzww/zyyy+8/PLL/PTTT7Rp08a9b32qVIahUCJbSitttxcWogx0LnGu8BQT/BqDNSMD/YaNqFqE4KmLaOzhXFes5TWYViu2CxdQt2rVuAMSKrFmZFC2fz9Kf3/8x41r8Os5LBZsOecImDqForVfo1+/nhZPPNHg162rcwteAqDTxobLfpfXJ3u0bev8f+dOmFPONNj1auv8okUYE5PouFa0sxOZ5TrQ6XRs3bqV+fPns2vXrhpXoVu7di29evUiOjqao0ePcuzYMfe222+/HYDevXu7A8idO3cyY8YMAMaOHUugKzsI8N5779GzZ0/69+9PRkYGp06dAkCpVDJ58mQA9u/fz5AhQwgODsbDw4MpU6ZUO66arrNt2zbi4+Pp06cPUVFRbNu2jTNnzhAcHEynTp3Yt28f+fn5nDhxgtjY2ErnlGWZZ555hsjISEaMGEFWVhbnz5+/5Ou4e/du9zi6d+9Ohw4d3MHy8OHD8ff3R6vV0qNHjyqLmJw8eZKOHTvStWtXAGbOnMnOnTsveb2Lr33XXXehVCoJCQlh8ODBHDhwgD59+vDpp5+yYMECDh8+jK+vL506deLMmTPMnj2bH3/8ET8/PwwGA7/99ht33nmnOwufk5MDQGxsLHFxcXz88ccNUhrjcGWSLa4yjCo1y0WFqAKbAYjWcY2kvMRATGKrquKEJesNPHnJdDLZ3QasKTEeOgyA6WT9JqFqYs3MBFnGKyoKn4ED0W/YWOWOgyUzi8zHHsNuKK3hLA3LYTRiPn0ac0oKDpOpwa5T/u9B7QqWNZ06Y0lPr/Ief62V7tqN6dAhbLm5jTqO60GTzSxfKgPcULp27UpCQgJbtmzhueeeY/jw4bzwwguV9jl79iyLFy/mwIEDBAYGEhcXh6nCPzKNRgM4g13bZW5v7Nixg59//pm9e/fi5eXFkCFD3OfSarUolcp6eV6yLDNz5kxee+21KtumTp3K2rVr6d69O5MmTUKSpErbV65cSW5uLvHx8ajVakJDQys937oqf32gdq9RfRk0aBA7d+5k8+bNxMXF8fjjj3PPPfeQlJTETz/9xNKlS1m7di3vvPMOAQEB7ox5RUuXLmX//v1s3ryZ3r17Ex8fT1BQUL2NsXx56/Ka5Sqt4woK8WjXHkC0jmsk5cGyNT29kUdy/bFmZrm/tmRk4tWnT72d25afT8n//kfA1KlV3qOutcw5s1EFBxP65ZeNOo66Mh0+BIA1LR27oRSlj3eDXs/i+jeibt8e31GjMOzYgeXMGTRdurj3Kdm6lZIffiRg4kR8Bg9u0PFUx3TiBDgcAJhPnMAzKqpBruMOltu0AUBzU2ew2bCkp6O56aYGuebl2PV6LGfPAlAWH4/f6NGNMo7rhcgs10F2djZeXl7MmDGDuXPnkpCQAICvry8lrl6RxcXFeHt74+/vz/nz5/nhhx8ue97yMgCAH374gUJXVwO9Xk9gYCBeXl6cOHGCffv2VXt8v379+PXXX8nPz8dqtfL111/X6TrDhw/nm2++4cKFCwAUFBS4M7qTJk1iw4YNrFq1iqlTp1Y5p16vp0WLFqjVarZv3+4+ruJrcrGBAweycuVKAJKTk0lPT6dbt26XfZ0AunXrRmpqKqdPnwbgiy++YHAd3kQHDhzImjVrsNvt5ObmsnPnTvr27UtaWhohISH84x//4L777iMhIYG8vDwcDgeTJ09m4cKFJCQk4OfnR8eOHd2vsSzLJCUlAc5a5n79+vHyyy8THBxMRj1nF6vULFdZlKTQXbOs8PJENl++Fc0AACAASURBVBqd7eaEa6Y8o2hJT680J0AAa1YWCn9/UCjqPbNctHYt5156GYvrfaGxWM+dw5qWjinpUJXl6K93xkOHQa0GwJx8sk7Hmk4mOwPLOrC6Jvd5tG+PZ8/IP8dQ8byuu7KmCndnryXT0T+vazx6tMGuY8nMRBUcjEKrBcCjU2eARi3FqPizKDvwR52OlWUZewOUIjYmESzXweHDh92T4F566SWee+45wFnDOnr0aIYOHUrPnj2Jjo6me/fuTJs2rUrZQnVefPFFdu7cSXh4OOvWraN9e2d2cPTo0dhsNsLCwnjqqafo379/tce3atWKBQsWMGDAAGJjYwkLC6vTdXr06MHChQsZOXIkkZGR3Hrrre7SgsDAQMLCwkhLS6Nv375Vzjl9+nT++OMPdDodn3/+Od27dwcgKCiI2NhYIiIiKk0SBHj44YdxOBzodDqmTJnCihUrKmWUL0Wr1fLpp59y5513otPpUCgUPPjgg7U6FpzBf2RkJD179mTYsGG88cYbtGzZkh07drh/dmvWrOGRRx4hKyuLIUOGEBUVxYwZM9yZ95UrV/LJJ5/Qs2dPwsPD3ZMa586di06nIyIigptvvpmePXvWely1UZ5Jtri7Yfy53LVsseAwGFA1Kw+WvUCWkRvw1qFQlSXV+WHRYTBgLypyTpqaNYviH39s5JE1PmtmJh4dOqAKCcGaVb/Bcnkg01hBVbmyAwcAkK1WTIcPX2bv64dstWI6dgzfEcMB6hT4liUkkDplCpmz59Tpmpb0dBReXiibNcMjNBSFjw9GV3a7nOl4ebB8vE7nrgtrTk6lMo+cBQvI+9A5md505AjKoCCUzZpVCpzrfQwZmagrzM3RdOoI0KiT/IyHkkCS0PaMpOyPugXL+vUbODVo8A3V+Ua6XrMfMTEx8h8X/YCOHz9eYyAoCE3B1fwOl/72G+n3/h9P3vIw/XJPcufpHYQddU4+tJ6/wOnBg2m54EUCp06lYOVKzr+ykC57dqO6TCmIw2LBnp8vJlzVg9PDR2AvKcFRXEzomtUo/f1JGT0G39GjafvO2409vEZ1etQoPMPDseXmITschK6svzKFU0OGYjt3jsB77m6UEr1yOc+/gH7zZuSyMoIffZTmDz6AfsMGHEYTgVOrn0tSW8akJPI+/Ig2i990fhiuR6Zjxzh7+2Rav7WY8y+/gu/IkbR65eXLH3fiBGl33+Os57Va6fy/n/BwJWEuJ/2BB7BdyKXTd+sASIubhaOkhI7fOluMOoxGTvaOAYcDddu23PTz1it/gjVwWCycHjIU7/79afPvt7BeuMDpwUNQ+PjQZfcuUu+4E1WrliCD7cIFOm1YX+9jADg9bDieMb1p88YblR/r1Ys2i99skGteTvr992PLycHvttvIfe8/dN37G8qAgFodm3rXNIwHD9Ls/+4l5KJk2fVMkqR4WZZjqtsmMsuC0EQ4KpRhWBQqsNvdE2LcS10HlGeWnfWGtalbLli+nNNDh5E69S6Kt9b/H6QbxeUmGTksFqw5OXi7FuyxpGdgPOLKeNbhFq5sszVqCYdss1G4alW91rzLdjvW7BzUbdqgbtsW61WWKNkNBsyuyc62vDxs584BNGj2rzbKDhzAu29fNF1uoiw+HkdZGecWvsr5f/3rqidJnX/zTQy//IJ+48Z6Gu2fym+5e/bsiSYsrFaZZdnhIOuJJ1F4edH+w6UAlNbQtrQ61vQMPCpkUz11OkwnT7rLV8wnT4LDgWdUFNbMTOx6fV2eUq0YftmOvaCAkq1bsRcVUfLT/0CWcZSUUPzDD5hTUtCGh6MN74H59OkGmeQnW61Yz51zd8Io59G5c6O105NlGVPSIbQ9e+IVEwOyXOv2eZa0NIwHDyJptei/Xef+u9XUiWBZEJoIuUI3DLPCObmzvG7ZXr56X3kZhmtRFkctVvEznTiJwt8fe2EhWbPnNMmZ/A3NdPIkyf36YaxmYmc5a0YGOBx4x94MkoQlPc19K96akVGrP/ayxULajLvJfuLJeht7XRl27uTcSy9TtO67ejun7cIFsFpRt2mLum0bbBcuXHFNryk5mbO3T+bMpNuxnr/g/iCi6dYN0/HjjVanb71wAUtqKl59+uDZqzfGgwedWeWSEmSLhYIvV17xucsOHMD4RzySWk3BF1/W+4cp4+FDKAMDUbdpg7ZbN8zJyZftr2vYvh1LSgotnnwSrwEDULdpg6GWwbJstzvLctr/GSxrdRFgs2F2BepGV0mN/2RnBylTPbeKBSj6bh0Kb29kqxX995sp/uEHNF1uQtWiBbnvvucM1sPD0YaHg93uDODrmTUnx5U9b1fpcU2nTljOnm2U32drWhp2vR7PyEi0kZFIanWtSzH0GzaCJNHyhRf+/AByAxDBsiA0EZX6LEuqSo/ZCwoAUJVP8PN23qZ1lF2+5ZI1IwPPyEjaf/E5qFQUrlpd72Nv6owHD4LdTsmOHTXuU16fp+3a1VmXm56B8cgRJA8PoHb1tLkffIAxMRHDjh3IFWrSryXDr85WjAbXYkH1oWJrrPIMmjUru87nKd3/O6lT78JRUgI2GyU//eisV5YkAu64A7msrNE+7BldwYRX3z54xfTGYTCQ++57aHqE4XvrCGe2vvTKWqDlLf0QZVAQIc8+iyUlhdLLrKBaF7aCAowJB9FG6pAkCU1Yd2Sz+ZL1prIsk//xMtStW+M3ZjSSJOEdG0vZvv21+r21nTuHbLWirlCy4RlZeZKf6dgxlAEB+A4f7v6+PlkvXKB0124Cp09HExZGwWefYUxIwG/sWPzG/Q2ba96ONiICz/Bw59hqcYfIdPw4BZ9/TvHWrc5MeWkplswszi18ldS7prmzxbIsY0xKIv/jjwFQt21T6TweN3VGNpk4/9oi0u/9PzIe/n/kPP88+Z+uoHTvXmyuBImtoICSX7ZT+vvv2PLzcZSVYTeUYi8uxlZYiK2gAFtuLpbMTOdETFdrQ0tqKubTpzEdP47RtShMeebceMhZO+7ZsycKjQZtz0iKt2wh76OPMezcifnsWexFRVgyszCfOYM1OxtHaSmyw4F+wwa8BwzAf+IE1O3bU7h69SUDftliwbBrNxfefZfSvXuv24nRTa51nCzLjd4aSBCuxNW+CVQMli1K5z9dh9mMEtxvnO5uGK7MslyLXsuWzEz8e0aibtECv5G3UvTddwQ/+oj7HIIzswxQtv/3Gvcpn9zn0aEDHu3aYU49izn5FL6jRlG8aROmo0fxHjCgxuPLDh4k/8OPULdvjzU9HdOxY3jW8yTRy5FlGYOrb3np779jNxhQulbrvBoWV9s4j7ZtsHk5f6+sWZnuiUy1Hdv51xehat6cDl98QcaDD1K8eQvKoCA8QkPxck1ANh09iqZTp6sec12VHjiAwtsbbViYe56AvaiIFk8+geammyjZ+jO5S97HZ9AglP5+eHTujKRQYD51CofBgDYiwtkfvawMW34+Cq0Wu8GAYcevlO7ZQ/ATj+N/+yRy//Mf8j/8CNPhI5Tu24clLQ17QQFanTOoM6emYkk5gyokBE2nTnj17YtXTG/Mp09TlpCA0scHVfPmGJOSMOzc5f4g4z9pEgBa15wK04kTaDo7uzIYk5Kw5uS47wxYUlIwJiYS8uyzSCrne5F3bCxFa9diPHwYr169Lvlalfchr1jfrAoJQRncHNMRZ7BsPnYcbY8wVM2aoWrZEtPx+p3kV7xpEzgc+E+aiKp5c87/618A+I0ejcNioeCT5SiDglCFhADO99aS/20FqxXzmbMYDx3CduECCo0GVUgI3rGx2PJyKVqzFi5+r5ckUKlQeHqSOm06wY/MQb9hA6akQ6BS4T1woDsgL6cN6wFA4cqVaLp1A4cDY1IS9q+/ce+jDAx031WsL0rX4mqSl5e7bV2z6dO58Na/yf33v2s+UJLwaN8ea1YWwY/MQVIoCJwyhQtvvsnJmD5obroJdevWKP39MaecxnI6xVlyZja7P2Dlf7AUj9BQQp59Fp+Bt9Tr87paTSpY1mq15OfnExQUJAJmoUmRZZn8/Hy0rtZAV3QOi/O2dXnrOOdjzjcZe6GzTY/StVBO+QSgy9Wd2vV6HHq9+xZg4LRpFG/5Af333xN4551XPNYbjfmkc9Ec4+HDOEpLUXhX7UFrSUtD6e+PMiAAdYf26Nd9Bw4HPrfEYkxIuGRWSpZlzr24AHWrVrRf9jEpI0dRuv/3eg2W7cXFlGz9Gf9JE5EU1d9UNCefwpaTg//EiejXr6d09+4691eV7XYKVqxA3b49frfeCrgyy5KEqnVrJE/n72ZdF24p3fMb5mPHafXqQtQhLfAfexsXFr+Fwtsbn2HD0HTuhKTRYDp6rMFXoLNmZ2PYtRtbXi6y2YLx4EHK4uPxGTQISaVC3bo1qtatcJSW4Td2LApPT7z69KFg+XIKli93nkSpRFJV6JeuVqNq1gxbNYs6abp2JfCuu1B4eBA4ZQp5779P2e+/o+kRhlefGJR+/hiTkij4ahWa0A5odRHYcnMp/ukniiq2EpUkdyAneXriPWAAgXfdhTY8HK+Y3s5rdeyIpFZjjI/Hf+xYbIWFpM26F7nie4lCgTIggABXiQSAd/9+oFBQ/OOPyFYbOOxoundHFRjorltVuO6ymJOd9eYVa5YlScJTF4nx0GFkiwXTqVMEzbwHAG2PHrXOLF+cUHOUlTnf50pLsebkYElNw5x8kpKft+EZHY2mY0eUAQGcf/NNNF1uwiM0FADP6GhULUPc5/Ls3QvDz9so27cPhY8PnpGReOp0yGYz5rNnyVuyBBQKAmfMIGhWHLb8AqyZGVjSM5DtNgJuvx3ZZiPj/gc4//IrqFq1ouWCBfiNGe1+367IMyKcjuu+Rd22LUo/P/fjtoICzCdOYDpxEnPKaTzad8Crdy8cRqOzbMNiAYUSFBKSQglKBZJCgaTRuv8uyGYTSJLzg45ajaRS4TCUYs3KwpqViSUzE6/oaCTXWg5+Y8bgN2YM9qIizCkpzhryoiIUPr5IGg2yyYj1/HmMiUkog4LwHTECgGZxM12dRI5iTjmN+fhx7EVFeHTqhO+oUSi0GlCq8OoTg1evXpRs307RqtXVvr82tiYVLLdt25bMzExyxWoyQhOk1Wppe9EkjrqovNx11TIMhb8/kqtPqlRes3yZzHL5qmrqds5xefbujaZrVwq/WkXAHXeID6U4//iak5Px6NABS1oaZQcT8bmlaktIS1qa+w+tR7v27sUMtDod2vDwS04+M/7xB+bkZFq9uhCP9u2dE8T274f7/1FvzyP3nXcp/OorFN5eNQbAhp2/AhD8yBwMO3Zg2L69TsGyw2gk68m5GLZtc55vwnhaPPUU1qwsVCEhKDw8kIKbI3l4VFqkpDbyP/4YVUgIfq5A2Hf0GC4sfgtHaSna8B5IKhWa7t3qNJmyJrLdjunIEUp/+43S338HuwOFjw+OkhJnL+WKgb5CgaZzJ4IeuJ9m06a5Hw6ZNx8kyX2Hpu1//4P51ClkhwN7Xh6mk8nIJhNanQ6FtxfG+HisFy6g6dgRVYsQ54djlQqvmBg8QkPd/xaD/nEfmps649m7N2pX1tM97osCRdlux3TsGMaEBDw634RX717INhu28+dRt2uHopqWnZKHB363jaHom28Juu8+ClevQTYaabv0A+dCGZmZWDOz8L55QKWuHEp/fzwjIyn8/AsKP//iz/NpNM4PBGo1nhERoFRg/CMeVatWqFq2rHRtT10Ehl9+IWvuPLBa0biy3NqwMAw7dpCzYAHGQ4fw6NABbVgPJA81ssWKNTsLa3oGlvR052S50A54hkdgTj2L6fAR979F91gDAtD2CCP4sccAZ/la61cXom7d2r1P++WfQIWFv9q88QbWc+dRNQtE4edX5QOnrbAQ2WJFHdICAHXr1tUueR+66itK9+3HZ+gQ94eHmmh79KjymKpZM1Q33+yeSFzJwIGXPN/VUgYE4NW7N/TuXav9JaWSgEkTYdLEWu0fMHEiARNrt++11qSCZbVaTceOtb9tJwg3Eke1wbIzK2UvKkRVoa3Pn5nlP4PlwjVrMScn0/L559yPWTOcwXJ5hkeSJPwnjOfCm4uxFxaiatasAZ/RlbMXF4MsV5uRqW+27GwcBgMB/+//ceGttyjbv7/GYNm7r3NVuvKJSwpvbzxCQ9H26EHJ//6Hvbi4UpaoXOGqVSj8/PC77TYAvPr2o2jdOmSLxV3zbC8uxmE0uf8Y14X1wgWKvnHevs1bsgTfkSOrzS6X/roTTVgY6lat8Bk82Fk7bbO5b7WXqxiUGY8cRb9unbMG8uxZbOfOEfLM09j1xeQtXYp+8xYUHh5oejgDH0mhQN22LcU//IDtwgU8Oobie+utaLp0wVFahqNYj12vx64vxl6sRzaZsBcWUrZ/Py3mzXMHGB5t2+AZFYUxMdEZhOEMLoo3bsKwew9Izj/uqmbNUDZrhqRQYDp6FOOhwzhKDTgsFiSlCkmpwJqd4+r9LIEkYTxyBIdrQqYmLAylt7dzURUfHzx1EQTedRc+QwY7g9gasvR+o0dV+l7p7+/sLFC+3fWzLuc7dGhtfpQoPD2rHFvu4g+3klKJp06Hp05XeSy+vpe8RvCcORT/8CPn/vUvyn7bi9+Y0fgOGXLZsbVc8CJlCQl4dOgAOOuM7QWFKP39cRhKKD1wAEdxMcGPP07A5NvdmctyAVOnYj51ipIdv4JC4b6z4hkVBQ4H+g0b8YyMxJR0iJIf/uxdrvD3dy5uEhmJ78iRmFNOY9i1C48OHQh64H482rRB8vRE3bIl6nbtUAUHV3mt/MePr/T9xWVoCi+vS5YNlc8XuRylvz9+o0bWal/h+tGkgmVB+Csr74ZhVf5Zs1yeWbZVWL0Pqi/DKFyzGkvKGUKeedr9R8qa6cyQqStkvMsn3Vhzcq7bYDnriSeRbVY6fPppg1/L5CrB8IzqiadOR+nv+wHXxJpt2yjdtdtZZ5qTg9oVJJS/htrwcCSFwjmbHmfw4H3R4kK2vDyKt/5Ms2l3uf9Ae/XtS+HKlRiPHMWrVzTWrCzS7pmJvaSE0NWr61TrC1DwyXJku53gRx8l9513KPnpJ/zGjHFvlx0OTIcOUXbwIEH33QeAz7Bh6DdsQL/pe2d2COcHtsIvviD/o4+R7XYUvr7YcnKQPD3RdO2CtkcPAhe86F6a2HfUSIo3bqTkl+34DBzkvl7A3++keOMmjElJFG/eTN5//uvM4rlaIVZHGRhIwN//XumxgL//HUtqqrvO1is6mqJVq8lwPYfKJ7jo/BVKEpSBgajbt0OSFMhWK74jhuN98814Dxhw3f4baEjqNm0IvHsGBZ84S0aCarnwk7Z7d7SuhakAfGqxKFdFqsBA2vz73zhMJmy5ue7JoN63xNJx/Xd4dOzozobbDQZwOJBUqnrvOy0IFxPBsiA0EbLFgqxU4pAU1ZRhFKJu8+dsanfrOKMzWLYbSjGfcPYttWZnuzPJloxMlAEBlSZxqVs5b0Xazp2DiyadXA9khwNjQgKy3V5t1rO+mZOdwbKmS1e8+vUl/+NlZM+fj37zFrDZULVuhSo4GK+YGHyHDQNcE5ckCc9IZ0ZPG+F8HQu+/BJJqUTTpQsKX18cRiMFn38BVisBU/5cTt7LlaE27NiBpFSQ9fgT2EtKkFQqMh58kNA1qy+bybJeuID5+HEcRhOFa9bg/7e/EfSP+9Bv2sSFxW9R9PXXGI8eQ/JQg82OvbAQSat1Zy19Bg9C2zOSnKefdrbzUikp3vIDtpwcvAcOxCM0FHt+Pp6zZuE/cUK1GXNt165on3ySFk9WboUXFBdHUFwcALbcXEq2bcOacw6lnx9Kfz8Ufn4o/fydX3t6IssyqsBAlD6VaxkDbp+E/8QJ7uyu39ixzpXQZBlcS+7a8vOxFxS6yjXC8ewVjSowEEmtdv8OVVeO8FfX/P770a/7Dq/+/dB27XpNr63QaqvUM1cMwoF6mXgqCLUlgmVBaCJkiwXUzlvQVYLlwkJ3QAYgqVRIHh7uSTmmQ0nuuj3z6dPuP0TWjIxKy6wCqFs56wit2TkN+Gyct/IvvLkY3xEj8OoVXevjLGlp7hZc5pQzaLvV3x/ysoQEsp96mg6fLnd/+DAnn3ROsvHxxrv/APKXfkjx/7YSeNddBEyaiCYsrMotXaWvL+2WfoDWdftbFRiI39ixFP/4I4afnfW8FTOb3rfcUilbrAoMRNO9O/kffUT+Rx+h8POj/SefINuspM+M4+yk29F07YLCyxt7fj6O0lIktdr5n4caW35B5Z6wKhVBDzyApFQS/OgjZM15BIW3N36jnKUCst2Gd9+++Awe7F6lS6HV0uGLLzi/8FUKPvsM1Gq8+/en1cJX6pwxvBRVcDCBU6defscaVCyDkJRKvKJr/7skKZVVSgEEJ6W/P52+33RdTrYShGtNBMuC0ETIFjOyq17TWqF1nCzLzvriizKNzjZUzprlsvgEd3BmOXMGXPWRlsxMPCMqZ4+VzZo5J2Cda9hg2ZqeTsHy5VizsuoULFecKGc6evSKgmXZ4cBhMFTJhhauXo01PZ38T5bT8oXnndc4mYzGlVnz6teXdh99iFanu2xmt7wUoVybtxY7azr378eanY29SI/C2wt1q1Z4DxpU5fhWr7yC8eBBVC2C8ezZ070cedsl/6Vw9RpsOTlYyzJQBgWhDG4ONjuy1YrDZEYVFITfE4/j1asXCk9PZ5mB63i/W2/FNynRXQt9KQoPD1q9/BKBM6ajbtmy2uyxcOMqb4EnCH91IlgWhCbCYbGAq9uFpULrOEdpKbLVijKwcm2l5O3lrlk2HkxA060btrw8zClnnMfabFizs6t0O5AkCVWrlu6m/A2ldL+z9rd03z5ku71Shs9uKEXhoa42oDMdPYrk4YGkUjk7H9w+qc7XLvxyJRfeeYfOmza6M8gOsxnDtl9ApaLo229p/vBDKHx9saSm4jvS2QJNkiR8qglsa0vp6+tuq3Q5nrqIamfT+wwciM9VznqvTaBc0bW+DS8IgnA9ESv4CUITIVssyCpnsFyxDMN+0YIk5RSeXjiMRmSbDWNiEl69op1LqLpWkLKeOw82m7ttXEXqlq2w5pxryKdD2T5nsOzQ6yv1UJUtFs6OH0/Oyy9Xe5zp6FE03bs7e68eOVJpm/X8BfcCIpeiX78euayMvKVL3Y8Zdu7EUVpKyDNPO5cn/uwzir//Hux2ESwKgiD8hYlgWRCaCNls+bMMo0LruD+D5YBK+yu8nJllc3IyjrIyPHv1xqNzJ8xnziDLsrsThsdFNcsA6latsJ5ruGBZlmVK9+/H21X7Wrrnz+V79Zu3YM3OpnjzlirLA8sOB6Zjx9D2CHP2Lj5xAtlmc2/PnjePtLumYXMt/10dS1qacynd4OYUrfsOS3o6AMVbtqBs1ozAv/8d39GjyP94GTnPPodH586XXHlPEARBuLGJYFkQmghnZrlyzbJssbgDw4tbXCk8PbFmZ6PfsBHAlVnujKOkBFturnsFtfLV+ypStWqJ7fz5SoFofbKcPo09Px+/28ag6d6d0t+cwbIsyxR8+ilKf39ko5HirVsrHWfNyHAuDRwejjYiwrl6litTbkpOpmz/fhxlZeR/vKzGaxe7+rO2W7IESaUi9933sGZlYdjxK76jRiKpVATPno33zQNo9dprdNqw3j3pTRAEQfjrEcGyIDQRssWCrK6uDMO11PVFZRiarl2xpKRQ8NlnqFq1Qt26NZqbOgNgOXMGS8oZUKlQt6y8Chg4yzBwOLA10GqZpft/B8CrX3+8b76ZsoMHcZSVOZc0Tk6mxbx5qNu1o3jjxkrHla/O5hke/mfv4iPOxwq/+grJwwOfYcMo/OorrOcvuI+zFRa6g+riH3/EMzoaz8hIAqdPp3jzZk4PH4FsNLp7D2s6daL98uUETJrY4K3pBEEQhOub+CsgCE2EbDZXCZYdZgvyRTXLhaXOdnIhzz5D0L2zMB496u6d7NHJGSyXHfiDom++wWfgwGqDQXVrZ+cEa06Ou4tCfSrbvw9127Z4tG2D9803U7B8OblLllD6215UwcH4jfsb1qws8j74AOu5czjKynAYjZQlHERSq9HcdBOoVCi8vTEdPYL91hHoN2zE729/o/nDD5Ey5jbOvfgiPkOGYDpxHP1365HNZrxjYzGfOEHIM08D0OLxx/CM6ontQi6SWo1Xnz71/lwFQRCEpk0Ey4LQRDisFmS1c/GEimUYjpJiUKtRuJr0z/v2ELIss2xmH9StW6Nu3dp9DlWLYBQ+PuR/9BGyw0GLJ5+o9lrqlq5eyw3QEUO22Sj9/QC+I4YD4BXTG8nLi4JPlqPw9aXl88+h8PDAf/w48t5/n7MTJ2EvKnIfrw0Pd3dz0EZEUPT1N5Tu249sNBI4fRoebdsSdO+95H/4oXNRD7Ua/4kTUIW0JH/5clAo8HX1F5ZUKvxGiqVnBUEQhJrVS7AsSdJo4F1ACSyTZXnRRdsHAe8AkcBUWZa/qY/rCsL1SrZaKf7xR/xuu63eFj2QLVZkb2dA7JAUyEqls2a5sBBVQIB7YYzcEjNKhVTtOSRJwqNzJ0xJhwiYOgVN587V7qdyZZNtDTDJz7BzJw693r3anUKrJXTVKkBGc9NN7tfLIzQU/wnjsWZl4ztmNKrgYCypaXj17uU+V8sXnqfom28p3b0L31tH4OkqzWjx2KME3fd/OIxGFF5e7tW+Au68E1tONuqQqqUngiAIglCdqw6WJUlSAkuAW4FM4IAkSRtlWT5WYbd0IA54suoZBOHGU/Lzz2TPnYfCywvf4c4MqsNkQqHVXvE5ZbMZR4D6zwfUHu6a5Yr1yiarHa265gBdGxaG5XQKwbNn17iP0scHha/vVa3i5ygtJfvpZ2j+8EOVlqotWrMWVXBwpUU7alpYpPXrr1/yGprOnQmZPw/mz6uyHYQWoQAAIABJREFUTenri9LXt9Jj6pAWqENa1OVpCIIgCH9x9THBry9wWpblM7IsW4DVwISKO8iynCrL8iHAUQ/XE/7CirduxZaf32jXNyYmkv3Ms8h2+yX3K5+IVrpnj/P7Eyc42acvxsOHr/jaFfssA6BWI1vM2C5cQFmhE4bJakd2LaNcnRaPP07HDesvuzrXpdrH2Q0GbIWFOIzGSo+dW/gqtrw8AIp/+IGS//2Pcy8ucI/Hmp2NYdcu/CffLibOCYIgCE1CfQTLbYCMCt9nuh6rM0mS7pck6Q9Jkv7IbaBZ+ELjsWRmkvHAg9hLSq7oeHtREVmz51D41ap6HlntFXy5Ev26dRgPHbrkfuWLbBh2O4Nl/YaNYLVirsWCGTWRLRYc6j+DZVmtRr/pe0yHD+MZ1fPPa1v/P3vnHR5Hee7te2a2adWbJbnIvWJswDbNVNM7gRBqEiCFGkIgOScxgZCQHALkBBIIHEK+QAKYFsDGmGKwsXHDuPcq25Jl9V62Tfv+WM1qtbtqq2r83telS9LOOzPvtpnfPPN7nsdA70AsKykpOIZHNyKJxJaXi1paEvV44xdfsHfmLPaddjr7Zp+Bd8sWAGpfn0fta69R9eLfAaibPx/Jbse7ZQsNH30UfOzd98A0Sfv2t7v2pAUCgUAgGGAGVek40zT/bprmTNM0Z2ZnZw/0dAS9jOerr2havhzPuvVxrR8oPhL8fehQL86q65i6TvOKFQCh3zHHmSa+HTuR3G7UoiIChYU0fBKs7RtPV7ya116n4LLL0Soq0BMSWxc4HBiNjWTcfjvZ990Xetin6ei9cA/HnpuHFjFf0zCo/MtfsefnkzN3LpLbTcWf/hcjEKDmtVcBqHvnHbxbt+Jdv4Gsu+/COXkyFf/7v9TMm0ftW2+ROHt2l8S6QCAQCASDgd4Qy0eA8K4Gw1seEwjaoFYE69769+yOb/3iYoBQx7X+xrtlK3p9PZLdTtPyL9sdp5WWotfXk37ddQBU/u1vaC1VJdTy7onlQGEh5b//PbLbTc7cX1H9rZtDy3xXXkfeH/5Azn/9Aklu/Sr7VB3DaD+y3FXseXnotbU0fPRRyHbS+Pnn+HfvJvvee8j43nfJuvNOPOvWUTr3IfTKqmCraJ+P4nt/ApJE6tVXk/Pf/41WUkr57x5DstvJuufuHs9NIBAIBIL+ojdMg+uA8ZIkjSYokm8AbuqF7Qq+YWgtYtm3K06xfKQlslxYiGmaoeoP/UXTl8tBUUi/5RZqXn4ZraoKW1ZW1DjLgpFy6SU0fv45DR8sRHI4sA8bhlZW3q191s2fD7LM8OeexZ6Tg7apOLTMe8V1pI1vu3/TNDu1YXSVlMsupX7BAo488CD2kX8h/YYbqX//fRyjRpFy6aUApH/nOmpefpmGDz/EOWkS6d/9Ls1fraVp6VLcp52KPS8Pe14eI9+Yhy0rC/vw4f3+vgkEAoFA0BN6HFk2TVMD7gU+BXYBb5umuUOSpN9JknQlgCRJsyRJKgauA16UJGlHT/crOPrQKoI+dF+8keUjQaFoNDS0qbvbXzR9+SUJJ55AyuWXBf9fsTLmON/OnaAoOCdOJPGMMwBIPOtMHGPHoHUjsmwaBvXzF5A4e3ao1Fm4vSKWIPZrwQG9EVl2jBjBmA8XMuyZZ7ClpVPxxBP49+4l6+67Qsl5ksNB1k/uBSDzB7cjSRJZP/4RSFIosg7gPvFEHCNGCKEsEAgEgqOOXklHN03zI+CjiMceCft7HUF7huAYxoosq4VF6E3NKEmJnazRlsCRVnePWliILaK9c3eoeOYZ/Hv3MfSJP0aVF4uFWl6Bf+cush94ANfkySjZWTSv+JK0b10dNda3YyfOsWORXS6SzjqTurfeIuWSS/Bu2oynpc1zV/CsXYtWWkrOL1orLhphAjmWIPapetS4niApCikXX0TKxRfh27MH3/btpFx2WZsxqVddhXPMGFzHHw9AwgknMG7ZMmxDRN6BQCAQCI5+BlWCn+CbjVZRgdJiW/Dvja4KoTc1U3zfT0OJfJGoxUdwjAs20Yj0LZf9/g80fv55l+Zhqiq1896gaelSCr/3/VCps45o+nI5AElnn4UkyySdeRZNK1dhNDdHjfXt3IlrypTg+HPPZfj/vUDKJZdgz83BaGxEb4peJxZ177+PnJJCUkudZqBNSTg9plgORpZ7w4YRiWviRNKuvTaqyYokSSRMm9YmamzPGSKiyAKBQCD4RiDEsqBfMHUdraqKpDPPBIJ1hyPxbd1C4+LFNH3xRfT6pol65AiJJ58CskzgUGFomVpeTu1rr1H98itdmotn0yaMhgbSb7qRwMGDHL7r7g7rEpumSd1bb+MYNQrnhGDzjLRrvoXR1ETxT+/HDARa51JRgVZZGRLLkiyTfM45SLKMLSfYQlqraN+3bOo6zV9/TfFP7qNh4YekXn4ZstMZWh6uj2MJ4lBkWVQ0FwgEAoGgVxBdAQT9glZdDYaB6/ipNC1dij9Gkp8VLfbv3x+1TK+uxvT5cIwejT0vr01k2fPVV0CwYYje0ICSktLhXJqWLwe7newHHsA5eTJlDz9C86rVJJ0xO+Z478aN+LZvJ/c3j4Sipe6ZM8l77HeUPvRrSuY+xNCnnkSSJHzbtwPgmjI5ajv23KDvWCsrwzF6NI2fLsa/dw9qSSmG34fp8+PduBG9vh45NZXMH/2IzB//qO3rYHRiw9D0qHECgUAgEAjiR4hlQb9gJffZc3JwTp6ML0ZzjkBRsLdNLLFslY2zDx+GY2Q+gcLWyHLz6jUgy6DrNK9eQ8rFF3U4l6bly3HPnIGSlETqVVdR9ddnqfnn/2tXLNe88i/k1FRSr2rTmJK0a69FLSuj6tnnSL/pRtwnnUTjkiXISUm4pk2L2o4tNxhZVsvK8e/Zw5H77wdZxjZkCLLLhWS3kXTuuSSddSZJ55yD7HZHbaONDSNmZLnvbBgCgUAgEByLCBuGoF+wkvtsQ4bgmjQJ/969US2jA0VBAezfty/KFmEl99mHDcM+cmQosmyaJs1r1pB83nnIKSk0rWi//jEEuwgG9heQfM45AMgOBxnf/x7Nq9fg3RFdpCVw6BCNS5aQfv31McVr5q23IiUkUL9wIaam0bRkaVDoOhxRY21DhgCglpXiWb8BgLGLP2X8si8Y+8nHjFm4kKF/fJyUSy+NuS+IsGF0lOAnIssCgUAgEPQKIrIs6BfCxbJz0kRMn4/AoUM4x44NjVELgwLYaGhAq6jEnjOkdVlL0p9j2DAc+SMx6uvR6+rQamrQKipIPPMMUBSaV6zssAZz0zIrUe/s0GNp119P1Qv/R/ljvyft+uuRbArNK1fi2bgJ9fBhsNtJvzl26XA5MZHkOXNo/OhjkufMQa+rI/nCC2KPdTpRMjLQysoJ7C/AlpODfVj3OsO3qYYRI3rs7eVqGAKBQCAQHOsIsSzoMnXz5+PduIm83/222+tqFRUgSdgyM0k47jgAvNu2hcSyaRgEDh/GOXky/l278O/f11YsHzmCkp6OnJiIY2Q+EGxO4t0W9Agnnn46kmKj8ZNP8O/di2viRAAaly2j8ZNPUUtLQz+OkSNxjBoV2raSnEz2Az+j4n//TOmvfhV8LC0N98knk/qtq0k686xQneNYpFxxOQ2LFlH+h/9BSkgIJTHGwp6bi1pehn/PXhJOOrHbFSPCA8aaHqPOsio8ywKBQCAQ9CZCLAu6TMOHi2hes4bcXz+EFMNm0BFaZQVKViaSzYZjzBjkxER8W7fC1Ve3LK/E9PlInjMH/65dBPbvh9mtHmK1uBj78GCpbsfIkQB4Nm6iec1q7MOH4xg+HMkenFPTsuW4Jk5Eq6zkyE/vR3a7cYweTcLxx5Ny0YUkzZkTNb+Mm28m/YYbCBw8iOH345o0KapEWnskzZ6Nkp5O4NAhki+8EDkhod2xttxcvJs2odfW4j5pRtdevDDC7RWxoseWZ1loZYFAIBAIegchlgVdJnDgAOg6gaIinOPGdWtdtaICe3YwUiwpCq6pU/Fu3da6vMWDnHDiiSjp6VFJfuqRIzgnBytM2IcPR3I4qHjiCQDSvvOd4OM5Q3DPmkXNK6+Q9p3rqH75FUxVZdSbb4QEdkdIitLt5wUg2e2kXHIJtfPmkXzhhR2Otefm0FRbG3yuJ53Y7X0ZbeosRy/3iciyQCAQCAS9ihDLgi5heDyoJSUA+PcXdFtUauUV2FuqQQAkTJtG9SuvYPj9yE5nKGHPMTIf57hx+PfuC401DQO1pITkC84Hgt7fkW/MI3DoEKbPT9LZZ4XG5vz6IQ5ecy1lv3mUppUrSbnssi4J5Z6S8f3vYTQ3kTzn3A7HWbWWJbc7ZBXpDl2tsyyqYQgEAoFA0DuIahiCLhE4dCj0t78gurRbZ2gVFaFqEACuaceDquLbuTO4/cIisNmw5+XhHD8Of0FBqCKGb8dOTFXFMaY1GTDhuONIvewy0q69BltLV0AIdpnLvO1WGhcvxvR6ybrzjm7PNR4cI0cy9Ikn2q1iYWHVWk6YPg3J1v1r1U7bXWtGu8sEAoFAIBB0HyGWBV3CX3Ag+IfdTqCgoFvrmoEAek1NG7GcMG06AL5tQStGoKgIx7BhQU/zuHEYTU1oZWUA1C9YgORwkHz+edEbj0HW3XfjGDuW1G99q021jcGAFVl2n3hSXOt33u5aVMMQCAQCgaA3ETYMQZcIHDwAskzirJmtwrmLaFVVANiGZIces+cMCSa7bdkKBD3L9pYqF5bFw7drF7bMTBoWLSLpvDmdduazkN1uxsx/H+KI3PY1rkkTcU2ZQvJFHXub2yPcp9xx6Tg6LKEnEAgEAoGgaww+NSEYlPgLDuAYMQLn5Ml4Xn0NU9O6bCOwaixHll9LOP54vFu3YpomgaIiUk8MJrwlTJ2KbcgQKp76E0azB722Nqp7XmdIdnu3xvcXSloao997N+71jU4iy37VCBsLitDKAoFAIBD0CGHDEHSJwIEDOMaMwTl2HGYgEGo/3RH+Awcpvu+nlD78MAC27Ow2yxOmT0M9fJj6997DaGoK1U+W3W6GPvkkgUOHKJ07FyUzk6TZsVtRH2t03u66tSuiqIghEAgEAkHPEWJZ0CmmprV02xuDc+wYAPyd+JZNVeXIgw/SvHo1SlYWad/5Ds7x49uMcZ98MgClD/0aoM3yxFNPIfPOOzBVldTLLx+0keL+Jlz/xkzwCxPLwrcsEAgEAkHPETYMQaeoR44Eq1GMHoOjJWHOv7+A5PPaT7ir/sc/8O/axbBn/0rKBbHbPydMm8a4L5ait9QdtuooW2Tfcw/2nNy4/b3fRAzTRJaCojl2neXWB0VkWSAQCASCniPEsqBTrIQ+x5jRKElJ2HJzCRxoG1m2kslMTaPx88+pfP4FUi69pF2hbGHPy8OelxdzmWSzkX7D9b3zJL4h6KaJTZYJ6EZsG4YmIssCgUAgEPQmQiwLOiVwMCiWnWOCFgzn2LF4t27Ds3EjWmUVNf/+N94tW7Dn5WH6/WgVFdhH5pPz618P5LS/kZgmyDLIRhdsGDEizwKBQCAQCLqHEMuCKGpefx216DDZP7sfTJPGpV+gZGWhpKYCwZbUzatWUXjTzUCw/XTGzTehVVYFPcZXXUnSOefE1XRD0DGGYaJIEoosxYwse8NtGCKyLBAIBAJBjxFqRtAGraqKiiefwvT78WzYgGS34928mbzHfhcak3XP3aRcdinq4cMgySSefhqSogzgrI8dDBNkSUKWpJiRZb+ohiEQCAQCQa8ixPIxjOHxgM2G7HCEHqv5178wVZWcub+i8i9/xQwEGPb006RcfFFojCRJOEePxjl69EBM+5jGME0kCWyy1GEHP2usQCAQCASCniHE8jGG4fXiWbeO+g8W0rh4MXJiIum33Ez6jTciKQq1894g5eKLyfje90g+7zwMrzfUUU8w8BimiSxLmLRXZ1lUwxAIBAKBoDcRYvkYwQwEKJn7EI2ffoqpqsgpKaReew1aaRlVzz5H1fMvYB8+DKO5mcw7fgyAfdiwAZ61IJJg6TgJ5HYS/DQdt0PBE9BFZFkgEAgEgl5AiOWjHL2hgaq/PU/K5ZeTcPzUmGNMw6DkV3NpWLSI9JtvJuncc3HPnIHscgHg37+f+oUf0rh4MalXX41r4sT+fAqCbmB5liXa7+CX5LQFxbKohiEQCAQCQY8RYvkoJFBYiJyYiBkIcPiOO/Dv20/de++R/8rLJBx3XJuxpq5T/sQTNCxaRPaDD5D1ox9Fbc85bhxDfnY/Q352f389BUGcmC1NSUxJimpKYpomPtVgSLINCIhqGAKBQCAQ9AJCLIdhmia18+Zhz8kh+fzzB3o6QLBtdONnn5Fw0knYMjIof/Ipal97LbhQlpETExn65BNUPPMMh2//Ae5TTkErL8c5ZTLJc+ZQ/feX8KxbR/r3vkvmD384sE9G0GN0o8WGAegRoWO/Fvzf7VBCYwUCgUAgEPQMIZbDkCSJujffxJaTOyjEsqlpHPn5L2j89FOQZWy5OWglpaTffDOOkfmopWWkXXsNznHjSDjhBI7c/zP8+/djy8ig/r33qXvjTWS3m7zHHyf16quQWkSW4OglaMMIflYjI8tWJYxEp61lrBDLAoFAIBD0FCGWI3DPmkX9/AWYmtalphqB4mK0ikrcJ53Yq/Mw/H5KfzWXxk8/JeveezF1Dc/ar8n5+c9JufTSqPGO/HxGv/du6H+9qYnm1atxTTkOx3CRqPdNwWhpKy7L0WLYqoQhIssCgUAgEPQeQixH4J41i9p5b+DbuZOEadM6HGtqGofvuJNAQQHpt9xC1j1349+zF8PnJfHUU0MJdN3BVFUaPvmUyqefRi0pYcjPH4zLPqEkJZFy4YXdXk8wuDFNUGQJWYoWw1ZkOUlElgUCgUAg6DWEWI7APXMmAJ516zoVy3XvvEOgoIDEs8+i9rXXWr3EgOR2k3rFFeT++iEkuz1qXc/69TR8upjkc8/BPWtW8P+PPqbxs8/Q6+pwTp5M/u8fI/H003v3CQqOaoyWBD85RrtrnxYUywktkWVRDUMgEAgEgp4jxHIEtuxsHKNG4Vm3nswf/KDdcXpTE5XPPod75kxG/N//4VmzBs/mzSRMnQqyQsNHH1H31luYmkre73+PJEkYHg96fT318+dT+exzYJrUvvoqkt0erH3sdpM0Zw4pl15C0jnnIMlyPz5zwdGAleCnxGh3bdkwEh3Br7WohiEQCAQCQc8RYjkG7lmzaPjkE0xdR1KUqOV6QwNlv/0dek0NQ158EUmSSDz99DZR4KQzZmPPzaHq+RcwGhoIFBbh37s3tDzl8svJeWguzStX4d20Efdpp5F05plxWTcExw6mCZIUtGK0Z8NwO4VnWSAQCASC3kKI5Ri4T55F3Tvv4N+zB9eUKW2W1S9aRPkf/ge9tpasu+9qtxEIQNZPfoJaVk79/Pm4TzqJ7J/eh5KRiSN/BO5TT0WSJFKvuJzUKy7v66ck+IZgmGaLZ1mK8iR7rWoYDuFZFggEAoGgtxBiOQbuWbMAaFq+HMe4ccgOBwDV/++fVDz1FAknnEDuP16KEtKRSJJE3h9+T+5Dc5ETE/t83oJvPla761iRZb8VWRbVMAQCgUAg6DWEWI6BPTcXx6hRVP7lr1T+9VlsWVkomZn4d+8m+ZKLGfrEEyEB3RmSJCEJoSzoJQyTltJxEnqEFg55lkU1DIFAIBAIeg0hltthxN9fxLN+A2pJCWppCWpJCZl33EH2fT+J6WMWCPoDwwhWw1AkYiT4tY0si2oYAoFAIBD0HCGW28GRn48jP3+gpyEQtKEjG4YvwrMsqmEIBAKBQNBzRG0ygeAowjCDNZZlKVad5ZYOfk4rsizEskAgEAgEPUWIZYHgKMJqSqLIseosWzaMlsiyEMsCgUAgEPQYIZYFgqMI06TVhhGjdJxDkbHJEiBsGAKBQCAQ9AZCLAv6FSv6KYgPvSXBT47Rwc+vGrjsMkqLWDaFWBYIBAKBoMcIsSzoNzYU1jLt0cWU1nsHeipHLYZpIkkSthiRZZ+q47IrIbGsi2oYAoFAIBD0mF4Ry5IkXSxJ0h5JkvZLkvTLGMudkiS91bJ8rSRJo3pjv4KjiyN1XgK6QWG1Z6CnctRimqBYdZYjxLAlllu0srBhCAQCgUDQC/RYLEuSpAB/Ay4BpgA3SpIU2druB0CtaZrjgKeBJ3q6X8HRh9ai7mqaAwM8k6MXwzSR5aBgjk7wC9owZCmolkU1DIFAIBAIek5vRJZPBvabpnnANM0A8CZwVcSYq4B/tfz9H+A8SWo5owuOGbSWlnPximVNNzj7qS/4aFtpb07rqCK8zrIW0XXEp0XaMIRYFggEAoGgp/SGWB4GHA77v7jlsZhjTNPUgHogsxf2LTiKUFvEXW2cYtmr6hRWezhQ2dSb0zqq0MPaXUdqYZ+q47IpociysGEIBAKBQNBzBlWCnyRJP5Ykab0kSesrKysHejqCXiYUWfbEG1kOrq/qx64INM3WdtfRHfwMnKIahkAgEAgEvUpviOUjwIiw/4e3PBZzjCRJNiAVqI7ckGmafzdNc6ZpmjOzs7N7YWqCwYSq9yyyrLWIw2PZXmCYZliCX9vXQTdM7IosqmEIBAKBQNCL9IZYXgeMlyRptCRJDuAG4IOIMR8A32/5+9vAUlOEvY45LLFb41HjXD+o/lTj2FWBhhG0YSiShBHxFdIME0WWkEQ1DIFAIBAIeg1bTzdgmqYmSdK9wKeAAvzTNM0dkiT9DlhvmuYHwP8DXpUkaT9QQ1BQC44xtJ5GllvsF9oxbMMIb3cdHVk2sMlBIQ2iGoZAIBAIBL1Bj8UygGmaHwEfRTz2SNjfPuC63tiX4OhF7WE1DMvGcazbMORQgl/syLKohiEQCAQCQe8xqBL8BN9sLBtFbbwJfoaV4HcM2zBMQnWWI8WwppvY5KCQDo4VYlkwuLjvjU38/sOdAz0NwSBB0w2qmvwDPQ2BoFOEWBb0G5Z9whPQ8al63OsLG4bUjg3DRJHlVhuGEMuCQcbe8kb2H8OlHwVteWdDMec8tQxPQBvoqQgEHSLEsqDfCC/5Fo8VQyT4Bdtdy5KELEXXWdYMA7sitdZZ7ubLVFDZxKtfFfbSTAWCaAzTFPYgQYiqRj9Nfo2CiuaBnopA0CFCLAu6zO6yBhbvKIt7/fCOc/GIZUtsH8snW92wEvyiXwe9xbMst3yruxNZ9qk6d7y6gYfnb2d/hYj8CfoGzRBiWdCKZa3bW944wDMRCDpGiGVBl/nnyoM8vGB73OuHR5bj8S1b1TSEDaOlznKMBL/wahjdESV//mxvSCR/uLWk9yYsEIShC7EsCMO6oN8nLtCpbQ7wX//ZQqMvvtKqgr5FiGVBl/GpBgEtfguEpvcssqyLBD9MM6zOcmRkWW/xLHezGsbGolpeWnGAG0/O55TRGSzcUiK6/wn6BCGWBeFYkeX9FSKyvOlwLW+vL2Z1QVS/NsEgQIhlQZdR9R6KZcMkyRmsVhhPrWW15cCqHQUn273ljUx55BOO1Hl7dbuGaaLILXWWY0WWFQlJCjYm6aoNY/GOchRJ4qHLJnPF9KEUVDazu0ycvOJl+5F6vIHuJ7AeC+iGeVR8fwX9g3XBLyLLrXdM94hj76BEiGVBlwloRhsrRXdRdYOsJAeSFLuLX4NP5dsvrOZgVexkj5AN4yg42RZVe/AEdEr7QCxbCX6mSZsIsGYYoahyrA5/7aHqBk6bTJLTxiVTc1FkSVgx4sQT0PjW86t4d2PxQE9lUKIbpqjSIghhHcuLajxxVUj6JmHdcdkj/NuDEiGWBV0moBsEdCPuW/SabuKwyaQl2GNGlouqPawvrGVHSX3M9dVQ6bjBb8NorQndu8JAt9pdx7BaWJ5lCFbM6OrLZCUGAmQmOTl9bCYfbi0VVow48KvBC8omvyiFFQvdMI/pnANBW6zjl2lyzCcWW3cKRWR5cCLEsqDLWBaMeAWgZhjYZJn0RAc1MRL8LC9ye55kq5rG0XCyteba2/5MM6zdNbQeYA3DxDRbH5flrtswgiXnWg8Fp4/NorAlMi7oHtZFkvDlxkZUwxCEE/5ZOObFcstrcbCqGb8mjr2DDSGWBV2mMzHb+fomdkUiw+2IGVm2RLiqxT6ZhhL8joI6y5ag7+25Bj3LrbWUrc1bIs0SvbE6/HU0V0tkAzhtcpttCrqO9ZofDRd0A4FhmFFee8Gxi2aYpCbYsckS+47xJD8trDTqsX7hMBgRYlnQZQItIjneJD9VN7ApLZHlmGK546YjR1Od5VCEsZdFk9HSlMQWEVm2XpPWyHI3xLJhtoks25TgNo4Gu8tgo/WOgnjtYiEiy4JwDMMkwa4wKiuRfeXHtkAMv4gUVozBhxDLgi7TasOITwhoetBTm+GOLZYtMa62I8Y1vWc2kP6kNRkx+Lu03suURz5hd1lDj7ZrmCaSFBTDEBbJbNmPJaIVuesJfpputIksd7f0nKCV1vdDvHax0EUHP0EYWku+xPghScd8NDX8eyHE8uBDiGVBl7FEaiBeG0aLNzY90UGtJxCVQBayLrQjhkOl4wYw4rmjpJ7fLNgeVeM4Ei1CNJXUefEEdAqrPT3av2EEq2G0BH9DB9jIyHJ3qmFYJecsLMEtBF/3EZ7ljhF1lgXhWLay8UOSOFQd7dUNaAYfbzs2ko2tY8eQZKco3TkIEWJZ0GWsyHK8NgxND4qyjER7zIoBndkw9EFQOm7Znkr+taaw0w6Ekd0GQ37sHgr9oA0jOvprbd8SulI3q2HY5HCxLLfZtqDriMhy+5gtUWVNWFQELViR5cwkJ4YJHn9bsbxsTwV3vb6RgsrY5US/SVgBmClDU0RkeRAixHIEAT1Ara92oKcxKAn00Aah6i3VMNwOAGqb1ajl0H6CnxZhORgIrDnWxqgTHU5kZNkSUT1p6gItdZZlKWTDMKI8yy0pzTZWAAAgAElEQVQJfjKdRr8t1JbOfxZWlPlY7pQYL9pR5Kvvb6yXpC8+Vl8dqOaCPy8/5mv1Hm3oLbXh5XbuZvlajpfHwvtqPffjhqZQ1uCjvpNzjKB/EWI5DM3QuOL9K3hm4zMDPZVBSY8jy4aJwya1iuWI6Ky13fbEcGud5Wghcs3zq/jnyoNxzas7WPvuNLIcYRnpaSURC7MlwU+ROvEsS9Ed/tpDNwzsivAs9waR7wfA4RqP6OhH3yY/7ilrZF9FE3VCYBxVWHe1bO0cc/Q+KsE5GLGe6+S8FAD2V4ro8mBCiOUwbLKNU4eeyscHP6Yp0LVkA93Q8ag986EeLYTEctwJfsHIssuuAOCPEN2WwGxv+x0l+O0tb6Kgsu8TRCyLSGftuiO7DWohv3fPDvpGS53lyAQ/67cVFZZlqcuRZetWqIXwLMdPrPralz+7kn+tOTQwExpE9KVFZTDcdfomcNdrG/h0R1m/7U+3cjBCx5yIc4Led5+ZrvL4R7t4/ONdfb4f67SXmmAHwKeKz/JgQojlCK6bcB1ezcuiA4s6HWuaJvd9cR/XLbwOv+7vh9kNLL1RZ9mmSDha6vhGRqg7s2GoHZwQA7rRrXkt2lrKK6u6H4m2Dt6dRbAi7RdaL9kwdNNsE1m2bBhayIbRWg2jq5FlTTexh9sw+tGz/ObXRe22Nz8aiayzbBgm9V61WxHPl1cd5PW1hX0yv4HEem26ehHXvW0fOxHIvuSzneVsLOo/G6LeklxsXaBHHtojgwEDwcaiWjYV1vX5fqzPsNMWDCaJYMXgQojlCI7LPI5JGZN4Z+87nWbgLilawpfFX1LUWMSbu9/spxkODIZh9ljwaYaBXZZDt/wjxW1nNozQCTEiOmuaJqpudMtL/ea6Ih5duJNPtncvimJFjGN1IGw7rm1Cn/WcemLDMM1gl75Y7a5DkWW5tSlJV4+1mhFROq6fPMumafKr97fx7obibq/7jxUHeGFZQR/Mqme0d5HUHevBgs0lLNpa2vuTG2D6J7IsBEa8mGbwGN/bteE7QusssjwI7hho/ZSUaj1XK5gkarUPLoRYjkCSJL49/tvsqd3D9qrtUcvr/fUcbjiMV/Py5LonGZ8+ntPyTuOlbS/REOhZDd3BTLg1okd1lhUp1AAj0m7RWcWI9rri6S2tnrtjD7H28Yv/bOFwTddtNFZ0uzPPshoR6QrNvQeRZevaTZGiE/ys7Suhahhdj+C1Vzqur6M51vsWT5fDxTvL+XxXeR/MqmdEi+Tuizg97ML0m0QostwHZcB0kVjZYwaikothWp7l2HezWu9G9NuUouivcoeGEayhbw81hRKf5cGEEMsxuGzMZSTYEnh779tRy+5dci+Xvn8pZ791NqXNpcw9eS4/m/Ez6v313Lf0Pi577zLOe+c8/rP3P+jGNyepJ1yI9qSDn12RQ+2UI7fTWdOR9hL81DiEqKqbjM1ORAIeXhB9UdQe1hzrmju+ra5HeO16I8HPEhmyRFiCX8u8YjQl6eoBPrJ0nNJPnuVIP3d30A1zUHYYjBbJbUsIdnUbg/G59ZR+iSwLgRE3A1EjXNOD1X2sBqKRn41YCbP9jab3z8WrZnR84SAYWIRYjkGSI4krx17JogOLqPRUhh7fUrmFzZWbuWLMFZyXfx73nHAPM3NnMjlzMleOvZItFVsYnjycoYlD+e2a3/L9T76Pqn8zsrPDhWjcCX4tBwMrshwpHDsTlCHhYZhtLDKBiGS6rqDqBvkZbs6ckN2tyLJ1Mu7UhhFx8rYOfP4eieXg7/CTS6QAsSwUcjeqYUSWjrPen74+WPfk5KzpxqCMvkZ6Z+MRiJpufCNPlNZrYJq971seDN7Wo52eCNN/rDjAXz7fF9c+bbIUOv60F1keyPe1vyLL0cmOA/9Zfm9jMX/8ePdAT2NQIMRyO3xvyvfQTZ3Xd70eeuy1na+RbE/m16f+msfPfJw7p98ZWvbb03/L6ptW8+IFL/LvS/7NI6c9wpbKLczbPW8gpt/rtLVhxPclDtow5HYT/AKd2TDCDh7hB694orYBLRjldihyt56PZcOo61QsR3iVrQhzO8mLXcGKLEtSUAyHP9bqWQ5rd93lyHLs0nF97VnWO3m/OyIYfR34k0kkkdn7auhiqevPcaBtGHWeAJc/u4JDvZx4Gf6d7eqFXFcZDN5Wi4NVzRyp8w70NLpNT6Lzy/ZUsnR3921ReksHv/Yq8AwGL7pm9M+FeWdl9AaCZXsqWbStZKCnMSgQYrkd8lPyOT//fN7e8zbNajNlzWV8VvgZ1064FrfdHTXeJttIsCUAQd/zdROu48xhZ/LClheo8lb19/R7nXBhG7cNo0WUdR5Zjn2QCL81rcUQy92Zl2aY2BUZmyx165a3NbbTpiQRoimy3nI8tNowohP8rP2FEvy6WQ0jVum4vj5YR/q6u0Pw1ujAC6NIWiNhEZ717kSW+ymS1R6Hqj1sP9LQ6y139XYudntn24OnGsYv3tnC7z/cOdDT6DY9ieLGKyh1I3jsaT2eReajDPz72l+RZS3itRgMkWW9nxM+BzNCLHfAbVNvo1Ft5JcrfsmvV/4aE5MbJ93Y5fX/a9Z/4df9/Hb1b/nk4CfsqN7Rh7PtW9Q2keXuixQrmcsmt0aWo+osd2bD0KMFMrRGa7szr6B/WsJuk7tV+9gS8p3VWW71V7c92PekdJx17AxP8NMjI8thNoyuHmutCweL/jpYR7bq7g6aMTitCpGRsO54lg3TwDCNAY8s95VAae/OUDwUNRTx+q7XQ3aswRCBtGjyazT5tYGeRrcJt7l1e109vjs90dHUyDkN/PvaX9UwrAsH6xg+GKphaIYRuit6rCPEcgdMzZrK+fnns6ZkDTuqd3DdhOsYmjS0y+uPSh3Fj6f9mGXFy/jFl7/ghg9v4Ml1T1Lrq+WJr5/gyvlXxqy4Ec6mik1896PvsqOqfaHdHzWe/T2MLFtC1qZIOEKR5bZfws5sGOFf2vADczxtuNUWG4Zdlrp1ILTG1nnVDm0OesSJp7NKH10h3IYRqrMcIcosoSt3oxqGdZC26C/Pshq6kIjPhhGvHagvifIqd6NKw52f3cmjqx8NRukGMMGv9a5I784hvApGT8XPvN3z+OPXf6TcE7z1Pxi8rRaRdwa+KPqCTw590uPtbq7YzK2f3NpnTbB6FlmOT1B21pSkL2tzWywtWsq+2la/9c+X/5xnNrR28e2v6GrQkiIPqsiypscXVTdNs02+1zcB20BPYLDz9LlP92j9u6bfxbfHf5uGQANv7XmLV3e+yrxd8zBMgzRnGj/49Ac8edaT5CbmUu4pp9JTSZPaxPFZx+PRPDyw7AG8mpcHlz/IO1e8Q2OgkRe3vsg1469hevZ0VhSv4IFlD/CzGT/jpsk39dKzjkaNIU67g/XFt3elKUmcNoxueZZ1E7tNxq7I3aqiES5+Gn0aqW57zHFqhFiy5h5vciS0njBi2TBieZa7epBTdSNmNYw+9yxbFxJxRrL6WxgdqGxiTHZSh2OiI8tdE55NgSbWlq0lxZGCqs9uE+nvb/qqskT49nr63u2qDnZU21m9k9zE3EERgbSIvDPwzMZnaFKbuGjkRUiS1MGaHfPxwY/ZUL6BrVVbOTXv1N6YahviuUj6n7X/gyIp6MbsuN7TUFMSJbb1q6/fV9VQ+e8v/5vTh57OX+b8BVVXWVq0lGFJw7h/xv2hfcf/rnUdXW9JdpQGj2c53so8Hx74kEdWP8LH13xMbmJuH8ys/xFiuR/IdmeT7c5m7ilzOWnISSwuXMwPjv8B2QnZ3PHZHdy79N521x2fPp57TriHB5c9yE+/+Cn7avdR569jYcFCbp58M/N2zUM1VF7Y8gJXjr2SJEfHJ/N4CRe2cSVkWZHllitnWYqnGkZ4ZCp6Pt21YTgUGZsid0ushe+j1hNoVyxHtru29tEbNgw5LMHPsmFYFxitkeWue5atE5ZFv3mWrQuPo8CGsbusgYufWcGCe2YzfURau+Paq4bR2Vw3VWzCMA3q/HVIShm6MbyXZt59+qqEWHhkuSfb1g2dXTWtYnlO/pywOssDf+ta1VvvDNT76zlQfwCA4qZiRiSPiHu7mys3A7ClYkufiOXuRpZ1Q2dhwUJkSSbZODVuz7IsSaHjWeQ2DKNv39eCugJ8uo/t1cE7vPvr9qMaKocaDtEUaCLJkYTeT2LZ8ixbeSeDIYE5Xr/2V6VfoRka68rWccXYK/pgZv2PEMv9zMWjL+bi0ReH/v/XJf9iSeESkhxJZCdkM8Q9BJfNxabyTRQ1FnHN+GtIdaZy74n38peNf2Fc2jj+dt7feG7Tc7yy4xUmpk/kgRkPcMfnd/Dqrle5a/pdfTLvcJEYnw2jNbIMwS5FkVFWj9aAe+TzeIxbY26jTWQ5hn+5O7fltZZoqkORUHUD0zS7FPUJP5jXegKMIjHmuNbb8G2tBr2S4BeWBGKdQ6I6+MlS1Pu0obCW4loPV50wrM3jwchy/3uWIyuGdIf+9vXWNAU96l0uGRiy33TNB7q+fD2yJAd9y879aL5hHY7vS0K5A70sUHrLs1zYWIhXC1ab2Fm9s822B5vA2Fq5NfT4hvINcYtlj+phT80eIFjCtC/ormd5b+1emtQmABxSCboxJI59RtQWjnj/+jqybL0/FZ4KKjwVoc8TwK6aXczKnYWmGz26I9BVjJbKIEo7UfaBIN7ETet13VixUYjlYwZdA1kJGkX7gBRHCt8a/62ox88beV6b/2+fejsT0ycyI2cGbrub589/ns8LP+e0oaeR6kzlvPzz+PeOf3PTpJtIdab2+jx7HFm2mma03F62K3KUmKvV96G4i2jybgaui7GNaIEcnFv3/cBqmA3DNKOjq+0+D90gzW2nzqN22MVPjfCqtra/jv8A2LYaRvAxPZTg1NazHKsaxqtrDrGhqDZKLEc2Jem3Ost6/CdCVTf71McYtb8uirF2q2F0st768vVMy5pGpbeSw0370ZrP7OmU46avIsvh0cF4/dB+TWdtcfBEPDljMjuqd2Ca5qCommChhV3IbancgiIpuO1uNpRv4OpxV8e1ze1V29FNnbzEPLZUbsEwDWSpd6063X3fN5RvCP3tU/aj6tnd3mcwX6J9n25fe9HDc4a2V21nZ/VOnIoTv+5nZ/VOZuXOCkaW+yG03HrhMHg8y/FElmt9tRxqOATAxvKNfTCrgUEk+HXGWzfDS+dCc/WATkOWZM4cfmaobJ1NtnHx6ItDwvjeE+7Fo3m45aNbWHVkVdT6HtXD9z/+Prd+ciuLDizqdpJITxP8LLFgmD4AnDEiy03GEQB88sGY2wgXwz2ps2yaJoGWboKWeO/qgUnVTbKTnADUdtDFL7JkmCWSe2LDsLRvUCwH521EnExsYTaMyKcU0I2YdZ41wwxFMyAsstzHnuWedvDr7chnx/vrWjJiq0e9655lj+phZ9VOZubOZGbOTKSEA2gD2P2zqwK/+9tt/Tvet+7VNYU8vvRznIqTy8dcTo2vhnJP+aDyLGu6EXrtNlduZkL6BGbmzGwjLruLZcG4efLNNAQaQmKkN4n87HbGxoqNDEsaRnZCNgFbQVxWiaBYpgPPcterycTDtqptzMqdhSIpIbE8LXsaOe6cUPWq/quGYbS5azg4LEXBCz+zG3XRrajy7KGzOVB/gFpfbV9Nr18RYjkM0zRZuKWElfta6iKrXti/BEo2wb+ugKbBm905Ln0cz5/3PIZpcOfnd/L85udDy0zT5LGvHmNTxSbKmsv45Ypfcsq8U7jgPxfw2s7XorbVFGiiIdDQ5rE2kdw4DlwB3cCWvJ2n9lzPnpo9MRPrPATFsmorjPnlbGu9iBbLXRWi1gnVoUghW0hXE+80wyA7uUUsdxhZbmsxsA58PUnws04k4e2uI0VCa+m46AxyVTdjXlBYEQ2L/ops6B3YMPbW7u0wm1rt5y53wc+b3umdgehqGJ1HPDdXbkYzNWbmzGRm7kwkxYNuK+udicdB5Ge3t9B6IbJc1RRAVQ4zPm0807KnAUErxmCrhhH01Otsq9zG9OzpzMiZweHGw5Q3d79xBwQ97ePSxnHGsDOAoG+5t+nOa2iaJhvKNzAjZwYn5ZyEai+Iz7McUQEi8m5YqBpGLzexAWhWmymoK2BWzizGp49nc+Vm9tbuZUrGFKZkTgklkfZnB7/wBL/BcOEXz/fKupvy3SnfBYKf3W8CQiyHIUkST3+2l1dWt0Q2SzaBocKp90DNAfjgJwM7wU6YPWw271/1PleMuYIXtrzAssPLME2Tebvn8eGBD7n7hLv56JqP+MeF/+C+E+9jaOJQnlr/VNSH+f4v7uf7H38fPSy61dOmJKpm4Mhagm5qvLXnLRw2OUq4eQl2CjKVhlBJqHA6S/DrenQ4ON6uyCHLQVcjF5pukp7oQJGlDsVyZNQ01MGvl5qSWBbj6GoYYU1JosSyEbX/4C1sM6Znub8S/CJfe83QuP3T2/nj139sd93wW93hlDWXhbydHWGaZsySizurd3LdwuuioiHFTQdImvQw/9w/lyVFS3h5+8vct/Q+HlvzGK/vep0Xt7zIo6sf5f3SR3GPfoY692ssP7ycT4vfIWHEy5QrH7R7N2d92XoUSeGEISdw0pCZABjOgk6fQ1/R3UYqpmmy6sgq9tbu7dJ2IX7xo2oaiusIkzKmMDFjIrIks7N656CKLFvian/dfjyah+lDpjMzJ/i+bqzo/m1pwzTYUrmF6dnTGZ06mmRHcp/4lrvTBfFgw0FqfDWcNOQkThxyIqZShy7XdHufochyO9HUnli1OmNH1Q5MTI7PPp7jMo9jfdl6AkaAKZlTOC7zOA41HKLR39jusaa3sUp4yi0J8IPlwi/8d1fYUrmFiRkTmZk7E7ts/8ZYMYRnOYIZI9P5fFd5MOGr6Kvgg2c+CIYGm14FXQUldgWEwYBDcfDIaY+wv24/c1fMZUTKCHZW7+SMYWfw42k/RpZkTsk7hVPyTuGmyTdx7QfXMnfFXN698l3cdjeH6g+xtmwtAEsPL+WCkRcArSLPGUPktkdjoJHlxcu5aNRFrK9YjeIqJdWexaIDi0hUTmkTZTVMA79Uhu7JR3EXsa1qW1TJGc0wQiKwbSm77glRa11bmFjubF1VV6n2VaMawSoaqanlLK3+kPuNv2CXoz8PUaXDeqGDn6UvJKlV0Bpm2/0obWwYESJUj65NHIpIx/As91tTkoj9bK/aTr2/nnVl62ImXpqmiZS0CQUT3bi0TY3oJ75+gg3lG1h2/bIOPZ3z98/nT+v/xKfXftqmgsySoiXsrtnN2tK1bRJxd9avRZIMSr0F3P9FsKRUfnI+68vW06gGO91luDJQ9HRMLRWfezP3Lg0eP2RHBrWOPVz+/tdMzZpKQA+Qm5jL9OzpVPuqWVCwgCmZU0i0J2KTXBiBdJT0L3hy3ZNMy55GrjuXFGcKUlhOvlNxkpmQiVNxdum1bgo0saVyC0WNRUxIn8DkjMkxO5GCJVAMfJqPZrUZt83dboJTja+GR1Y9wvLi5UDwgn1i+kR8mo9hScM4deipjE8bjyS1vXiL97NVr5UjKX4mpE8kwZbAmNQx7KjegW6cAvTvrWuP6sGu2EPff9VQaQw0okm1BPT0kKCdnj2dvMQ83DY368vWc8noS9psp8pbRa2vlnFp42K+zgfqDtAYaOTEISciSzLTsqe1EctWMxub3LPTeXQFF51PDn1CZkJmVPUNSwDNyJmBTw9a6wzngbj2aQv3LEccn/ryjsG2qm0ATM2cSnlzOe/uexeAKZlTSHYkA7CjJbpsmsE7dbLcd+ZlqxoGBIMeg+PCr3u5AJqhsa1qG1ePuxqn4uT4rOPjukAcjAixHMHMUem8s6GYgspmxh3+GjLHQ2ImjDwdvn4RSjbDiFkDPc0OcdlcPH3u09y06CY8qoeHT32Yq8ZdFSUeEu2J/OGMP3DbJ7fx1Pqn+M1pv2FBwQJkSWaIewgvbX2J8/PPZ/7++ayo3A9MIclp67Lge3bTs7yx+w3e3/c+NZ5mDDWV709+iL/u+hnOhPUEtItCY8uayzAlP2rDdJSEI2yr2hYS6gDFjcU0SdtIyKgkQC0v71nNfu80Lhp1Eaqmg6Si6rYuVbWw5u8Iq+/ZWbT8b5v/xrzd85CNh7HJmShpqylWv2JT+SZOzjs5anxk6TitFzzLlvhVYtTi1HQDZB8B3QM4UWQJzQgQ0AM4FEdw37rRbtF/W6wOfn3sWW6vKcnKIysBqPXXUlBXwLj0cW2WB3Qd55BFIIGq/xKlRSRYt4Zr/bXsqdnD5MzJ7e57efFyGgINrC9fzzkjzgk9bomQDeUb2ojlg43b0P1DuPv4FxmbX87YtLHkJuZimibVvmqS7Em4bC6e+Xwvz2zdx9A0G//7vRSKKpz895uljBleydi8VRxpOoJDdrC1amvo5DwpYxL3nnhvy2th4iu9Fkfmct7aHazL3hGJ9kQyXZm4bC7q/fV4VA+aqaFICmnONFw2FzW+Gmp9tZi0PeEl25NJcaagmzoBPYBqqKi6il8PkDzZ4OUj8PK8YH5EdkI2E9MnMilzEkcag9/POn8dTYEmZEnm5zN/TkAPMG/3PL4u/RqX4gpdRGS6Mjk572SkQA729CokOcD/bV+DstuLX/eTYEtgePJwJCQONRyiwd+Ay+bCZXPhVJwk2BJwKS6SHcns9QWbR4xPC763x2Uex5KiJcg8jXt0JQuPTCch4yyyXFkkOhJDd8eGJw8n0Z7Ixwc/ZknRktBrNyxpGKNSRjE6dTTDk4dT4amgsKEQp+IkzZlGnb+O0uZSNEPDJtvw634a/A1sqtjElsotuG1uTh92Oh7Vw/ry9Xg1L84x0GgqPLcphUxXJsOThiNJEifnnczbe99mb+1eTh96OumudHbV7GJhwUJUQ+W4zOM4L/88/LqfZrWZxkAjNb4a9tUFn/P07Omh388feZ45b8+hzl+HagRzJzJcGWS4MjBMA83QyHZnk5+czwUjL2D2sNlsKN/Au/vepaSphFpfLTbZhktx4Tf8eFQPafZh2DOyaJBH8GWxwQubXwiVVJs9bDbnDj+XQMsxZUnREjJdmYxMGYlhGmC4kJK2sKVyC3mJeWQlZLV7sVrlreL1Xa+zq3oXekoi9fq5KPKY0Oc/nFACcy95luv99XxZ/CWJ9kTWla8jPzmfNFcaU7Omhj4T+Sn5IbG8rWobkt2GJBk0qz4SHU5qfbUE9ABuuxu3zY09InhmmiaNaiMSEh7Vw4H6A1R5q0i0J5KTmMOUjCmhc1Sdr47GQCOqoRIwPCAHeHHLizjy57OhYQYFdbdhmAaVnkokScJlczEieQSZrkwM06DWXxtK9vRqXpoCTbjtbrISsnDIDkxMbLIt7mRQ1dCQlEbKmytpaqhkVckqPKqHK8ZewYT0CVHj99Xuw6t5Q5/Vk3JO4pXtr7CubF0wHyPGudkwDTZXbGZt2VpOGnISs3Jn9Xryam8gxHIEM0ZmALDhUDXjDq+FiZcGF4ycHfxduGrQi2WAYUnD+Ozbn3X6RZmRM4Nbp97Ky9tf5uzhZ/PB/g84Y9gZnJd/Hr9Z/Rvu+vwuVpUEEwZtyTeRaD+VRq2CL4u/5IxhZ7S77RpfDe/ve58pmVPYVLEpeDCovoIJqdOYlDGJA/pK/Pr5ofH76/YDYPiGYfqHtslSrvXVcsX8K9CSNaRkcJgS6yoSWV76QcutepnkSQF07zBWH8liavZkDtYfDCafuKMztMNtGI4uRFFVQ2X+/vl4NS82x3YUOR/VGUz+WHFkRUyxHC5iPaqHBr0M2VlCQB/T7n46w5qiHNbu2hLQumHiHvEyj65dzEsXvYgsS9Ql/Zs57zzK3dPv5qJRF1Fv7kS3maELigP1B1hauALZYWCTW4Vlf3nm2kskW3VkFXmJeZQ2l7KufF2UWN5RtRPZHhRie2r2MT0nOPdDDYeo9QftE1+VftWuWDZNM2Q9Wlu6NiSWdUMPfe7CoyG6oVPk2YHuOR4JG7OHzQ4tkySJrISsqOekGwqnDT2NxtoyoBSbOpqXLrw1NM4wDQ41HCLFkdJmfc0w0T3j8HrG8fVvz6XUU0xZcxlNgSZeWX2Ikjovcy+djFfzUuOrodpXTbW3Gp/uY3LGZJIcSciSjG7o1Afq8Wk+ThhyAjnuHGpr8vj7kiaevCmLWq2Qam81DYEGbLINuxyMkDoUB7tLm1m+p5ZTRw/hnIk51PvrKWsuY2f1TpYVLyPTlRlKgkq0J3LpmEtDJ84fTftR6LmUNZfxVelXfFX6FWtL11LlrcLVcrPo64pUstwZuBQXTWoTnxd+jonJiOQRpDnTaAg04NW8+HQffs2PT/eFysWZupMRScHv0elDT2fhgYXYqcbUktnVuIKHVn4a832XkDAxGZkykiR7EocbD7OkaAma0b3W1LIkMzF9IrdNvY0aXw0ri1fitru5etzVjEwexaMf7CYxsZbczHJmD5sdEgiPnf4Y/9n3Hz46+BHPbwnmlDgVJ98a9y3GpI3h7T1v89dNf0VCItGeSJIjiTRnGtOypnHL5FsYmTISgMtHX86+2n0kO5KDF0SKCxOTKm8V1d7q0DG/wlPB0sNLeX//+yTbk2lUG0lzpjEhfQIT0idgmAZe3YtTduKyudhYtgNXztdUAfcsCYrvP575R6q8Vby45cWoxPEbJ92IJEkokgKeCdiStnLLR7cAYJftpDnTkCQJWZKRkUN/lzWXoRkao1JHQfohPm9YTNLW74AykY21i9m9upgThpzAcZnHUaatw55eSKXfRkAfydKipXxx+AuS7ElkJmTiUBwokkJjoJEmtSl0AaAZGj7dR0APhH43BhrZUL4hdHEBcOno4Pl9bNpYXIqLyRmTkSWZzIRMchNz+eumZ0hqOfyc/uafgw1YzLbJtzbJRk5iDqfmnUqKI4XFhYs50nSk3cM283YAACAASURBVM/PmNQxnDb0NNaWrg2d94IbAhSJ5zabSNJQdnsXcvWCBTG3kWhPxKf5oubSHgm2hNCPaqg0+BvQTC30vbfJNmySDSRw29xkuDLwal6qMveSlKVz1cI/AMHvkCIrvLLjFUaljGpzoeBVvZQ2lwJwwpATALhk9CW8vedtbv/0dvIS83AqTlRDDe1TMzTq/fWh4zYEtcvcU+Zy1vCzuvTc+gshliMYm51IuttO4d6t4K2B/ODtPZKyIWsCFK6GM+4f2El2ESui2Bn3nnAvK4pX8PPlP8ev+/nVuF9x9vCzeWHLC6wqWcUtk29hccFaynPng9/Odt7hniUeJmVM4ubJN5PqSGVo0lAmZkwMbfON3W/g0308fsbjNKlN/HPze8zfPR27Tea6CdfxWM1j1Ad2A8HX90Bd8BaeHsjG8I5gR9VmdENHkRU2VWxCMzQSG25G9o2jpNrBc7edSm52DYsPLWZzcTUr99ZjT9vAnUtaT9Z22c4146/h+onXMzZtLBAUVO/sXETi2Hd55VAWN42eC3Rsj1h1ZBXVvmoUSUFL2EITk9ClRjDtrChewYMzH4xax9pek1nE7DfvCs5/DPiqfgycFzW+I97d+y52xc6YhNOQbHWsrnqXpJSgYLPEWV2gCsVdyLryYur99Rj48Du24SKRx79+nMe/fhwSICE/iYB+K06bjec2PcdnhZ+ROBZeLX6Lxg0XMmfEHKZlT+sXz5waw49Y66tlR/UO7jrhLt7d+y7ry9Zz46Qb26y3vHg5pikhSSZrSteExLIlgJMdyawtXcttU2+LuV/Lb6lICl+VfhV6vKC+gGa1mVEpo9hXu496fz2pzlT21O7Bb3jQPaM7TfCLLL/Vnv9XlmTGpEZfOIVfONhlZ0jYALy/cj1lnkYuHXNuh3Noj1e/KsTUtnNi1mzGDbmo3XH/XHmQz6p2MnnKGG6f2vaCw6t5cSmuLtWdzU3M5epxV4fKpX249TD3vvkVmHb+ddfZnJSfHhqrGRomZkxLk4VP8/Gzd5fxybYqJBQALh1zKReNuoibXvqatYdrePCyicyZJlPvr6cpEBRPhmlQ2FBIpbeSs4efzfTs6aH5a4ZGSVMJhxoOcbjxMEPcQxiVMgrVUKn11ZLmTCMvKQ+7bEcztFCku73nH9AMHqr7GAJ23v7JhW2WpbnS+OHxP+SHx/8QVVepD9TjUlwhG9BNk24Kvr42V4cBjhEpI/jzOX/u9PWHoH1sSdESlhQtYVbuLK4ceyUumyvm2C/2VHD7v5eSP0Tlye9MYFLGpFC1pe9M/A5NgSYciiP4IztQZCW0rlZ2I355Di/dPoZKbzklzSU0+BswMUM2EdM0MTA4Z8Q53DDxBvJT8pnw8LtMn7aO+QfeJmm8xMdlJglVCaG7LgCuXFhQ9QEfzbOjGmooel7nrwuNkSWZJHtSSCRbF34uJXh3wrpLcf3E67lszGWohsq2ym2cOTxYotEm27h/xv1t6mA/OONB1pdt5pVlfjAV7rsoAySNzIRMXIoLr+bFo3nwqB4O1h/kk0Of4Nf8nDL0FK6feD2KpOBQHIxOHU2OOwePFqyV/Z+9/+GN3W8wI2cGD8x4gMyETBRJ4c9Lv0aXvPz9mh/ynWcPceHxLmYdV06yPTkU9PGoHooaiyhqKCLRnki2OxubbEM3dNx2N4n2RDyqhypvFaqhIiERMAJ4VW9ovnbZToojJSRWg1HtQOg98mgeanw1pDnTcDSfTUNjCr+8ZBLDU7M4OfdkJCQWFCxgc8XmNnerHLKDS0ZfwtSsqQxLCpYnnZA+gc+v+5xPD33Kl8Vfhl4TVVdRDTV4cWNzcWreqZw29DS+KvmK9/e/T7qz9dgwWBBiOQJJkpgxMh0Ot1zRjTildeHI02H7e2DowdrL3xAcioM/nvlHblh0A+nOdM4efjZ2xc6fzv4TFZ4KLhh5Ad7qL/iP7wFqE/+F0xjOr059gJd3vMzDqx4ObeeGiTfwwMwHMEyDN3a/wbkjzmVMWlAQXDsqj/nm19gVmStHXsnja56lTJmPaX4PSZIoqC9A0pNBTyTgGY5HW82B+gPBLOWKzdhlO3LzibgdLqAJ3TCZlDGJSRmT+KfnIEurdhKoPoff3NSMJKmMTBnJ8uLlvLv3Xd7a8xYpjhRMTBoDwYikoY6hJnCEv+25FyXpGgLa7FgvDQAL9i8gw5XBBSMv4M1d73JEXY2EjNJ4DgXSZ5Q0lTA0aWibdSxxVCuvQUJiDLdTYPwb1b67W+9Nja+GR9c8CoBLSSBxnI+PS0yKfOuAG0Ji7KBnPQC6qbPyyErqzGKQNJ4+5+nQrcC/r9xOU8JidlTtYvqQKXxd9jVnDD2HzzemkjqmkFd3vMrL218m05WJM/sE1B5EwbtCZMUIgDUlazAxOWPoGRQ1FLG6ZHWUtWblkS/RvflIipevy9ZwJ7cDQR9lujOdi0ZdxIKCBai6GnWL1BoHcOXYK3l///tUeavISsgKlTy69bhbeXTNo2yu2MzZI84OlfzSPaM7TX6KFMfdrV0bmbjqsocJkrDOcPGgdbHKRUcJfgm2hLj3b5oKGEGfdGSllq74bV02F04zB1PT21xUKLISmrNpxr4ICb8bELnf/JR88lPyu/w8OiJ0kdTJRZVdsbe5owDBc097PvJ4sSv2qEZY7aHpJqaejBRwc0reKW2WWVHJdtc1ZAw1l9lDz2rzme10n5qLU1Ju445Zl3PPe69x0/GX8fD5l7Gvdh97a/fy+koPa/epnHdSPaOH1nHOiHM4behpyJIcEnqGaZBgS+j2rfsTh5zY5v+bJ9/c5v+LR1/MKUPm8I8FnwHw/9l77zA5qjN7+FTq7hlpRjmCckIBEEIIEYQAE4xINphgsMEE489gFuN1zjZO2Ot1ANbAz2sTDGtjgk3O0SAEkkACITAoEIRy1mhmuqvu/f6ounVDVXVXVccRfZ5HT8+MumJXV5177nnPe97kY9G3NVqAYjamYp/hlAFT8MkJn/TVVRF/fqQfspaOif0mwtTfhYV+OGNifdXVgxc+jsKObpww6mgM78s///Onno/zp54fax0tZos0aC6GeWPnYd7Yean3t5poPGNIA+DAUf0xsuM1kFw/17PMMOpwoHsHsP716IV7KCb1n4RfHfEr/OjQH/kEY/9B+/u+4V76MOTXnYmB5Gjs3fk1nLXPWbjvk/fhnlPuwd9O+hs+M/kz+Otbf8WRfzsSh/7fodjevR0XTrvQXz97uJm667saTk9Cp/GO709duW0ltII7R+t0ug+ul9e9DMBVDKcMmAKHGGjxbsShDUqohZNGfwqfm/Y5HDXyKPzw0B/i4dMfxlWHXYVjRx2L40cfjx8f+mNcd8Sd6HzvElw+6Rq0Z/qjdcTN+OaLF+KBlQ9IU3SAq3Q+/cHTOGnsSZg3Zh403caK7kcx2JqM3ZtdXxY7BhHu8VLsNBbh0OGHYgA9HE7nSNBcspSDl9a9BAD45qxvYs6w45HfdDQOGXgy3tiyBJq5w7dhrN69ECTfFwNyA/D0+09jM30FGmnFAYMPwNwRc3HBtAuQ3T0XAPDCh/Px5tY3sb17Ow4fdjQKWw/Bp0f+FM+c/QyunnO1e7Me8AS25N9PtK9JEdYx7PkPn0ffbF9MGTAFM4fMxJauLVi1fZX//+s71uPf296Es2synI4JWLLxFT/V4pUNr+CAwQdg9vDZ6LQ7IxMDFq9fjP65/jhz0pkAgJfWuud4ycYl6Jfth3lj58HUTSza4JLkResXoY85FNTuU1pZVlp4J82JFUl1MHOWJmrNHrVvpfZFLUqtFMSC07QWn6jmMI2ShsGyv2uZAV4pOCHfx7hIc/4ppSDUrZE4ZNhsdG84GcOzU12bS/9JOHncyWghY0DtfhidOQrfmf0dHLbXYT4pNnUTLWYLelm9quZxlROYSgyAdCv2YCeqKNwokmZUDzRSJGO90VSWQzBzdD/01ldgc9/9MEiI1MKoQ9zXd18Ahu1fn52rIo4ZdUzk/+VtAmP3ARijDcBGj5xYuuX7SacMmIK5I+bikdWPoH+uP/YftL/vWwJknzAADNPnYo3zEK555RrMHj4bK7avAAruSJ8WBmJsn3F4ZPUjOH3i6Vi2eRnOnXwuljuUk+WQnGX1ZwAY0mtIYFT72gfbAbyDYb1G4Fv734Av3HMj8uMW4pvPfRO/WfQbzBo6Cyu3r8TG3RvhUAc2sXHq+FMxvu94kEIbdGsnxveehXe6BmB0r73w3AfP+cSLwSYEeu59OPpWHDf6OPx9NYGzewyMgU9iZ36nX0BSCgvWLkBvqzfOmnQWZvTdjbsfew6HDR6I+Zvug9n2GhxyGLrsLnzYvRR2xwwcOXU4Hln9CLodCqNrqqTYEbsNTtdQvLRuvn8e9xswE8BSmIaG9kw75o2dh2kDp+HEe07E+vxyAMeF7te6jnXok+1TUml8fdPr2Kf/Pv5U4fdf+D5Wb1+NnYWd2NixHb0n7sIO5HDmfddjc+dmbOjcgBPGnABDN3DQULc24KHVD+HwvQ5Hn0wff/Bg75wMLbMV+f7PY/H6xZjQbwLe2/kezpx0pl8gsmDdAswcOjOwT4s3LMaMwTMwuf9ktGXa8OLaFzFv7Dws3bgU+w3aDy1mC6YOmIrF6xf7RYPDsjPwAUoTTZVw2AppLgVx/erDuVCmsuwTuRLrUItTKwXx2NI+fNVBiL/umE1jqg22fz2RXKTt3EgI9ZN6SinqIth2DKERh3rN1XsQVGzwWo1tsd5QjZKGUe/z30hokuUQ7Du8HVRbj6U4HFJ5WJ+9gb6jgNX/AmZ/sV67VxcUHArL0JAJaVPNMHvY7EDEEIPaNCNrZdC6cx6Wb7kVR/7tSHQUOoDuwf77j977OPxx2R/w1PtPoUAKmD54Om52bGQtVpAX3iQlTlIHi6yzTB1ZM4fCtln4/gGXwckuxy1v3IL5a+djfN/xmNBvAmxiY0TbCLcghlDYO6ch038+JvedjYexFQcNORQPrf4nLn/ycli6hT7ZPhjeazgKZCis9qUANXDkiCPxf84bcHaPhaY9gUXrFuPIkXNL7ifgqp4zh8z0vJfucQ5tHYUx7ePw9u7X4FCKl9a9BJt2Ax2TceSIA3y/X2b3NGldBYfA6ZiI1za9AIICJvSbgDbLLWgVo+NGtI0A7HZsKLwRuk/PffAcLn3iUndfeg3FzR+/GcN7D8f27u246sWr8NWZX8XQXkOxYtsKfPqBT+N7s7+HMyediWWbl+HeFfdi34H7YkLfCeijAS+t70Aua2NgSwYT+k3AuL7j/KKbEW0jMLTXUFy/5Hpcv+R6f/tDWobhnfxgoNAXpmbi+TXPY1dhFwB3arU9046pA6bi8Xcfx0ljT0Jbpg23LLsFa3atwanjT8WaXWtw7uRzXUI+5CC8uPZFrOtYh5XbV+KksScBcKu4b33jVvxo/o+wrXsb9m91vbslO/hFeJbj2zCKKMtOeXmvTohHvNg+VL7ddfnEI2w2AqhuHm8S9GRykTRfm0Ga5UswWHHEdJ+InOV6K5tyI50akOVGU5b92aieN1NSaTTJcghy3ZsALY8XtrRhlhpFNu5o0KV34PZ/vYnH3t4Bh1AMac/hE9P3wiHjBkiZr3sSum2CjGnASpCzLIItw5pfZA0dWseB+P0pB+Hh1Q9j6calWP3uOOQsHV0Fgrl7HYM/LvsDfrvotwCA6YOmwyEv+4poWAc/9edS+2IJHfwcomHuiLmYOyKaxBYIQX7TxzBvwmEY1jICwFYcO+IUrNr5FtZ1rEPeyWNb9zZs6doCDBkCU+tEprAP2jPtbqOCzpGgxMBL616ORZbX7lqL93a+5xe4sfu2rmk4au9jsWrH9die34TH330cJrIodI/DwcMORs7IIe84QJcc7VNwKOyOCbAHPIslG5fgvCnnCYMYPoOiaRr0/Dhsst4MjeJ78v0n0Wq24rNTPosblt6Ap95/CudOPhdPvvckHln9CKYOmIoLpl3gq8DPrXGV9wVr3fzuaz92Lfrn+uO2Be/iufmvo6XVwv8cE1SwNU3D9cdcj1XbVyFjZLC5czPe2fYORrZOwzcWE4BmMW3ADNz8xs1o+XeLX80OAGdNOgs/fOGHOOmek5DRMyiQAnpZvfDw6ocBuGQYcGdTnnz/SRx/l1vwxrrCzdlrDv78+p/x0KqHMH3QdAzXZwDYWNIGwQkHm45PRkAcEn0tFwgtK0IrysKgwt/3Sre7ppUgy+5y6rmpN6liYOeOUt5koqfATqmKpx0EScpyRAJPWgJfKUjHVqWW2+K2TD9nWWuIAVdPHvxVGk2yHIatqwEAr+7qh5dXb8WsMf39/ypMOgXWoj/jmQf/ig8GHoW2nIlXl63DnYs+wKgBrfjOvMk4dsqQWNXiPQkFhyDjKcvdKbKC2Y2YkVPL0GE7FEeNPApHjXSr+8cvehDtLSa6CnkMbR2Jyf0nY/mW5RjZNhIDWgagQKhfPCKRCqm7YOkvNc9ZFjv4lT4mtwCmNya1zUTGdJcb2XsCbpt3m/S+BWsX4KKHroBudMDa4VpRCoQA1ILTNcIvGCsFltTAoul4Bz/g6BHH4E9vXI9b3r0CnWQ7RmYOx7uaW6l/+sTT8cKKDfiAyA0rCg6B0zUaGT2DPMlj9rDZQuc/+Xq18uPQ1foKPtj5AUa08wpxSileWPMCDh52ML50wJfw8OqH8fya53Hu5HN97/aCdW4SBTvOl9a+hAIpYMHaBZjYbyL65/r75xMo/hAa13ecn2TC8M6GnQCeBQBcsd8P8MaOp7Fw/UKM7zve99ufOv5UHDr8UNzzzj3YuHsjPj350+iX7YffLf4dVm1fhUn93OSWk8aehKG9huLJ957E2o61fj7oQUMPwoJzFvjJB7965E0AG0teJ4xgEupOTzssSzpuh8iiynIwJzsJ4hb4cWU5/bb++eoajB/cG1OH9wmsV/05CaIIXb1JFYNsoyFSYkSjI6zgNg7Sqq9iQ6SornVOBa7FchDVNbY62yL+4MpVluuv5nJ7U5Msl0WWNU3rD+BvAEYDWA3gTErp1pD3PQxgNoB/UUpPKmebNYFHljdZw/D3he/7ZJkQisvnt+JntDeuHL4cky79AXRdQ1fBwaNvrMc1T7yNS25dhGMmD8a158wIVgUTB3js+8BrdwITjgEOvADYO+ipbETkbYKMqSNjaqmUZfalYwpmxpTtHIS4U8ytGQNbOlxid/zo47F8y3Lf+2w7pKSyHOeG5qvcCTr4udvnN3e2XJgl5eBhB6Pw3n+A9noFbc4B0rLO7jF4a+uz6Ch0oJfVq+j2FqxbgP65/pjQ1y0y9cmyrmFM37GwO8ajtV8BXz/wy3j1jbH4QN8EwC0G/MHa1/Eu+VDZf5ewT+43Hcu2LsaBQw7Eyg1uQaOqgFmFCegGsHD9Qoksr96xGh92fIiL9r0IgJtz+493/oHdhd2Yv3Y+ALeAruAUsGj9IvTP9ceWri14ee3LeGXDKzhrn7P4/jCVsMhn9o9X1mBonxxmjx0QWA4AWo2+OG/qeThv6nmBZQe1DsIl+10i/e2Hh/5Q+l3TNBw09CDfHy1CLNbx25bH9CwDrpIa5xhFFPMssw6McRrvhK47prLMiH05xYRX3f8Gjp0yFD8/bV//b6TIQCAuolTvallHkqKWHtdKI+05TKu+suuhWNc6vk+JdqliqOXnyYodAfe1Uo1YykFTWeYot4T0mwCeoJROAPCE93sYfgXgs2Vuq3bYsgqAhunT9sUDr63Frm43tP5//7UKDy/fjI17HYPJO56HTvIAgJxl4JT9h+OhK+bgO/Mm4/HlG/CVO16Vv1xdO4DbzwLmXwsMnAAs+wfwx2PcYsEeAEaWLSOtDcNTlnWuLEuE13sItmYYGSY4YcwJyBpZHDb8MBDiVk7nrKASnNSzXBBUbqZ0x5lyZvtoGbqvLEdtz873RX7zkXCIIb3P2T0GDnXw8bs+jhPvPhFn3ncmLn7kYvzPq/+DZZuW4Yl3n8DVL12NK5+6Ek+99xQOHnqwT4zY5aRr7rRl53sX49PDf4dPTfwUKLUkdVjXtUA8FzvGM8Zdgh8f+mO0Wq3+NcrOA0OGDoWFNixcv1D6O2tMcOjwQ/3XTrsTt75xK3bmd+KE0Seg0+7E/Svvx6bOTfjc1M/B0Axct+Q6X832z5FTWrX47eP/xq3z35XPbQUKxZKCZ0LHU2UBd9+SEpBiD+dCmSoPuwZLFvixfS7jYd1tk8B2iqnmcVFSWa4zwZCj/3oWwUibhlFQ1PS4sBWyHObTrbuyXGTwWvFtEcJtGIYmpcfUA1LhZgOo3PVGuTaMUwEc6f18M4CnAXxDfROl9AlN045U/96w2LoaaN8Lp80ah9sWrceNz6zAuMG9cfXDb+LjU4di4uzPALf9A1jxJDDpBH8x09Dx+SPGQtOAnzywHEPa38APTp4KdG4Dbv0ksG4pcNJvgZkXAF3bgT8cDtz3ZeD/+xdgxmsgUi8UHOJ3vEvTstkW1FwgqCyzG25rxvR/H9t7OJ444wm0Z9r9/89likTHIb0NI4myLJLlMEsKpTSywMvpGIfPTLoE3XQrdhV2oaPQgS2dW3D9kuvxhyV/AODmUg7vNRxTB07F2fucLa0XcG0YfkGM0MHPFAivoWmSR5RS6hc2juo9CdO9/HD2cDN0edxsGQb6aJPw4ocv4rpXr0PBKeCifS/C8x8+j9Hto7F3294AgFlDZ8HUTfzp9T9B13RcfsDleHj1w7hx6Y0AgLkj5uKp95/CKxtegamZOHDIgfx8+v7TaLW04PD9VpcT97/aiDsd6RD54ZrUHiBNaavqqVDEZqaY3Y9bBBdVRJd0W+p0vqq6pwH3XTdmGkYlBgT1QiFiIFIK6jUfF0FlOUiW661s1jQNw6F+Z1ajAdIwpPtsDxv4VQPlkuUhlNK13s/rAAwpZ2Wapl0C4BIAGDmyMiHxqbB1NdB/DGaM7IuJQ3rj90+6LSn37teCqz+1HzSLArk+rjoskGWGi+eMxQdbO/Hn51fj5ImtmPHMBcD6ZcBZtwGTvHD4XB/gxF8Dt58BvPA74Iiv1fAAkyPveMqyqadSTNQ0jIyhIe8QnySxh5+oLAPwO0jZxG3rWTRnGXGVZa4QmwnIMrdvaH6b7LCBg7xv6s3ewBnjLsDYQb2lZTZ1bsKLa1/E8F7Dse/AfUObabCbta5p0JnaLDxMTIHwqiqNE3G+bEXxZzB1Db0wFSs7F/rE95kPnsEHOz/AaRNO89/XarVi+qDpWLh+IaYPmo4R7SOwT/99sHzLcvTP9ceY9jE4dPiheGXDK5g2cJpkPVEVYtMIkuV8SFya+HutHigisY/zPsB9+LFjpNT9rHS9uH1C9bzK/8eV4SSNH9T1lY6Oi6eiF0PBIYFzJW42Lally6kWkUb1LPckiOcwidWn2ACv+HIeWfa2YxhhynJ9PbM1TcOgcoFfvQdbaQdBeypK2jA0TXtc07TXQ/6dKr6PurJXWWeUUnojpXQmpXTmoEGDSi9QLWxdDfQbBU3TcNvFs/G3S2bj9s8fjAcun4M+LZarAk85FXjjn0DHptBVfP3jkzC0PYctd38NdN3rwFl/4USZYeJxwJRPAM/8Ctj+QfWPqwzkbUFZ9khuEnAbBleWAf4lzEeQZXX5nJ+zHE6QY5FeT322TD2RDYPtq2VoyJhsuRCyHGITcAmOR7BDlhnYMhAnjT0JM4bMCCXKALdhaKKyTPh2DNWGIXxG0R5vWd1hMHQNA+kcPPjJB/HSuS/hhmNvwLqOdehyugLd0Njv7JVZLWYMngFN03zLhtoVzInxILJDSFc9FA+/OK7E9SU/YOSCvDi+5WIPKEYQK21hCLyvzO1Qz6sdTKxIR6qkffOPIXy2od4EoyerceK+JzmNlUjDAFzSHMzPbiRlubqDH/Ee3gie5VoOFHoCSpJlSukxlNJpIf/+CWC9pmnDAMB73VDtHa468ruBXeuAfqMBAIPasjh47AAcOm4g+rQKJOaQLwF2J7DghtDVtGZM/PDwFhzZ9QRWjD4bmHh8+PaOuwogNjD/ugofSGWRdwiypujVTfblsQVVFkCgQC7MhhG2fC5k+3mbgokgiXKWdc0n7/FsGN4x6DoyhiHtv/Q+39rAiyEdQvmxxbCKhIENUAzNrRwH+HR2wSGyZ1mTH3giUZM9hkzxl28FpqHDIRpGtI9A1shi9rDZuPWEW/HF/b+IQ4YdIr332FHHYu/ee+O40W78G0vvYJaLfQfui2/N+pYfgcf3qbRyEWrDqIN65xf4xSSagOxZZr+X3E6R93NlOSXRjOnrFQd4ZW0nYkodQGo/ZpTCHzdDutqQrCY9jGDIg9fk3uOkywXIcoiaymbO1PqLWqGWgx9bIMuNoCzLok/PmiWpBsot8LsXwPnez+cD+GeZ66s/tnnFRP3GFH/foEnAPicBL90IdO8KfcvxW26Do5m44v252NFVCH0P+o4E9v0UsOhmYPeWMna8umDKMldik315GMlghE4tkGPxb72yQeUYkEmdelMtOAStISkZUWDrtgzXVuL+rfRyYmEg2/8wlZitK2fqAvHg3QfDlokDdsi6rrlZyBp/iKjKsqEp50gg9eK5ZTdBNTrODFE2xvcbj0unXxpQvke1j8JDpz+EsX3GAnCV5SsPvBKnjD8FgJs4cc7kczCgZYC0XJwq+rwTLBQr1EHxiPLKqlDV8qRTmU6Rh3O59gj/u1ZieXGAlwY8OUTeDkl4LsLXHb5vDaMs17AgrNJI67dOqyyrs1ph95zGUparuw8BZbmB/Pf1VrkbAeWS5V8AOFbTtLcBHOP9Dk3TZmqa9kf2Jk3TngPwdwAf0zTtA03TImTWBoAXG8eU5aI47MtA1zZg8c3B/9uyCtqS/8P2Kedi+a5W/Pi+8G5o7nquAAodwMv/m2aPawI3Z1kv6tUtBttTPpkPTlWW2Y2hqnae1AAAIABJREFUxXLV16iiLsvQYOqaopQStGZN/+fSxxK0YcQhsH78nS4sF3Ie2L7lLMP3/9mE+BaTNAWSAFeRGa81dF7EZ5OgDQOQPc3+/km2lWgbRtqbtambuHDahWjPtBd9n7QfEdtyWzyHq4jqz9VEnOQO933yw1U8xjj7WiwGkZ2jci0McZXltAQlqq12JYrfChFEvGE8yxLB6FlqXFqPapitKw7Y7AKrtQj3LNd3EFSJbPC4UJuS1Hvg15NjEKuBsgr8KKWbAXws5O8LAVws/D6nnO3UFD5ZLqEsA8CIg4DRc4AnfgzkO1xrRmE3sPxe4JlfAoaFwR//Bi7ruwPXPPkOjp0yBMdPHRpcz5CpwITjgAXXu220s72D76kz/Oi4EpFpUSg4RCrgUpVZlmLBPcvhU9AuUdWlB37BIeiVMbAx5n75NgwjmQ2D7ZNZQllmNxbeQMUt9IryY8cFe7iwAYeosIelYQAuwdahhSaPiPtqqTYMPV2edhKUuhk7xI0uCpKudNPF5SBugZ96TGUpywEbRrn2iJgFfjHtGlFgsxiBQU4FOvhFRcQ1YhpGvYl7UqjFqXFRbDak6Pb8gbr7e7Gc5XqprLW01dik0dIwRDGjZ13L1UC5yvKehy2rgEwb0Nq/9HsB4PQ/uokYT/0U+Nkw4FfjgPuvdO0V598HtA/D5UdPwNTh7fjW3a9h3fau8PUc8TVg92bgviuAOucrhqHgUL/ADwiPTIuzPIOqULMHeEsm3IYhElXT0KDmLDM/cD6BemfpOnRdi11MIdo3fLIc5ln21pW1eBGjTYh/bGnJDo+O4wUxoqImxr+xm26Y/zTsZ1VZNo3q36yjig7VvxWPjqutspwkZ9lWPMtxHvhRhIUKDU5SK75xo+P8Y02pYLNrTlUJK5CPHRbhxzLYgfpPF0tJLT1s6lqybdXAs8wG/+y+Jc6UqftUN2W5hrYa0tDKcs+aJakGmmRZxdbVrgUjboestqHAGTcB590LzP0mcMIvgfP+CVz4CDDCLXTKmDp+d/YB6Co4uPS2ReHT8CNmAUd/F3j9TtcH3WDo9jv4pVOWbUJksqwU6rH19cqE+3pFC4Spa4HpTuZ1LsQg8QXHbSuqCzemWAV+gu86W7TAz1OWTR5zZ4uEPqUNg92vmGqsS8oykeLfGPklNEiQwtI61KYkYf7BSqOUasOugSjvbtRy1UBqZdkJP+9xlo8aFKT9XOISD64sl+eNDqs7MJVBXFJwIi5cOxVQrCuFemSAVwppv1fle5bd3w1NCyja5Q7cykWtCCMbDBuCJaX+ynL595w9CU2yrMKLjUuMsXOBo74FHPwFYOyRAbI9fnBvXH36flj83jb87MHl4es4/CvApHnAI98G1hfxONcBrmdZE5p4JPvy2A6Visgi0zCyUWkYgrKsB20YLR4RjaXeOVQihxkjXna01CbbjPY62yEquU2o/3v6Aj9mw3B/N4R4ONtRPMsaW8Z9FQm6uP1i0XFV71glfYbBbRWUWQd/OVJ8uWogbtGbSpYKCadxo8hWJRJAYnfwS9mcQl0+UOBHKbJKZGTadUcRtHoTjJ7s80ydl5xSfeVpGFxZVpdn72mINIwq7gNbNRNCGkFZroco0chokmUVMz4LTDu9Kqs+ef/huOCw0bjphdW46v43pBvArm4bi97fjgfGfhd5PYeux39alX1IC+ZZTlvgF7BhKJ5f9mBtjbBhSAV+hlrgR31FOg55yjuyym0aMZVlMQ2DDRrCCvz8TGjZspIksSMMpIQNQ/Qss/eEFT6FTRWbetCzXO2pt1IFX1HFXGFpHtUGL44rvj2HEJ8QpvEsy8cWPi1eq+i49HaPiBkBQv3vffoCP3lwzdbLUO+HeljxbE9BrdMw2HvFFs/q99kJmRmrJWo1+PHPhdFIaRjpCjf3VJTbwW/Pw6GXV3X13z1xCigF/vdfq/DaB9vRmjXw7ubdWL25w7cqX2kehyvevhu/vvUuTNjvEBwwoi9G9G+t6n6VgtjBj/2eBDaRC/zUNAmVLEcRpLACv7zN/cBsfd+4cyn2GdaGCw4LFmqyZA++L3pMP6lgBTF06FqJAj/PhtFVcKRjS23D8A6Z8Vqx8YjqWfZtGKGe5eDUoto9rxaeZamApIhnuVhTkloRkrg2DJu46mm3Tcr2LEuKnfRzygK/mNFxfrvr1DaMoFUCcD3LbJBafoGfMKioQ+52FHqysuykVIhLfY9LLadrIkEMV5YbIg2jivcaNXPa1IOWlFpDLtzsWZaiaqBJlmsMQ9fww1OmYmT/Vvy/51ZiQO8MJg9rw2kH7IV9hrVjzMBW6N37o/PPj2K/d/6Azy/LAQBOmDYU3543uS6kmSlkVlnRcbINw1dmA8pyuA3DL/DzCvLUnOWsFwPH1vPMvzdiR1chnCzbssptGbqfxlEMYs4y4KrjpaLjAKCrwGwZ8ePtwlBMWXaI3JTE7/DHmpaIaRghKmWcnOVKo9Q0X5RtIEp9rSZ8AlmCjDmEImMaAGwvBSXZ1HakvSBlPJcIv8AvBuEvZzucbCvEh7rfu7DmE3EQVeRoJ7S6VBM92rOc1nuccrqenR42UA+zHtQ/Z7k215bfyMofODRCGkbj2JsaAU2yXCdcePgYXHh4VDxdG3DE5Tj26Z/jqTMs/GPLaNz47Eo88eYGfH7OGFx65Hj0ytbuo2NkxS3wS9mUxAkv8At28ItQln0FVA8U5LF1W4bu/72z4KDTU3TD9kVVueMV+PF9cJfTQ1NB1DSMTkVZTk+W3Vddio7j25Q9y4qyLA4u7ODDTe3gl5bQJEEpYhFpw6jDTTxuRrHtCL5chyZWGiXVPCK6qdzouFIqUbme5WJZyLruDfJSJP5ERZQ1kmc5ajagJ6AyHfySK9J6SBQm4N672GXSEMpyNT3LrHi7YdMweta1XA00PcuNikMuA/qPxZinLseVs/vgqa8eiRP3HYbrnlqBo/7raSz7cHvNdoVZDTIeIQXSpGFQhaBGKctRZJl7uizFIsD80C6Jdv/emXfQmQ8ny/mUNgxVhc2aeuh5sAPKsmLDKDs6zv1d17naLAbaA8WVZTk/k1lLQpTlGnqWQwv8BBsGpeEPrVpND/o+3BIPDYdQf5DkBGwYcZTlcCUraapGGKIi3YL7EE524yLKPuNeo+mVZVn5DFfa6/1Qj9qvnoBae5Z5UxJGEPXIgU/9cpbDr7lKw1eW9WhLSq3Rk7tRVgNNstyoyLYBZ94KdG0H7rwQQ3sb+M1Z03H3pYdC04Cv/G1Jau9rUrDtiNFxyQv8iFREFlXg1xJhw/CL63Q9UJDHCvYyHnm1HYK8QyKVZVspNjRj2jB4gZ+7bMYIt2HwdtcyWVZ91UnBbty6xh8u7CbmNiURcpaVNAw7xHoBcBIWzFmugbJcwk4R1RmsHkpi3Dg117PMIgNJ4inqQgQprkSLb18xLkG22bbKTcNQz5VDKHQNsXPNA/slziZF2lUaZ+q63sQ9KdJ+r9IWgtnKvUdXCKKsMsdebUVRK8IY6lmuN1luIHtTI6BJlhsZQ6cBJ/8OePdfwM0nAzvWYsbIfvj5afvirfU7ce2Tb8vvr1Izk0KIspy4wC8krg3gxJE1E8mE2Czc5XkhmngjoZRKsXYFh2C3R06jlOWCQ/zoN3eb8VRUbsMQPMvFCvw8hZF5lqOOLS5UG0bW1NHtHauYYSu+h9kwxGYtYSQ0qCzHi9MrB2I6QniBX/g+S/tfqwI/v2itlCorp2HIcVylP/doz3L5KlvcAj9fWU5rw/A7BSoDXkJ8ZZmUbcNoTGW5Ep9TvVARZTnBvY03JQkniPXo1KmiZmkYispuxHwmVRNNz7KMJlludOx/FnD6/wJrlwA3zAGW3YOjJw3GaQfshduffhV3LXzffXC89yLwXxOBX08G7jgfWLO4YrvACG1ZBX4kXFn2/Y22TMjVL2dBIHVizrLj+dq4Z5n6JDlKWQ5Gx4XbKVQUBHUbYIWBYSRPtmF02463Hc3fxzRQc5ZZ6gLgkgcjzIZB5PPL3uv/TNzlNCUXvCaeZYcgVyRKTCLIis9a19zzULPouLh+XyJnCTskWbZw1AOqEgME9h2K47sGKpCzrKZhEPe6SmvxiY6Lqz+p4ttvHJU7KdIq9Ok9ywpBVJRl8aPc0z3L7HzrDaQsy9avnjXwqwaaBX49Aft+ChgyFbjr88DfPwfsNRO/2r0FRmYllt47Bjc+ehS+QP4PRvtwYPgBwMqngbcfA866FRj/sbI3H2bDSEr4Cg5FzgprSsKUUUE5NrQACeXKsmvD6PKW8wmsl4aRd4hPlruKFPhZuuhZ1iQyFgVR3Qbg2z4C7wsoyx5Z1r19TGnDoIoSk7UMf92qsqx6lkUSkVdIh6oqu/tafWXDIRQ5y8COLjv0ehJVezXBw9R1UNCaKR5xC/xcz7Lh/1xweJRcnIefE6FERxHnJIjfspvbMCilgYFUKbD9o9RdBx+48c6ZaYhAtNLu/pw19foTjB48dS1eV2nTMNI0JZEJYvjAZ09Pw1AzpxstDaOnXcvVQJMs9xQMngxc8jSw+CZg/nUwBowD3fcMjF90G/br+BP+jZHInnoXRo0aA+xcB/zlU8DtZwETjwda+wP7fxoYdWiqTTPSYpVV4Feq3TX1t5EJUXpFFUKMNVP3rWAT7M6XsmFQn8iyZXfZdoxj8PZBIMuhaRg+WZaj4yxDi7RuxIFqw8hZBrZ3FgDIpER8j2rDyBh6IM4slCzXwLPsfg6cWAb+X4y7UxqRmIYGQmv3EI1b4BeqLFsG0GXHVpZZJGFkI5lyFd+YTUnYttRW6KWgWmYM3fuMKVeW01xbUR52P33GrD/BkKxDPcyGwYqEbcU+VHq5dIQySBDDPctZU69b5nDNcpZDLCmUuvdvPeT+XAs0UspMI6BJlnsSDBM46GL3HwANQOvcr2H94vtw8UM66N9W4e4vDsegtqHABQ8AD/wnsO51YPVzwKu3Ayf+N3Dg+Yk3y5TQbBkFfsF213K7aG710AJNR9jy7v/rUsOMgqMsRyg6Cy7x3V1wQpUx2yGwcvzSt2LaMPx9LGHDYMSGFfgxO4ihewOBsgv84K1fx4a4yrJfQGkEFCQ1No7tK0uhSKosxoWrLDNiGT3oAGQbRsGLyTNo9bOg1X2Jk7PMPcsu4c1Z/PeS23EoLF2DE8gSL39KNG6Roty0hsIbz8TfjuI15wMiNwtc19JV+pfyLGcto+4KWE+O22IDPTvvJFOWy7Rv6EKLZxJC0NzUoXopy67li6K6li+1wI+92oQiUyey3Czwk9H0LPd0GBaGHHQafve5udi4sxsX3fwyOrptINcHOP2PwGUvAl9+DRhzBHDffwDP/DLxJtiNKuNZHYDkBX6BnGXF++z+v+udDWs/Ldo0LEPjxUqSsuwux5RlShGq/OYdtSlJPNJlEwJd49OG0dFx7OGt2DAMDVYZyjL1PcvchsGOz1WWQ9IwvE2J0XwF6YFEIm0YAFezq4ECIWixWHvzEGVZSTxhYLMURkh73GpBVGVpRHEapcyjzI/J9WXHb3POZghUla0SU9K8sUp8ZTmN5UMuapSJlK6732+S4hiizgG7BhpBWe7pnmVmIUpjp3B/TlDgp8zURaVhZC0jVS53JeCKELqvuFdtO0osKXutJ0ntycWq1UCTLO8hOGBkP1x3zgy8vmY7vnT7Ylk9yvUBzvk7sN/ZwFM/BRbdlGjdYoEfU1WTF/jJOcuapvkeY/b/UiSb8pAWM47FyDSmODIbRt4mkv0izLestrs2Q7YXegyOrMKG7Sd7HxBiw9D12Cp2GNh9k6kOOVMXPMtEmi73bRiUkzzAU5ZtmcyosXHiNip5k6SU4sZnV2DNtk4A6sM5ZFATkQ3NCKWpa6kTG5IijsriKIMkx7dhxG/x7LaF12HpGlS7TNjPceGmxjBlufjyBYf4M0hpHtZS8opw3gilfgfONMQjqsBPtGHUWwFTi2d7EmxCixbcFluOIcn3kS1nCMpy2NR/PT9XcfBazX3gySC691r5+29SNFIkYyOgSZb3IHxs8hBc9YlpeOqtjTj52udx/9IP+QVvmMCp1wLjjwHuv9ItAIwJsYOfrmuxO96JsL2iLBGiJSFvc4XTDFF6HV9ZluPXuGdZ8wvuxBSMsEQMtYNfmEc6DAVvitxfLqLddVRTEkPXvGzmdDceojQlyQkFfqpnOZCGISjLaoW3FWLDYMS7kg+I7Z0F/OzBN/Hg0rXutgXPchh5kvyfig3D8gZNtfAyEkJBKATLSPg2xYc7+90lIPHVOuYbVQmlnDGc/AEaFckV9d6cWfxYiyGS5DPioVWiwC84eMmaRt2r9lXPek+CQ7hlJsn9XfQep2lKwq0H8swAu+dnPLIcNaNTTTD7oFnlgjt/4ODdihtCWRY+1552LVcDTbK8h+Hcg0fhd2dPR7ft4Eu3v4Kv/X0Jv8kYFnDGzcDgKcA/L3MbnsRAt+Andl+Tq6PMZiFCtCSIalbY+iVlWSg+UzOgCw71bRgApJ/9ddlKdFzMJgk2IbBMuTCweFMS970sOs4q04bB7leamLNsE181lHKW1Q5+jjsYUc+tGjnHwBSOSnoFmcLeLSSg+KQsZDtygoeiLBsuoaxFERXbRkuJKWqRtLHfZWU55uyFrsE0inQzS/GZlOqWyECpS/CzRewxJbcVkY9drkonFw6GKJBW/ZVlKSqwh6lxtkNTzShIBZZJPMv+PV33XuXrgn3cbBawHp+tQ4h/r6luGoZ7sL6ybNR/wCXam+r9vWoENMnyHohTp++Fx66ci8uPHo+7X1mD//3XKv6f2d7AKdcAuzYAT/401voYUWEPgShFtRhUGwYgK7qip9ky9MB0nlrgx1M0oj3LQHgiRkFRU60I73FgOUUdz0QUngSVZa6KZwwtdYEf8/ipyjIvDuH7ZihpGK6FJIQsh3wuQHWUDUaSeTY0LapkSTYMJWHB8mL4anETV201UeplmLJccIhPnuMMPGxhIBCds5z8+pE/8+jl1YY6qfKQo7pFCqp5ucpyWAJDY3iWSSrfbyMgrWfZJgSa5t5HE3mW2UyZd9syVOsRI2t1PJ+scLraUZrssBvKsyx4xnvatVwNNMnyHgpD13DlMRNx/NQh+PlDb+K+JR9yhXmvGcBBFwEv/z/gw1dLrqsgeJbZaz6haqKquWw9InHi6w8SSnZDNnTN9XMSTrIB90bNmot05nkMXLRnWbVhxCAyijoeNzqOEXZT1yKzmeOA2zBYdJwOQoEumxc/MrD3sJsta8TiDiiCBEZFNTxz7Fz5nzkRkyJK2DCUfQ4rgqsWVLIcda0EPctESvyI51kWCooi7AxpfNpx/Yf+tWtWRllWvd665kXHpZhSj/QsC4p+vRUwlmaiZgb3BMhJLknIskgoky0HFFOW5cFnfZTl2niW2feE3beZ2FFfZZnPkPa0a7kaaJLlPRi6ruHXZ07H5GFtuPz/XsHZN76IN9ftcP/z6O8BrQOBuz8PdG4tuh6mLLMpukyE/aAYCiRIlsVIoLzgI46yYUiB7YF8Zs0nvSU9yyE2jDgEVlVh3fMQXD/bNzZt3xXo4FehAj9v/bu7benvAFdr2DKsqNFScpbdGLbaeJbZNeO36BZixcI9yyTiZ+p51/VUKmtSqDMFUQMIXwkTPMrJPcuuLUbNuS43ximKaAa3r6joZdolbHVgVsaUNjsHmqauV07DqIe3lUGaGehhNoyC0Ko9aRqGocudVeMuBwjKsiEPotSZmnokYsiD12raMLyBgyFHx9UrXxrg94ycZdQtuq+R0CTLezh6Z03cc+lhuOrUqXh7wy6ccs3z+ONzK0GyfYAz/gxsWQXccR5g5yPXIfqCgejOdcUQ1vzCEsimmFARZsNwhHbZlhFUljNJbBgOlb3HMb12aue/qAYjtjK4kDv4havRcRDW7hoAdnlkWcpZVtIwmA3D1OVZAYcEveQAt3RU8gERsGEQocAvzM4SQZbZPteicQrbTwBosYqfk4AS5kXHpfIsKwVFssqe/PqR49yil+cqeno1T9zXgNdc11MTSVH1DouRY9P19RSXbUJh1SBqrBpwHB576CS4xlw1XU8808MLAz2frhalLLN9qqOyXOV7jZqzzEhzfdMw5ALLjzqaZPkjAMvQ8dlDRuOxK4/A3EmD8JMHluOwq5/El15oxTOTfwCsehbOQ9+IXN6PjjMFm0SCmykrGlKbX1gmtwQUStgwCg5XdcW0DNmzzGwY0coypdS1JIgNUnQ3wq6UImU7QWU53LPMirTc90od/MpRln3PMs9ZBoCObp62waCmYTAbRsZUfYHh0XGMeFfygd/tF/hxsuxma4c/FKQIspBUhVoREvZ5tWSKq63smswI6pybjpAwDSNEfY2yZMSFHXEuA+9T1PFUxLxYGoZnpUqjEopEXrXluPuc3mddKfCZgZ5HMNzCzjTKslsEZybMPedNSdzf1e+zWDiYdJ8qBW4xqa4f3ifLmqIsN4JnuQFqARoBzQ5+HyEM6J3FjZ89EPcu+RCPvbEei9/divu3j8W3zRNxyaI/4UtLR4GMnoNPHrA35k4cxLv1hSjLSWwY7ItmKaQsWOAXbcMQ22WzGxel1I9hE3OWd+cdfx9VZZndfOSmJPxmXKy1ry2o2wAfcauxbcyuwVRoKTqujG5U7H4ltrsGgA7Poy0ORtQ0DOYJV2/6TBVSwW/W1fAsO34DD1N3s7uT2DDYPtsgNZnq5ukmxQv8+LXFya5TQj1XUfDUV1OngUJMwCWKqYruHGYlKb68uB3xmJJAvr6CyrKp66H2qFJg12KL0qlPJVX1JBisu2Rca1cjoVzPclJlmXj3Tc0niLrU4pntQzmZ3+WCDX5olbfPo+PkAr96e5Z1zX0+Nj3LTbL8kYOmaTh1+l44dfpeAIC12zvx6orJ2PboEvzAuRGnrhiPB19bh+kj+uKOLxwiEWPRJpEk/syPCCpS4OdmH/P1qzcJ0cYh3kh4BrTX2Y+4nuUBvTJYu70r8FD2Pc6KDYNto1hrX1f9lgv8AFd5Z6ojOxZLdzvMAWL0nmsVSer3ZlBzltmDraOYDYNwBZ61BFebfaj51+6+VkFZ9mwYeZtIGZ6moYUS0GId/ExdB7R0xDEpVM9y1GCHP/B0P9bOJtQvJo3rWTZ1DY4R3u66JaV/kFtJjFgFfsWaxZSC+FmJdiqXLAc7tcWF7KEMS8OofwqFQ7xBaY0sQpWEZItKQpaFAUJSki2KDKbwPckIsw/1nDEQmzZVc/tqN0Nmg6u3smyWYZva09C0YXzEMaxPC06YMRZ9z/oDBhXW4LnZL+Hnp+2LV9/fhl8/+hYA3jCEqZVR+cJRYFFSqmorep8LDvXJuBlCKJl64f4/v5FE2TD698oACNow8sL7Gdh6Sw0AWHc1BrYOdV9ZDjBbL/csl1fgRymFpvGcZfZg2xVW4KekYbBoPtHv7R5TeHRcdTzL3IbBC1qivY5RhWJsn6tddCNuDyhd4Cc2ZzB1fg2zRjpxPctu9b06A8BV1TRFjbZEtqOXd9Sp71SWD4Es2/K1Znh+3nLaXWdVZdkn+NwrXi9wi1DPm7ouT1nWA9dsKRBK/UE9ELQeiIWbSfepUii3KDUu/IG21ljKsuE9s3rawK8aaJLlJlyMOQI44LMw5l+LT4/YinMPHokbnl2Jf729SWoYArAUixTKcogNIy8oy4xMZ4ygYmhLyrPmLxMkyxQdeRt9Wy1oGtCVV5VlppIHFeJSJERt/KHaVOT36f57GWE3DT2yKDAOCOUkGOBNT8IK/IJpGF6CRMzouKp6lgv8c2MDiDBSVnConyiiEmfWuKOW0XEtmeKNWtg1y6KmuoVIv7hT1I5nBVJVuoJA2NNEx4nqeLH9KCgqeiobRkR0HGt3rWvplGXRsxzW6a0ROufZXvMfNTO4J8AmvMg6sWdZ92b2Ej4XxHsPv+cQ//8BPstRD8ImDvBq4ln2E58qb4NLCtbp1p0la5LlJlluguO4q4BeA4F7L8d3T5iI8YN74yt3vIp1O7oDHt8kU8HsBqraMERlOS/EuamEDnDJgl/gx26qDvWLwCyPiALAjs4CWiwTLZYR6OBXCNkXZkModUy2I0fOZY1wssym0tkNT1SW08Tu+eulFCKvzfrRcdEFfkTo4JcJia6Lio6rjmfZ8V/FqKRiynKvbAhZJlRQa6t/Ey8Iqi4Q/dBWlWU2OPAzaOM2JWE2DqURid+9MoVyWhCIR6zoOFbgl8YuEWIfAeB3i0yvLPN9C/Nzi50T6wV/1sPogWkYwoxN0kK9JANCvj3iz1YCwj2L8P0BGkBZ1mvRwY/dO3jmNFDZmb00+8TuXU3PcpMsNyGipR8w71fA2iVoefgr+PNB72Ns5+t4a+kC9DW6/bclJXzs4Rlody2sxyZiGoYebEoixLaxVqAFQvz3seg4ANjeaaM1Y6A1YwQ9y0JBIN8PrlQXg2pZsEzPvhHYV/d9mteAgXfwKzdnmfoWDIAXYXFlOdjBj92EbU/tVrcfFR3n+werZMMQ25dbESpcweFecJV0sQdYLYqoAjaMEh38mI2CDQ4MXY+djmAT1pZczVnm0X9pvJPcxlE8m1osBATSDZYKNp+JEj8fQuHHcKU6Bua7zsg2jMZKw+CzSj1t6locqCWNgOOkKsFyVFaWVW8w24c0anelYLN0k4QZ0kkRyJxukDSMcqIe9zQ0C/yakDH5FGD6Z4BX/4IR+Av+agIwgR12b+CVHcD0c2K3h2bwlWU9TFmWC9AALyUjYMPghRaWcCPhHfw0nwDv6CqgNWMgZ4WQ5RD/NFuu1DEF2l0b4fFaYjtt8cFj6a76TSj3gyUBpZCUZab+dYQ2JZGV5bxD0J6xvKlSkXhGRcdV/gEV6VmOKIbK2xStlnuLUgu6TEOHppGaPEw4gSyutorqkCnaMJiynNizzK9dVjQaNusSByLRJJQnDkRi7H4CAAAgAElEQVQdQ6lixuLbImixDLeQU7FkmLoWyNONvV6ByIc1WcmWkeBRKdgOgZE1YejpU2/qAYdQUMqv1SSzF+WQ7DBlmXuW6/+5soJNSmuThuEry1UosE4Kx7e79byBXzXQJMtNyNA04BPXASdcDWxdDbJzPW58dDHmbrsH7f+8FHjpBpzZPQ5vdWWAO2913z/7UreFdgR8FTFEWfbTMErYMMRYN2ahsB1Olk1PvQN4OkWLZQTaXasNVth+iPsZhbB212x7IpiHTzxGTXMJrEjMDb1I9EYICJELYnh0nONtK5iGIRX4CYVH1FOpRUVfBFd5KulZdvxX9jm47cuDTWjcbRPkLB26psTIeaQLqI1nOdDVLkpZdkRlWeP2mwQFQkylc3Q1Z1koaiwjOs73gBOCbMj1F+ialsaG4VC0Zgxs7yxIg15GjtK2uxYtIo6yXnef65+Gwe5TVsLM4XpD9NsnrQXgynKySEC1XkKtk2gMZZkiZ7FUo+Rxh3HBvg/cs9wAAz/CB+5Nstwky01EIdsbGDoN+tBpuGTc0bCdK4FXbwaW/g2z1j+EOaQT+GAk0LUDeO3vwOg5wMjZbqHgmCOkVdm+mhvW7trzLAtd9Vj1rah+iUkUom2Ce5Y1qWivxTLQkgnxLJdtw5BzlgFXKSWEYsXGXZgwpE0qBORquLzv3TZv9RwXhMoFfmp0XLE0DJazzKfHKTKmFqlwm77KU0nPclBZZpnEUTYMN4ZLURKF81uT6DghScLdZgll2fOsBpTlGGodU81ZQxN/H7zrXz0XSY9BzHzOhtz9A+9L5S3m9plAu2vdTdUp23etXA9AY+Qs+x3fqlwQVmmofvsk36uCw1u0213JvM5ynYX8+TFfOy/wq/3gg12zFNW9rhxBPAAaJQ2DeP0CtMBM70cRTc9yEyWh6xoylgkcdBFw0aP49YzHMDV/M+4+4iHQLy8FPvZ9YPdm4LlfAzefDDz1M9cz4CEqDUPMHBYrsX31VfiChhFQlrNsef5gkQC3esqy2pSEFeOZqWwYSuc/g3uWH1u+Hsf99ll8uK3TL5QR38N+D/NyxgXxouMY1Og40SLCfmQfQ8EhsEye0CE2gwlNw6iyZ5krWXqkklWwPYKv+KzF6LhaxITxDn7F49Si0jAML3M7zoOPRSSqyQIFh8JKkTggrhcQCH+JY8j5rb3TeJaDKSaEUH+wlzZZQLJhSMqyO3NjGcU/n1rA/T7pNYs1rBRkv31C77HgWU7TlIQhSlnmg6DYq64YbKc2aRhqU5KGSMMgQnFjD7qWq4UmWW4iMS6cMwFTRg7GV+5Ygsvuegfdh3wZuHQ+8K01wAGfAZ65GrjvCr+sWYx3EyHlLNuctHGlVy7qEjv4uX+jkn1DXH9LxkRLptI2DLnALysQ33Xbu0ApsHFnt//ABIJKQSYmMQ8DobLHr5iy7N9sWRoGIR7ZkgmFSOxFVNez7Ag+bqa6Bs9H3osuUtur24yQGLWJNBJTGNzfwz+7YBoGt8dEdSkMbMtT6dSpTxadaKacElUbq5Q6hnKU5YLnWQb4d4pdh+zhS9K0uxb2LVD8qPNc83ory6wQsydNXTMyZBnJiT6fri+vKYlKEHkkYPoGOeWiVmkYRPh+iK/19yzrPTLZpRpokuUmEmNonxz+eskh+MbH98GDr63DZbctdhXiTCtwyrXA4V8BFt8MPP9bAELxQohn2fbsFgXFhgHIqpZoF+Dd5Yg/VS8uBwjKskKW2UNA7ODHVdRSNgy5KQkr8MvbBDs6CwDc4kJb6PTHCzbkfUwTH0colWwYuu5G0fF210U8y55K66vhkrJcI8+y5/krONQ/fn/6NkxZdtzZBjNSWa4NIQkU+JXs4MfSMGSlOVZTkgiVjh2zlbbAT8goFvc16n3lxLDZDhVsGIz4eOcmRcSYv16WFKK754BSPuBjKRvsffWC35TE6FlT19K1KxR0feK653HNE28XXdbvWpiwKYlDopqSyPtUz3bXNiFeg6nq1kdEK8uN4FluFvgBTbLcREoYuoYvHjkOP/nENDy+fAMuu30xtncW3IK/j30fmPpJ4MmrgFXPSkV4IsSmHnlROQ7JLxZbTZuCEpx3xMg5fuONIsthKrcVsr0wsKnwsP3f0eWS5Z1dtkTs1WxoqywbhuxZBtxK8Y6QnGU1DaPgq7RyvFZUUxJ2LqvhWQbg73MxJcv3WUd4lmvV+CFugV8gDUPI144bv+REPKBsh7ppGCmj4wIFfiXi77Ll2DAcQVkmnNAC7iAu7cNXLDgCeMMd1kGuEZRl9jn1NIIhz4pwYrhy4y6s2tRRdNm0pEqtl1Cj44KRgLU/nzXLWXZksizOntYLzLPcE7tRVgNNstxEWfjM7FH40SlT8cTy9Tj+N8/isTfWgwLAKdcAA8YDd5yP4UuuxTBsDra79ogb8xWzAr1MmA2DcAWUB7YTv9kGIBPgnFfgF+VZlqPj4vlzxS6C4nJ5m2Bnl6vu7uxyEwC4ZUTet4yvLCe/+VClKQngHmdYB7/QNAyDJ4awQscCob4iJ8J/cFXSs1wQyTK3jkQpxMxnrdowWBFctX2EDLyDX3Frgur75AV+euz4JZZOYuqK9cR7cBlGOi+sb2EIKbyT3sdU9DKSJfxrTTgGsdo/fXScd26UglxxgJF2nysFm1CuRPYgn6fqt2efT5dNpEFuGFgTpqSFgSpZVgc7AWW5DueTty9Pl0ITF/73w7tvG0YDDPzEY+9h3SirgWYaRhNl4/xDR2P6iL742p1L8PlbFmLUgFYcNn4gtjlfxUXdv8eBS3+Dp7MWVm8d46ZmeGA3QW4jiLZhiH5h8abKCJW4HBBd4FdMWS5Z4Kf4e6OUZTG7WPUuZ1gjkxQ3H4fQgLKcs3Rs2ZWXtgEE0zDc9AtuwygI6o0VYsOojmeZfxaidcQ0NHSFxDLlHe6zDouOozVqd81bQBdXW5kKz4gD88sz8hTHX80IhEquWbvytJXptqIsRz34uS84vbLFLSO8AQojOqauxS52DKzX4W2VxX21fbJW/zQMOyVxrDekJBdv3wlx7VJq3YcKdr8zEw7kopuSyLMRjaAsU1pdsu4Q6seLAg3iWWaqetOzDKCpLDdRIew/oi/uu/xw/Oas/TGsTw7/eGUNNuVG4nzn+ziB/h470IqRz38LcGx/GUZSd+cd6XczhLxKaq2flkF9tQngRBTwyLLXwY9SkXQUIcslbghiF0F3e9x/vKPTPa4dnQV/WhgQptUU9Tu9DUP+W9Y0hJzl6DQM3hmOkyBKaWR0XHU8y0EbRrFuZ2HRcYSw5gl6zRSPuNFxas6yGh0Xx9LC0knUqU/bK9A0jXSKpZqGEeV7rkRmMesWKeali6p7Oe2u3ahBWWnkjVwagGB4x97TCEbBv3a5hYRdv2EDWRG83XUKz7KkLMuDHfVaTFMUWi5Yw5VqF7mxWgWGRkjD8L/HPcxSVC00leUmKoasaeCTB+yNTx6wt/+3O15+H1+/y8aP9PNw7aZrgJduBA65FAC3JDAbQTEbhuOEFPgpXmcpDcMykbPcbmV5h/g3XLbOMBuG2mJbBIu+ktIwxAK/LlbgZ0vNS/xXXbZhFNtW5D4o7a4BrgAC0WkYlHoFlIYukXXeNaqIZ7mCZFQky7slZTk8O9h2KCzTzc/2U1OYemtoIFQr2o2uUgj6fYsTTaaWq7aMcjzLBYcX+KUlsICcsxyGirS7dggyXkdNpq4SKpAxLf0xuC3b5dmRYBpGfQv8WDZtTyIYwZxl6ivKXYXi59MhPF6tIp5lZYDFRJC6Ksuo7oxFVIxevWdJspbpD4JoyPPno4SmstxEVXHGzL3xsX0G434yG52jjgae/AmwZjEAXuzWuXs3hmKzXzwXpr4WCC/wYzeVgtfBL8yzzDr4AZCsGMWj46IfCrxNtrCcYKlgaRhqgV9AWWaNTFKQUEoRUIGZtxSQSa9owxAHCKLfU2w5raJaHfzYLoo+6yjVlQ2ERBuG+FD3CX2VFSfedrdEBz+JHMvXVxzPMqVUIn5qXJ6fOJCm6E61kpSIjsuWGBgUg1hwx7zx/Nzw5hNJ1WXuoQzGH7KUDfHv9YBNePRfT/cs+8pyKRuGN+uRtOBWjY5j9yauLHvt0fX0A7dywaw/tUjDCEsGqbcNg80EAbyg9qOKprLcRFWhaRp+feb+ePj1dchN/D1w0zzgppOAU36Pff69FA9l7sakv63Fizkb7y05Adj/BrR1vIfLjH9g4MIFwHuDgX3P8LN1AU5Yfc9yRBpGq1fM1Flw0Nf7O1N0RdLrE8gidwO/ZbdwcxdV4h1RBX5KoV+5ynLAhlFCWSaE+g9ClizBjoenlATVgmp4lvM2QXuLhW27C771hrUpDyMWPDqOT+fz6WINhHLSlLAZYiIEfLwR50RNw2Bg5KlAipMOtlrT0OF4FhlxH8wUvlB/+Zid+fz8a0ODrqVsd23z5BXfGy9M8/tRjYQik2BGwCbujI3avdH2fPcq2aoH3H3pwZ5lnRcncmW5+HXLZv3SKMuWcP/yB/iUD7BEslaPwUfNOviVsKTUA+yeI6aUGHoVb7QNjiZZbqLq6NuawdmzRrq/XPQY8JdPAXddhIkAXsRkvD3+Qjy1fC0uWfsg8Nv9MKtrG2ZZAF71VvD8b3GQ8wWYxl4AID0sWX4wEKIsZ8KUZT5VzhCHwDLFRCbZOnQtqCyL7Zj9V285VqzSlYIshxb4ScqyQJyFBw9T91iCBDse0Tagoho5n902QXvOJcu+smzwaV8V3Hqg++/nbbJ1UMrIEgFQvZu473P3O3nFUZblKVUrhmdZVvfkqU+/wM/Qyyrw43akiGMQjzXltthg0RKIvdiURC0+jX8MvJCM/e6uh8ikqk4Eg/np2YCgJ9kweBY+9ywzr3IpG4afe27EK2JlYPYNBtVG4zgyWatPzrKXFkSjZ2MqAZbww9BIyrKYFpX9CDPGj/ChN1EXtA0FLngAeO1OvGxMx9l3rMOXBo7HtfY72P+Yc3DIh7fgg15T8Mn5Y/HfFx6LOX02AXddjOt3/RTzP3wfcH4tddzLOwTtGQuAkoZhGb6KJmYt5yNIL1D8ZhjmdWbr2ba74N/UdnQV/IeHuG72IGhvcfd1p+dxTgJKAdUylhMkVZGgsfcRQn3CkzE03waSL+FZroaa0207GNonB4BHx/keSYW8EUL9Zgdiu2v2vjDSVC3Yjqvo6yW8x2oaBoPpFU2Vjibkn4cjTH0aGrNhxPc+q2A2plIxiT7hL6OdOGvKYwqeZXZudNFbnNA+Y3skXCUSvMCvvmqc6Kd3bTQ9hyxLyrKhodt2fJJcOjouvbIs3k7DPMvi9VL3NIwq2zBEIaQxPMtesWoVZhl7IsryLGua1l/TtMc0TXvbe+0X8p7pmqbN1zRtmaZpSzVNO6ucbTaxByDXBzjoIoybtC+Gtudw7VPvAAA6Bh8InPNXbJrxH9iIfigQAEOmAhc/gTvoUThs7c3ATSfC2r0eACJzllnXN+ZZFqcR/Q5+IXnJxR5utv8gDDZW2dzR7f++0yvwUzOhGbFrz7lkmaVnJAGhweQK0YZhSmTZnUYnVE4AsQS/py1MjavQdXf5Sqop3QWCtqx7/FIaRkjbatEjbhl6wPsqT8dX9yZeEDo3WhHFiOJ+BJRlIxgFF3d5f5DgqXCWp8LThETTYT7iEgNDcQCVJsea+65dFZ7libOxkKlrfjFmUiJuCzMN4jHwFtP1VeNEwqm2K290iEkujPSy+2Z3CRsG6wLKFOm412ZAWVZsNITKynKt0zDYtWzo1c90Z/5sBl3XoGn1Jai2Q6TvVU+6nquBcgv8vgngCUrpBABPeL+r2A3gPErpVAAfB/BbTdP6hryviY8Y+vfK4LGvHIHPzxmDPi0WRg/sBSBIXqnVgm/mL8b9E38KrHsd/R++FBpcZVT0LDM7BfOWMs/ybqXATyUzjEAWa0Etqn4isqaOTV7Ocb9WCzs9ZZnnLLNXvm+WofnpGUmgtrt2ty/YMBTV29C1gA2DFSW6aRgkdDl/fRUuaum2Cdpy7mRWh2TDCBIL9tn7nmXmT5W8r7VREm2hc6ObchFRHOfIRJPBiEmeQosXCfNq8+g4IPlDtKA8+EomejAvecLBkjgDkzH58qLFRPUcx4VIXABZgWSRZ+4x1McrLA52rCLXSSNCJfpSGkaJ6Di1ECzu91HtHqoOdvjnXZ9udn4NgXdOKE1elBoXDgkWb8eNm6wWxM6M7u8953quBsoly6cCuNn7+WYAn1DfQCn9N6X0be/nDwFsADCozO02sYegLWfhOydOwZIfHIfxg3sDCKZTsJvvyiHHA/N+icwHL+BzxiOessy9XowItmZcQpaLSMNQrRT+9HpRG0bQvgG4ZG7TLldZ3qtfi+9Z5q25mertvmqahvac5Xuck4BQFI2OU9uJ65om2TAsgycJsIxqd7lwslzJFq+EeJYZz4biR8d551710PJCTE22YQjHoqqv1YLUvrzIACIyDUOP10xEnL1Qpz6Zqpq2iI356EstL14TaT5/8RjENuZqu2sgpQ1DtN8Qvm5Tr7+yrJ67npSGUVAGM66y7MU1OrTodeDnLBvJzr9D1eg4JWfZu2bZV6nWyqY0wEt4bEnBfPcijCqr2aWgfq+aynJ5GEIpXev9vA7AkGJv1jRtFoAMgBUR/3+JpmkLNU1buHHjxjJ3rYmeCmZVuH/JWuRtwVtraMD0c1EYdxy+Yf4Vo9fci1H5FWB8kRFBpii3ZMI9y2Ed6yyjuMdQ2gdxOVPHpp0uWd67bytsQtFZcASvstycBAD6tFjYnoYsk/B21wxhN1uWGAIoaRjSeQ2/DaRNXggD84qzz5YX+LlKe1BZJsL/B20YaZSstFATV6I7+LkPf03TIJ5SFmtWynZghyjTvPUv8Tr4pWtqw5Y3SyxvE+J3ErNS+G65ssyaycgDXjHiLXWBn6808gGUqIDV66EeGCiksMvUC3xWxG2oUnCI1HGzWCJG2sFKIGdZC1OWhc+7xp+rrLZX99pSm5K429Xr0uKbgTdkqY+y32goSZY1TXtc07TXQ/6dKr6PuneFyLOpadowALcCuICyMnYFlNIbKaUzKaUzBw1qis8fVQztk8O35+2Dh5etw0U3v4z5KzYD8OwSmgb7xN9iK9pw5Bvfx035r+Dctb8AKFdzGUkO8yyL7bFFWLpelIDwmLWgssxi4/bq1+L/PSoNAwDaWix/mSQIt2GEe5YB9+HjUCr5tLlCItgwIpTlSsZfsSKh9hZX9WfWmKhMWuZhZjYMth/i55DWkpAUYqv1YoqhaL9Rq/zjdABzhIEAL3bi6p6kqqYgsZYer8CPEXIjRaIDT43RvAFoMB87NVn2Cj5VUsbJWn0f6nKRnJcl3UP4hToIFT3LQDRZln29cmfF0tuU1VSmTBP/cyVeZ8D62GvCBubVsiKE1aM0mrL8US/wK5mGQSk9Jur/NE1br2naMErpWo8Mb4h4XzuABwB8h1L6Yuq9beIjg0uOGIe+rRl86+7X8NzbmwAAOY8Em32GYW73b/DDw7Igi27FZ7Y9ACy+GdqBn0Nfowu9zHYAXGGWbBg2DdgwAFchLkaWOeGUl80IZHWvviJZlhVlS7gRtufM1DYMtVMdU5aZGihC07j9ge07I1t5J44No3KeZaZSqcoyI1XqQ8i3YXid4JifnEfHuR38gOp76QpEzviOiscSi3RUL2YcS4M4e8GPjdkwPGXYb82eUFlWlo8s8HOIcM0m//wLgkJpGTp2Els6DkPTeKxhCiJuZM2A0mUTipzFbQB1U5b9WErun3ZrJBo/m1aMkWQkTYyMi4q6FH294kA8Doji0w31LGv1I2tiDQLbctWUZSdIluvdZprdC+pdC9AoKDc67l4A5wP4hff6T/UNmqZlANwD4BZK6Z1lbq+JjxDOnDkCR0wYhBUbd2FzRx5HTXJnG0xdA4wsfvmKgV35czB74CaMf/DrwLJ/YKH5DF7dORugD/pEcreoLAsNQ0SI/sqwjLZiaRgMkrKsRscJy7W3WFizrTPZyUB4U5KcKaduiHAbd8iNWDIGn8IulrPM1lmpacBu78HbOxemLAfbVkvWEZMnUIhNSWrVsU3yoOvRNgxZWRZJgKu6lbJOOL5HUvdtHH6BH1ESH5ImSRDehhmILvATYw+TdmRz1yvGp/HPjamF5fg/XWVZ6EIppmE0gAKmJomwfesJUAswVWU5KhGDfd5pbDA2IaFd6xzhczV0zU+GqFZxXfT+MeuQ7j4TUL1rS7WkAPVXlv3vW1NZBlC+Z/kXAI7VNO1tAMd4v0PTtJmapv3Re8+ZAI4A8DlN0171/k0vc7tNfEQwtE8Oh40fiFP2H442T5XUNA1/ufhgHDZuIDRNx4LpPwfahgBbVuI5bQZmdr4ALL4FWVPHJP19DNz4EtDhqtMFh0qtrhksQ8fo7QuAG48ErjsY6N4p/b/vxVRuaIx4Z00dA3tn/L8HbBiSsmyljo4LNCXxBgTqjZb9zfGmSdm++sqkkIYRFh0HIJZ1IC6YDSNnGciaPBnC8hRIQL4Z5wW7Rdh0Pmt6oS5XDUgFfka02ipW90tpGIYWSyXyP6cQ4md7XvvUZNmhfqMRtr5Sx5DmYc3enzHkzy0sFi9xu2uHR9IBXPlzVbn6p2E4IuHsYT5P8fvI0zAEZTmiMYkTMkCI71nm1gu2DnF5ubC29sRRTQgR/1bxbdEwz3IjpGHoNRMlGh1lKcuU0s0APhby94UALvZ+/guAv5SznSaaUDFrTH/MGtMfedtLtzjyVUDT8fWfPIrbsr/AxIe/Be2dx/FI5l5gGdx/ow5HL1wO02iRV7Z7C35R+AXmvLcAaN8b2Pkh8Mi3gVOu8d8iTrGKYJ7h9hbLtxiI7+MKs0CWW8x00XEEke2uw4oWWRpGXlD7xFg+O2IA4B9DRT3LrjKVNXVkTR3dNi8kE/2AGcgkI2NqfgIFpVRqSsIfYFW2YcQs8GMPFyBIAowYAw87RDV3CAEhFISyz690A53wfSOxVN2Cw4/BMvTEyrItXWt8eZZ8YehaoJAr0TEYYsGXrCynXW+lIMbmVdvjWmkEmtEQKkXGRcXHhSXAxI+Ok7OFVWWa5WcD7r2s1iq9eM0yH0Y1lWXVRhfnnlFN+DM2NRIlGh3lKstNNFFXZEzdjVPTDUDTcPTkYXjrkKsBwwTefgx/1E7HTWN/DRz9PWDtq/jO+5dgrjPfnVajFFjxJHD9HMx2FuHuAZcA/7EYOPQ/gMW3AG897G9H7M4lbd8jL20501e+gTBlWbBh5CzkbVK0wjwMxGt9LIK1uzaMIOFlPlmfeCppCuKDLgyVnAZkynLW1JH11HBL8XWL21JtGO7fRJWcWwqqbsNQrQlRynJIm3P2cxJlWc1DFhu0pH1wFZx4U6oO4dGKaT5/cUbAFBJmpLSFlBYFt/gw+PBmJJo10qmX9YEXaPLPqafYMNQCTNshsQr8wmZ64ia1OETtWqfEJQqDz7ooy0oUoPi3SsN2ItIw6nT9UEqF/OzaiBKNjma76yb2KFz9qf3cH6Y9B1gtuOm61zEr1x84Yjow5VRsveFMfGfXz4Hr7wPMHLBmIdBvNL7a9it0tO+L08wscNS3gXceB/5+PjDtdGD2F2E7gwEEFVxG5Npzlt9wAwjxLIs2DC9reEdXQYp+KwVKg8SWLR/mWda9NAyReIotm0tGx1Uwuoh5lrOm4avxfiFZyJS12JZcfAjbkq+3uj5CBtahDCiemiKTan5O/cQPTx1XBzwMomeZ9ZoRBzti4kO50XFxfNdpCozEQli3QFC2Yeh6UEFMsu6wpiSy/aWyjXSSgCe1CIOdHkKWw9Mw+DXSHWHDEL3OST9XtSmJmqesWoLqmbOs/q3SUDOn2XYboRtlWuvXnoamstzEnol+o4Deg9ErY+L1NduxbnsXVmE4ztWuxrV9vgoQB+jcApz438ClC7A6O5EXPZlZ4NN/BfY/G1j2D+CPx8Dc+aH7X2rOssFtGK0ZQyIa0qtY4OeR6qS+ZUIpVLeFSjxF6LrrCxWLrtg+FQjxyVlkdFxFPcueDcPS/X0W1VpAfhBJeb0hLbprOT2oNpmJemiEpWGw3GUrBpEIy1m2RbIs+rRTRMeJnQEjC/wc6l/TaXK2paxhQVlmrYpFZTmNDcMSrChyK/D6kSoGOVEiWYxavWELRJ9N/3cnUJbTeJbFz81dh2zjEKPlTKP2KqvYzbLaOcthBX6VLLBOCtGWU+/88kZBU1luYo/Gl44ej6/fuRTH/eYZt7jP1DH9jC8CE74nvc8yFMWw7wjg5N8Bh18JXDMTY5b/D4BPREbHtedMaJqG3lkT2zsL/o2fR46FK8tJ4ITlLPvKcnDca2heGoZgw2CvBZtKyRJhqEbOsutZltXwMAIodvBjudh5R25Q45DiKmml4CpcPNVEbJ8uIiwNwyfNArE3IyYTROLhCJ5lsQNj2tazDqFosQyvYUqRlt1E7la42042oBN9u+J3SlYuWQZxigK/kIe3WlhZLwVMbXft/q1nTF070ufjDji6beIPPiI9y5LPPplnWc0WZj/K+dlcDKi1yhpmU6vWPoiWE4Z6Ksu82Di99WtPQ1NZbmKPxsn7D8eDV8zBPsPaMWtMfzzy5SNw+ISBgfeZRrDlMgCg32hg5gUY+e5dGKWtC5DSjFDg576a/vqkV8WzDCBx1jKh0e2uw5Vl2YYh7pNNeHRcWJQeW2el1ATZsyzH6RmCcsxgCz7djGTDKF+hTIqC1xwBcIshozOKBVIdMcNQ7HwWhIGA6FnmirMuqKpJiSaRZhaiCKVrOeHXSXobhu4VQzLiw6e02VchVfyd8PBm50strKyXt1L8nGp1bVYK4veKWVm6Cg76eve1aBtG8JpNqyxrGiPqfIAlXrO1/lxrmYZBSGOlYThlDIL2VDSV5Sb2eIwZ2At3fOGQou+xDB1dhTVMBGgAACAASURBVAgVbc5XQRbegivNO2Ean5b+i6m1jAC3ZS0AnUHSJCjL/bSd2FdbiZZVW4GdeWDXRkDTgUwvYL+zgF4DQneDhuUsF/EsG14ahi34f9lrwSFSRmoYzJDOemnBpnRFz3JAWRYeDHmJdIk2DMEXWqMiKrH4Jn4HP/W1dLfBKM+ynF1cng0DYJ9/lJVEJijJ212Lnw/3LLNxqOi7TtOUxNQ1v27AEeIEJWW5bmpc+Z9TvaAqywDQkbfRp9XC5o58ERuGWEPAjrk0wSOEhtZgiGoqEb5PulZHZdnQqp6GYZPgrGF9leXgtZy0TmJPQ5MsN9EEPGtC1M2gbQjeGn0uTlnxJ2zdshzodyBfzleW3a8SK/JT/bg+mX39Loy5+xLcl7WBsF6Wm95y7R8hIJRKIf5Acc8yU4Z94il2oXOo1A0vDGk8q1EItWEEPMtBG0ZGyIZWbRgmSVfslhQFr/sdEGLXEeAQWb1l7xd/j+tZdoRzIiaAlOrAF7luoQuhUUSxklTaFDMLYjGp5ZFtt7Le/TsrMAV4NFfsdRPqeijVNAyh62BDeJbLGBDUC+K1x85vRzdXlqM6+EnZ4Amm6x3KtydC9Ona4iAoxSxHuRBrOtilWi2lV43Rc7dbvzQMafDUw5JdqoWmDaOJJlCaGC4ddT52oQVtL/xS+ntAWfZeW+xtwKPfxcT1D2IItiCjOcDy+4G7Pg+610G4OP+fuHPGLcBXlgPf2+T+O/BzwKu3AzvXhe4DIWE2DEY8g19lTdNAxDQMkyu5caPjkrZVjkLeJ8uisiy/Sp5lQU2VbBjCVHetuqSxTlZsf6LIgKQsG/IgxicSRYh9mLrnECKo6ekL/JjfF/A+/8hjEG0YevK22oK1R2w2IyrLaZuHOMRtrMLjD8M8y/VMwxCmrpUug40OhxDompx73tFtow8jyxHKsuhZTjJAYO9Rs4V1QU0V/fN18Sw7we9jtWYKHG8gKKIRPMuN0BmzUdBUlptoAu7DfevuPDbs6EKfVgv3vvohVm7qwGHjBuKgMf3QobfhBvskfG3FHcD7LwMjDgIQ4ln2lOUJS/8LWPF3HAFgQQ7Ak96G9poJ7dw78OxVz2O8NQZoH8534rAr3HznF/8HOPbHgX0MbXddzIahs+gx2YZhGrqULBHlWU4THRYFX1m2dKEoUVZhpTSMENJVcIQED0ODSWpzE5cK/IpYU8I6+AWPsYiyLCjIDhE8y6KyrKdTlsU27+4xFPNdiz7RlDYMUQV3uLIsDwTir5flvsoFfkIahjBLUW81zk1w6VlqnE3EYjr3dVe3jdaMiYyhl+7gJxWfxifL4T5dUVnmMzO1bnctpmFQSqW/VWNb6qyhaWjI5+vrvze8jo5Az7mWq4UmWW6iCQAHjuqHB15bi8OvfgptORObO/LQNOAPT6/A4LYsDhrdH085H8dX+zwJ7dHvAmf9Beg9iEfHeSS5vcXCNG0lhq64E5h9GR4x5mD+0w/iE1P7YPqoQcCM86Hl2sO7+PUfC0z5BLDwz8Cc/wRyfaT/Dmt3XdSG4aVh5B35wWQZOvIC8YxWlivoWfaq6TOGGB0nR8iVsmEUHJ7gYek6bJazXGVfqFj0ZhlFCvzCPMtK7nJRG4bg/7RCc5bFzOlkxyw1TCkaf0d5AWaKz1/8fCxBXfVnMTTNv4aTKMui0qWeg0bzLIsqa0/yLKuWsY5u2496jFSWCSdVph7/c/VztwM+Xd23aIhd7Yw6zBjwYwOA6g7MRcsJQyN4luM0MvqooEmWm2gCwAWHjcFRkwbjT8+vwsad3Tjn4JGYMbIfXlixGT954A088NpaADk4R30P5gNXAP89GRgxC59ZvwqfyW5F/uUTAONcjCIFnGjdAjvXH9aR38DOZTtxk0MwZcJ+mH7QCH977TkrPA3j8C8Dy+4GFtwAzP269F+EIpCzbBm6pNaJ0HXXhmE7bj4ts3Cwls0FhUSrsCroE+y2CTJeUxS1wK+YDcMyRXIkNCUxNBhOuun8pBAtDMUK/BxCkVFUc0uXj7WoshzhWRa7R6aNjisQKvuuI/ajQCh6SV3T0re79guDbJ68Yhjp8rHFrGkWf+cQrwV6wGddXzVOjtvqGTYMkayxa6wj77i2KcvwB7uB5cR85gRWBULC7z2yZ1nOLa9XzrIY6VbNNIzQnOV6pWEIlrAkhZt7MppkuYkmPIwe2As/PnWa9LdjpwzBzFH9cNnti7Hswx0wZp4PjD4EWHQT8N58bOq7HxZ17MZp7z4KvHM3LgAAHVh94NUYnesDU98FQE7DAIC2Fgs7ukLSN4btD+xzEvD871wPc+/B/n+FKcv4/9s77/A4qqsPv3eLerGsarn3XnBvGEw1zXRCIKGDIYR0EkI+voSUL71CgBAgFEPoAULHxhgDtsHG3cbdcpMlWVavW+b7426Znd1Vs1Yr4fM+jx9Zs7Ozd2d2tOee+zu/AyQ5bFEzy343BbN1nbXAL1oHv87ULDe5vIEgOcHSlCRSYBHS7tokw2gpwxgr/N3v/ONpqYNfNDeM49MsR5KktFezHGxjra3jWihSNGXF2y3DMMtnHGbNsqmArAMShUB1vum8ukz2h21xK4k15glBj7OO83gDqyDmADbJaSPJaYtqHWf+zJo16q2+XpR6CXM21eMxuWHEQ7McIaCPrc+yRb+t4u8Z3pXNn7o7EiwLQitkpSaw+MYZ1Da7dXY2dyQs+DUAZXuPcd+LGzjntimklqzl4007eebTAywadQUQ7vjgJyPJEd1n+Yx74YEZ8MGv4fw/BzYbRviyJejGJNZgHHw+y14tXTA7XmhLsGCzi6gd/DpVs+wJLO+3pSmJWToSah13fEFXR3B5jEAHvpbOSWgHP0uBXzs0y+bMsquT7PLcnrYVS5n387tZtO91TM1kTK25/UvrNqUC2sx2BctWKZEttGW7eVISL21lyESup3Xwi5BZBl0TkeS0R29KEmES1Jagyt+QxtqIw2HyyfYYVv18V/ssh8vUYueGESGzHMfPsjmr3tOcXWKFuGEIQhuw2VTA8cLM9MG9WX7nfFLTMmDofGqGLeQN70ySEvS+VgsxPxnJzugd/HKGwZTrYe0TULY9sNnjDS/wA39mOfxWtikCbhj+bC7ozK7L4w18kUcLljtXs+wNBMlWN4xIel6zdMQfIJqt40J0oV1R4GcqjozqJOEJzyz7r3tbNMvmTL85KA5tVtKxroXmAj+nvYUCP68RMuaOF/iZpAgeI+Sz1pFJTlCKEmprF55Zjp8bhjn73VG5TLwI0SybJtZJDjtJzrYU+HVMs9ySTjfMDaOLJx5d6Qhhnhj46cj911l0dBL0ZUaCZUHoRE4fnceDV09meF4aYLZHs2aWnVQ3tNBK+NS7wJEEn9wX2BRVhuG0BzKfZvwBhVWG4S+Cas06rrM1y/4gOdjBLzTrapY3uDzB4C5EhuErtjMH0bHW0oUV+EWVMBhh76k9mWVzkxjzRCCSdVz7M76WAr+o2XFTZrmFYsbo7yF8RcBqU9iRYNkaFPvlMGGZ5ThoW/2E2Br2sKXrSG4Y4JNhOOytFvh1VLNstY7zS8eCYwpe1/a2Rz9eIul2Y+aG4THCzkVci1Uj1B7ESz/dXZBgWRA6Eafdxjnj+wSK6QI6QItMIqIbhpnUHBh7MWx5BZrrAS3DsPosA/TNSiY/Mylsu83nhuH2GAGPZf8Ym926WM5uUxGPCf4sT2dplj2B7LZVhhFJWqClI8FMKPgtyMID0lh/oZjb7tpt+pxGsrHyeEPbLpvH2BZv4dCmFr79zYWYHWxK4i+CC+iubdF11+aJQUeyedZ21/qYRsD20N/SWL+HjhX46Z8q8HkAi2Y53kVRPbSDX1gDJUwyjFY7+LVPItSmzLIntHAzXj7LDpMUIVbXM5obRrwzy+aJQqxrQ7o7EiwLQgwJuiFYZBhJTprd3qhfQgBMugqaa+CL1wFfB78Id+w/r5nKzy4YG7Y9JcHOpkNVLNlWEhgH+JbhfZnlaBIMPebO+4JqcnsD/spBu7vQc2OWNzSbC9JMMgzdtlnvb3ZFiBUBf19baOAeqfDRmgkzj93RhoxwyBeUKTMZtHEKLnW354sr4P9rziy3sd2122sEPGbbgtvX3CI0O+61ZC718duTKbQGV/7mI0F3FJODR5y+1M01AF0lEeosrBNCP0kB67iWO/iZM8vRZEpmzB0dzTjsQT9lXfRG4Phxc8MwdY2MWWbZMAjTb3eDiZ9ezRLNMkiwLAgxpVeKM+SnH38TkxazywNmQdYgWP800LIMw6xJ9vOzhWO5ce5g7DZFYa/kwHa/lKCq3tVisGy32TqtQKnJ7QnKMBz+wNOSITZlO91RZBjmltJ6jJ3n2BGJgG+wZayRvjisGku9f+iEoE2aZYtG0lzc1pGMZbBFeOu6a5cpm+fowJdks6k1uHmCoD1z9T7tWa4PvAfTsrD/pzuaG0a8iqIitIzuKUvXocWp4Znl6NZxwQlesKix9fds7uhoxqw5D1mp+bJrlr3hiZDukln2X6KeMvGLFeKGIQgxZFzfTN769smM7pMRst3fxKS6wU1eepQn22ww8avwwW+g8gCeCO2uW6JPZjJ3nzuaO88eGRJkO+w2io7Vs6+8npOH50R9vrMFbWt7aXJ7SUvU79mfYQ6zVWtFhuFye7VfsOkL1uzLGgus9nrmBilW3C24YbSlSCakeNEIBqpmG72OtIoOFN359b626M4CHm8wo28es0850ypuk3NI6CQnPLPcrgI/05K4/xjWiYT/Z3cIMPznoKcsXbsjTJLA156+hQK/YEBpC1kNafX1/Jlla9c6m1mzHCoJ6vrMcte4YfhXryJnluM08TOtkvilUz1l4hcrJLMsCDHGGigDZLYlswy4xn0FMNj62p9webwR3TBawxxkAfTJTCLBbuPOs0fy6LXToj6vTfpPT8vj92P2WQ5mlkOzrqHWceEyDLfX8HWiCy9WjBVWe72gzV34eYmUWQ7PSEc/nyHFi6bsazAzrB9z2lWblrqDxw0+3z+2aFk6s99rS1n0Ft+DZWLh1xZbz42nHTKMsAI/X8tuc7tlfez4umH4r19X2Rp2FpFatYPfZzl6Zjmk/Xw73rM3amZZ/80xDAOvEbpC09WShK7KLPsPGdENI14+y6YVA/9PySwLgtDlBGQY0byW0YH07a+WcpHnZC7c/TiDmgdhtxUe92vfcdpwFp0yNJDpjUbUQLS5HlY9ANteg5Kt8NV/w/AzWzyWlmFYNcvWAj+TG4bbG5ZZbvZ5Q5sDf4c9tl+i1sxlSxniSJpla4DYUqbRHFDabAql/E1Jgm4Y/p/tcQCx2q612FglxDWj/UVNLm9wRSAgw/DJJcKC5fZklr0RZBghPstBzXLcOviFTDR6lmbZFeH6gE+G4bC3S7Pcnsyy3R4aIPo94CPJa7r6VJrdMKzbOhN3hAw2tOxaE2v8r+s01QL0FM/wWCHBsiDEgayUBAD+9fE+8tKTGFOos88Vdc38+q1t7CqtZf+xeirrXVx8/m/xfnwZ//I8wqGJVxz3ayc4bBE1zlYcdhuGz/khUIhjGPDq7bold7/pWlP98s2waAWk9IY9y2HoaeAMdedo9pgzyzpotmZdzYGk2e83KMPQGUqnRbMcy+xdUCsbdJLQY42QWbZU70MwaGpLkYy14NJh0xnkMN20vX3NQoJtmM1635YK/ELtDtszGdGTnGD2178tUiDZMes4UyGfN4IbRjwDDIs9n97WM5auo2mWEx02n89ytMyySTrUDseIQFMSiwzDblM0uEyToJBi03hllm1h2zqTSEG5//fuICkCf1JCgmVBELqYQdkpfO/METz84R7O/dsK5o/M5aKT+vKHd7dTUtXEtMFZzB6aw9UzBjBjSDYUPETvJxfSe9fDMOinXTJGc6Yowf+HfPVDOlA+/adw8vegfDc8fCo8dRHUH4OGY9B/ps42p/QOHKvJ5Q128HNaM8vhwZPLE6qd9S/Puj1GiKayI13m2kOwfbM1Cx4ls2zZrz2aZXOgCvpL2mNyfDBnhtsTOITprltoNhOiu+6AV7D5HPjtCnVQ6w0ERh0pGHKFFfjZfC4boVm5eGqWzdevpzVyiNRQB4IFfn6/b/Pn0/880J8p/9Pa1JQkSkMka7MZ/2fGHofMZldllq0TPj/xdcMIlZ/FcyzdBQmWBSEOKKX41unDuXb2IBavKuLRj/aybHsZ+RmJPLdoJicNyAp9wpBTYPRCWPs4nPKjsMxtLPD/oWz2eKlpdJFduQne/R8YdT7M/a7eKXsoXPh3eOE6GHYGDJoL7/8CHj0TLn8cCsYDLXfwC2SWTX+Mm00yDP8+uoOfN+RLxR7jlrBhEogWCvwiedVaPaFb1Cxb3pvfBs1lCSzaa48WLBAM6qgjZca9XqtOtP0Bn8vjDbNL1MvqweMGLf/a/uXrtmbXbS34LMdL5xkhO9uTNMuBZkEhTUnsgfu10e0lzRosm+QUfj19Wz4v/vMSqRGHniDGXzMbKaCPxWcrcC4iZNm91pW9LiKSZrmnfJZjhQTLghBHMpOd3D5/GDfMGcy7W48wa2g2eelRAuFpN2qd8NZXYOKVMR+b/w/lxX//mIPlVazL+yVJaQVw0QNg/sM+ZiHcfTgYwPebBs99Df5xCsy4Fc74qcU6ztKUxB9YmL6IXB4vqSZNdYLdhstt+DLL5oAyuv62MzAX10FQyhApyxJavR86EWiLXZo5uwf+iYA3pNGH/5jtkmGELalGDjysOsVAi+52Sj6C8pnge45k+deeyxbM8gX10I2u7tfBz7qi0FNkGG6vQYpFQgTBAj/QjYWsdQ5W/+u2BlX+4s7WMsvxdsNQKhjQ+2sIOhvr3xg/gb+LhoGNrg2WrYWz8fQv7y6IG4YgdAOSE+xcOKlv9EAZYPApkD0M1jzWJWNKSdBfjA0uD7c43yKpYjuc+3tIygzf2ZzpHjgLvvkZTL4GVv0dY/U/QttdO0KztJGaGbg8ofpdp8O/7B5qsRTr4Mi8zGweq/WLI1pW1m7RLLdmHRdJs+z2fWmbZSsds45ruUDQKmkIdOBrTwbYFBQHs/C+62aaYOm2xh23v/O/B6uWOb5uGKENc+Jp/dVeQtwwTEFbksNOkjOYWQ57nsc6WWnb+ffvE55ZbqHZTFdnliPcjzFxw4iqWY5fM5CwzH6MV/B6ApJZFoSeglIw9QZ45244shkKxsX05RZOKiQ7LYFT8+qwPfgyb7mmkZ04k+lteXJKb7jgL1CyBWP90xjGPcEOfs7QrGskH0+XJ1yG4fLJMJymL5VYL89atbJOUwBoxpopswacbdUs2y1Zc49PhmHuwOjooHVciIShhcyy9T20rymJqa22PTgJ0t0nOx54RMx0eY2w5eJ4umGYZTj+MfWUAMMcGEbSLAMRi/w6mln2Wp7nx/98q7Wc3WaL2GI+lnTV9QwEphE8p82PdyX+ybRZUtWevzlfRiSzLAg9iYlfBUeS1i7HmLREB2ePyibx1UU4nE4eSLqZ3779RbvaHzPpq9jKvmC82hsuwzAFhk67jc2Hqmlo1l/ILo8Xp8mxw2m30ew2fB3mQp8Xy6XuoDSh5QK/MJmAJfAISk1a0iyHBsXBxhuhEgZnlMxw9OOGFgg67JEL/Dxh2ujozh9RX8sTnMz434vb4w2TmNhsql3BjysQXAVlGCGaZbOHdJy+1F3W62S39ZimJGaZjNUNw3+/RgqW/QGlv1mSf1LbGtbMpZ9AIa+1cDMOLifmlQJoe9a8vbTkhgHtk0F1FpHcSKQpiSAIPYeU3jBigdYteyPbOXUqH/waDn6GuuCvXH3mLNYWVXDr4rXUNrnb9vyxl2DYE7nMvjxgV5fk1JXz/owVwKJThrB8Rxnn/m0Fj6zYQ2W9K9ABDrTd3eHKBg5VNITY3sU6OIrkcQzhBX7RsrJmSzPzfpHwhHlI+zTL1uVge/v0g1brON3UJPyLzxVh6VuPq2OaZYdJs3z8meXQIsVu64YRlonsGQGGWd7kP5cJDhs2mwrKMCJ4Lbst2VdHG5fro0sPtOtF99Asd01m2Trh8xN0o+n6z1B3KpztLkiwLAg9jbEXQ10ZFH0c29fZ/T6s+BOc9HUYdylfmdaf/zlvNEu2lbLw/o94bcNh6prc/GfdQb7173V8caQ67BBGUiZrU+aw0L6S6f1TAZ1Zfuy6aVwxtX9gv++cMYJnbp6Bx2vwyze2UV7XTO/UhMDjTrti5Z5yKuqbuX7OoMD2mGuWrQV+Ub7AImk39fPaHnhas+Z2m99n2SJJsbdvSTT4HoIBv2FEz4472xHgh70HU4bS3EzGGvDrSv/2uGyEyy26nc+yRU/fme3iY02IR7TvPSQFJrfBAj8rZn9m/3Pbo1mObJcWwQ1DxcNnOYI7TQwL/CK5YUD30Cy3dRL0ZUY0y4LQ0xh+FjhTYMsrMHhebF6jbAc8fx3kjYZzfgtobfFNJw9hTGEGd7+8iW/9ex02RaCwbem2Eu69cBy7Smt5d8sR5o3IpW+vZD4qn8YTCe+TVfYu9LsagFNH5oW95OyhOSy/81SqG9yU1TYxoHdK4LH+WSk4bDbuu+okhuamBba3N8vaVgzD4LN9Fbyy7pB+HUvW7c/v7eCJT4oor2viWF0z/bKSA+OB0OVL8/Nb91kO/XL2eMIdQJw21b4Ofp7wJW3/drstmN236n+DMor2BLXhnRf9cgmbJVjuUHbc4hXdvdwwgt7g/jH1lGxcpA6L/iA5oFmO0PI60gSvTW4YLQSIXsMIkybEw0atyzPLttDcZTw1y92pcLa7IMGyIPQ0ElJ0wLztNe1OYQp4OoW6cnjmcnAkwFXPQUJqyMOzh+bw/vdPZfmOMj7cWcZpo/IYnpfOoqfW8IMXNmBTMGVgFotXFeH2GswYNA/D/j7q7btg0Bzd9S8KSikyU5xkpjhDtj98zVRsioA20o/DZuNofROPfrSXvUdrSU1wMKpPOgsn9g180e0pq+Xp1fs5VtfMDxeMpE9mctTXNwyD1XuP8es3t7HhYBVKwUkDejEsTwfoQ3LSmDYoi5pGN/XN9eSkJVLYK5ltxTqr7j92NDlGi9ZxYU4f+gvK5fWGfJG2V3oSqcAPwjNW4dZx7V8GNmcotZZVPz+yRKH9MozQzHJkNwyP18AwjLDPSqwJkyT0oAAjkkd0oImQL8PcFEGGYb2ubZXXtBQgmgs3w+wlu9BGzfxZDowthj7L3dENwz8kaUoiwbIg9EzGXqx1y0Ufd352edkvoeoQXP8W9BoQcRebTTF/VB7zRwUzxM8tmsWr6w8xe2gO/XuncOBYPa+sO8QV0/qjPI/AQ/PgxRvghnfA7oSqg/D2j+Gkr8GIs1sckvWLxLz9iyM1/OL1rWSlOKlr9tDs9vLYR/tYMK6A978oZW1RBU67bse7bHsp3z9zBB6vQXF1I0eqGqmsd5GaaMftMdhwsJKS6ib6ZCbxu0sncPa4AjKTg4F7ZoqTF26dHXEsjS5PIAvntNvITHaSm54YOF+t+bR6LE4fAc2yJ7TFt9Nuo765jZpxTAV+FmmI9YvfGpD6X7M9X9ZhhZk2XeRmDSTtNhVwEGnbcf0SkdACv0iZZf+YrRrQWBMWONp7ToAR0lDHd96SHK1nlnXHRstErg2rHsGmJKHb7T4HGGvBrN30WXR2cm6gpTGa3WliZZ8WLViOtxuG0x5auNlTVklihQTLgtAT8UsxNj7XucFyzRFYt1gHsP2nteupSU47X5kWDK77907hjtOH+34bBAv/qjv9PTgHJlwBqx6E+qNw4FO4Yy0kpoUe0OuBxZdCaq5uhGIPzTYDXDd7EGMLM7hoYj4jGjZgbPg3R0rL+FvZJP76zgSG9cnmzrNHcvnUftQ2urnj3+u459UtgG50kp+ZSK/kBA5VevB6DWYOyWbG4Gwumdw3pACxre/fj92mWPaDU8lICv6J1V0IDQ5W1JOS4AjRZEOEpiQB/WZom+H2OgO4LJnlaP7J1k6BVk9pt8fLU6uKOHtsAYW9ImfntaOHObBXvg5+Bk5nx7PjgYDCpIf2t9GOOGavgaOLgio/OhPZdcWnnYlZUx7QLAdkGNEL/Kya5ba+56DVoiWz7PtsW60Y49ERsavcMNyWz7CfoGY5PgV+1sLNSCsLJxISLAtCTyQhRdvIff4kzLuzRWlDu1h5P3jdMOfbnXM8M2Mv1j9X/FG3xM4ZCWf+HF79Bnz8VzjtJ6H7r3kM9izT/3fV6/bZ/oDZ1QDb32L+xueZf/hzWFUKGKjETPokpPJr91J+kT8Ax9de0S25gbx0eOX2OewprSE7PYneKQkx1T9ag2GHzcZDy3fz0PLdAGSnJjCmMIOT+vdi4aRC3F4jEJjo/RUNzR68hnU5uH2WZMFq+1BHj/DMcpT9fNsf/GA3f3xvB69tOMyLt84OfJk2NHu46+WNnDOuDy63NbAPNg+xNpNpn3VceFDst6Tz/25+PD5L114SnaGTo3jYfnUEjyfcDcP/WUxqwTrOumLgtNvadO6jWcfZlL+DX+gqh93yWewKukqz7C90jZZZjkcTSOtEQWuWu8B9qRsjwbIg9FTm/UBngZf/Tmdej5f6Y/DZYzDuUug9+PiPF4mxF8OYi+DwOsgdqfXQu5fCJ3/TvzdUQN4YyBmhA+rBp8DIc+HtH8GjZ+kgvq4MPvw91JZAeiEMOxMyCiF/jLbVsyfAzvdwvPoNeOxs+NpL0GciAM4Vv2Pkuqdh0XKwJcbmPUbh22cMp6S6kWF5aTQ0e9hZUsumQ1Xcv2wX/1yxlySnjfH9egX2d9hsrNxTDsDJw3MC2532jhX4WTu0Wf1wrQ4F5v02HKjkr0t3MiI/jXX7K3n4wz3cdupQDMPgBy9u4I2Nxby3tQTDwCIZ0Y4eVomCTXVUdx2UYbhMrglhbabjVBQVrlnuGdk4t9cI07SHFfi1QbPc1sxyNOs4+XtqRQAAIABJREFUvzbWOgnyf6S6NrPcRW4YnsjnoiM1A51F+GdZ3DAkWBaEnkpGIUy7CVY/CHO/CznDW39OS3zyN3DV6WPFEqWg7+Tg72f8DL54A166MbgtMVNnk8/9A+SOgOQsWPYreOFa/fiA2XDxQzqYjlTgOHKB1kY/eRE8fj589VnwNGnfaNDZ7bN/Fat3GJFbTxkacXtpdSPfePpz1hRVhHw53zB3MOP6ZjBzSDazhmYHtjvsNpo9XoqrGjAM6JOZhFKKspomdpTUUNfkJtFpZ8bg3iQ57dT7Gr0EXSr0azz60V4amj0crmrgcGUDxVWNQLCgKy1Rfz1897n1JDnt5KYn8sKi2dz18kb+/N4OmtweSmuaeGNjMbfMG8Lzaw5QWe8KW7quanBpNwxTwV17ragiFRx5IliMtZZZdnm8vLPlCKeNygu0c+8sXBYZRlvlMhV1zZTWNDGyID1ke32zm9LqJgZmp8S8WNEcHPn19f5mJP7PQ1syy21tXtFSUxKvEandsj+z3HWBY1e7YYRNHDpQM9BZWCcKPUlSFCskWBaEnszc7+pufq9/F65+EZxJHTvO0V3wyf0w4UrIH9upQ2yVXgPg1o+gqQbS8nWm+fMnYdR5OlAGmPgVGH8Z7FoCjiSt024tgMgZDje+A09dDIsv0VnsnJH6/X36T5hxK/Tq3/IxuoC8jCSeuXkmf1myIyRgOnNMPmeOyQ/b32lXFJXXM+vX7wOQk5ZIRrKDPWV1IfslO+30753MjpJaHDZFaqIOfvLT9Wfk8U/2kZueSGFmEiPy0zl1ZB6DclKZOqg3oDXnT980g6XbSll/oIIfLRhFZoqTX140jr1HV/OXJTsBuHRyP358ziimDszilqfWBoIrgIn9M3ljYzFKEWIFaFeK2kY3S7eV0ODyMChbO67sKKnBa8Ck/r3ol5VMbZM70KUxtOBIL/fX+5rjBNwwWgmq/r5sF39ZspOTh+fwyLVTAwFhSzS5PThtthDJzvtflPD7d3aQYFcsGNcHpeBQZUPoe7SpwEQFYPmOMtbvr6RvVjJDclMZ0yeDj3cd5UcvbeRobTNXTO3HDXMHU1Rez/IdZby2/jC1TW7GFmZw5fQBXDipkIwkLUMyDIOtxdV8tPMopTVNNLk9nD22gDlDc1AK6po92HwBb7TiWDORsqh+GYbNpkhw2CIX+HnCnVraIhGyyizMrwvQ7LashsRNs9x+p4/2YtVn+4mH9CQwpgiToFh2Su0JSLAsCD2ZtFw47w/wym3w/NfhK4vB0U55gWHAW3eCMxnO+kVsxtka5qz45Gv0Pys2e6uuGWFk9oPr34anL4OSLXDNa5CUAV+8Dst/Axf+/fjG3UkkOGz8cMGoNu17w5zB9O2VTEFmEh6vwfr9lVQ3urhian8m9MskI8nJsbpmlmwrYU9ZHRed1JcFYwtI9wVas4flsOZ/ziA9ydFqsDhnWA5zhuWEbMtOS+Tt78yj0eXhWF1zILN91tgCHrx6MmMLMwP73n/VZB5ZsZe/Lt1BTrq5yYyNT/cd49N9x9r0npOdoUGfP/D/9VtfAMEsXEtB1e6yWh5YtptRBems2HmUO55Zxy8uGkd+RugEs7iqgbomN01uL8+s3s8Law+Sl57IBRMLMQxYW3SMz/ZVMCQ3FafdwW/f1mMYkpvKeRP6BI4zrjCTp1YV8eOXN5KS4ODRj/aGvI4/+BpVoK0On1y5j+fXHAS0Xvi88YWM7pPOy58f4p5XNvN/b2xj7vAcXB4ve4/WUVReHzg3NgWLV+0nNz2RxmYPNb5JhNOuuPnkIXznjBE0e7y8sfEwS7aVsmp3OamJDvpmJXOwoh6vAYmWAlVzwWqiwxbdOs7sDW5XEeUa4c8LngMz/gLOJl9gbvV+7spgubMyy4ZhUFnvIstSw+AnsGoSVbPc9cGyy9SJE/RqlsgwBEHo2Uy6CtxN8Pp34OH5+veJV0JqTuvPdTfDyvt0t75zfgdp4c1Cejyp2XDD21BfrqUrANNv0cWMvYfq7HwXe/IeD+P6ZjKubzAgvWZW5P3mjciNeoyctOPXayc57WGuGOeM7xPyu9Nu47ZTh3LVjAEh7cu/c8YINh6qZMqALNKTnBSV12EAI/LTMAxYt7+S0ppGMpKd1DS6Wbe/IqRg8qoZA8lJS+RwZQOpiY5AxtUfYNy2+HP69tKZaZfHy5SBWazcXU6S08aTN07nrU1H+OlrW3h3awlDclK5YGIhZ47J5/FP9vHi2oOB10mw27hwUiElNU38Y/lu7DbF8Lx0/ue80VwzaxAJDhvFVQ047bawc/rTC8aQnuTggQ90Qed1swfxg7NHUlrdyI6SWjYerCQtycGNcweT6LBz1Yz+fF5UyfD8NEYVZJCcoIPVG+cOZtOhKv796X5W7i4nPcnJ8Lw0Fs0byllj88lJS6TR5eGdLUdYuq2U3qkJFGTqCcDWw9U88MFu3t58hJLqRuqaPfTtlcz5E/vQ7DY4UFHPrCHZTOjXi0sn9wuM3WGzhRSbJjnt7C6r5b8bDrP/WD3biquZNTQ7Qva1bYVgrWWWmwKZ5Y63Xj9erO4m/kY47TuGl28/t563Nx/htlOGcsfpw8ImqB5PtMxyqBtNVxIxsxzDc7/9SA3ff2E9d58zmtnD2vC9FQckWBaELwNTr9cZ04//Bu/+BFb+Ha57PeAEEYbXC1v/A0t/ARV7tRXd1Bsj7/tlwJEYDJQBTrsHaoph6b1QshkmXwv9p+vsutDpmL2qAeYOz2GuqWhxTGFGyOPD80P1u1bSEh1cYgru/MwbkctlU/rpgO5INWmJDgxDyy+8Bvzq4nHkpSdx7exBTBvUm492lfHhjqP87f2d/HXpTpx2xS3zhjC+byYer8GsodmBzHN1o4skh50Ek8wEiNrkxmHXqwVzhuXQ7PYGPMnTctMYkpvGgnEFIfsPy0tnWF74+1ZKMaFfLyaYij+tJDntXDipLxdO6hv22LnjC/jLkp2cM74PV80YwEn9e7Wqgb5sSr+QotLctERW7DzKip1HAe308vrGYtITHQzPN3XUtCmaXJ5Wm8IE2pdb9vHr2v3BsjWzHG83DJfHy9qiCkqrG+mXlcLAnJTARM2P2+PlSHUjTruN/3tzG29sLGb64N7cv2wX72w5wu8um8BJA7LweA22FVezylfEG23i8ODy3XznuXUk+yan18wayNljC/Aa8Pn+ClweL0lOOztLathwsCow+bOumLSHSO3pG1we9h6tIzc9MVDPYGVXaS0f7zrKpVP6Rd3HSkVdMzc9+RkHjjXoicW3Tya7EybznY0Ey4LwZWHcpfrfobWw+DJ44gKtY84bHZo5LVoJb98Fxeshb6zeZ9gZPSq7etw4k+DSR7WG+cPfweaXICkTvv5KaPFhd8bVCBgS4JvIz0jiD5dPDNte1eBid1ktJ/UPBpxjCjMYU5jBLfOGcuBYPR9sL2Xu8FwG56SGPR8IC4railXG0tUsGNeHBeP6tL6jiZ8tDK1bWHzTDA5XNpDosJGXkURKgp2bnljD8h1lIdnX3PREln5Ryhl/Ws654/uQlZJATnoig7NTSUtycLiygXe2HOHZTw/QK8UZVXoQyCzbOy7DaHR5SHTYQoL26kYXa4sq+HTvMdYWVZCblsi8ETkMy0snPcnBgN4pJDntGIZBo9sTMslz2BUr91Rw6YOfhLzO4JxUxvfNZEK/TOqaPDz72f5AsSzAnWeP5Pb5w1i2vZS7X97EpQ9+wumj81l/oJKymiYAslKcYcGtf2Vh5e6jnD4qH4ddseVwNbcu/pzpg3pzsKKew6bXAT2JrG1y86s3t5Hgk070zUpmyoAshualkZeeyIGKBlbvKcfl8TIwO5WslAQSHDYS7Eo3PHJ5WH+gImAZ6D9uWU0T8//wAYkOGxdN6sslk/tS2CuZ5AQ7x+qaeX1jMQ9+sAuXx+D+Zbu4esYANh+qYmdpLaeOyOXMMQVUNbgoq2kkJz2RnLREKutdPP7JXkqqmvjNJeP539e28MMXN/LItVO7vANnayijHV2UupKpU6caa9asifcwBKFncmSzDpYbjunmJX0mwbhLoHwXrP6H1vKe9j8w/vLOb5fd02iqgaJP4M0f6AD0pvc6z7e6s/F64Z27dQGkqw6UXdvi5Y4CwwMeF3iaIb1AX9/krHiPWPiSUtfk5muPrmZwTip/umISoLXGb2ws5olP9rHhYFXE5zlsisun9uf2+UPpl5US8thTq4q455XNnDE6jyXbSlnyvVMYlpfG25uLuXXx5/ziwrGsLapg8sAszp9QGJDmHKyoZ/2BSrJSEshMdrJ4VREvrj1IfkYS80flUt/sYevh6kABqcOmGNs3kyNVDZRUNwVePzXBzvxReew9WseWw9VcMrlv4L09vbqID3eUsWBcAcPz0jlU2cCuUi2n2XSwKhC4njw8h7PH6lWDPplJnD46WKRb0+ji1299wVubipk5JJuzxxYwoV8mA7NTwzLLhmGwdFspE/pnkucrynV7vDy5soiHP9zDiIJ0Lp/Sj5y0ROqb3QzKSWVITip7jtbx9uYjVDe4sNkUe8pq+Xx/MDC3KRhbmElaooP9x+qpanDR7PbS7BOSO+2KgswkLp7Ul++dNRLQk4zVe45R1+Tm033HePnzgxG16RdOKuSSyf3483s7WH+gkv69kxmWm8bHu8sDRZtWbAp+e+kELp/an399vJd7/7uV31wyniunR+4eG0uUUmsNw5ga8bHjCZaVUr2B54BBwD7gCsMwKiz7DAT+A9gAJ3CfYRgPtXZsCZYF4TipKIJd72mniz0fQNk2vX36LdquLSFyBu2EpWy79nJOy4Mb34Pk6MvenYrHBbuWwqYX9GvPuxNSekP1Ye38kaLdKfB64fVv60B53KXa1aOpFg6shmN7dcMWe4L+eXQnZPbVjVz6TDqxVg2ELsPrNVCKiFlAt8dLTaObkppG9h2to7ZJ66WH56dF1cxvOljFrYvXcqiyAYDVd59OfkYS720t4eYndTyQ7LTT4PKgFKQnOkhw2Dla2xRynAS7jUun9OVobTMrdpaRkeRkVB/dAGjG4N6cNCCL5ASdQd5VWsuhygaqGlys2lPOu1tK6JXi5OaTh3DRSW3v5FlW04Tb640qy4k3Dc0eSmsayUpNiLhKYhi+tvRKtdqsqbK+mbVFFZTXNlPf7CY7LZHBOamBWgqv1+BoXRO5aYkopahqcLFufwV56UnkpidSXtdEeW0zmclOCnslByY9hmHw0PI9XDV9AJkpHVvJOR5iGSz/DjhmGMZvlFJ3AVmGYfzIsk+C73WalFJpwGZgtmEYh1s6tgTLgtDJlGzVP/PHxHcc3Zl9H8OTC3Vzk68s7vwg0+vRnQmrDmqNeclWbZXXUAHJvaGxUstBeg2A4g2QkA7n/h4KJ8F7/ws734V5P4T5d7c8tgOfaXeUmmJ9jH5T4Yon9LHrj8HG52HKtW2TcLibtcf1tJu6hdWe8OXncGUDx+qaA8HX1sPVnH/fCq6eMZAfnTOK/eX1LNlWQnltE/XNHsYUZjB1YG+qG10UVzUyd1hOoMjR6zVi2qlT+PIQy2B5O3CqYRjFSqk+wAeGYYxsYf9sYB0wU4JlQRC6JSv/rqUOZ/68c9t+N1TASzdpr2ibQ7cVT83VHQhHn6+LLI/ugCU/g8YqHbDvfA/2+zSSiRlw6o9h5m1tC+Jry2Dzi1p689mjMOt23Yjlua/Dttf0633laXBEtrQKsPEFePkmbee38L7jPg2C0BHcntBW6oLQ2bQULB9vgV++YRjFvv8fAcId9PUA+gNvAMOAO1sLlAVBEOLGzG9oacOSe6HvVBg0J/hYRRG89k2YdQeMOKvtx6w+rDsJVu6H8/8MU64HV4OWWZgKpMgfC1e/EPx9zre1xrz+KMz6ZlCS0RbScnVgDVrHvPoh/fxtr8GQU3WW+rmrdZvx5no4+ftaumHls0f0z40vwBn3tm8MgtBJSKAsxJNWM8tKqSVAQYSHfgI8YRhGL9O+FYZhRK0oUUoVAq8AFxiGURLh8VuAWwAGDBgwpaioqE1vQhAEoVNprIaHT4XmWli0AtLzdfOWxZdq2YSywYLfwIxF0Y/hbtYZ4IYK+Ne5WhJx9YswMIoxciypOwr3TdYZ64LxcPMyHYS/dw/YfNrA9AK49r+QNTD4vCOb4aE5MPEq2PAMnPVLmH1H149fEAQhxnQbGYbvOY8BbxqG8WJL+4kMQxCEuFKyBf55OvSdAlc8CXuWwUs3wuk/hYNrYPsbcPb/aXmDldJt8Ph5OlB2+CyhvvYSDJzdte/BzNrH4Z2faP/twpP0tuY6cCRD8Tp46hI91gEzITEdxl4M2/4LG/4N39sGz14NNYfhjs+DDiqVB3SQbe/6Ypw242rU8pamahgwOzSTLwiC4COWwfLvgXJTgV9vwzB+aNmnn2+fBqVUFrAauNQwjE0tHVuCZUEQ4s6GZ+E/i3T21ZEIOSPgpiX6seevge1vwdf/A4PnQfluLWNwNcA/52tpw5TroK4UJn5VB6Hxxt0UvR36kU3w5g91p8PaEl1sCDqrfPGDsPllePF6HUQPnAPb39SdH/vPhCufblvHyEh4vbDij1D0sQ68kzIBpZ1BRizQPuFet872N1WBM1Vn+iNx6HNY/4x2elFKF2weWqtt9QDmfAfOvFf/3+MGt8+nNjEt8vEEQThhiGWwnA08DwwAitDWcceUUlOBWw3DuEkpdSbwR8AAFHC/YRgPt3ZsCZYFQegWlGyFdYth34dw0UNQME5vb6rRmefaEkhIg+qDOkubmgu1R+C6N6H/tPiOvaO4m3Vx4JZXtPQid4S2uHvj+zrb3HAM0gpgzEJtZZeWBzNvh4w+MHCubjHeFppq4OVFOkufN1ZnfxurAUP/H8CeCB6TNZiywYQrYfLXdWa7plif89KtsOoBvb/XrQPkwskw+GQomKDlM+sWw3l/gsoiWPVQ8LgD58K0G7RfdWI6ZPYXuz1BOMGIWbAcSyRYFgSh23N0Fzx7FeQMh6HzoWwH7FsBc78HEy6P9+hig9erW6Rn9tNZ6kNr4blr9GQBdBOcKdfr4sT0fG2T9/w12gs6d6SWbNSW6klGQ0VQ/z39ltAAteaIztwf262dQBIztN3ekc2w5tFgVtjMlOu0i0lihg6YzfIQjwuevAiKPtK/T/gK5I/TuvQN/9bFl34mXgUL/9a95SVfdsyrINXFcPBT7eDiTNb+4uuf1qs4zmS92pGWF9/xCj0eCZYFQRCE2OH16mzzsb06kN34vNY/T7sRNr2oA9IxC/XjXrcObNLyITUPhpzSfolKdTEcWgPZw3TQXl+ut7fWebGuHD76E4y/LKjbBu1/vX8V1JXBwc9g5f0w4hy4/F/STrwz2f6Wlsqc/L3geXU16tWJQ2vgpK9B7mjdTXPrK3oykzUIdrwDXhfkjYFTfgTv/0JbIvpJSNPHnHqD7lrZVKtlPX0mRZfsCIIFCZYFQRCErqN8Nyz9uQ540grgay9qF46ewmePwBs/gAGz4KpntY66oUIHZZJtDtJQAV+8qSUuNgeMu0yvsPjP0e73dXCcO1J3qVz7L729YAKc9QsoWqkD5ZrDWovuqtMyGgw46eu6MLPsCxh7iW6s887dekKTVgCXPAz9pukVgaX3ag29zaHtHo9s0sdyJMP0m3XmOW+0DtANQyQ2QkQkWBYEQRC6niObfVnkHrhEvvklrafOHanHv3uZDppHX6AdUPJGx3uE8aW5Hv4xD8p36lUCT7MOnlNydDv2msNa3x5AwZxvQb/p8Oo3tI0hSmvKT/6+LhRd95SW9cz5duTzW1uqJTOTrg4vKD28Tmvs9yzTGeWR58KWl/UqB4aW+yi71rLP+qaW60jQLJiQYFkQBEEQ2suupbrjYUq2lm5UH9KZVK8LzvktTL629YCroULLVNpa9NhTePOH8Ok/dBfIUedpTfiuJbDxWdj+tg5O5/1ASyMq9uosb/4Y/dyKIjjwqXaRibVMovKADsBLt+mAvnyXbsxz1i9hxm36seRe2ulGgucTGgmWBUEQBKEjNNXqokW/P3NtKbx8i85gFk6GcZfoYsFI2fPSL+DJC3U287o3dJa6o2x7HZb9n5YvDDu948fpDHYthcWXwPRFcO7vwh9vrAbDq4PQ7obXqy0Qt74CSb2CFom9BkKvAbqwcNAcOOUucCbFd6xClyLBsiAIgiB0Fl6v1jWvXwzFG7SW+ZQfQnqhttzzeqD/DFj9oNbR+rn2dW3DB1BbpjPVBeMBpQvS6o/CyPPAkRD6etXF8MBMbadneGHud+G0e4LNYTqTqoNQ9Ikunuw7OXT7hmdh66twZKN+fNEKSEjp/DHEGlcjvHaHzn6PXKCz/zuX6CJVgP0rtZXhpKv0NTK8upA0f6z2GI/mVd4WvF44tgdyhnXOexE6DQmWBUEQBCEWlO2A9/4Xdrylf8/srwvJju6AjH5w7WvaAeTx83TGdeKVumnKmse0/V1yls5cVx/Sz8/op10hnEk6CB8wE5bcC/s+ghvf1W4jax+H0Qvh0keCgZvHBXs/hEFzQ4O5vSv0c7IGQ//pOrjfv0o3ypn4leB+zXW6nfv+lcFt/Wfq9uflu7VcAUNrjkefr5/fE7XobWHHu/Dq7bqhkNPX4Ka5Vj+WkKYLBk+7JyghqTygHVQaq2HhfWB3RD/2ij/pgsQLH4CTro79e+lKXI26uDIhVa8qJPXSrjiNlVqKs+NdKNmsNecjz4kse9n6mn4sDoW0EiwLgiAIQizZv0pnIPvP1JKNmhJdEOhfyq8ogo/+rAvUPC4dNA+eB3s+0M1Zxl6s/aFX/AEOrA4//oLfwsxb9f9XPgDv/Fg3Uznr55DeB164Hg6s0hnf0/9XZ0u3/VfriJOz9Gt43YCCjEIdnM+4Tcs67E54/buw5l9w+j0w9HQdNH/2iJYlZA3SrdonfhV6D+6iExpnXI06QE7xac0bK7XOett/YeNzOggcsQCqDmi7QcPQcptT7oL5P45+zL+M1ysIyu7r/nlyx8ZXtFKvbAw7E8ZepLfVHdUrAA0VUH1Yjy05S1vwZQ/TzXtsNj1WCA1Wqw7qSdihtdpxJCEdhp6mP5f+LHjNEf15UjY9acgZro9Zc0TLkj65T/unR0Xp81l/VEuYeg/R98jJ39fdT7e8Ai9cC+f8Hmbc0rHzchxIsCwIgiAI3YH6Y7rQLL0g+j7NdYDS/tH7PtLygBm3BXXTABueg9e/A6563Y7dnqDlGRue0cv8oF0qZn4DZizSgfLh9bpLYXIWvHeP7niYO1rrrpf9SrtQnPnzmL79LwVHd8G7P9FZ+qzBWq4y41Z9Djc+Bxf/w5fBX6nPe0ZfnXHe/ha89k244kl4/1c6sPzay9BvSvhrVO6HD3+vdeH+rqEAxRv1SsaeZVri43Vryz5XPex4W0/YomFP1BOj5jq9+pCaq7PAoFdCAPpM1J+bmiNQvF4XZl71rA5s/3WuDsCjMXieLug0DD25aKjUjWOSs3QWftA8nXFet1ivrDTX6qC+1wC9SvLEBdB7qF5Bkcxy25BgWRAEQRBaoLFKW9wdWguzv6ULCF2NOpDqPVRn/lpyePjiDXj7x7r9d8F4uGnp8elxT3SaauChuVCxTweyA2Zpic3eD3VG3uPSKw2LVuhg+IkLdBZ34X06yD26E4afqeULT12ks//OFDj/z7oN+7bX9cpEcpZuwjL5Gvjkfr0akZKtvan7TdXZ2vQ+WhJUX66lDxX7dKDr9ehjuhv1azfXAQZkD9cdMLMGBt9P1SF4+jI9+UrN1Zr5K5/R+u2GCj3euqO6zX3OCK3pbi/7PtbFop5mPa5FH0L20M65Hu1EgmVBEARBEMJxNcKm52HIfOjVP96j6fmUbdee3OMuCWq693wAz1wJ7ga4+OGgVry2VAejxRtCj6HskNJbZ6iX/zYoy3EkwbSbYN6doU4j9ce0LMJaGNoZ1JVrR5eKfXDNKzoY72x2vAMv3Qzn/QEmXNH5x28jEiwLgiAIgiDEi30fB/2dzRKDphrY+a7O7GYN1AVue5dr7XPOMHA36+dl9NVyj3hk/v36bWsjmM7E64mNu0s7kGBZEARBEARBEKLQUrBsi7RREARBEARBEAQJlgVBEARBEAQhKhIsC4IgCIIgCEIUJFgWBEEQBEEQhChIsCwIgiAIgiAIUZBgWRAEQRAEQRCiIMGyIAiCIAiCIERBgmVBEARBEARBiIIEy4IgCIIgCIIQBQmWBUEQBEEQBCEKEiwLgiAIgiAIQhQkWBYEQRAEQRCEKEiwLAiCIAiCIAhRUIZhxHsMEVFKlQFFcXr5HOBonF5biI5cl+6HXJPuiVyX7olcl+6JXJfuSVdfl4GGYeRGeqDbBsvxRCm1xjCMqfEehxCKXJfuh1yT7olcl+6JXJfuiVyX7kl3ui4iwxAEQRAEQRCEKEiwLAiCIAiCIAhRkGA5Mg/HewBCROS6dD/kmnRP5Lp0T+S6dE/kunRPus11Ec2yIAiCIAiCIERBMsuCIAiCIAiCEAUJlk0opRYopbYrpXYppe6K93hOZJRS+5RSm5RS65VSa3zbeiul3lNK7fT9zIr3OL/sKKUeU0qVKqU2m7ZFvA5K8zff/bNRKTU5fiP/chPluvxMKXXId8+sV0qda3rsx77rsl0pdXZ8Rv3lRinVXym1TCm1VSm1RSn1bd92uV/iSAvXRe6XOKKUSlJKfaqU2uC7Lvf6tg9WSq32nf/nlFIJvu2Jvt93+R4f1JXjlWDZh1LKDvwdOAcYA3xVKTUmvqM64ZlvGMYkk3XMXcBSwzCGA0t9vwux5XFggWVbtOtwDjDc9+8W4MEuGuOJyOOEXxeAP/vumUmGYbwJ4Ps7diUw1vecB3x/74TOxQ183zCMMcBM4HbfuZf7Jb5Euy4g90s8aQJOMwxjIjAJWKCUmgn8Fn1dhgEVwI1XXmLjAAADbElEQVS+/W8EKnzb/+zbr8uQYDnIdGCXYRh7DMNoBp4FLozzmIRQLgSe8P3/CeCiOI7lhMAwjA+BY5bN0a7DhcCThmYV0Esp1adrRnpiEeW6RONC4FnDMJoMw9gL7EL/vRM6EcMwig3D+Nz3/xpgG9AXuV/iSgvXJRpyv3QBvs99re9Xp++fAZwGvOjbbr1f/PfRi8DpSinVRcOVYNlEX+CA6feDtHxDCbHFAN5VSq1VSt3i25ZvGEax7/9HgPz4DO2EJ9p1kHso/nzTt6T/mEmmJNeli/EtEZ8ErEbul26D5bqA3C9xRSllV0qtB0qB94DdQKVhGG7fLuZzH7guvsergOyuGqsEy0J3Za5hGJPRS5W3K6XmmR80tI2LWLnEGbkO3YoHgaHoJc1i4I/xHc6JiVIqDXgJ+I5hGNXmx+R+iR8RrovcL3HGMAyPYRiTgH7o7P2oOA8pKhIsBzkE9Df93s+3TYgDhmEc8v0sBf6DvpFK/MuUvp+l8RvhCU206yD3UBwxDKPE9+XjBf5JcOlYrksXoZRyogOypw3DeNm3We6XOBPpusj90n0wDKMSWAbMQsuRHL6HzOc+cF18j2cC5V01RgmWg3wGDPdVYiagBf6vxXlMJyRKqVSlVLr//8BZwGb09bjWt9u1wKvxGeEJT7Tr8Bpwja/KfyZQZVp+FmKMRe96MfqeAX1drvRVkw9GF5R92tXj+7Lj008+CmwzDONPpofkfokj0a6L3C/xRSmVq5Tq5ft/MnAmWk++DLjMt5v1fvHfR5cB7xtd2CjE0fouJwaGYbiVUt8E3gHswGOGYWyJ87BOVPKB//i0+w7gGcMw3lZKfQY8r5S6ESgCrojjGE8IlFL/Bk4FcpRSB4GfAr8h8nV4EzgXXRBTD1zf5QM+QYhyXU5VSk1CL/PvAxYBGIaxRSn1PLAV7Qxwu2EYnniM+0vOHODrwCafDhPgbuR+iTfRrstX5X6JK32AJ3xOIzbgecMwXldKbQWeVUr9EliHnujg+/mUUmoXurj5yq4crHTwEwRBEARBEIQoiAxDEARBEARBEKIgwbIgCIIgCIIgREGCZUEQBEEQBEGIggTLgiAIgiAIghAFCZYFQRAEQRAEIQoSLAuCIAiCIAhCFCRYFgRBEARBEIQoSLAsCIIgCIIgCFH4fxA4GjPr3ucvAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "g 3.1984 \ts -0.1126 \tc 0.5444, score 1.3760, [특히 피해가 큰 어린이·노인 등 나쁨 단계부터 문자로 알리는 주목을 시민강좌 역시 호평 속에 운영 중이고 이상 신청 미세먼지 발생시 등을 알려준다.]\n",
            "g 2.3138 \ts -0.0715 \tc 0.5611, score 1.2530, [특히 피해가 큰 어린이·노인 등 나쁨 단계부터 문자로 알리는 주목을 시민강좌 역시 호평 속에 운영 중이고 이상 신청 미세먼지 발생시 알려준다.]\n",
            "g 2.8772 \ts -0.1055 \tc 0.5667, score 1.3074, [특히 피해가 큰 어린이·노인 등 나쁨 단계부터 문자로 알리는 주목을 시민강좌 역시 호평 속에 운영 이상 신청 미세먼지 발생시 등을 알려준다.]\n",
            "g 2.6967 \ts -0.0654 \tc 0.5667, score 1.3174, [특히 피해가 큰 어린이·노인 등 나쁨 단계부터 문자로 알리는 주목을 시민강좌 역시 호평 속에 운영 중이고 이상 신청 미세먼지 등을 알려준다.]\n",
            "g -1.7773 \ts -0.0962 \tc 0.5722, score 0.5354, [특히 피해가 큰 어린이·노인 등 나쁨 문자로 알리는 주목을 시민강좌 역시 호평 속에 운영 중이고 이상 신청 미세먼지 발생시 등을 알려준다.]\n",
            "g -0.9565 \ts -0.1482 \tc 0.5611, score 0.6312, [특히 피해가 큰 어린이·노인 등 단계부터 문자로 알리는 주목을 시민강좌 역시 호평 속에 운영 중이고 이상 신청 미세먼지 발생시 등을 알려준다.]\n",
            "g 4.2812 \ts -0.0801 \tc 0.5722, score 1.5613, [특히 피해가 큰 어린이·노인 등 나쁨 단계부터 문자로 알리는 주목을 역시 호평 속에 운영 중이고 이상 신청 미세먼지 발생시 등을 알려준다.]\n",
            "g -1.6347 \ts -0.0951 \tc 0.5889, score 0.5436, [특히 피해가 큰 어린이·노인 등 나쁨 단계부터 문자로 알리는 주목을 시민강좌 역시 속에 운영 중이고 이상 신청 발생시 등을 알려준다.]\n",
            "g -0.9843 \ts -0.0789 \tc 0.5833, score 0.6737, [특히 피해가 큰 어린이·노인 등 나쁨 단계부터 알리는 주목을 시민강좌 호평 속에 운영 중이고 이상 신청 미세먼지 발생시 등을 알려준다.]\n",
            "g 4.8505 \ts 0.0828 \tc 0.6944, score 1.6968, [특히 피해가 큰 등 나쁨 단계부터 알리는 주목을 호평 속에 운영 중이고 이상 신청 발생시 알려준다.]\n",
            "g -1.7021 \ts 0.0259 \tc 0.6778, score 0.5644, [피해가 큰 등 단계부터 문자로 알리는 주목을 시민강좌 호평 속에 운영 중이고 이상 신청 발생시 알려준다.]\n",
            "g -1.3381 \ts -0.0461 \tc 0.6000, score 0.6309, [피해가 큰 등 나쁨 단계부터 문자로 알리는 주목을 시민강좌 역시 호평 속에 운영 중이고 이상 신청 미세먼지 발생시 등을 알려준다.]\n",
            "g 2.5242 \ts 0.0156 \tc 0.6167, score 1.3197, [특히 피해가 큰 어린이·노인 등 나쁨 단계부터 알리는 주목을 시민강좌 속에 운영 중이고 이상 신청 미세먼지 발생시 알려준다.]\n",
            "g 4.3329 \ts -0.0708 \tc 0.6111, score 1.5402, [특히 피해가 큰 등 나쁨 단계부터 문자로 알리는 주목을 시민강좌 역시 호평 속에 운영 중이고 이상 신청 발생시 등을 알려준다.]\n",
            "g 2.3796 \ts -0.0734 \tc 0.5833, score 1.2399, [특히 피해가 큰 등 나쁨 단계부터 문자로 알리는 주목을 시민강좌 역시 호평 속에 운영 중이고 이상 신청 미세먼지 발생시 등을 알려준다.]\n",
            "요약률 0.5722 유사성 -0.0801 문법성 4.2812 요약 [특히 피해가 큰 어린이·노인 등 나쁨 단계부터 문자로 알리는 주목을 역시 호평 속에 운영 중이고 이상 신청 미세먼지 발생시 등을 알려준다.]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('특히 피해가 큰 어린이·노인 등 나쁨 단계부터 문자로 알리는 주목을 역시 호평 속에 운영 중이고 이상 신청 미세먼지 발생시 등을 알려준다.',\n",
              " '특히 피해가 큰 어린이 노인 등 미세먼지 나쁨 단계부터 문자로 알리는 알림서비스가 주목을 받았으며, 이 역시 호평 속에 운영 중이고, 6개월 이상 신청 시 미세먼지 비상저감조치와 미세먼지 발생시 경보 등을 알려준다.',\n",
              " 1.5612659535474247,\n",
              " 4.281235218048096)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "di_qtu_05Zcf"
      },
      "source": [
        "# 측정 도구..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxXeH4yjTjW0"
      },
      "source": [
        "# 한국어 rouge score..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFgYYBgh5b-P",
        "outputId": "f383dd4f-37aa-44fe-8b0d-7254bcb41223"
      },
      "source": [
        "!pip install rouge-score"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.7/dist-packages (0.0.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.15.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge-score) (3.2.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.19.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge-score) (0.12.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1gLnjcb937W"
      },
      "source": [
        "# 종합 Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt2N7xL1QYuy"
      },
      "source": [
        "import re\n",
        "import six\n",
        "\n",
        "from nltk.stem import porter\n",
        "import six\n",
        "from six.moves import map\n",
        "from six.moves import range\n",
        "from rouge_score import scoring\n",
        "#from rouge_score import tokenize\n",
        "\n",
        "def tokenize(text, stemmer):\n",
        "  \"\"\"Tokenize input text into a list of tokens.\n",
        "\n",
        "  This approach aims to replicate the approach taken by Chin-Yew Lin in\n",
        "  the original ROUGE implementation.\n",
        "\n",
        "  Args:\n",
        "    text: A text blob to tokenize.\n",
        "    stemmer: An optional stemmer.\n",
        "\n",
        "  Returns:\n",
        "    A list of string tokens extracted from input text.\n",
        "  \"\"\"\n",
        "\n",
        "  # Convert everything to lowercase.\n",
        "  text = text.lower()\n",
        "  # Replace any non-alpha-numeric characters with spaces.\n",
        "  text =  six.ensure_str(text) #re.sub(r\"[^a-z0-9]+\", \" \", six.ensure_str(text))\n",
        "\n",
        "  tokens = re.split(r\"\\s+\", text)\n",
        "  if stemmer:\n",
        "    # Only stem words more than 3 characters long.\n",
        "    tokens = [stemmer.stem(x) if len(x) > 3 else x for x in tokens]\n",
        "\n",
        "  # One final check to drop any empty or invalid tokens.\n",
        "  #tokens = [x for x in tokens if re.match(r\"^[a-z0-9]+$\", six.ensure_str(x))]\n",
        "\n",
        "  return tokens\n",
        "\n",
        "class RougeScorer(scoring.BaseScorer):\n",
        "  \"\"\"Calculate rouges scores between two blobs of text.\n",
        "\n",
        "  Sample usage:\n",
        "    scorer = RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "    scores = scorer.score('The quick brown fox jumps over the lazy dog',\n",
        "                          'The quick brown dog jumps on the log.')\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, rouge_types, use_stemmer=False):\n",
        "    \"\"\"Initializes a new RougeScorer.\n",
        "\n",
        "    Valid rouge types that can be computed are:\n",
        "      rougen (e.g. rouge1, rouge2): n-gram based scoring.\n",
        "      rougeL: Longest common subsequence based scoring.\n",
        "\n",
        "    Args:\n",
        "      rouge_types: A list of rouge types to calculate.\n",
        "      use_stemmer: Bool indicating whether Porter stemmer should be used to\n",
        "        strip word suffixes to improve matching.\n",
        "    Returns:\n",
        "      A dict mapping rouge types to Score tuples.\n",
        "    \"\"\"\n",
        "\n",
        "    self.rouge_types = rouge_types\n",
        "    self._stemmer = porter.PorterStemmer() if use_stemmer else None\n",
        "\n",
        "  def score(self, target, prediction):\n",
        "    \"\"\"Calculates rouge scores between the target and prediction.\n",
        "\n",
        "    Args:\n",
        "      target: Text containing the target (ground truth) text.\n",
        "      prediction: Text containing the predicted text.\n",
        "    Returns:\n",
        "      A dict mapping each rouge type to a Score object.\n",
        "    Raises:\n",
        "      ValueError: If an invalid rouge type is encountered.\n",
        "    \"\"\"\n",
        "\n",
        "    target_tokens = tokenize(target, self._stemmer)\n",
        "    prediction_tokens = tokenize(prediction, self._stemmer)\n",
        "    result = {}\n",
        "\n",
        "    for rouge_type in self.rouge_types:\n",
        "      if rouge_type == \"rougeL\":\n",
        "        # Rouge from longest common subsequences.\n",
        "        scores = _score_lcs(target_tokens, prediction_tokens)\n",
        "      elif rouge_type == \"rougeLsum\":\n",
        "        # Note: Does not support multi-line text.\n",
        "        def get_sents(text):\n",
        "          # Assume sentences are separated by newline.\n",
        "          sents = six.ensure_str(text).split(\"\\n\")\n",
        "          sents = [x for x in sents if len(x)]\n",
        "          return sents\n",
        "\n",
        "        target_tokens_list = [\n",
        "            tokenize(s, self._stemmer) for s in get_sents(target)]\n",
        "        prediction_tokens_list = [\n",
        "            tokenize(s, self._stemmer) for s in get_sents(prediction)]\n",
        "        scores = _summary_level_lcs(target_tokens_list,\n",
        "                                    prediction_tokens_list)\n",
        "      elif re.match(r\"rouge[0-9]$\", six.ensure_str(rouge_type)):\n",
        "        # Rouge from n-grams.\n",
        "        n = int(rouge_type[5:])\n",
        "        if n <= 0:\n",
        "          raise ValueError(\"rougen requires positive n: %s\" % rouge_type)\n",
        "        target_ngrams = _create_ngrams(target_tokens, n)\n",
        "        prediction_ngrams = _create_ngrams(prediction_tokens, n)\n",
        "        scores = _score_ngrams(target_ngrams, prediction_ngrams)\n",
        "      else:\n",
        "        raise ValueError(\"Invalid rouge type: %s\" % rouge_type)\n",
        "      result[rouge_type] = scores\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def _create_ngrams(tokens, n):\n",
        "  \"\"\"Creates ngrams from the given list of tokens.\n",
        "\n",
        "  Args:\n",
        "    tokens: A list of tokens from which ngrams are created.\n",
        "    n: Number of tokens to use, e.g. 2 for bigrams.\n",
        "  Returns:\n",
        "    A dictionary mapping each bigram to the number of occurrences.\n",
        "  \"\"\"\n",
        "\n",
        "  ngrams = Counter()\n",
        "  for ngram in (tuple(tokens[i:i + n]) for i in range(len(tokens) - n + 1)):\n",
        "    ngrams[ngram] += 1\n",
        "  return ngrams\n",
        "\n",
        "\n",
        "def _score_lcs(target_tokens, prediction_tokens):\n",
        "  \"\"\"Computes LCS (Longest Common Subsequence) rouge scores.\n",
        "\n",
        "  Args:\n",
        "    target_tokens: Tokens from the target text.\n",
        "    prediction_tokens: Tokens from the predicted text.\n",
        "  Returns:\n",
        "    A Score object containing computed scores.\n",
        "  \"\"\"\n",
        "\n",
        "  if not target_tokens or not prediction_tokens:\n",
        "    return scoring.Score(precision=0, recall=0, fmeasure=0)\n",
        "\n",
        "  # Compute length of LCS from the bottom up in a table (DP appproach).\n",
        "  lcs_table = _lcs_table(target_tokens, prediction_tokens)\n",
        "  lcs_length = lcs_table[-1][-1]\n",
        "\n",
        "  precision = lcs_length / len(prediction_tokens)\n",
        "  recall = lcs_length / len(target_tokens)\n",
        "  fmeasure = scoring.fmeasure(precision, recall)\n",
        "\n",
        "  return scoring.Score(precision=precision, recall=recall, fmeasure=fmeasure)\n",
        "\n",
        "\n",
        "def _lcs_table(ref, can):\n",
        "  \"\"\"Create 2-d LCS score table.\"\"\"\n",
        "  rows = len(ref)\n",
        "  cols = len(can)\n",
        "  lcs_table = [[0] * (cols + 1) for _ in range(rows + 1)]\n",
        "  for i in range(1, rows + 1):\n",
        "    for j in range(1, cols + 1):\n",
        "      if ref[i - 1] == can[j - 1]:\n",
        "        lcs_table[i][j] = lcs_table[i - 1][j - 1] + 1\n",
        "      else:\n",
        "        lcs_table[i][j] = max(lcs_table[i - 1][j], lcs_table[i][j - 1])\n",
        "  return lcs_table\n",
        "\n",
        "\n",
        "def _backtrack_norec(t, ref, can):\n",
        "  \"\"\"Read out LCS.\"\"\"\n",
        "  i = len(ref)\n",
        "  j = len(can)\n",
        "  lcs = []\n",
        "  while i > 0 and j > 0:\n",
        "    if ref[i - 1] == can[j - 1]:\n",
        "      lcs.insert(0, i-1)\n",
        "      i -= 1\n",
        "      j -= 1\n",
        "    elif t[i][j - 1] > t[i - 1][j]:\n",
        "      j -= 1\n",
        "    else:\n",
        "      i -= 1\n",
        "  return lcs\n",
        "\n",
        "\n",
        "def _summary_level_lcs(ref_sent, can_sent):\n",
        "  \"\"\"ROUGE: Summary-level LCS, section 3.2 in ROUGE paper.\n",
        "\n",
        "  Args:\n",
        "    ref_sent: list of tokenized reference sentences\n",
        "    can_sent: list of tokenized candidate sentences\n",
        "\n",
        "  Returns:\n",
        "    summary level ROUGE score\n",
        "  \"\"\"\n",
        "  if not ref_sent or not can_sent:\n",
        "    return scoring.Score(precision=0, recall=0, fmeasure=0)\n",
        "\n",
        "  m = sum(map(len, ref_sent))\n",
        "  n = sum(map(len, can_sent))\n",
        "  if not n or not m:\n",
        "    return scoring.Score(precision=0, recall=0, fmeasure=0)\n",
        "\n",
        "  # get token counts to prevent double counting\n",
        "  token_cnts_r = collections.Counter()\n",
        "  token_cnts_c = collections.Counter()\n",
        "  for s in ref_sent:\n",
        "    # s is a list of tokens\n",
        "    token_cnts_r.update(s)\n",
        "  for s in can_sent:\n",
        "    token_cnts_c.update(s)\n",
        "\n",
        "  hits = 0\n",
        "  for r in ref_sent:\n",
        "    lcs = _union_lcs(r, can_sent)\n",
        "    # Prevent double-counting:\n",
        "    # The paper describes just computing hits += len(_union_lcs()),\n",
        "    # but the implementation prevents double counting. We also\n",
        "    # implement this as in version 1.5.5.\n",
        "    for t in lcs:\n",
        "      if token_cnts_c[t] > 0 and token_cnts_r[t] > 0:\n",
        "        hits += 1\n",
        "        token_cnts_c[t] -= 1\n",
        "        token_cnts_r[t] -= 1\n",
        "\n",
        "  recall = hits / m\n",
        "  precision = hits / n\n",
        "  fmeasure = scoring.fmeasure(precision, recall)\n",
        "  return scoring.Score(precision=precision, recall=recall, fmeasure=fmeasure)\n",
        "\n",
        "\n",
        "def _union_lcs(ref, c_list):\n",
        "  \"\"\"Find union LCS between a ref sentence and list of candidate sentences.\n",
        "\n",
        "  Args:\n",
        "    ref: list of tokens\n",
        "    c_list: list of list of indices for LCS into reference summary\n",
        "\n",
        "  Returns:\n",
        "    List of tokens in ref representing union LCS.\n",
        "  \"\"\"\n",
        "  lcs_list = [lcs_ind(ref, c) for c in c_list]\n",
        "  return [ref[i] for i in _find_union(lcs_list)]\n",
        "\n",
        "\n",
        "def _find_union(lcs_list):\n",
        "  \"\"\"Finds union LCS given a list of LCS.\"\"\"\n",
        "  return sorted(list(set().union(*lcs_list)))\n",
        "\n",
        "\n",
        "def lcs_ind(ref, can):\n",
        "  \"\"\"Returns one of the longest lcs.\"\"\"\n",
        "  t = _lcs_table(ref, can)\n",
        "  return _backtrack_norec(t, ref, can)\n",
        "\n",
        "\n",
        "def _score_ngrams(target_ngrams, prediction_ngrams):\n",
        "  \"\"\"Compute n-gram based rouge scores.\n",
        "\n",
        "  Args:\n",
        "    target_ngrams: A Counter object mapping each ngram to number of\n",
        "      occurrences for the target text.\n",
        "    prediction_ngrams: A Counter object mapping each ngram to number of\n",
        "      occurrences for the prediction text.\n",
        "  Returns:\n",
        "    A Score object containing computed scores.\n",
        "  \"\"\"\n",
        "\n",
        "  intersection_ngrams_count = 0\n",
        "  for ngram in six.iterkeys(target_ngrams):\n",
        "    intersection_ngrams_count += min(target_ngrams[ngram],\n",
        "                                     prediction_ngrams[ngram])\n",
        "  target_ngrams_count = sum(target_ngrams.values())\n",
        "  prediction_ngrams_count = sum(prediction_ngrams.values())\n",
        "\n",
        "  precision = intersection_ngrams_count / max(prediction_ngrams_count, 1)\n",
        "  recall = intersection_ngrams_count / max(target_ngrams_count, 1)\n",
        "  fmeasure = scoring.fmeasure(precision, recall)\n",
        "\n",
        "  return scoring.Score(precision=precision, recall=recall, fmeasure=fmeasure)  "
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzMFSPQuRYgS"
      },
      "source": [
        "\n",
        "scorer = RougeScorer(['rouge1','rouge2', 'rougeL'], use_stemmer=True)"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "iUXcwD9OQeex",
        "outputId": "638bd0be-8b2e-4fd7-c274-6d8d58a92c24"
      },
      "source": [
        "@@@"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-49-7e1bb4465bf8>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    @@@\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_AzsvMX6KEq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9705123f-15a3-46b5-cf09-0bf27d009de3"
      },
      "source": [
        "import io\n",
        "\n",
        "result_data = {}\n",
        "\n",
        "result_data['ID'] = []\n",
        "result_data['Original'] = []\n",
        "result_data['Step1'] = []\n",
        "result_data['Step2'] = []\n",
        "result_data['Step3'] = []\n",
        "result_data['Ground Truth'] = []\n",
        "\n",
        "result_data['S1.R1'] = []\n",
        "result_data['S1.R2'] = []\n",
        "result_data['S1.RL'] = []\n",
        "result_data['S1.Similarity'] = []\n",
        "result_data['S1.Grammar'] = []\n",
        "\n",
        "result_data['S2.R1'] = []\n",
        "result_data['S2.R2'] = []\n",
        "result_data['S2.RL'] = []\n",
        "result_data['S2.Similarity'] = []\n",
        "result_data['S2.Grammar'] = []\n",
        "\n",
        "result_data['S3.R1'] = []\n",
        "result_data['S3.R2'] = []\n",
        "result_data['S3.RL'] = []\n",
        "result_data['S3.Similarity'] = []\n",
        "result_data['S3.Grammar'] = []\n",
        "\n",
        "statistics_columns = ['S1.R1','S1.R2','S1.RL','S1.Similarity','S1.Grammar','S2.R1','S2.R2','S2.RL','S2.Similarity','S2.Grammar','S3.R1','S3.R2','S3.RL','S3.Similarity','S3.Grammar']\n",
        "\n",
        "atten_rate = 0.2\n",
        "similarity = 1.0\n",
        "std_factor = 3.0\n",
        "\n",
        "#es = ExtactiveSummarizer()\n",
        "\n",
        "pd.set_option('display.max_columns', 100)\n",
        "\n",
        "logout = False\n",
        "for rate in range(0,1):\n",
        "    comp_rate=1.5\n",
        "    #result_data['comp_rate'].append(comp_rate)\n",
        "    for try_count in range(10):\n",
        "        \n",
        "        full_text = get_prepared_doc(sentences_dataset[try_count])\n",
        "        '''\n",
        "        try:\n",
        "            del model\n",
        "            print('delete model')\n",
        "        except Exception as ex:\n",
        "            pass\n",
        "        model = EncoderDecoderModel.from_pretrained(\"/content/drive/MyDrive/GAN_ENDE/en_sentence_complete_model\")\n",
        "        '''\n",
        "        try:\n",
        "\n",
        "            #org_text = besm(full_text,num_sentences=9)\n",
        "            step1 = es.generate_summary(full_text,top_n=6,min_length=0)[0]\n",
        "            \n",
        "            org_sentences = np.array(nltk.sent_tokenize(step1.strip()))\n",
        "            if logout:\n",
        "                print('BESM Summary sentance length:',len(org_sentences))\n",
        "            (step2,step3),grammar,simil = summary(full_text,step1,steps=2,top_rank=2,comp_rate=comp_rate)\n",
        "            '''\n",
        "            tmp = []\n",
        "            for txt in np.array(nltk.sent_tokenize(step2.strip())):\n",
        "                tmp.append(sentence_correct(txt))         \n",
        "            step3 = ' '.join(tmp)\n",
        "            '''\n",
        "\n",
        "            ground_trouth = gold_summary[try_count]\n",
        "            \n",
        "            print('='*50 + ' Original Document ' + str(try_count) + '='*50)\n",
        "            for txt in np.array(nltk.sent_tokenize(full_text)):\n",
        "                print(txt)\n",
        "            print('-'*50 + ' Step1 ' + str(try_count) + '-'*50)\n",
        "            for txt in np.array(nltk.sent_tokenize(step1.strip())):\n",
        "                print(txt)                \n",
        "            print('-'*50 + ' Step2 ' + str(try_count) + '-'*50)\n",
        "            for txt in np.array(nltk.sent_tokenize(step2.strip())):\n",
        "                print(txt)\n",
        "            print('-'*50 + ' Step3 ' + str(try_count) + '-'*50)\n",
        "            for txt in np.array(nltk.sent_tokenize(step3.strip())):\n",
        "                print(txt)                \n",
        "            print('-'*50 + ' Ground truth' + '-'*50)\n",
        "            print(ground_trouth)\n",
        "            print('-'*120)\n",
        "\n",
        "            #with io.open('/content/drive/MyDrive/GAN_ENDE/CNN_Daily_summary_result.txt','a',encoding='utf8') as f:\n",
        "            #    f.write(org_text + '\\r\\n\\r\\n')\n",
        "            s1_rouge = scorer.score(ground_trouth,step1)\n",
        "            s2_rouge = scorer.score(ground_trouth,step2)\n",
        "            s3_rouge = scorer.score(ground_trouth,step3)\n",
        "            s1_simil = similarity3(full_text,step1)\n",
        "            s2_simil = similarity3(full_text,step2)\n",
        "            s3_simil = similarity3(full_text,step3)\n",
        "            s1_gramm = grammar3(full_text,step1)\n",
        "            s2_gramm = grammar3(full_text,step2) \n",
        "            s3_gramm = grammar3(full_text,step3)  \n",
        "            #print(scores['rouge1'].fmeasure)\n",
        "            #if s2_rouge['rouge1'].fmeasure > 0.1:\n",
        "\n",
        "            result_data['ID'].append('CNN/DM-'+str(try_count))\n",
        "            result_data['Original'].append(full_text)\n",
        "            result_data['Step1'].append(step1)\n",
        "            result_data['Step2'].append(step2)\n",
        "            result_data['Step3'].append(step3)\n",
        "            result_data['Ground Truth'].append(ground_trouth)\n",
        "\n",
        "            result_data['S1.R1'].append(s1_rouge['rouge1'].fmeasure)\n",
        "            result_data['S1.R2'].append(s1_rouge['rouge2'].fmeasure)\n",
        "            result_data['S1.RL'].append(s1_rouge['rougeL'].fmeasure)\n",
        "            result_data['S1.Similarity'].append(s1_simil)\n",
        "            result_data['S1.Grammar'].append(s1_gramm)\n",
        "\n",
        "            result_data['S2.R1'].append(s2_rouge['rouge1'].fmeasure)\n",
        "            result_data['S2.R2'].append(s2_rouge['rouge2'].fmeasure)\n",
        "            result_data['S2.RL'].append(s2_rouge['rougeL'].fmeasure)\n",
        "            result_data['S2.Similarity'].append(s2_simil)\n",
        "            result_data['S2.Grammar'].append(s2_gramm)\n",
        "\n",
        "            result_data['S3.R1'].append(s3_rouge['rouge1'].fmeasure)\n",
        "            result_data['S3.R2'].append(s3_rouge['rouge2'].fmeasure)\n",
        "            result_data['S3.RL'].append(s3_rouge['rougeL'].fmeasure)\n",
        "            result_data['S3.Similarity'].append(s3_simil)\n",
        "            result_data['S3.Grammar'].append(s3_gramm)\n",
        "\n",
        "            print()\n",
        "            df_result_data = pd.DataFrame(result_data)\n",
        "            print(df_result_data[statistics_columns].iloc[-1])\n",
        "            print()\n",
        "            df_mean = df_result_data[statistics_columns].mean()\n",
        "            print('** Mean result **')\n",
        "            print(df_mean)\n",
        "            del df_mean\n",
        "            del df_result_data\n",
        "        except Exception as ex:\n",
        "            print(ex)\n",
        "\n",
        "print('** Total mean result **')\n",
        "df_result_data = pd.DataFrame(result_data)\n",
        "df_mean = df_result_data[statistics_columns].mean()\n",
        "print(df_mean)\n",
        "df_result_data.to_pickle(\"/content/drive/MyDrive/GAN_ENDE/df_result_93.pkl\")\n",
        "\n",
        "df_result_data[['ID']+statistics_columns]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================== Original Document 0==================================================\n",
            "구리·남양주시가 타 시·군과 차별화된 미세먼지 저감 대책을 세웠다.\n",
            "7일 구리시에 따르면 2022년까지 미세먼지 농도를 현재 24㎍/㎥에서 19㎍/㎥로 낮추겠다는 목표로 단기 및 중장기 대책을 마련했다.\n",
            "먼저 버스정류장 저감 시스템을 설치한다.\n",
            "시는 한국철도기술연구원과 함께 버스 정류장에 사물인터넷(IoT)을 기반으로 한 미세먼지 집진 모듈을 설치하는 기술을 개발하고 있다.\n",
            "올해부터 3년간 총 10억원의 예산을 들여 관내 버스 중앙 차로 버스정류장에 시스템을 설치할 계획이다.\n",
            "지난해 말부터는 미세먼지 취약계층 이용시설에 휴대용 미세먼지 측정기를 보급하고 있다.\n",
            "올해까지 시립어린이집 12개소와 지역아동센터 15개소에 미세먼지 측정기 설치를 완료할 예정이다.\n",
            "또한 미세먼지 농도에 따라 구리타워의 조명 색상을 파랑(좋음), 녹색(보통), 노랑(나쁨), 빨강(매우 나쁨) 등으로 바꿔 시민들이 대기질을 즉시 알 수 있도록 했다.\n",
            "시 관계자는 \"현재 관내 미세먼지 취약계층이 상주하는 경로당, 어린이집 등 시설에 공기청정기 898개를 보급하고, 미세먼지 저감을 위한 계획과 실행 방법 등을 담은 소통형 스마트폰 앱(App)을 개발하는 등 미세먼지 대응에 총력을 다하고 있다\"고 말했다.\n",
            "남양주시 역시 미세먼지 측정망 운영 등 미세먼지 저감 10대 중점과제 등 맞춤형 미세먼지 종합대책을 마련해 시행 중이다.\n",
            "특히 미세먼지 피해가 큰 어린이·노인 등 민감 계층에게 나쁨 단계부터 미세먼지 예보를 문자로 알리는 맞춤형 예·경보제가 주목을 받고 있다.\n",
            "찾아가는 미세먼지 시민강좌 역시 호평 속에 운영 중이다.\n",
            "연간 300회 실시되는 이 강좌는 10인 이상 신청 단체에게 미세먼지 위해성과 고농도 미세먼지 발생시 행동요령 등을 알려준다.\n",
            "또한 교육시설에는 미세먼지 알림 시스템을 구축해 운영하고 있다.\n",
            "관내 어린이집, 유치원, 초중학교 등 교육시설 700개소에 남양주시 자체 대기오염 측정소에서 측정된 해당 지역의 실시간 미세먼지 정보를 제공하고 있다.\n",
            "시 관계자는 \"미세먼지 저감은 단기적인 계획이나 한두 가지 정책만으로 효과를 볼 수 없다\"며 \"종합적인 대책 마련은 물론이고 시민들의 피부에 와 닿는 정책을 마련하기 위해 애쓰고 있다\"고 말했다.\n",
            "-------------------------------------------------- Step1 0--------------------------------------------------\n",
            "특히 미세먼지 피해가 큰 어린이·노인 등 민감 계층에게 나쁨 단계부터 미세먼지 예보를 문자로 알리는 맞춤형 예·경보제가 주목을 받고 있다.\n",
            "찾아가는 미세먼지 시민강좌 역시 호평 속에 운영 중이다.\n",
            "연간 300회 실시되는 이 강좌는 10인 이상 신청 단체에게 미세먼지 위해성과 고농도 미세먼지 발생시 행동요령 등을 알려준다.\n",
            "또한 교육시설에는 미세먼지 알림 시스템을 구축해 운영하고 있다.\n",
            "관내 어린이집, 유치원, 초중학교 등 교육시설 700개소에 남양주시 자체 대기오염 측정소에서 측정된 해당 지역의 실시간 미세먼지 정보를 제공하고 있다.\n",
            "시 관계자는 \"미세먼지 저감은 단기적인 계획이나 한두 가지 정책만으로 효과를 볼 수 없다\"며 \"종합적인 대책 마련은 물론이고 시민들의 피부에 와 닿는 정책을 마련하기 위해 애쓰고 있다\"고 말했다.\n",
            "-------------------------------------------------- Step2 0--------------------------------------------------\n",
            "피해가 큰 어린이·노인 등 계층에게 문자로 알리는 시민강좌 역시 호평 운영 중이다.\n",
            "연간 이 강좌는 10인 이상 신청 단체에게 위해성과 미세먼지 또한 알림 구축해 있다.\n",
            "관내 초중학교 등 남양주시 자체 대기오염 측정소에서 측정된 지역의 실시간 제공하고 있고 시 관계자는 계획이나 가지 정책만으로 볼 수 없다며 물론이고 시민들의 와 정책을 위해 있다고 말했다.\n",
            "-------------------------------------------------- Step3 0--------------------------------------------------\n",
            "피해가 큰 어린이 노인 등 취약 계층에게 문자메시지를 문자로 알리는 시민강좌 역시 호평 속에 활발하게 운영 중이다.\n",
            "연간 2회 열리는 이 강좌는 10인 이상 신청 단체에게 위해성과 미세먼지 발생량 또한 알림 시스템을 구축해 운영하고 있다.\n",
            "관내 초중학교 등 남양주시 자체 대기오염 측정소에서 측정된 지역의 미세먼지 농도를 실시간 제공하고 있는데, 시 관계자는 이번 계획이나 몇 가지 정책만으로 볼 수 없다며 시민은 물론이고 시민들의 정서와 정서에 와 닿는 정책을 마련하기 위해 노력하고 있다고 말했다.\n",
            "-------------------------------------------------- Ground truth--------------------------------------------------\n",
            "구리시는 미세먼지 농도를 낮추기위해 단기 및 중장기 대책을 마련하고 버스정류장에 저감 시스템 설치 및 휴대용 미세먼지 측정기 보급 등을 실시할 예정이다.\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "S1.R1            0.050420\n",
            "S1.R2            0.000000\n",
            "S1.RL            0.050420\n",
            "S1.Similarity    0.836638\n",
            "S1.Grammar       0.999929\n",
            "S2.R1            0.027397\n",
            "S2.R2            0.000000\n",
            "S2.RL            0.027397\n",
            "S2.Similarity    0.825355\n",
            "S2.Grammar       0.999916\n",
            "S3.R1            0.064516\n",
            "S3.R2            0.021978\n",
            "S3.RL            0.043011\n",
            "S3.Similarity    0.810965\n",
            "S3.Grammar       0.999943\n",
            "Name: 0, dtype: float64\n",
            "\n",
            "** Mean result **\n",
            "S1.R1            0.050420\n",
            "S1.R2            0.000000\n",
            "S1.RL            0.050420\n",
            "S1.Similarity    0.836638\n",
            "S1.Grammar       0.999929\n",
            "S2.R1            0.027397\n",
            "S2.R2            0.000000\n",
            "S2.RL            0.027397\n",
            "S2.Similarity    0.825355\n",
            "S2.Grammar       0.999916\n",
            "S3.R1            0.064516\n",
            "S3.R2            0.021978\n",
            "S3.RL            0.043011\n",
            "S3.Similarity    0.810965\n",
            "S3.Grammar       0.999943\n",
            "dtype: float64\n"
          ]
        }
      ]
    }
  ]
}