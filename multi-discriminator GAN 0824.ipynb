{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "korean_frame_token_0_1.0_gamma_10.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e0a4690f59a64cdd8e982dbe49026c76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0ddc7e842c194c56bb75603da6dce365",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d77b71fb05b24cb4910395d6ece596c1",
              "IPY_MODEL_6023471cebc14635982d90180d9f516f",
              "IPY_MODEL_6bdfafb2b0ae4fd9a512dd6444f0f4f7"
            ]
          }
        },
        "0ddc7e842c194c56bb75603da6dce365": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d77b71fb05b24cb4910395d6ece596c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9410f7ca4ba24d45904c35f4a7c17451",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7efccaf87ec643eeafdab75d0f8bd2b5"
          }
        },
        "6023471cebc14635982d90180d9f516f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2016c290f9a04aaf9554376dccc9d7b1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 795,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 795,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f6ee093161064e1ca4721bf40b962bbf"
          }
        },
        "6bdfafb2b0ae4fd9a512dd6444f0f4f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_59dec3a1eef64e8eaaaea29523cfcdeb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 795/795 [00:00&lt;00:00, 31.4kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_995560843c064d70ba10075cac9481f5"
          }
        },
        "9410f7ca4ba24d45904c35f4a7c17451": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7efccaf87ec643eeafdab75d0f8bd2b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2016c290f9a04aaf9554376dccc9d7b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f6ee093161064e1ca4721bf40b962bbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "59dec3a1eef64e8eaaaea29523cfcdeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "995560843c064d70ba10075cac9481f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "68a5cc21e99e4d778949428c1d699fd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6e282483bb284c2bb4aa21376b0e6d11",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f21261246ab14ca2b0815d77016fb115",
              "IPY_MODEL_721cdd3b8de6419389543e0d48e1d16f",
              "IPY_MODEL_507b8ab17e53493cb897562fe52f60c1"
            ]
          }
        },
        "6e282483bb284c2bb4aa21376b0e6d11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f21261246ab14ca2b0815d77016fb115": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6f96f960b53648c489b95f296ebd57a7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9749f68dd09a44c3ab2bfb6ac2ef2dea"
          }
        },
        "721cdd3b8de6419389543e0d48e1d16f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_744d0e095fea49f1aaef002bcf9efe02",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 3971,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3971,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3b6d67c28393497e8344443b9233dc97"
          }
        },
        "507b8ab17e53493cb897562fe52f60c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_04acccf1544a4f02b62a262342f022f6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3.97k/3.97k [00:00&lt;00:00, 165kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bee604b5a42b4915a10ba3547cc3bb14"
          }
        },
        "6f96f960b53648c489b95f296ebd57a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9749f68dd09a44c3ab2bfb6ac2ef2dea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "744d0e095fea49f1aaef002bcf9efe02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3b6d67c28393497e8344443b9233dc97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "04acccf1544a4f02b62a262342f022f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bee604b5a42b4915a10ba3547cc3bb14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bfc749d039fc48d19464a8220483e593": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8d604e9e156845469e9e29a2bf23f236",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_522955f556124fda9ad8ac09331bbf75",
              "IPY_MODEL_d29fab4003c5404fa87b0779aaaed8e5",
              "IPY_MODEL_ea575589e1f144b99c8ac5ad187db305"
            ]
          }
        },
        "8d604e9e156845469e9e29a2bf23f236": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "522955f556124fda9ad8ac09331bbf75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9374bd1f52bf403aaa18543f503b0bbe",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eeb1ab0fbf9045b2af1c4782f7937a39"
          }
        },
        "d29fab4003c5404fa87b0779aaaed8e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3ac86c33a70441ac8895f54de078a7a9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 733,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 733,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_927135024a7449c9a03dacc56dfa397a"
          }
        },
        "ea575589e1f144b99c8ac5ad187db305": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_73516fa006d141a382144ba23cdcb69a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 733/733 [00:00&lt;00:00, 32.1kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_695c17d7bf0e4582be8f5de2c71f7d06"
          }
        },
        "9374bd1f52bf403aaa18543f503b0bbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eeb1ab0fbf9045b2af1c4782f7937a39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3ac86c33a70441ac8895f54de078a7a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "927135024a7449c9a03dacc56dfa397a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "73516fa006d141a382144ba23cdcb69a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "695c17d7bf0e4582be8f5de2c71f7d06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "41adff6bd84c4d5b8ca88509e47a4ec8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ab806100057846b891bbec906dd60cc8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fb9457c1fdef45cea3785183e9064cb4",
              "IPY_MODEL_623853cfb34543cf8306a1fce8e55493",
              "IPY_MODEL_b68e4b63a28b4d849019a96c5f3ca5cb"
            ]
          }
        },
        "ab806100057846b891bbec906dd60cc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fb9457c1fdef45cea3785183e9064cb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b52371d59b624aceaec8d7c494357685",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c633ce0bca804bb0ba4b2757d4c883aa"
          }
        },
        "623853cfb34543cf8306a1fce8e55493": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e3e580b7cf62496eb9ecb0702441157d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 122,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 122,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_33d3d9f237c6465886cd5fe9695951a1"
          }
        },
        "b68e4b63a28b4d849019a96c5f3ca5cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6feb107f141442a6880debe95ea606c5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 122/122 [00:00&lt;00:00, 4.45kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_31bfde8636334955a8936caf8b48b64c"
          }
        },
        "b52371d59b624aceaec8d7c494357685": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c633ce0bca804bb0ba4b2757d4c883aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e3e580b7cf62496eb9ecb0702441157d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "33d3d9f237c6465886cd5fe9695951a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6feb107f141442a6880debe95ea606c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "31bfde8636334955a8936caf8b48b64c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "78758051efcd4fc084b07a84038b5b19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_09b5266c879f416781be89b29e6d5a9c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ea43c2553f10483f8cdd07f098e1bd8e",
              "IPY_MODEL_0bee704f2c484c3abfb7126ef5611701",
              "IPY_MODEL_833df243ab844b38aa798434ee6b23f6"
            ]
          }
        },
        "09b5266c879f416781be89b29e6d5a9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ea43c2553f10483f8cdd07f098e1bd8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3e099e0056a44bca8b2274c4478f6f0a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c9a286f4528c4c9e9486eed2e2859e4c"
          }
        },
        "0bee704f2c484c3abfb7126ef5611701": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3591faad15eb4e489be71e61845d57ac",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 229,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 229,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e4bb481c088442fe99bc48e1cec70762"
          }
        },
        "833df243ab844b38aa798434ee6b23f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f58214b43cb547d5ad63ef6e3eeef67d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 229/229 [00:00&lt;00:00, 9.67kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_05c7ddea8f2248f09f7cb698bd8f0cb7"
          }
        },
        "3e099e0056a44bca8b2274c4478f6f0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c9a286f4528c4c9e9486eed2e2859e4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3591faad15eb4e489be71e61845d57ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e4bb481c088442fe99bc48e1cec70762": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f58214b43cb547d5ad63ef6e3eeef67d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "05c7ddea8f2248f09f7cb698bd8f0cb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ed9f4716f4c54a56bf899306d03b49e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9cd6651e738d4d818614a0a19e209578",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_94a0e1602f354ec39a67c59970b6fccc",
              "IPY_MODEL_8a3d3edcedd5450b9040f77114c0f518",
              "IPY_MODEL_27830f24a9da4ba79e2d9e271eeacaa0"
            ]
          }
        },
        "9cd6651e738d4d818614a0a19e209578": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "94a0e1602f354ec39a67c59970b6fccc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_95032f16a45a453cb4de0458859780dd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_552f004af0544e599f8513d24487e22f"
          }
        },
        "8a3d3edcedd5450b9040f77114c0f518": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d5273c903c9649b2a0cd06ea8c8bac53",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2239713201,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2239713201,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_581a1e4a4ea44d039bc487c07a6c2eac"
          }
        },
        "27830f24a9da4ba79e2d9e271eeacaa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f2568987a4084cf9ba67fcc8a29d2f9c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2.24G/2.24G [00:58&lt;00:00, 39.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3e81761278314ce4876f7e74c2a250d6"
          }
        },
        "95032f16a45a453cb4de0458859780dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "552f004af0544e599f8513d24487e22f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d5273c903c9649b2a0cd06ea8c8bac53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "581a1e4a4ea44d039bc487c07a6c2eac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f2568987a4084cf9ba67fcc8a29d2f9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3e81761278314ce4876f7e74c2a250d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e36b30b4ad564813ab8bee0a4e68a9f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_aee89f781c8a4fb380e2a0750ece52b0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b0709de72aed489bbeff7feba6e55a2a",
              "IPY_MODEL_2788e247e8f146d78ee372b8f5c8f481",
              "IPY_MODEL_403782c2dd964b8f8884f6c487c5ef75"
            ]
          }
        },
        "aee89f781c8a4fb380e2a0750ece52b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b0709de72aed489bbeff7feba6e55a2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_37c4a1a264cb4e92b07f6cc792135750",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6da97c72fd70454bb4e88727a2fcbff3"
          }
        },
        "2788e247e8f146d78ee372b8f5c8f481": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a0e95a3340444c6e9c2557b15e96acd8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 53,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 53,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3ca8f80f42b44269a4f9c695ca0f8c83"
          }
        },
        "403782c2dd964b8f8884f6c487c5ef75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4646d5cad24e47aaa0cdb4d377e0fac5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 53.0/53.0 [00:00&lt;00:00, 2.03kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1b945686f09c455ab5ad9fcb8e614ee3"
          }
        },
        "37c4a1a264cb4e92b07f6cc792135750": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6da97c72fd70454bb4e88727a2fcbff3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a0e95a3340444c6e9c2557b15e96acd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3ca8f80f42b44269a4f9c695ca0f8c83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4646d5cad24e47aaa0cdb4d377e0fac5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1b945686f09c455ab5ad9fcb8e614ee3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f3ae4d8813ed40e0b8a6a64c29530bba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_04efab9bf6f841a18eaa11a63f2fa0bb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0108240159264bd0b2db9c579bea6e44",
              "IPY_MODEL_4af166179809493b9502b704a37c96aa",
              "IPY_MODEL_a5a9b20c881843dba1d5f7a8221617b8"
            ]
          }
        },
        "04efab9bf6f841a18eaa11a63f2fa0bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0108240159264bd0b2db9c579bea6e44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_88db2814d036493d807ae7382588e690",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e0a7b83e1db54188aa93a466164c1656"
          }
        },
        "4af166179809493b9502b704a37c96aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1ea6f3d9dba4443595c64909e521f3f3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 5069051,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 5069051,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e46c8fe544a347e6a15c24c1343996d5"
          }
        },
        "a5a9b20c881843dba1d5f7a8221617b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_25cb88644dec441e9e1c98827be599b8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5.07M/5.07M [00:00&lt;00:00, 5.55MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_978da94885664bdb8549049dd5f42566"
          }
        },
        "88db2814d036493d807ae7382588e690": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e0a7b83e1db54188aa93a466164c1656": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1ea6f3d9dba4443595c64909e521f3f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e46c8fe544a347e6a15c24c1343996d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "25cb88644dec441e9e1c98827be599b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "978da94885664bdb8549049dd5f42566": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6c0b2d5d51004bba9283a8bf9550baf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_00ff0b227ed143debdded0e337e94b29",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8c27b993425a4f3ea1c7d5582af3c2c6",
              "IPY_MODEL_903b1291b3414b7ba060e8c46ea24f88",
              "IPY_MODEL_f803321e9cb74ab38860bfa95c5b359f"
            ]
          }
        },
        "00ff0b227ed143debdded0e337e94b29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8c27b993425a4f3ea1c7d5582af3c2c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d130d9d563e94120880c324704beb6d3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_502f854d8e6b41a0926daa81728813f6"
          }
        },
        "903b1291b3414b7ba060e8c46ea24f88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3c2db1f53f1c4d19a373ca274bae4626",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 150,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 150,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1a5b71a528024adba646e239c7dc2c48"
          }
        },
        "f803321e9cb74ab38860bfa95c5b359f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cc8f1e1c4971419aa8260d500edd7749",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 150/150 [00:00&lt;00:00, 5.48kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_972956db98ec4620bc6f01f8275195a6"
          }
        },
        "d130d9d563e94120880c324704beb6d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "502f854d8e6b41a0926daa81728813f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3c2db1f53f1c4d19a373ca274bae4626": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1a5b71a528024adba646e239c7dc2c48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cc8f1e1c4971419aa8260d500edd7749": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "972956db98ec4620bc6f01f8275195a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "367e67d87dec4f04b45bfec54d71777b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6c7cf8e1c172461995f183eb99bca71e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b636ad58b09d41f28089607027a5bffe",
              "IPY_MODEL_637ddfc05ecd420b88591ea79e8d979e",
              "IPY_MODEL_ee550de80ab444c5928996c6ea5907d8"
            ]
          }
        },
        "6c7cf8e1c172461995f183eb99bca71e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b636ad58b09d41f28089607027a5bffe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_29bb24363e294ee38dfc2c1384f85adb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ded690a2f6be46cdb7473e863d442382"
          }
        },
        "637ddfc05ecd420b88591ea79e8d979e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d685e26087e349af8463cfcdacad60cf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 9096735,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 9096735,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_76c73beb322a4cc0a21fc3721cb2f97a"
          }
        },
        "ee550de80ab444c5928996c6ea5907d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e7c0b6f965f9486f870e7d7e2109f40d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9.10M/9.10M [00:00&lt;00:00, 21.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_61f00dfeabbf48e395f149990773a7ad"
          }
        },
        "29bb24363e294ee38dfc2c1384f85adb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ded690a2f6be46cdb7473e863d442382": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d685e26087e349af8463cfcdacad60cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "76c73beb322a4cc0a21fc3721cb2f97a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e7c0b6f965f9486f870e7d7e2109f40d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "61f00dfeabbf48e395f149990773a7ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5607d59b87e04285a5914b05d5e21668": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f8392076ac474b91ab5566a12756f00a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_78e28e5372fa472b8a7ba49775b2a10b",
              "IPY_MODEL_6c2301db406b43e8a3ac5312f08a1810",
              "IPY_MODEL_ea539075646547369e35b405175f7cd4"
            ]
          }
        },
        "f8392076ac474b91ab5566a12756f00a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "78e28e5372fa472b8a7ba49775b2a10b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d4082039517c439bbba70c404fb646d2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c9c2f8faa40c4ae9afb47fbabcbf0150"
          }
        },
        "6c2301db406b43e8a3ac5312f08a1810": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7b98d284115b4cdaaa4c0e1fadfb5482",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 502,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 502,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5425c989fe464489afc493a0c65536d6"
          }
        },
        "ea539075646547369e35b405175f7cd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8fe4848bae784b7aa3c51e8c8e282243",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 502/502 [00:00&lt;00:00, 21.1kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_45124ac94efb4acc88e3b6570c7b8950"
          }
        },
        "d4082039517c439bbba70c404fb646d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c9c2f8faa40c4ae9afb47fbabcbf0150": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7b98d284115b4cdaaa4c0e1fadfb5482": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5425c989fe464489afc493a0c65536d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8fe4848bae784b7aa3c51e8c8e282243": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "45124ac94efb4acc88e3b6570c7b8950": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dc9d1d2de0cc417fa6dec46e9c6a2d92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_eab763435e564100a61de685e1100a06",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0b858c3b74544d7188e8a5596b4cfa69",
              "IPY_MODEL_f3c41913a37a4208bf1ef7e9207fc335",
              "IPY_MODEL_f765836bf30b4782b1fb594d282076b5"
            ]
          }
        },
        "eab763435e564100a61de685e1100a06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0b858c3b74544d7188e8a5596b4cfa69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_01248e30f0354eae90e51a7a697fc9bb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a857817b6f1f46b38151d531509fb1ec"
          }
        },
        "f3c41913a37a4208bf1ef7e9207fc335": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d89c380ac93041c5be7dab88fb7b0c7f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 191,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 191,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3accdf41c5ba41b5a68cbfff8ea69f2d"
          }
        },
        "f765836bf30b4782b1fb594d282076b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_83ebb6adcc044f27866621e27ac0b13b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 191/191 [00:00&lt;00:00, 7.66kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b9efd5e44e6d4ce98cf8ab4d64fdd297"
          }
        },
        "01248e30f0354eae90e51a7a697fc9bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a857817b6f1f46b38151d531509fb1ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d89c380ac93041c5be7dab88fb7b0c7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3accdf41c5ba41b5a68cbfff8ea69f2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "83ebb6adcc044f27866621e27ac0b13b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b9efd5e44e6d4ce98cf8ab4d64fdd297": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dolmani38/Summary2/blob/main/multi-discriminator%20GAN%200824.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkQCxNatSIOk"
      },
      "source": [
        "# Multi-Discriminator GAN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZjIW9VwyjDf"
      },
      "source": [
        "ABSTRACT\n",
        "\n",
        "----\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c20I_OEErmP9"
      },
      "source": [
        "# Related works\n",
        "\n",
        "두개 이상의 discriminator를 사용하는 GAN 연구에 대하여 알아본다.\n",
        "\n",
        "어떤 목적으로 복수의 discriminator를 사용하고 그 효과는 무엇인지 알아본다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uFOIbzcFagq"
      },
      "source": [
        "## 0. N개의 discriminator를 활용한 연구 \n",
        "\n",
        "Generative adversarial networks (Goodfellow et al. (2014))\n",
        "\n",
        "Generator를 multi로 한 연구들...\n",
        "\n",
        "1) Q. Hoang, T. Dinh Nguyen, T. Le, and D. Phung, “Multi-Generator Generative Adversarial Nets,” ArXiv e-prints, Aug. 2017.\n",
        "\n",
        "2) Multi-Agent Diverse Generative Adversarial Networks\n",
        "\n",
        "Federated learning의 한 지류가 될수도 있을 것...\n",
        "\n",
        "H. B. McMahan, E. Moore, D. Ramage, and B. A. y Arcas, “Federated learning of deep networks using model averaging,” CoRR, vol. abs/1602.05629, 2016."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa-0-i84rqoX"
      },
      "source": [
        "## 1. Automatic Image Colorization based on Multi-Discriminators Generative Adversarial Networks [품질향상]\n",
        "\n",
        "GAN은 흑백의 이미지를 입력하여 Color화 된 이미지를 생성해 낼 수 있다.\n",
        "본 논문은 두개의 discriminator를 이용하여 더 produces\n",
        "more realistic quality results.\n",
        "\n",
        "Different from conventional GAN network architecture,\n",
        "Park et al. [13] (S.-J. Park, H. Son, S. Cho, K.-S. Hong, and S. Lee, “Srfeat: Single image super-resolution with feature discrimination,” in Proceedings of the European Conference on Computer Vision, 2018, pp. 439–455.) introduce architecture based on combination of one generator associated with two discriminators. For colorization task, we propose an extended model, illustrated in Fig.1, which uses two discriminators: <font color='red'><b>an image discriminator Di and a feature discriminator Df</b> </font>. The first one discriminates real images (RGB) from colorized images by inspecting their pixel values, while the second discriminates real images from colorized ones by inspecting their feature maps, noted respectively\n",
        "VGG(y) and VGG(G(x)) .\n",
        "\n",
        "본 논문의 Proposed Loss functions 중 GAN에 대한 Loss은 다음과 같다.\n",
        "\n",
        "$$ L_{M-dis}(G,D_i,D_f) = \\lambda_iL_{GAN}(G,D_i) + \\lambda_fL_{GAN}(G,D_f)  $$\n",
        "where lambda_i and lambda_f denote a defined weighting factors\n",
        "\n",
        "실험에 있어서도 lambda_i and lambda_f 의 값을 특정 값을 설정하여 실험 하였다.\n",
        "그 값은 최적의 값이 였을까??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxfz7tKoDcZQ"
      },
      "source": [
        "## 2. UMLE: Unsupervised Multi-discriminator Network for Low Light Enhancement [품질 향상]\n",
        "\n",
        "Low-light image enhancement, such as recovering color and texture details from low-light images, is a complex and vital task. For automated driving, low-light scenarios will have serious implications for vision-based applications. To address this problem, we propose a real-time unsupervised generative adversarial network (GAN) containing  <font color='red'><b>multiple discriminators, i.e. a multi-scale discriminator, a texture discriminator, and a color discriminator.</b></font>\n",
        "\n",
        "본 논문에서 loss function 은 Adversarial loss + Cycle loss + Color loss + Preserving Loss + Reconstruction loss 로 구성된다.\n",
        "\n",
        "$$ L_{all} = \\omega_1L_{adv}+\\omega_2L_{cyc}+\\omega_3L_{color}+\\omega_4L_{pre}+\\omega_5L_{idt}$$ \n",
        "\n",
        "하지만 각각의 omega는 huristic하게 특정 지었다. 최적화된 값인가??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jbt0ApbS8V_Q"
      },
      "source": [
        "## 3. GENERATIVE MULTI-ADVERSARIAL NETWORKS [품질 향상+mode collapse]\n",
        "\n",
        "N개의 복수 discriminator를 사용하여 안정적으로 더 빠르게 GAN 학습을 할 수 있다. 또한 mode collapse에도 robust 한 특성을 보인다.\n",
        "\n",
        "본 논문에서는 loss function을 three classical Pythagorean means 을 응용하여 정의하였다.\n",
        "\n",
        "$$ AM_{soft}(V, \\lambda) = \\sum_{i}^N \\omega_iV_i $$\n",
        "$$ GM_{soft}(V, \\lambda) = -exp(\\sum_{i}^N \\omega_ilog(-V_i)) $$\n",
        "$$ HM_{soft}(V, \\lambda) = (\\sum_{i}^N \\omega_iV_i^{-1})^{-1} $$\n",
        "\n",
        "하지만, 논문에서는 omega에 대하여 다루지 않았다.\n",
        "저 omega는 어떻게 최적화 할 수 있겠는가?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5qpgtDUySyC"
      },
      "source": [
        "## 4. Dual Discriminator Generative Adversarial Nets [mode collapse]\n",
        "\n",
        "GAN에서 발생하는 치명적인 mode collapse (https://developers.google.com/machine-learning/gan/problems) 현상을 개선하기 위해 두개의 discriminator를 사용한다. - dual discriminator generative adversarial network (D2GAN)\n",
        "\n",
        "it combines <font color='red'><b>the Kullback-Leibler (KL) and reverse KL divergences</b></font> into a unified objective function, thus it exploits the complementary statistical properties from these divergences to effectively diversify the estimated density in capturing multi-modes.\n",
        "\n",
        "본 논문에서 제안하는 D2GAN의 목적함수는 다음과 같다.\n",
        "\n",
        "$$ \\min_{G} \\max_{D_1,D_2} J (G,D_1,D_2) = \\alpha \\times E_{x \\sim P_{data}} [logD_1 (x)] + E_{z \\sim P_z} [-D_1 (G(z))] + E_{x \\sim P_{data}}[-D_2 (x)] + \\beta \\times E_{z \\sim P_z} [logD_2 (G(z))] $$\n",
        "\n",
        "여기서 alpha, beta는 hyperparameter로서, 본 논문의 실험에서는 다양한 값을 대입하여 각각의 성능을 확인하였다.\n",
        "\n",
        "이렇게 값을 찾아야만 하는가?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBQHO9hOBho-"
      },
      "source": [
        "## 5. MD-GAN: Multi-Discriminator Generative Adversarial Networks for Distributed Datasets [성능 향상]\n",
        "\n",
        "we address the problem of distributing GANs so that they are able to train over datasets that are spread on multiple workers. MD-GAN is exposed as the first solution for this problem: we propose a novel learning procedure for GANs\n",
        "so that they fit this distributed setup. We then compare the performance of MD-GAN to an adapted version of Federated Learning to GANs, using the MNIST and CIFAR10 datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPUM8QvQCWzi"
      },
      "source": [
        "## 6. ParallelWasserstein Generative Adversarial Nets with Multiple Discriminators [성능 향상]\n",
        "\n",
        "In this paper, we solve the computation cost problem by speeding up the Wasserstein GANs from a welldesigned communication efficient parallel architecture. 그리고 이것을 Multiple Discriminators 로 구성하였다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjztFvs812EB"
      },
      "source": [
        "## Proposed methods\n",
        "\n",
        "ref : https://realpython.com/python-ai-neural-network/\n",
        "\n",
        "colab 수식입력 : \n",
        "\n",
        "https://wikidocs.net/1679\n",
        "\n",
        "https://en.wikipedia.org/wiki/Help:Displaying_a_formula#Formatting_using_TeX\n",
        "\n",
        "Original GAN의 목적함수\n",
        "$$ \\min_{G}\\max_{D} V(D,G) = E_{x\\sim p_{data}(x)}[logD(x)] + E_{z\\sim p_{z}(z)}[log(1-D(G(z)))] $$\n",
        "\n",
        "Multi-Discriminator GAN은 discriminator가 각 목적에 의하여 여러개 (N개) 있다.\n",
        "MDGAN의 목적함수\n",
        "\n",
        "$$ \\min_{G}\\max_{D_{i\\sim N}} V(D_{i\\sim N},G) = \\sum_{i=1}^N \\{E_{x\\sim p_{data}(x)}[logD_i(x)] + E_{z\\sim p_{z}(z)}[log(1-D_i(G(z)))]\\} $$\n",
        "\n",
        "여기서\n",
        "\n",
        "$$ L(D_i,G) =  E_{x\\sim p_{data}(x)}[logD_i(x)] + E_{z\\sim p_{z}(z)}[log(1-D_i(G(z)))] $$\n",
        "\n",
        "이라하고 단순화 하면\n",
        "\n",
        "$$ \\min_{G}\\max_{D_{i\\sim N}} V(D_{i\\sim N},G) = \\sum_{i=1}^N L(D_i,G) $$\n",
        "\n",
        "와 같이 된다.\n",
        "\n",
        "문제점은 GAN의 특성상, 특정 Discriminator가 학습에 있어 지배적으로 loss 함수에 영향을 미치게 되어 각각의 Discriminator가 골고루 학습에 참여하지 못하고 의도하지 않은 결과를 만들게 된다. 이러한 문제점을 극복하기 위해 다음의 두가지 제안을 한다.\n",
        "\n",
        "1) 목적함수에 각 Loss 에 대한 표준편차 (standard-deviation) 를 반영하여 각 Discriminator에 대한 Loss가 상호 유사한 수준을 유지하면 학습이 진행되도록 한다.\n",
        "\n",
        "$$ \\min_{G}\\max_{D_{i\\sim N}} V(D_{i\\sim N},G) = \\sum_{i=1}^N L(D_i,G) + \\sigma(L(D_i,G))$$\n",
        "\n",
        "2) 각 discriminator에 의한 loss를 제어하기 위해, adaptive discriminant factor (ADF) 를 적용하고, 학습의 진행 과정에서 이를 최적화 한다. \n",
        "\n",
        "$$ \\min_{G}\\max_{D_{i\\sim N}} V(\\lambda_{i\\sim N},D_{i\\sim N},G) = \\sum_{i=1}^N \\lambda_iL(D_i,G)$$\n",
        "\n",
        "3) 1)의 제안에 2)의 제안을 추가한다.\n",
        "\n",
        "$$ \\min_{G}\\max_{D_{i\\sim N}} V(\\lambda_{i\\sim N},D_{i\\sim N},G) = \\sum_{i=1}^N \\lambda_iL(D_i,G) + \\sigma(L(D_i,G))$$\n",
        "\n",
        "\n",
        "여기서\n",
        "\n",
        "$$ \\lambda_i = adaptive\\ discriminant \\ factor \\ for \\ discriminator \\ i  $$\n",
        "\n",
        "중요한 것은, 학습과정에서 L_i을 작게 (학습의 방향)하기 위해서는 lambda_i는 역으로 커져야 한다. 그래야, 전체 Loss function에서 비중이 증대되어 더 적극적인 학습이 이루어 지게 된다. 따라서, lambda_i의 최적화 방향은 기존의 gradient decent와 반대 방향이 되어야 한다.\n",
        "\n",
        "$$ \\lambda_i^{t+1} = \\lambda_i^t + \\gamma \\nabla V(\\lambda_{i\\sim N}^t,D_{i\\sim N},G)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K87VNBbeRLFF"
      },
      "source": [
        "#4. Implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQZeBAf8NxAR"
      },
      "source": [
        "## 4.1 기본 설정..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAdXzWGuKSBT",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c31c108-3970-4e13-82ab-590ccb266901"
      },
      "source": [
        "if True:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "newO0mBXKVnE",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2b3f530-3d47-4550-9b66-092002e796aa"
      },
      "source": [
        "#!pip install keybert\n",
        "!pip install transformers\n",
        "!pip install sentence-transformers\n",
        "\n",
        "#!pip install sentence-transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 8.3 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 41.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 62.9 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 81.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.2\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.0.0.tar.gz (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 3.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.9.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.62.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.9.0+cu102)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.10.0+cu102)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 62.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.0.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (5.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.45)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.10.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.6.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.0.0-py3-none-any.whl size=126710 sha256=52aa035d7490d2614a4bed8e542f9d6a5813ad6c6f600aee622b20c048debf93\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/c1/0f/faafd427f705c4b012274ba60d9a91d75830306811e1355293\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentencepiece, sentence-transformers\n",
            "Successfully installed sentence-transformers-2.0.0 sentencepiece-0.1.96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmIxp0FnKXif",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d520b5d0-7a5b-43bd-f7a3-f2f3388cdc66"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# set seeds for reproducability\n",
        "from numpy.random import seed\n",
        "#seed(1)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string, os \n",
        "\n",
        "import urllib.request\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3J0n_lhKcgm",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10a9a623-53d9-4740-9284-876dae725e31"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ue_4ZfdRKfdX",
        "trusted": true
      },
      "source": [
        "# Print iterations progress\n",
        "class ProgressBar:\n",
        "\n",
        "    def __init__(self,total=20, prefix = '', suffix = '', decimals = 1, length = 20, fill = '|', printEnd = \"\\r\"):\n",
        "        self.total = total\n",
        "        self.prefix = prefix\n",
        "        self.suffix = suffix\n",
        "        self.decimals = decimals\n",
        "        self.length = length\n",
        "        self.fill = fill\n",
        "        self.printEnd = printEnd\n",
        "        self.ite = 0\n",
        "        self.back_filledLength = 0\n",
        "\n",
        "    def printProgress(self,iteration, text):\n",
        "        self.ite += iteration\n",
        "        percent = (\"{0:.\" + str(self.decimals) + \"f}\").format(100 * (self.ite / float(self.total)))\n",
        "        filledLength = int(self.length * self.ite // self.total)\n",
        "        bar = self.fill * filledLength + '.' * (self.length - filledLength)\n",
        "        if filledLength > self.back_filledLength or percent == 100:\n",
        "            print(f'\\r{self.prefix} |{bar}| {percent}% {self.suffix}  {text}', end=\"\", flush=True)\n",
        "            # Print New Line on Complete\n",
        "            if self.ite == self.total: \n",
        "                print()\n",
        "        self.back_filledLength = filledLength    "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNHI0G6JKc5h",
        "trusted": true
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Zsv-LVkKmfL"
      },
      "source": [
        "##4.2 Grammar Discriminator Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_tZFAMYjC3C"
      },
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team and Jangwon Park\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\" Tokenization classes for KoBert model.\"\"\"\n",
        "\n",
        "\n",
        "import logging\n",
        "import os\n",
        "import unicodedata\n",
        "from shutil import copyfile\n",
        "\n",
        "from transformers import PreTrainedTokenizer\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "VOCAB_FILES_NAMES = {\"vocab_file\": \"tokenizer_78b3253a26.model\",\n",
        "                     \"vocab_txt\": \"vocab.txt\"}\n",
        "\n",
        "PRETRAINED_VOCAB_FILES_MAP = {\n",
        "    \"vocab_file\": {\n",
        "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/tokenizer_78b3253a26.model\",\n",
        "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/tokenizer_78b3253a26.model\",\n",
        "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/tokenizer_78b3253a26.model\"\n",
        "    },\n",
        "    \"vocab_txt\": {\n",
        "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/vocab.txt\",\n",
        "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/vocab.txt\",\n",
        "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/vocab.txt\"\n",
        "    }\n",
        "}\n",
        "\n",
        "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n",
        "    \"monologg/kobert\": 512,\n",
        "    \"monologg/kobert-lm\": 512,\n",
        "    \"monologg/distilkobert\": 512\n",
        "}\n",
        "\n",
        "PRETRAINED_INIT_CONFIGURATION = {\n",
        "    \"monologg/kobert\": {\"do_lower_case\": False},\n",
        "    \"monologg/kobert-lm\": {\"do_lower_case\": False},\n",
        "    \"monologg/distilkobert\": {\"do_lower_case\": False}\n",
        "}\n",
        "\n",
        "SPIECE_UNDERLINE = u'▁'\n",
        "\n",
        "\n",
        "class KoBertTokenizer(PreTrainedTokenizer):\n",
        "    \"\"\"\n",
        "        SentencePiece based tokenizer. Peculiarities:\n",
        "            - requires `SentencePiece <https://github.com/google/sentencepiece>`_\n",
        "    \"\"\"\n",
        "    vocab_files_names = VOCAB_FILES_NAMES\n",
        "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n",
        "    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n",
        "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            vocab_file,\n",
        "            vocab_txt,\n",
        "            do_lower_case=False,\n",
        "            remove_space=True,\n",
        "            keep_accents=False,\n",
        "            unk_token=\"[UNK]\",\n",
        "            sep_token=\"[SEP]\",\n",
        "            pad_token=\"[PAD]\",\n",
        "            cls_token=\"[CLS]\",\n",
        "            mask_token=\"[MASK]\",\n",
        "            **kwargs):\n",
        "        super().__init__(\n",
        "            unk_token=unk_token,\n",
        "            sep_token=sep_token,\n",
        "            pad_token=pad_token,\n",
        "            cls_token=cls_token,\n",
        "            mask_token=mask_token,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "        # Build vocab\n",
        "        self.token2idx = dict()\n",
        "        self.idx2token = []\n",
        "        with open(vocab_txt, 'r', encoding='utf-8') as f:\n",
        "            for idx, token in enumerate(f):\n",
        "                token = token.strip()\n",
        "                self.token2idx[token] = idx\n",
        "                self.idx2token.append(token)\n",
        "\n",
        "        try:\n",
        "            import sentencepiece as spm\n",
        "        except ImportError:\n",
        "            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n",
        "                           \"pip install sentencepiece\")\n",
        "\n",
        "        self.do_lower_case = do_lower_case\n",
        "        self.remove_space = remove_space\n",
        "        self.keep_accents = keep_accents\n",
        "        self.vocab_file = vocab_file\n",
        "        self.vocab_txt = vocab_txt\n",
        "\n",
        "        self.sp_model = spm.SentencePieceProcessor()\n",
        "        self.sp_model.Load(vocab_file)\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return len(self.idx2token)\n",
        "\n",
        "    def get_vocab(self):\n",
        "        return dict(self.token2idx, **self.added_tokens_encoder)\n",
        "\n",
        "    def __getstate__(self):\n",
        "        state = self.__dict__.copy()\n",
        "        state[\"sp_model\"] = None\n",
        "        return state\n",
        "\n",
        "    def __setstate__(self, d):\n",
        "        self.__dict__ = d\n",
        "        try:\n",
        "            import sentencepiece as spm\n",
        "        except ImportError:\n",
        "            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n",
        "                           \"pip install sentencepiece\")\n",
        "        self.sp_model = spm.SentencePieceProcessor()\n",
        "        self.sp_model.Load(self.vocab_file)\n",
        "\n",
        "    def preprocess_text(self, inputs):\n",
        "        if self.remove_space:\n",
        "            outputs = \" \".join(inputs.strip().split())\n",
        "        else:\n",
        "            outputs = inputs\n",
        "        outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')\n",
        "\n",
        "        if not self.keep_accents:\n",
        "            outputs = unicodedata.normalize('NFKD', outputs)\n",
        "            outputs = \"\".join([c for c in outputs if not unicodedata.combining(c)])\n",
        "        if self.do_lower_case:\n",
        "            outputs = outputs.lower()\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def _tokenize(self, text, return_unicode=True, sample=False):\n",
        "        \"\"\" Tokenize a string. \"\"\"\n",
        "        text = self.preprocess_text(text)\n",
        "\n",
        "        if not sample:\n",
        "            pieces = self.sp_model.EncodeAsPieces(text)\n",
        "        else:\n",
        "            pieces = self.sp_model.SampleEncodeAsPieces(text, 64, 0.1)\n",
        "        new_pieces = []\n",
        "        for piece in pieces:\n",
        "            if len(piece) > 1 and piece[-1] == str(\",\") and piece[-2].isdigit():\n",
        "                cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))\n",
        "                if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n",
        "                    if len(cur_pieces[0]) == 1:\n",
        "                        cur_pieces = cur_pieces[1:]\n",
        "                    else:\n",
        "                        cur_pieces[0] = cur_pieces[0][1:]\n",
        "                cur_pieces.append(piece[-1])\n",
        "                new_pieces.extend(cur_pieces)\n",
        "            else:\n",
        "                new_pieces.append(piece)\n",
        "\n",
        "        return new_pieces\n",
        "\n",
        "    def _convert_token_to_id(self, token):\n",
        "        \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\n",
        "        return self.token2idx.get(token, self.token2idx[self.unk_token])\n",
        "\n",
        "    def _convert_id_to_token(self, index, return_unicode=True):\n",
        "        \"\"\"Converts an index (integer) in a token (string/unicode) using the vocab.\"\"\"\n",
        "        return self.idx2token[index]\n",
        "\n",
        "    def convert_tokens_to_string(self, tokens):\n",
        "        \"\"\"Converts a sequence of tokens (strings for sub-words) in a single string.\"\"\"\n",
        "        out_string = \"\".join(tokens).replace(SPIECE_UNDERLINE, \" \").strip()\n",
        "        return out_string\n",
        "\n",
        "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
        "        \"\"\"\n",
        "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
        "        by concatenating and adding special tokens.\n",
        "        A KoBERT sequence has the following format:\n",
        "            single sequence: [CLS] X [SEP]\n",
        "            pair of sequences: [CLS] A [SEP] B [SEP]\n",
        "        \"\"\"\n",
        "        if token_ids_1 is None:\n",
        "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "        sep = [self.sep_token_id]\n",
        "        return cls + token_ids_0 + sep + token_ids_1 + sep\n",
        "\n",
        "    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n",
        "        \"\"\"\n",
        "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
        "        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n",
        "        Args:\n",
        "            token_ids_0: list of ids (must not contain special tokens)\n",
        "            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n",
        "                for sequence pairs\n",
        "            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n",
        "                special tokens for the model\n",
        "        Returns:\n",
        "            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\n",
        "        \"\"\"\n",
        "\n",
        "        if already_has_special_tokens:\n",
        "            if token_ids_1 is not None:\n",
        "                raise ValueError(\n",
        "                    \"You should not supply a second sequence if the provided sequence of \"\n",
        "                    \"ids is already formated with special tokens for the model.\"\n",
        "                )\n",
        "            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n",
        "\n",
        "        if token_ids_1 is not None:\n",
        "            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n",
        "        return [1] + ([0] * len(token_ids_0)) + [1]\n",
        "\n",
        "    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n",
        "        \"\"\"\n",
        "        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n",
        "        A KoBERT sequence pair mask has the following format:\n",
        "        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
        "        | first sequence    | second sequence\n",
        "        if token_ids_1 is None, only returns the first portion of the mask (0's).\n",
        "        \"\"\"\n",
        "        sep = [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "        if token_ids_1 is None:\n",
        "            return len(cls + token_ids_0 + sep) * [0]\n",
        "        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n",
        "\n",
        "    def save_vocabulary(self, save_directory):\n",
        "        \"\"\" Save the sentencepiece vocabulary (copy original file) and special tokens file\n",
        "            to a directory.\n",
        "        \"\"\"\n",
        "        if not os.path.isdir(save_directory):\n",
        "            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\n",
        "            return\n",
        "\n",
        "        # 1. Save sentencepiece model\n",
        "        out_vocab_model = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_file\"])\n",
        "\n",
        "        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_model):\n",
        "            copyfile(self.vocab_file, out_vocab_model)\n",
        "\n",
        "        # 2. Save vocab.txt\n",
        "        index = 0\n",
        "        out_vocab_txt = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_txt\"])\n",
        "        with open(out_vocab_txt, \"w\", encoding=\"utf-8\") as writer:\n",
        "            for token, token_index in sorted(self.token2idx.items(), key=lambda kv: kv[1]):\n",
        "                if index != token_index:\n",
        "                    logger.warning(\n",
        "                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n",
        "                        \" Please check that the vocabulary is not corrupted!\".format(out_vocab_txt)\n",
        "                    )\n",
        "                    index = token_index\n",
        "                writer.write(token + \"\\n\")\n",
        "                index += 1\n",
        "\n",
        "        return out_vocab_model, out_vocab_txt"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VQdGLciKc_y",
        "trusted": true
      },
      "source": [
        "from transformers import BertTokenizer, AutoTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset, random_split\n",
        "\n",
        "import time\n",
        "import random\n",
        "import datetime\n",
        "\n",
        "# 간단한 전처리\n",
        "def clean_text(txt):\n",
        "    txt = txt.replace('\\n',' ')\n",
        "    txt = txt.replace('\\r',' ')    \n",
        "    txt = txt.replace('=','')\n",
        "    txt = txt.replace('\\\"','')   \n",
        "    txt = txt.replace('\\'','')\n",
        "    #txt = txt.replace(',','')\n",
        "    txt = txt.replace('..','')\n",
        "    txt = txt.replace('...','')\n",
        "    txt = txt.replace(' .','.')\n",
        "    txt = txt.replace('.','. ')\n",
        "    txt = txt.replace('  ',' ')\n",
        "    txt = txt.replace('  ',' ')    \n",
        "    txt = txt.replace('  ',' ')   \n",
        "    txt = txt.replace('  ',' ')           \n",
        "    txt = txt.replace('  ',' ')\n",
        "    txt = txt.replace('  ',' ')    \n",
        "    txt = txt.replace('  ',' ')   \n",
        "    txt = txt.replace('  ',' ')             \n",
        "    return txt.strip()\n",
        "\n",
        "def shuffling(txt):\n",
        "    txt_list = txt.split(' ')\n",
        "    random.shuffle(txt_list)\n",
        "    return ' '.join(txt_list)\n",
        "\n",
        "def collect_training_dataset_for_grammar_discriminator(sentences_dataset):\n",
        "\n",
        "    sentences = []\n",
        "    labels = []\n",
        "\n",
        "    for txtss in sentences_dataset:\n",
        "        txtss = clean_text(txtss)\n",
        "        txts = txtss.strip().split('.')\n",
        "        for txt in txts:  \n",
        "            txt = txt.strip()\n",
        "            if len(txt) > 40:\n",
        "                #ko_grammar_dataset.append([txt,1])\n",
        "                txt = txt.replace('.','')\n",
        "                tf = random.choice([True,False])\n",
        "                # 정상 또는 비정상 둘중에 하나만 데이터셋에 추가\n",
        "                if (tf):\n",
        "                    sentences.append(txt) # '.'의 위치를 보고 True, False를 판단 하기 땜에...\n",
        "                    labels.append(1)\n",
        "                else:\n",
        "                    sentences.append(shuffling(txt))\n",
        "                    labels.append(0)\n",
        "\n",
        "    return sentences,labels\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "class Grammar_Discriminator:\n",
        "\n",
        "\n",
        "    def __init__(self, pretraoned_kobert_model_name='monologg/kobert', input_dir=None):\n",
        "\n",
        "        if input_dir is None:\n",
        "            self.tokenizer = KoBertTokenizer.from_pretrained(pretraoned_kobert_model_name)\n",
        "            self.discriminator = BertForSequenceClassification.from_pretrained(\n",
        "                                    pretraoned_kobert_model_name, # Use the 12-layer BERT model, with an uncased vocab.\n",
        "                                    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                                                    # You can increase this for multi-class tasks.   \n",
        "                                    output_attentions = False, # Whether the model returns attentions weights.\n",
        "                                    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "                                )            \n",
        "        else:\n",
        "            self.__load_model(input_dir)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def set_dataset(self, sentences,labels):\n",
        "        # Print the original sentence.\n",
        "        print(' Original: ', sentences[0])\n",
        "\n",
        "        # Print the sentence split into tokens.\n",
        "        print('Tokenized: ', self.tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "        # Print the sentence mapped to token ids.\n",
        "        print('Token IDs: ', self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(sentences[0])))   \n",
        "\n",
        "        # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "        input_ids = []\n",
        "        attention_masks = []\n",
        "\n",
        "        # For every sentence...\n",
        "        for sent in sentences:\n",
        "            # `encode_plus` will:\n",
        "            #   (1) Tokenize the sentence.\n",
        "            #   (2) Prepend the `[CLS]` token to the start.\n",
        "            #   (3) Append the `[SEP]` token to the end.\n",
        "            #   (4) Map tokens to their IDs.\n",
        "            #   (5) Pad or truncate the sentence to `max_length`\n",
        "            #   (6) Create attention masks for [PAD] tokens.\n",
        "            encoded_dict = self.tokenizer.encode_plus(\n",
        "                                sent,                      # Sentence to encode.\n",
        "                                add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                                max_length = 64,           # Pad & truncate all sentences.\n",
        "                                pad_to_max_length = True,\n",
        "                                return_attention_mask = True,   # Construct attn. masks.\n",
        "                                return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                                truncation = True,\n",
        "                        )\n",
        "            \n",
        "            # Add the encoded sentence to the list.    \n",
        "            input_ids.append(encoded_dict['input_ids'])\n",
        "            \n",
        "            # And its attention mask (simply differentiates padding from non-padding).\n",
        "            attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "        # Convert the lists into tensors.\n",
        "        input_ids = torch.cat(input_ids, dim=0)\n",
        "        attention_masks = torch.cat(attention_masks, dim=0)\n",
        "        labels = torch.tensor(labels)\n",
        "\n",
        "        # Print sentence 0, now as a list of IDs.\n",
        "        print('Original: ', sentences[0])\n",
        "        print('Token IDs:', input_ids[0])\n",
        "\n",
        "        # Training & Validation Split\n",
        "        # Divide up our training set to use 90% for training and 10% for validation.\n",
        "\n",
        "        # Combine the training inputs into a TensorDataset.\n",
        "        dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "        # Create a 90-10 train-validation split.\n",
        "\n",
        "        # Calculate the number of samples to include in each set.\n",
        "        train_size = int(0.9 * len(dataset))\n",
        "        val_size = len(dataset) - train_size\n",
        "\n",
        "        # Divide the dataset by randomly selecting samples.\n",
        "        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "        print('{:>5,} training samples'.format(train_size))\n",
        "        print('{:>5,} validation samples'.format(val_size))\n",
        "\n",
        "        # The DataLoader needs to know our batch size for training, so we specify it \n",
        "        # here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "        # size of 16 or 32.\n",
        "        self.batch_size = 32\n",
        "\n",
        "        # Create the DataLoaders for our training and validation sets.\n",
        "        # We'll take training samples in random order. \n",
        "        self.train_dataloader = DataLoader(\n",
        "                    train_dataset,  # The training samples.\n",
        "                    sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "                    batch_size = self.batch_size # Trains with this batch size.\n",
        "                )\n",
        "\n",
        "        # For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "        self.validation_dataloader = DataLoader(\n",
        "                    val_dataset, # The validation samples.\n",
        "                    sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "                    batch_size = self.batch_size # Evaluate with this batch size.\n",
        "                )        \n",
        "\n",
        "\n",
        "\n",
        "    def train(self,epochs=4):\n",
        "        # Tell pytorch to run this model on the GPU.\n",
        "        self.discriminator.cuda()\n",
        "\n",
        "        # Get all of the model's parameters as a list of tuples.\n",
        "        params = list(self.discriminator.named_parameters())\n",
        "\n",
        "        print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "        print('==== Embedding Layer ====\\n')\n",
        "\n",
        "        for p in params[0:5]:\n",
        "            print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "        print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "        for p in params[5:21]:\n",
        "            print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "        print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "        for p in params[-4:]:\n",
        "            print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))  \n",
        "\n",
        "        # Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "        # I believe the 'W' stands for 'Weight Decay fix\"\n",
        "        self.optimizer = AdamW(self.discriminator.parameters(),\n",
        "                        lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                        eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                        )\n",
        "\n",
        "        # Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "        # We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "        # training data.\n",
        "        #epochs = 2\n",
        "\n",
        "        # Total number of training steps is [number of batches] x [number of epochs]. \n",
        "        # (Note that this is not the same as the number of training samples).\n",
        "        total_steps = len(self.train_dataloader) * epochs\n",
        "\n",
        "        # Create the learning rate scheduler.\n",
        "        scheduler = get_linear_schedule_with_warmup(self.optimizer, \n",
        "                                                    num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                                    num_training_steps = total_steps)\n",
        "            \n",
        "        # This training code is based on the `run_glue.py` script here:\n",
        "        # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "        # Set the seed value all over the place to make this reproducible.\n",
        "        seed_val = 42\n",
        "\n",
        "        random.seed(seed_val)\n",
        "        np.random.seed(seed_val)\n",
        "        torch.manual_seed(seed_val)\n",
        "        torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "        # We'll store a number of quantities such as training and validation loss, \n",
        "        # validation accuracy, and timings.\n",
        "        training_stats = []\n",
        "\n",
        "        # Measure the total training time for the whole run.\n",
        "        total_t0 = time.time()\n",
        "\n",
        "        # For each epoch...\n",
        "        for epoch_i in range(0, epochs):\n",
        "            \n",
        "            # ========================================\n",
        "            #               Training\n",
        "            # ========================================\n",
        "            \n",
        "            # Perform one full pass over the training set.\n",
        "\n",
        "            print(\"\")\n",
        "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "            print('Training...')\n",
        "\n",
        "            # Measure how long the training epoch takes.\n",
        "            t0 = time.time()\n",
        "\n",
        "            # Reset the total loss for this epoch.\n",
        "            total_train_loss = 0\n",
        "\n",
        "            # Put the model into training mode. Don't be mislead--the call to \n",
        "            # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "            # `dropout` and `batchnorm` layers behave differently during training\n",
        "            # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "            self.discriminator.train()\n",
        "\n",
        "            # For each batch of training data...\n",
        "            for step, batch in enumerate(self.train_dataloader):\n",
        "\n",
        "                # Progress update every 40 batches.\n",
        "                if step % 40 == 0 and not step == 0:\n",
        "                    # Calculate elapsed time in minutes.\n",
        "                    elapsed = format_time(time.time() - t0)\n",
        "                    \n",
        "                    # Report progress.\n",
        "                    print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(self.train_dataloader), elapsed))\n",
        "\n",
        "                # Unpack this training batch from our dataloader. \n",
        "                #\n",
        "                # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "                # `to` method.\n",
        "                #\n",
        "                # `batch` contains three pytorch tensors:\n",
        "                #   [0]: input ids \n",
        "                #   [1]: attention masks\n",
        "                #   [2]: labels \n",
        "                b_input_ids = batch[0].to(device)\n",
        "                b_input_mask = batch[1].to(device)\n",
        "                b_labels = batch[2].to(device)\n",
        "\n",
        "                # Always clear any previously calculated gradients before performing a\n",
        "                # backward pass. PyTorch doesn't do this automatically because \n",
        "                # accumulating the gradients is \"convenient while training RNNs\". \n",
        "                # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "                self.discriminator.zero_grad()        \n",
        "\n",
        "                # Perform a forward pass (evaluate the model on this training batch).\n",
        "                # The documentation for this `model` function is here: \n",
        "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "                # It returns different numbers of parameters depending on what arguments\n",
        "                # arge given and what flags are set. For our useage here, it returns\n",
        "                # the loss (because we provided labels) and the \"logits\"--the model\n",
        "                # outputs prior to activation.\n",
        "                loss, logits = self.discriminator(b_input_ids, \n",
        "                                    token_type_ids=None, \n",
        "                                    attention_mask=b_input_mask, \n",
        "                                    labels=b_labels)\n",
        "\n",
        "                # Accumulate the training loss over all of the batches so that we can\n",
        "                # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "                # single value; the `.item()` function just returns the Python value \n",
        "                # from the tensor.\n",
        "                total_train_loss += loss.item()\n",
        "\n",
        "                # Perform a backward pass to calculate the gradients.\n",
        "                loss.backward()\n",
        "\n",
        "                # Clip the norm of the gradients to 1.0.\n",
        "                # This is to help prevent the \"exploding gradients\" problem.\n",
        "                torch.nn.utils.clip_grad_norm_(self.discriminator.parameters(), 1.0)\n",
        "\n",
        "                # Update parameters and take a step using the computed gradient.\n",
        "                # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "                # modified based on their gradients, the learning rate, etc.\n",
        "                self.optimizer.step()\n",
        "\n",
        "                # Update the learning rate.\n",
        "                scheduler.step()\n",
        "\n",
        "            # Calculate the average loss over all of the batches.\n",
        "            avg_train_loss = total_train_loss / len(self.train_dataloader)            \n",
        "            \n",
        "            # Measure how long this epoch took.\n",
        "            training_time = format_time(time.time() - t0)\n",
        "\n",
        "            print(\"\")\n",
        "            print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "            print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "                \n",
        "            # ========================================\n",
        "            #               Validation\n",
        "            # ========================================\n",
        "            # After the completion of each training epoch, measure our performance on\n",
        "            # our validation set.\n",
        "\n",
        "            print(\"\")\n",
        "            print(\"Running Validation...\")\n",
        "\n",
        "            t0 = time.time()\n",
        "\n",
        "            # Put the model in evaluation mode--the dropout layers behave differently\n",
        "            # during evaluation.\n",
        "            self.discriminator.eval()\n",
        "\n",
        "            # Tracking variables \n",
        "            total_eval_accuracy = 0\n",
        "            total_eval_loss = 0\n",
        "            nb_eval_steps = 0\n",
        "\n",
        "            # Evaluate data for one epoch\n",
        "            for batch in self.validation_dataloader:\n",
        "                \n",
        "                # Unpack this training batch from our dataloader. \n",
        "                #\n",
        "                # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "                # the `to` method.\n",
        "                #\n",
        "                # `batch` contains three pytorch tensors:\n",
        "                #   [0]: input ids \n",
        "                #   [1]: attention masks\n",
        "                #   [2]: labels \n",
        "                b_input_ids = batch[0].to(device)\n",
        "                b_input_mask = batch[1].to(device)\n",
        "                b_labels = batch[2].to(device)\n",
        "                \n",
        "                # Tell pytorch not to bother with constructing the compute graph during\n",
        "                # the forward pass, since this is only needed for backprop (training).\n",
        "                with torch.no_grad():        \n",
        "\n",
        "                    # Forward pass, calculate logit predictions.\n",
        "                    # token_type_ids is the same as the \"segment ids\", which \n",
        "                    # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "                    # The documentation for this `model` function is here: \n",
        "                    # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "                    # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "                    # values prior to applying an activation function like the softmax.\n",
        "                    (loss, logits) = self.discriminator(b_input_ids, \n",
        "                                        token_type_ids=None, \n",
        "                                        attention_mask=b_input_mask,\n",
        "                                        labels=b_labels)\n",
        "                    \n",
        "                # Accumulate the validation loss.\n",
        "                total_eval_loss += loss.item()\n",
        "\n",
        "                # Move logits and labels to CPU\n",
        "                logits = logits.detach().cpu().numpy()\n",
        "                label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "                # Calculate the accuracy for this batch of test sentences, and\n",
        "                # accumulate it over all batches.\n",
        "                total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "                \n",
        "\n",
        "            # Report the final accuracy for this validation run.\n",
        "            avg_val_accuracy = total_eval_accuracy / len(self.validation_dataloader)\n",
        "            print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "            # Calculate the average loss over all of the batches.\n",
        "            avg_val_loss = total_eval_loss / len(self.validation_dataloader)\n",
        "            \n",
        "            # Measure how long the validation run took.\n",
        "            validation_time = format_time(time.time() - t0)\n",
        "            \n",
        "            print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "            print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "            # Record all statistics from this epoch.\n",
        "            training_stats.append(\n",
        "                {\n",
        "                    'epoch': epoch_i + 1,\n",
        "                    'Training Loss': avg_train_loss,\n",
        "                    'Valid. Loss': avg_val_loss,\n",
        "                    'Valid. Accur.': avg_val_accuracy,\n",
        "                    'Training Time': training_time,\n",
        "                    'Validation Time': validation_time\n",
        "                }\n",
        "            )\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"Training complete!\")\n",
        "\n",
        "        print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "            \n",
        "\n",
        "        return training_stats\n",
        "\n",
        "    def save_model(self, output_dir = './model_save/'):\n",
        "        # Create output directory if needed\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "\n",
        "        print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "        # They can then be reloaded using `from_pretrained()`\n",
        "        model_to_save = self.discriminator.module if hasattr(self.discriminator, 'module') else self.discriminator  # Take care of distributed/parallel training\n",
        "        model_to_save.save_pretrained(output_dir)\n",
        "        self.tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "        # Good practice: save your training arguments together with the trained model\n",
        "        # torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n",
        "\n",
        "    def __load_model(self, input_dir = './drive/MyDrive/Colab Notebooks/summary/en_grammar_check_model'):\n",
        "        print('Loading BERT tokenizer...')\n",
        "        self.tokenizer = KoBertTokenizer.from_pretrained(input_dir)\n",
        "        self.discriminator = BertForSequenceClassification.from_pretrained(input_dir)\n",
        "\n",
        "    def transfer_learning(self, sentences, train_for = True):\n",
        "        \n",
        "        input_ids = []\n",
        "        attention_masks = []\n",
        "\n",
        "        # For every sentence...\n",
        "        for sent in sentences:\n",
        "            # `encode_plus` will:\n",
        "            #   (1) Tokenize the sentence.\n",
        "            #   (2) Prepend the `[CLS]` token to the start.\n",
        "            #   (3) Append the `[SEP]` token to the end.\n",
        "            #   (4) Map tokens to their IDs.\n",
        "            #   (5) Pad or truncate the sentence to `max_length`\n",
        "            #   (6) Create attention masks for [PAD] tokens.\n",
        "            encoded_dict = self.tokenizer.encode_plus(\n",
        "                                sent,                      # Sentence to encode.\n",
        "                                add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                                max_length = 64,           # Pad & truncate all sentences.\n",
        "                                pad_to_max_length = True,\n",
        "                                return_attention_mask = True,   # Construct attn. masks.\n",
        "                                return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                                truncation = True,\n",
        "                        )\n",
        "            # Add the encoded sentence to the list.    \n",
        "            input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "            # And its attention mask (simply differentiates padding from non-padding).\n",
        "            attention_masks.append(encoded_dict['attention_mask'])\n",
        "        \n",
        "        if train_for:\n",
        "            b_labels = torch.ones(len(sentences),dtype=torch.long).to(device)\n",
        "        else:\n",
        "            b_labels = torch.zeros(len(sentences),dtype=torch.long).to(device)\n",
        "        #print(b_labels)\n",
        "        # Convert the lists into tensors.\n",
        "        input_ids = torch.cat(input_ids, dim=0).to(device)\n",
        "        attention_masks = torch.cat(attention_masks, dim=0).to(device)    \n",
        "        #if str(discriminator1.device) == 'cpu':\n",
        "        #    pass\n",
        "        #else:\n",
        "        #    input_ids = input_ids.to(device)\n",
        "        #    attention_masks = attention_masks.to(device)        \n",
        "\n",
        "        outputs = self.discriminator(input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=attention_masks, \n",
        "                                labels=b_labels)\n",
        "\n",
        "        #print(outputs)\n",
        "        #return torch.sigmoid(outputs[0][:,1])\n",
        "        #return outputs[0][:,1]\n",
        "        return outputs['loss'], outputs['logits']\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk93tR2Nuk8t"
      },
      "source": [
        "# 문법 discriminator의 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7Zf2oRMMXmH",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28eca812-3561-4d2b-b0ab-a0d184391dc7"
      },
      "source": [
        "use_pretrained_model = True\n",
        "\n",
        "if use_pretrained_model:\n",
        "    #g_discriminator = Grammar_Discriminator(input_dir = '/content/drive/MyDrive/Colab Notebooks/summary/model_save')\n",
        "    g_discriminator = Grammar_Discriminator(input_dir = '/content/drive/MyDrive/Colab Notebooks/summary/ko_grammar_model2')\n",
        "else:\n",
        "    sentences,labels = collect_training_dataset_for_grammar_discriminator(ko_sentences_dataset)\n",
        "    print(len(sentences))\n",
        "    g_discriminator = Grammar_Discriminator()\n",
        "    g_discriminator.set_dataset(sentences,labels)\n",
        "    g_discriminator.train(epochs=1)\n",
        "    g_discriminator.save_model(output_dir='/content/drive/MyDrive/Colab Notebooks/summary/ko_grammar_model2')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
            "The class this function is called from is 'KoBertTokenizer'.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-kyzdkT-G2m",
        "outputId": "93c861fc-57d7-463e-b81d-3d9b874a4689"
      },
      "source": [
        "txt = ['최근 날씨가 포근해지면서 산을 찾는 사람들도 늘고 있는데요','서비스를 음원 플랫폼 스포티파이가 국내  론칭한다']\n",
        "g_discriminator.discriminator.to(device)\n",
        "g_discriminator.transfer_learning(txt)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(3.9366, device='cuda:0', grad_fn=<NllLossBackward>),\n",
              " tensor([[-4.5301,  4.1747],\n",
              "         [ 4.0265, -3.8462]], device='cuda:0', grad_fn=<AddmmBackward>))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d96kaCAHKuUc"
      },
      "source": [
        "##4.3 Static similarity discriminator class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZDpXe7XKxeg",
        "trusted": true
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import BertTokenizer\n",
        "from scipy.signal import find_peaks\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.misc import electrocardiogram\n",
        "import scipy\n",
        "\n",
        "\n",
        "class Similarity_Discriminator:\n",
        "    '''\n",
        "    _instance = None\n",
        "    _embedder = None\n",
        "    def __new__(cls,pre_trained_model_name='stsb-roberta-large'):\n",
        "        if cls._instance is None:\n",
        "            print('Creating Similarity_Discriminator object')\n",
        "            cls._instance = super(Similarity_Discriminator, cls).__new__(cls)\n",
        "            # Put any initialization here.\n",
        "            cls._embedder = SentenceTransformer(pre_trained_model_name)\n",
        "        return cls._instance\n",
        "\n",
        "    '''\n",
        "\n",
        "    def __init__(self,pre_trained_model_name='xlm-r-large-en-ko-nli-ststb'):\n",
        "        print('Creating Similarity_Discriminator object')\n",
        "        # Put any initialization here.\n",
        "        self._embedder = SentenceTransformer(pre_trained_model_name)  \n",
        "        #self.cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "    def encode(self,texts):\n",
        "        return self._embedder.encode(texts,show_progress_bar=False)\n",
        "\n",
        "    def similarity(self, query_text, org_text_emb):\n",
        "        queries = nltk.sent_tokenize(query_text)\n",
        "        query_embeddings = self._embedder.encode(queries,show_progress_bar=False)\n",
        "        #query_embeddings = self._embedder.encode(queries,show_progress_bar=False)\n",
        "        #print(queries)\n",
        "        #print(org_text_emb)\n",
        "        \n",
        "        if len(query_embeddings) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        cos_scores = scipy.spatial.distance.cdist(query_embeddings, org_text_emb, \"cosine\")\n",
        "        similarity_score = 1.0 - np.mean(np.min(cos_scores,axis=0))\n",
        "        '''\n",
        "        for query, query_embedding in zip(queries, query_embeddings):\n",
        "            distances = scipy.spatial.distance.cdist([query_embedding], [org_text_emb], \"cosine\")[0]\n",
        "            results = zip(range(len(distances)), distances)\n",
        "            for idx, distance in results:\n",
        "                scores.append(1-distance)\n",
        "        '''\n",
        "        return similarity_score  \n",
        " "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sQZ36GuMumP"
      },
      "source": [
        "###4.3.1 한국어 문장 유사도 pre-trained model 적용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9Miao14Muww",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418,
          "referenced_widgets": [
            "e0a4690f59a64cdd8e982dbe49026c76",
            "0ddc7e842c194c56bb75603da6dce365",
            "d77b71fb05b24cb4910395d6ece596c1",
            "6023471cebc14635982d90180d9f516f",
            "6bdfafb2b0ae4fd9a512dd6444f0f4f7",
            "9410f7ca4ba24d45904c35f4a7c17451",
            "7efccaf87ec643eeafdab75d0f8bd2b5",
            "2016c290f9a04aaf9554376dccc9d7b1",
            "f6ee093161064e1ca4721bf40b962bbf",
            "59dec3a1eef64e8eaaaea29523cfcdeb",
            "995560843c064d70ba10075cac9481f5",
            "68a5cc21e99e4d778949428c1d699fd4",
            "6e282483bb284c2bb4aa21376b0e6d11",
            "f21261246ab14ca2b0815d77016fb115",
            "721cdd3b8de6419389543e0d48e1d16f",
            "507b8ab17e53493cb897562fe52f60c1",
            "6f96f960b53648c489b95f296ebd57a7",
            "9749f68dd09a44c3ab2bfb6ac2ef2dea",
            "744d0e095fea49f1aaef002bcf9efe02",
            "3b6d67c28393497e8344443b9233dc97",
            "04acccf1544a4f02b62a262342f022f6",
            "bee604b5a42b4915a10ba3547cc3bb14",
            "bfc749d039fc48d19464a8220483e593",
            "8d604e9e156845469e9e29a2bf23f236",
            "522955f556124fda9ad8ac09331bbf75",
            "d29fab4003c5404fa87b0779aaaed8e5",
            "ea575589e1f144b99c8ac5ad187db305",
            "9374bd1f52bf403aaa18543f503b0bbe",
            "eeb1ab0fbf9045b2af1c4782f7937a39",
            "3ac86c33a70441ac8895f54de078a7a9",
            "927135024a7449c9a03dacc56dfa397a",
            "73516fa006d141a382144ba23cdcb69a",
            "695c17d7bf0e4582be8f5de2c71f7d06",
            "41adff6bd84c4d5b8ca88509e47a4ec8",
            "ab806100057846b891bbec906dd60cc8",
            "fb9457c1fdef45cea3785183e9064cb4",
            "623853cfb34543cf8306a1fce8e55493",
            "b68e4b63a28b4d849019a96c5f3ca5cb",
            "b52371d59b624aceaec8d7c494357685",
            "c633ce0bca804bb0ba4b2757d4c883aa",
            "e3e580b7cf62496eb9ecb0702441157d",
            "33d3d9f237c6465886cd5fe9695951a1",
            "6feb107f141442a6880debe95ea606c5",
            "31bfde8636334955a8936caf8b48b64c",
            "78758051efcd4fc084b07a84038b5b19",
            "09b5266c879f416781be89b29e6d5a9c",
            "ea43c2553f10483f8cdd07f098e1bd8e",
            "0bee704f2c484c3abfb7126ef5611701",
            "833df243ab844b38aa798434ee6b23f6",
            "3e099e0056a44bca8b2274c4478f6f0a",
            "c9a286f4528c4c9e9486eed2e2859e4c",
            "3591faad15eb4e489be71e61845d57ac",
            "e4bb481c088442fe99bc48e1cec70762",
            "f58214b43cb547d5ad63ef6e3eeef67d",
            "05c7ddea8f2248f09f7cb698bd8f0cb7",
            "ed9f4716f4c54a56bf899306d03b49e7",
            "9cd6651e738d4d818614a0a19e209578",
            "94a0e1602f354ec39a67c59970b6fccc",
            "8a3d3edcedd5450b9040f77114c0f518",
            "27830f24a9da4ba79e2d9e271eeacaa0",
            "95032f16a45a453cb4de0458859780dd",
            "552f004af0544e599f8513d24487e22f",
            "d5273c903c9649b2a0cd06ea8c8bac53",
            "581a1e4a4ea44d039bc487c07a6c2eac",
            "f2568987a4084cf9ba67fcc8a29d2f9c",
            "3e81761278314ce4876f7e74c2a250d6",
            "e36b30b4ad564813ab8bee0a4e68a9f5",
            "aee89f781c8a4fb380e2a0750ece52b0",
            "b0709de72aed489bbeff7feba6e55a2a",
            "2788e247e8f146d78ee372b8f5c8f481",
            "403782c2dd964b8f8884f6c487c5ef75",
            "37c4a1a264cb4e92b07f6cc792135750",
            "6da97c72fd70454bb4e88727a2fcbff3",
            "a0e95a3340444c6e9c2557b15e96acd8",
            "3ca8f80f42b44269a4f9c695ca0f8c83",
            "4646d5cad24e47aaa0cdb4d377e0fac5",
            "1b945686f09c455ab5ad9fcb8e614ee3",
            "f3ae4d8813ed40e0b8a6a64c29530bba",
            "04efab9bf6f841a18eaa11a63f2fa0bb",
            "0108240159264bd0b2db9c579bea6e44",
            "4af166179809493b9502b704a37c96aa",
            "a5a9b20c881843dba1d5f7a8221617b8",
            "88db2814d036493d807ae7382588e690",
            "e0a7b83e1db54188aa93a466164c1656",
            "1ea6f3d9dba4443595c64909e521f3f3",
            "e46c8fe544a347e6a15c24c1343996d5",
            "25cb88644dec441e9e1c98827be599b8",
            "978da94885664bdb8549049dd5f42566",
            "6c0b2d5d51004bba9283a8bf9550baf6",
            "00ff0b227ed143debdded0e337e94b29",
            "8c27b993425a4f3ea1c7d5582af3c2c6",
            "903b1291b3414b7ba060e8c46ea24f88",
            "f803321e9cb74ab38860bfa95c5b359f",
            "d130d9d563e94120880c324704beb6d3",
            "502f854d8e6b41a0926daa81728813f6",
            "3c2db1f53f1c4d19a373ca274bae4626",
            "1a5b71a528024adba646e239c7dc2c48",
            "cc8f1e1c4971419aa8260d500edd7749",
            "972956db98ec4620bc6f01f8275195a6",
            "367e67d87dec4f04b45bfec54d71777b",
            "6c7cf8e1c172461995f183eb99bca71e",
            "b636ad58b09d41f28089607027a5bffe",
            "637ddfc05ecd420b88591ea79e8d979e",
            "ee550de80ab444c5928996c6ea5907d8",
            "29bb24363e294ee38dfc2c1384f85adb",
            "ded690a2f6be46cdb7473e863d442382",
            "d685e26087e349af8463cfcdacad60cf",
            "76c73beb322a4cc0a21fc3721cb2f97a",
            "e7c0b6f965f9486f870e7d7e2109f40d",
            "61f00dfeabbf48e395f149990773a7ad",
            "5607d59b87e04285a5914b05d5e21668",
            "f8392076ac474b91ab5566a12756f00a",
            "78e28e5372fa472b8a7ba49775b2a10b",
            "6c2301db406b43e8a3ac5312f08a1810",
            "ea539075646547369e35b405175f7cd4",
            "d4082039517c439bbba70c404fb646d2",
            "c9c2f8faa40c4ae9afb47fbabcbf0150",
            "7b98d284115b4cdaaa4c0e1fadfb5482",
            "5425c989fe464489afc493a0c65536d6",
            "8fe4848bae784b7aa3c51e8c8e282243",
            "45124ac94efb4acc88e3b6570c7b8950",
            "dc9d1d2de0cc417fa6dec46e9c6a2d92",
            "eab763435e564100a61de685e1100a06",
            "0b858c3b74544d7188e8a5596b4cfa69",
            "f3c41913a37a4208bf1ef7e9207fc335",
            "f765836bf30b4782b1fb594d282076b5",
            "01248e30f0354eae90e51a7a697fc9bb",
            "a857817b6f1f46b38151d531509fb1ec",
            "d89c380ac93041c5be7dab88fb7b0c7f",
            "3accdf41c5ba41b5a68cbfff8ea69f2d",
            "83ebb6adcc044f27866621e27ac0b13b",
            "b9efd5e44e6d4ce98cf8ab4d64fdd297"
          ]
        },
        "outputId": "223e10f1-ddbd-4400-d917-0a84085d3675"
      },
      "source": [
        "s_discriminator = Similarity_Discriminator()\n",
        "#s_discriminator = Similarity_Discriminator()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating Similarity_Discriminator object\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0a4690f59a64cdd8e982dbe49026c76",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/795 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "68a5cc21e99e4d778949428c1d699fd4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/3.97k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bfc749d039fc48d19464a8220483e593",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/733 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "41adff6bd84c4d5b8ca88509e47a4ec8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/122 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "78758051efcd4fc084b07a84038b5b19",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/229 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ed9f4716f4c54a56bf899306d03b49e7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e36b30b4ad564813ab8bee0a4e68a9f5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f3ae4d8813ed40e0b8a6a64c29530bba",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c0b2d5d51004bba9283a8bf9550baf6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/150 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "367e67d87dec4f04b45bfec54d71777b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5607d59b87e04285a5914b05d5e21668",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/502 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dc9d1d2de0cc417fa6dec46e9c6a2d92",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/191 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xnk9GsQ0K1t1"
      },
      "source": [
        "# 4.4 Document source class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhvXMrSO-CiD"
      },
      "source": [
        "## 두 문장을 합치는 rule 변환기... --> 이거... ML로 나중에 바꿔야..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_W1Wqq26MjQ"
      },
      "source": [
        "combine_matching_table = {}\n",
        "\n",
        "combine_matching_table['어요.'] = '고'\n",
        "combine_matching_table['지요.'] = '고'\n",
        "combine_matching_table['답니다.'] = '고'\n",
        "combine_matching_table['보거라.'] = '봐,'\n",
        "combine_matching_table['간단다.'] = '가니,'\n",
        "combine_matching_table['돼.'] = '되,'\n",
        "combine_matching_table['해.'] = '하며,'\n",
        "combine_matching_table['다.'] = '고'\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giRiIsfR6DV6"
      },
      "source": [
        "def combine_sentence(txt):\n",
        "    for c in combine_matching_table.keys():\n",
        "        if txt.endswith(c):\n",
        "            txt = txt.replace(c,combine_matching_table[c])\n",
        "    return txt"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwhMHwwefy-N"
      },
      "source": [
        "\n",
        "conjunction_table = ['그러던','그래서','그러나','그런데','그리고','그랬더니','그러니까','하지만','그래서']"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnBm6RCvNIWG"
      },
      "source": [
        "## 4.4.2 frame term 추출을 위한 source class 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsJKbtc2K4xN",
        "trusted": true
      },
      "source": [
        "\n",
        "\n",
        "class Source:\n",
        "\n",
        "    def __init__(self,org_text):\n",
        "        self.org_text = org_text\n",
        "\n",
        "    def __crean_text(self, txt):\n",
        "        txt = txt.replace('\\n',' ')\n",
        "        txt = txt.replace('\\r',' ')    \n",
        "        txt = txt.replace('=','')\n",
        "        txt = txt.replace('\\\"','')   \n",
        "        txt = txt.replace('\\'','')\n",
        "        txt = txt.replace(',','')\n",
        "        txt = txt.replace('..','')\n",
        "        txt = txt.replace('...','')\n",
        "        txt = txt.replace(' .','.')\n",
        "        txt = txt.replace('.','. ')\n",
        "        txt = txt.replace('  ',' ')\n",
        "        txt = txt.replace('  ',' ')    \n",
        "        txt = txt.replace('  ',' ')   \n",
        "        txt = txt.replace('  ',' ')           \n",
        "        txt = txt.replace('  ',' ')\n",
        "        txt = txt.replace('  ',' ')    \n",
        "        txt = txt.replace('  ',' ')   \n",
        "        txt = txt.replace('  ',' ')           \n",
        "        return txt.strip()\n",
        "\n",
        "    def set_key_rate(self,s_discriminator,comp_rate=0.2):\n",
        "        #self.full_text = self.__crean_text(self.full_text.strip())\n",
        "        self.org_text = self.__crean_text(self.org_text.strip())\n",
        "        print('-'*50)\n",
        "        print(self.org_text)\n",
        "        print('-'*50)\n",
        "        self.org_sentences = np.array(nltk.sent_tokenize(self.org_text))\n",
        "        for i,sents in enumerate(self.org_sentences):\n",
        "            for cj in conjunction_table: \n",
        "                if sents.startswith(cj):\n",
        "                    self.org_sentences[i] = sents[len(cj):].strip()\n",
        "\n",
        "        #self.full_sentences = np.array(nltk.sent_tokenize(self.full_text))\n",
        "        \n",
        "        #self.org_term_set = (' ' + self.org_text + ' ').split(' ')\n",
        "        self.org_term_set = (' '.join(self.org_sentences)).strip().split(' ')\n",
        "        self.org_source_length = len(self.org_term_set)\n",
        "        self.term_table = {}\n",
        "        self.seps = []\n",
        "        #morp_table = {}\n",
        "\n",
        "        for index, word in zip(range(len(self.org_term_set)),self.org_term_set):\n",
        "            self.term_table[index] = word\n",
        "            if word.endswith(('.','?')):\n",
        "                self.seps.append(index)\n",
        "                if self.org_source_length - 1 == index:\n",
        "                    pass\n",
        "                else:\n",
        "                    self.term_table[index] = combine_sentence(word)\n",
        "\n",
        "        #print(self.term_table.values())\n",
        "        #print('------------------------------------------------------------------')\n",
        "\n",
        "        self.s_discriminator = s_discriminator\n",
        "        # 원문의 embedding...\n",
        "        self.org_text_emb = self.s_discriminator.encode(self.org_sentences)\n",
        "        #self.full_text_emb = self.s_discriminator.encode(self.full_sentences)\n",
        "        top_n = int(len(self.term_table) * comp_rate)\n",
        "        #print('top_n',top_n)\n",
        "        self.story_peaks = [i+1 for i in range(top_n)]\n",
        "\n",
        "    def get_org_sample(self, num):\n",
        "        return self.org_sentences[np.random.choice(len(self.org_sentences), num)]\n",
        "\n",
        "    def get_source_embedded_code(self):\n",
        "        return self.org_text_emb"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XY59mdNK8ub"
      },
      "source": [
        "# 4.5 Generator class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5CLF3WcK6lp",
        "trusted": true
      },
      "source": [
        "from functools import reduce\n",
        "\n",
        "\n",
        "# custom weights initialization called on netG and netD\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Linear') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.06)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.05)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "        Simple Generator w/ MLP\n",
        "    \"\"\"\n",
        "    '''\n",
        "    def __init__(self, input_size=1024):\n",
        "        super(Generator, self).__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(input_size, input_size*2),\n",
        "            nn.BatchNorm1d(input_size*2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(input_size*2, input_size*3),\n",
        "            nn.BatchNorm1d(input_size*3),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(input_size*3, input_size*3),\n",
        "            nn.BatchNorm1d(input_size*3),\n",
        "            nn.LeakyReLU(0.2),            \n",
        "            nn.Linear(input_size*3, input_size*2),\n",
        "            nn.BatchNorm1d(input_size*2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(input_size*2, input_size),\n",
        "            #nn.BatchNorm1d(term_length*4),\n",
        "            nn.Tanh() # -1 ~ 1\n",
        "        )\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, input_size=1024):\n",
        "        super(Generator, self).__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(input_size, input_size*4),\n",
        "            nn.BatchNorm1d(input_size*4),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(input_size*4, input_size),\n",
        "            #nn.BatchNorm1d(term_length*4),\n",
        "            nn.Tanh() # -1 ~ 1\n",
        "        )\n",
        "\n",
        "    def forward(self, x, bias):\n",
        "        #biased_noise = torch.randn(N,_NOISE_DIM)\n",
        "        # stroy peak에 해당하는 term에게 평균값에 해당하는 bias를 추가 한다.\n",
        "                 \n",
        "        y_ = self.layer(x)\n",
        "        y = torch.add(y_,bias)\n",
        "        #y = nn.Sigmoid()(y)\n",
        "\n",
        "        return y, y_\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co1MnRG8a4Vq"
      },
      "source": [
        "# multi-discriminator에 대한 Adaptive discriminant factor 를 구하기 위한 학습\n",
        "\n",
        "ref : https://realpython.com/python-ai-neural-network/\n",
        "\n",
        "colab 수식입력 : \n",
        "\n",
        "https://wikidocs.net/1679\n",
        "\n",
        "https://en.wikipedia.org/wiki/Help:Displaying_a_formula#Formatting_using_TeX\n",
        "\n",
        "Original GAN의 목적함수\n",
        "$$ \\min_{G}\\max_{D} V(D,G) = E_{x\\sim p_{data}(x)}[logD(x)] + E_{z\\sim p_{z}(z)}[log(1-D(G(z)))] $$\n",
        "\n",
        "Multi-Discriminator GAN은 discriminator가 각 목적에 의하여 여러개 (N개) 있다.\n",
        "MDGAN의 목적함수\n",
        "\n",
        "$$ \\min_{G}\\max_{D_{i\\sim N}} V(D_{i\\sim N},G) = \\sum_{i=1}^N \\{E_{x\\sim p_{data}(x)}[logD_i(x)] + E_{z\\sim p_{z}(z)}[log(1-D_i(G(z)))]\\} $$\n",
        "\n",
        "여기서\n",
        "\n",
        "$$ L(D_i,G) =  E_{x\\sim p_{data}(x)}[logD_i(x)] + E_{z\\sim p_{z}(z)}[log(1-D_i(G(z)))] $$\n",
        "\n",
        "이라하고 단순화 하면\n",
        "\n",
        "$$ \\min_{G}\\max_{D_{i\\sim N}} V(D_{i\\sim N},G) = \\sum_{i=1}^N L(D_i,G) $$\n",
        "\n",
        "와 같이 된다.\n",
        "\n",
        "문제점은 GAN의 특성상, 특정 Discriminator가 학습에 있어 지배적으로 loss 함수에 영향을 미치게 되어 각각의 Discriminator가 골고루 학습에 참여하지 못하고 의도하지 않은 결과를 만들게 된다. 이러한 문제점을 극복하기 위해 다음의 두가지 제안을 한다.\n",
        "\n",
        "1) 목적함수에 각 Loss 에 대한 표준편차 (standard-deviation) 를 반영하여 각 Discriminator에 대한 Loss가 상호 유사한 수준을 유지하면 학습이 진행되도록 한다.\n",
        "\n",
        "$$ \\min_{G}\\max_{D_{i\\sim N}} V(D_{i\\sim N},G) = \\sum_{i=1}^N L(D_i,G) + STD_{i \\sim N}(L(D_i,G))$$\n",
        "\n",
        "2) 1)의 제안에 추가하여, 각 discriminator에 의한 loss를 제어하기 위해, adaptive discriminant factor (ADF) 를 적용하고, 학습의 진행 과정에서 이를 최적화 한다. \n",
        "\n",
        "$$ \\min_{G}\\max_{D_{i\\sim N}} V(D_{i\\sim N},G) = \\sum_{i=1}^N f_iL(D_i,G) + STD_{i \\sim N}(L(D_i,G))$$\n",
        "\n",
        "여서서 \n",
        "\n",
        "fi = adaptive discriminant factor for discriminator i \n",
        "\n",
        "중요한 것은, 학습과정에서 Li을 작게 (학습의 방향)하기 위해서는 fi는 역으로 커져야 한다. 그래야, 전체 Loss function에서 비중이 증대되어 더 적극적인 학습이 이루어 지게 된다. 따라서, fi의 최적화 방향은 기존의 gradient decent와 반대 방향이 되어야 한다.\n",
        "\n",
        "$$ f_i^{t+1} = f_i^t + \\alpha \\frac{\\partial V}{\\partial f_i}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLVVmQdxLBHZ"
      },
      "source": [
        "##4.6 Summarizer class (GAN training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0RQOPpQgUTE"
      },
      "source": [
        "# 학습기..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nd8GTS7HKz1H",
        "trusted": true
      },
      "source": [
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "from scipy.special import expit\n",
        "\n",
        "class SAM_Summarizer:\n",
        "\n",
        "    def __init__(self,g_discriminator,s_discriminator):\n",
        "        self.g_discriminator = g_discriminator\n",
        "        #self.c_discriminator = c_discriminator\n",
        "        self.s_discriminator = s_discriminator\n",
        "        self.m = nn.Sigmoid()\n",
        "\n",
        "    def ready(self,source):\n",
        "        self.source = source  \n",
        "        #self.source.analysis_frame_terms(self.s_discriminator)\n",
        "        self.generator = Generator(input_size=self.source.org_source_length)\n",
        "        self.generator.apply(weights_init)\n",
        "        return self\n",
        "\n",
        "    def summarize(self,epochs=10,batch_size=2,frame_expansion_ratio = 0.8,init_bias = 1.0,learning_rate=2e-4, method=1, display = False):\n",
        "        self.frame_expansion_ratio = frame_expansion_ratio\n",
        "        history = self.__train(epochs,batch_size,init_bias,learning_rate,method,display)\n",
        "        if display:\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            plt.plot(history['gen_g_loss'],label='generator grammar loss')\n",
        "            #plt.plot(history['gen_c_loss'],label='generator completion loss')\n",
        "            plt.plot(history['gen_s_loss'],label='generator similarity loss')\n",
        "\n",
        "            plt.plot(history['total loss'],label='total loss')\n",
        "            plt.plot(history['losses std'],label='standard deviation of losses')\n",
        "            \n",
        "            #if 'dis_loss' in history:\n",
        "            #    plt.plot(history['dis_loss'],label='discriminator grammar loss')\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "        return history\n",
        "\n",
        "    # text의 생성 for torch\n",
        "    def __text_gen2(self, p_txt, gen_length):\n",
        "        gtext = []\n",
        "        sorted_noise, i = torch.sort(p_txt, descending=True)\n",
        "        order, i = torch.sort(i[:gen_length], descending=False)\n",
        "        #print(len(order))\n",
        "        #print(gen_length)\n",
        "        assert len(order) == gen_length\n",
        "        order = order.cpu().detach().numpy()\n",
        "        for k in order:\n",
        "            gtext.append((self.source.term_table[k],k))\n",
        "        return gtext\n",
        "\n",
        "    # text의 생성 for torch\n",
        "    def __text_gen3(self, p_txt):\n",
        "        gtext = []\n",
        "\n",
        "        for order,p in enumerate(p_txt):\n",
        "            if p > 0.0:\n",
        "                gtext.append(self.source.term_table[order])\n",
        "        return gtext\n",
        "\n",
        "    def __discrete_gradient(self,weights,gen_length,use_gpu=False, verbose=0):\n",
        "        fake_gen_out = torch.zeros(weights.shape).to(device)\n",
        "        #fake_com_out = torch.zeros(weights.shape).to(device)\n",
        "        fake_sim_out = torch.zeros(weights.shape).to(device)\n",
        "\n",
        "        real_text = self.source.get_org_sample(weights.shape[0])\n",
        "        fake_outs = []\n",
        "        real_outs = []\n",
        "        apply_order = []\n",
        "        for i, noise in enumerate(weights):\n",
        "            gtext = self.__text_gen2(noise,gen_length)\n",
        "            tw = \"\"\n",
        "            tk = []\n",
        "            fake_scores = []\n",
        "            for (w,k) in gtext:\n",
        "                tw += w + ' '\n",
        "                tk.append(k)\n",
        "                if w.endswith('.'):\n",
        "                    fake_outs.append(tw.strip())\n",
        "                    real_outs.append(real_text[i])\n",
        "                    apply_order.append((i,tk))\n",
        "                    tw = \"\"\n",
        "                    tk = []\n",
        "                    \n",
        "            if len(tk) > 0:\n",
        "                fake_outs.append(tw.strip())\n",
        "                real_outs.append(real_text[i])\n",
        "                apply_order.append((i,tk))\n",
        "\n",
        "        D_z_loss, fake_gmr_out=self.g_discriminator.transfer_learning(fake_outs,train_for = False)\n",
        "        #D_c_loss, fake_cpt_out=self.c_discriminator.transfer_learning(fake_outs,train_for = False)\n",
        "        #D_x_loss, real_gmr_out=self.g_discriminator.transfer_learning(real_outs,train_for = True)   # not use of 'real_gmr_out'\n",
        "\n",
        "        '''\n",
        "        f_sim_out = []\n",
        "        for fake_text in fake_outs:\n",
        "            f_sim_out.append(self.s_discriminator.similarity(fake_text,self.source.full_text_emb))\n",
        "        '''\n",
        "        o_sim_out = []\n",
        "        for fake_text in fake_outs:\n",
        "            o_sim_out.append(self.s_discriminator.similarity(fake_text,self.source.org_text_emb))\n",
        "        \n",
        "\n",
        "        #if use_gpu:\n",
        "        #    apply_order = torch.FloatTensor(apply_order).to(device)  \n",
        "        \n",
        "        #print(fake_dis_out)\n",
        "        \n",
        "        for j, (i,tk) in enumerate(apply_order):\n",
        "            #fake_gen_out[i,tk] += fake_dis_out[j].numpy() --> 이거는 tf 용...\n",
        "            #fake_gen_out[i,tk] += fake_dis_out[j] #.cpu().detach().numpy()\n",
        "            # \n",
        "            try:\n",
        "                #print('fake_gmr_out:',fake_gmr_out[j,1])\n",
        "                #print('real_gmr_out:',real_gmr_out[j,1])\n",
        "                #fake_gen_out[i,tk] += torch.sigmoid(fake_gmr_out[j,1])\n",
        "\n",
        "                fake_gen_out[i,tk] += torch.tanh( fake_gmr_out[j,1])\n",
        "                #fake_com_out[i,tk] += torch.tanh( fake_cpt_out[j,1])\n",
        "                #fake_sim_out[i,tk] += f_sim_out[j] + o_sim_out[j]\n",
        "                fake_sim_out[i,tk] += o_sim_out[j]\n",
        "                \n",
        "            except Exception as ex:\n",
        "                print(j,i,tk)\n",
        "                print(fake_gmr_out)\n",
        "                raise ex\n",
        "\n",
        "        return fake_gen_out, fake_sim_out #fake_com_out, fake_sim_out #, D_z_loss, D_x_loss\n",
        "\n",
        "\n",
        "    def __train(self, epochs=10,batch_size=10,init_bias = 1.0,learning_rate=2e-4, method=1, display = False):\n",
        "        # In the Deepmind paper they use RMSProp however then Adam optimizer\n",
        "        # improves training time\n",
        "        #generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "        # This method returns a helper function to compute cross entropy loss\n",
        "        #cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "        # Set the seed value all over the place to make this reproducible.\n",
        "        seed_val = int(random.random()*10)\n",
        "\n",
        "        random.seed(seed_val)\n",
        "        np.random.seed(seed_val)\n",
        "        torch.manual_seed(seed_val)\n",
        "        torch.cuda.manual_seed_all(seed_val)\n",
        "        \n",
        "        criterion = nn.BCELoss()\n",
        "        #D_opt = torch.optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "        G_opt = torch.optim.Adam(self.generator.parameters(), lr=learning_rate)\n",
        "        D1_opt = AdamW(self.g_discriminator.discriminator.parameters(),\n",
        "                        lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                        eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                        )\n",
        "\n",
        "        \n",
        "        gen_length = len(self.source.story_peaks) + int(len(self.source.story_peaks)*self.frame_expansion_ratio)\n",
        "        pb = ProgressBar(epochs,prefix='Train...')\n",
        "        gen_gmr_loss_history = []\n",
        "        gen_com_loss_history = []\n",
        "        gen_sim_loss_history = []\n",
        "        dis_loss_history = []    \n",
        "        total_loss_history = []\n",
        "        losses_std_history = []\n",
        "\n",
        "        #model 들은 cuda로 보낸다.\n",
        "        self.g_discriminator.discriminator.to(device)\n",
        "        self.g_discriminator.discriminator.eval() # 학습하지 않는다...\n",
        "        #self.c_discriminator.discriminator.to(device)\n",
        "        #self.c_discriminator.discriminator.eval() # 학습하지 않는다...\n",
        "\n",
        "        self.generator.to(device)       \n",
        "        self.generator.train()\n",
        "\n",
        "        self.bias_w = init_bias\n",
        "        initial_bias = 0\n",
        "        G_s_loss = torch.tensor(0)\n",
        "        #G_c_loss = torch.tensor(0)\n",
        "        G_g_loss = torch.tensor(0)\n",
        "\n",
        "\n",
        "        dfs = torch.tensor([ 1.0, 1.0], device=device, dtype=torch.float, requires_grad=True)\n",
        "\n",
        "        for i in range(epochs):\n",
        "            '''\n",
        "            noise = torch.randn(batch_size,self.source.org_source_length).to(device)\n",
        "            bias = torch.zeros_like(noise).to(device)\n",
        "            bias[:,self.source.story_peaks] += self.bias_w \n",
        "            with torch.no_grad():        \n",
        "                sw, sw0 = self.generator(noise,bias)\n",
        "\n",
        "            self.g_discriminator.discriminator.train()          #discriminator는 evaluation 모드로 전환\n",
        "            fake_gmr_out, fake_sim_out, D_z_loss, D_x_loss = self.__discrete_gradient(sw,gen_length)\n",
        "            \n",
        "            D_loss = D_x_loss + D_z_loss      \n",
        "\n",
        "            self.g_discriminator.discriminator.zero_grad()\n",
        "            D_loss.backward()\n",
        "            D1_opt.step()\n",
        "            self.g_discriminator.discriminator.eval()\n",
        "            seps\n",
        "            '''\n",
        "            if True:\n",
        "                noise = torch.randn(batch_size,self.source.org_source_length).to(device)\n",
        "                bias = torch.zeros_like(noise).to(device)\n",
        "                #bias[:,self.source.seps] = -1.0 #+= self.bias_w  '~~~~.'에 해당하는 token은 제외 되도록 -1의 bias를 삽입\n",
        "                #bias[:,self.source.seps[len(self.source.seps)-1]] = 1.0 # 마지막  '~~~~.'에 해당하는 token은 반드시 포함 하도록 +1의  bias를 삽입\n",
        "\n",
        "                sw, sw0 = self.generator(noise,bias)\n",
        "\n",
        "                with torch.no_grad():                \n",
        "                    #fake_gmr_out, fake_com_out, fake_sim_out, D_z_loss, D_x_loss = self.__discrete_gradient(sw,gen_length,beta)\n",
        "                    fake_gmr_out, fake_sim_out = self.__discrete_gradient(sw,gen_length)\n",
        "                    #print(D_z_loss)\n",
        "                '''\n",
        "                if int(i/10)%2 == 0:  # grammar와 similarity를 각각 한번씩 교대로 학습한다?\n",
        "                    sw1 = sw * fake_sim_out\n",
        "                    G_s_loss = -torch.mean(sw1)\n",
        "                    G_loss = G_s_loss    \n",
        "                else: #if i%2 == 1:\n",
        "                    sw1 = sw * fake_gmr_out\n",
        "                    G_g_loss = -torch.mean(sw1)\n",
        "                    G_loss = G_g_loss\n",
        "                '''\n",
        "\n",
        "                sw1 = sw * fake_sim_out\n",
        "                G_s_loss = -torch.mean(sw1) * 5\n",
        "                sw2 = sw * fake_gmr_out\n",
        "                G_g_loss = -torch.mean(sw2) * 3\n",
        "                #sw3 = sw * fake_com_out\n",
        "                #G_c_loss = -torch.mean(sw3)\n",
        "\n",
        "                dsc_loss = torch.stack([G_g_loss,G_s_loss])\n",
        "                #dfs[1] = 1.0\n",
        "                #tdfs = torch.Tensor(dfs).to(device)\n",
        "\n",
        "                #G_loss = adf(dsc_loss) / torch.std(dsc_loss) # torch.dot(tdfs,dsc_loss)\n",
        "                #G_loss = torch.dot(tdfs,dsc_loss)\n",
        "\n",
        "                if method == 1:\n",
        "                    G_loss = torch.dot(dfs,dsc_loss)\n",
        "                else:\n",
        "                    G_loss = torch.dot(dfs,dsc_loss) + torch.std(dsc_loss)*2\n",
        "\n",
        "                #print(dsc_loss)\n",
        "                #print(G_loss)\n",
        "                \n",
        "                #G_loss =  G_g_loss #+ G_c_loss + G_s_loss*1.5\n",
        "\n",
        "                ##Gs = dsc_loss.cpu().detach().numpy()\n",
        "                ##total_loss = np.dot(dfs,Gs)\n",
        "\n",
        "                self.generator.zero_grad()\n",
        "                G_loss.backward()\n",
        "                #print('backward:')\n",
        "                G_opt.step()\n",
        "                #self.generator.eval()\n",
        "                if method == 3:\n",
        "                    learning_rate = 0.1\n",
        "                    with torch.no_grad():\n",
        "                        dfs += learning_rate * dfs.grad\n",
        "                        dfs.grad = None                    \n",
        "                        dfs[dfs < 0] = 0.1\n",
        "\n",
        "\n",
        "                        \n",
        "                ########################################\n",
        "                \n",
        "                #d_objective = np.mean(Gs) - Gs \n",
        "                #dfs = dfs - d_objective\n",
        "                #dfs = [0.01 if i<0.0 else i for i in dfs]\n",
        "                \n",
        "                ########################################\n",
        "\n",
        "            #print('step:')\n",
        "            gen_gmr_loss_history.append(G_g_loss.cpu().detach().numpy())\n",
        "            #gen_com_loss_history.append(G_c_loss.cpu().detach().numpy())\n",
        "            gen_sim_loss_history.append(G_s_loss.cpu().detach().numpy())\n",
        "            #dis_loss_history.append(D_loss.cpu().detach().numpy())\n",
        "\n",
        "            #pb.printProgress(+1,f'{i+1}/{epochs} epochs, beta:{dfs} Generator / grammar loss:{G_g_loss}  similarity loss:{G_s_loss}') #,   Discriminator grammar_loss:{D_loss}        ')\n",
        "            pb.printProgress(+1,'{}/{} epochs, grammar loss:{:.4f}  similarity loss:{:.4f}'.format(i+1,epochs,G_g_loss,G_s_loss)) #,   Discriminator grammar_loss:{D_loss}        ')\n",
        "            \n",
        "            total_loss_history.append(torch.sum(dsc_loss).item())\n",
        "            losses_std_history.append(torch.std(dsc_loss).item())\n",
        "            \n",
        "        self.generator.eval()\n",
        "        #self.g_discriminator.discriminator.eval()\n",
        "        if display:\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            xs = np.arange(self.source.org_source_length)\n",
        "            plt.bar(xs+0.0,sw0[0].cpu().detach().numpy(),label='before activation weights',width=0.2)\n",
        "            plt.bar(xs+0.2,sw[0].cpu().detach().numpy(),label='after activation weights',width=0.2)\n",
        "            plt.bar(xs+0.4,bias[0].cpu().detach().numpy(),label='bias weights',width=0.2)         \n",
        "            plt.legend()        \n",
        "            plt.show()\n",
        "\n",
        "        return  {'gen_g_loss':gen_gmr_loss_history,'gen_s_loss':gen_sim_loss_history,'total loss':total_loss_history,'losses std':losses_std_history} #,'dis_loss':dis_loss_history }\n",
        "\n",
        "    def get_summary(self, count):\n",
        "        #texts = []\n",
        "        self.generator.cpu()\n",
        "        self.generator.eval()\n",
        "        #gen_length = len(self.source.story_peaks) + int(len(self.source.story_peaks)*self.frame_expansion_ratio)\n",
        "        noise = torch.randn(count,self.source.org_source_length)\n",
        "        bias = torch.zeros_like(noise)\n",
        "        #bias = torch.randn(1,self.source.org_source_length)\n",
        "        #bias[:,self.source.story_peaks] += self.bias_w #self.last_bias_max.cpu().detach().numpy()\n",
        "        #bias = 0\n",
        "        #bias[:,self.source.seps] = -1.0 #+= self.bias_w  '~~~~.'에 해당하는 token은 제외 되도록 -1의 bias를 삽입\n",
        "        #bias[:,self.source.seps[len(self.source.seps)-1]] = 1.0 # 마지막  '~~~~.'에 해당하는 token은 반드시 포함 하도록 +1의  bias를 삽입\n",
        "\n",
        "        with torch.no_grad():\n",
        "            sw,sw0 = self.generator(noise,bias)\n",
        "            #sw,sw0 = self.generator(noise)\n",
        "\n",
        "        max_score = 0\n",
        "        max_sim = 0\n",
        "        best_text = \"\"\n",
        "\n",
        "        for p_txt in sw:\n",
        "            gtext = self.__text_gen3(p_txt)\n",
        "            text = ' '.join(gtext)\n",
        "            #print(text)\n",
        "            loss, out=self.g_discriminator.transfer_learning([text],train_for = False)\n",
        "            sim_score = self.s_discriminator.similarity(text,self.source.org_text_emb)\n",
        "            if out[0,1].item() > max_score:\n",
        "                best_text = text.strip()\n",
        "                max_score = out[0,1].item()\n",
        "                max_sim = sim_score.item()\n",
        "            #texts.append([text.strip(),out,sim_score])\n",
        "        return best_text, max_score, max_sim\n",
        "\n",
        "    def get_samples(self,count):\n",
        "        self.generator.cpu()\n",
        "        self.generator.eval()\n",
        "        noise = torch.randn(count,self.source.org_source_length)\n",
        "        bias = torch.zeros_like(noise)\n",
        "        with torch.no_grad():\n",
        "            sw,sw0 = self.generator(noise,bias)\n",
        "        samples = []\n",
        "        for p_txt in sw:\n",
        "            gtext = self.__text_gen3(p_txt)\n",
        "            text = ' '.join(gtext).strip()\n",
        "            #print(text)\n",
        "            loss, out=self.g_discriminator.transfer_learning([text],train_for = False)\n",
        "            sim_score = self.s_discriminator.similarity(text,self.source.org_text_emb)    \n",
        "            #print(text,out[0,1],sim_score)\n",
        "            samples.append((text,out[0,1].item(),sim_score))\n",
        "            \n",
        "        return samples\n"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCdfO9iuLH6D"
      },
      "source": [
        "#5. Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_eAwIPLb4aj"
      },
      "source": [
        "## 비교 대상 요약 알고리즘 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7Ty6_5gb_zR",
        "trusted": true
      },
      "source": [
        "\n",
        "def similarity(query_text, org_text):\n",
        "    sentences = nltk.sent_tokenize(org_text)\n",
        "    #print(\"Num sentences:\", len(sentences))\n",
        "    querys = nltk.sent_tokenize(query_text)\n",
        "    #print(\"Num querys:\", len(querys))\n",
        "\n",
        "    #Compute the sentence embeddings\n",
        "    org_embeddings = s_discriminator._embedder.encode(sentences,show_progress_bar=False)\n",
        "    query_embeddings = s_discriminator._embedder.encode(querys,show_progress_bar=False)\n",
        "\n",
        "    #Compute the pair-wise cosine similarities\n",
        "    cos_scores = scipy.spatial.distance.cdist(query_embeddings, org_embeddings, \"cosine\")\n",
        "    similarity_score = 1.0 - np.mean(np.min(cos_scores,axis=0))\n",
        "\n",
        "    return similarity_score\n",
        "\n",
        "def grammarity(text):\n",
        "    \n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    sentences = np.asarray(nltk.sent_tokenize(text))\n",
        "    # For every sentence...\n",
        "    for sent in sentences:\n",
        "        # `encode_plus` will:\n",
        "        #   (1) Tokenize the sentence.\n",
        "        #   (2) Prepend the `[CLS]` token to the start.\n",
        "        #   (3) Append the `[SEP]` token to the end.\n",
        "        #   (4) Map tokens to their IDs.\n",
        "        #   (5) Pad or truncate the sentence to `max_length`\n",
        "        #   (6) Create attention masks for [PAD] tokens.\n",
        "        encoded_dict = g_discriminator.tokenizer.encode_plus(\n",
        "                            sent,                      # Sentence to encode.\n",
        "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                            max_length = 64,           # Pad & truncate all sentences.\n",
        "                            pad_to_max_length = True,\n",
        "                            return_attention_mask = True,   # Construct attn. masks.\n",
        "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                            truncation = True,\n",
        "                       )\n",
        "        # Add the encoded sentence to the list.    \n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "        # And its attention mask (simply differentiates padding from non-padding).\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    # Convert the lists into tensors.\n",
        "    input_ids = torch.cat(input_ids, dim=0).to(device)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0).to(device)\n",
        "    g_discriminator.discriminator.to(device)\n",
        "    #if str(discriminator1.device) == 'cpu':\n",
        "    #    pass\n",
        "    #else:\n",
        "    #    input_ids = input_ids.to(device)\n",
        "    #    attention_masks = attention_masks.to(device)        \n",
        "\n",
        "    with torch.no_grad():        \n",
        "        outputs = g_discriminator.discriminator(input_ids, \n",
        "                               token_type_ids=None, \n",
        "                               attention_mask=attention_masks)\n",
        "    #return torch.sigmoid(outputs[0][:,1])\n",
        "    return torch.mean(outputs[0][:,1]).detach().cpu().numpy()\n",
        "    #return outputs"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLbWuwKXcMyk",
        "trusted": true
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def evaluate(method_name, text, g_summ, org_text_1,org_text_2,org_text_3):\n",
        "    result = {}\n",
        "    result['method'] = [method_name]\n",
        "    org_text = org_text_1 + ' ' + org_text_2 + ' ' + org_text_3\n",
        "    result['comp ratio'] = [len(text)/len(org_text)]\n",
        "    result['intro'] = [similarity(text,org_text_1)]\n",
        "    result['body'] = [similarity(text,org_text_2)]\n",
        "    result['ending'] = [similarity(text,org_text_3)]\n",
        "    result['var'] = [np.var([result['intro'][0],result['body'][0],result['ending'][0]])]\n",
        "    result['total'] = [similarity(text,org_text)]\n",
        "    result['grammar'] = [np.tanh(float(grammarity(text)))]\n",
        "    #scores = scorer.score(g_summ,text)\n",
        "    #result['R1'] = [scores['rouge1'].fmeasure]\n",
        "    #result['R2'] = [scores['rouge2'].fmeasure]\n",
        "    #result['RL'] = [scores['rougeL'].fmeasure]\n",
        "    return pd.DataFrame(result),result"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7BT0KdG8EXW"
      },
      "source": [
        "def sam_wgan2(g_summ,text,comp_rate=0.4, method=1, display = False):\n",
        "    source = Source(text)\n",
        "    if comp_rate > 0.8:\n",
        "        comp_rate = 0.8\n",
        "\n",
        "    source.set_key_rate(s_discriminator,comp_rate=comp_rate)\n",
        "    summarizer = SAM_Summarizer(g_discriminator,s_discriminator)\n",
        "    summarizer.ready(source)\n",
        "    hist = summarizer.summarize(epochs=60,batch_size=16,frame_expansion_ratio = 0.0, init_bias=0.0,learning_rate=5e-3,method=method,display=display)\n",
        "    summary_text, score, sim = summarizer.get_summary(100)\n",
        "    #print('-'*50)\n",
        "    #print('gold summary:')\n",
        "    #print(g_summ)    \n",
        "    print('-'*50)\n",
        "    #print('sam_wgan summary:')\n",
        "    #for txt in summary_text:\n",
        "    print(summary_text,score,sim)\n",
        "    #print('-'*50)\n",
        "    if score < 2.9 or sim < 0.65:\n",
        "        return sam_wgan2(g_summ,text,comp_rate+0.02, method, display)\n",
        "    #df,arr = evaluate('SAM+WGAN',summary_text,g_summ,org_text[0],org_text[1],org_text[2])\n",
        "    return hist, summary_text"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2k1mLxSQwB7G"
      },
      "source": [
        "def summary(text):\n",
        "    org_sentences = np.array(nltk.sent_tokenize(text.strip()))\n",
        "    summary_text = []\n",
        "    for i in range(0,len(org_sentences),2):\n",
        "        txt = org_sentences[i]\n",
        "        if i < len(org_sentences)-1:\n",
        "            txt +=  ' ' + org_sentences[i+1]\n",
        "        hist,st = sam_wgan2('',txt.strip(),comp_rate=0.4,method=2, display= False)\n",
        "        summary_text.append(st)\n",
        "\n",
        "    return ' '.join(summary_text).strip()\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "0S301yeelEvG"
      },
      "source": [
        "full_text = \"\"\"\n",
        "옛날 어느 집에 귀여운 여자 아기가 태어났어요.\n",
        "아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요.\n",
        "그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요.\n",
        "소녀의 아버지는 홀로 남은 소녀가 걱정되었어요.\n",
        "그래서 얼마 지나서 새어머니를 맞이했어요.\n",
        "새어머니는 소녀보다 나이가 위인 두명의 딸을 데리고 왔어요.\n",
        "그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요.\n",
        "새어머니는 소녀가 자기 딸들보다 예쁘고 착한게 못마땅했어요.\n",
        "그런데 이번에는 아버지마저 돌아가셨어요.\n",
        "소녀는 쓸고, 닦고, 하녀처럼 하루 종일 집안일을 도맡아 했어요.\n",
        "집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시 쉬곤 했어요.\n",
        "그러던 어느날, 왕궁에서 무도회가 열렸어요.\n",
        "신데렐라의 집에도 무도회 초대장이 왔어요.\n",
        "새어머니는 언니들을 데리고 무도회장으로 떠났어요.\n",
        "신데렐라도 무도회에 가고 싶었어요.\n",
        "혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요.\n",
        "그때 어디선가 마법사 할머니가 나타났어요.\n",
        "신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요.\n",
        "할머니는 소녀를 무도회에 보내줄테니 호박 한개와 생쥐 두마리, 도마뱀을 가지고 오라 했어요.\n",
        "마법사 할머니가 이것들을 보면서 주문을 외웠어요.\n",
        "그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금마차로 변했어요.\n",
        "이번에는 생쥐와 도마뱀을 건드렸어요.\n",
        "그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했어요.\n",
        "신데렐라의 낡은 옷은 구슬 장식이 반짝이는 예쁜 드레스로 바뀌었어요.\n",
        "할머니는 신데렐라에게 반짝반짝 빛나는 유리구두를 신겨 주었어요.\n",
        "그리고 밤 열두시가 되면 모든게 처음대로 돌아간다고 알려주었어요.\n",
        "황금마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼어요. \n",
        "그러니까 반드시 밤 열두시가 되기 전에 돌아와야 해요.\n",
        "신데렐라는 황금마차를 타고 왕궁 무도회장으로 가서 멋진 왕자님을 만났어요.\n",
        "왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요.\n",
        "왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고 신데렐라하고만 춤을 추었어요.\n",
        "신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요.\n",
        "어느덧 시간이 흘러 열두시가 되었어요. \n",
        "벽시계의 열두시를 알리는 종소리에 신데렐라는 화들짝 놀랐어요.\n",
        "신데렐라가 허둥지둥 왕궁을 빠져나가는데, 유리구두 한짝이 벗겨졌어요.\n",
        "하지만 구두를 주울 시간이 없었어요.\n",
        "신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리구두 한짝을 주웠어요.\n",
        "왕자님은 유리구두를 가지고 임금님께 가서 말했어요.\n",
        "이 유리구두의 주인과 결혼하겠어요.\n",
        "그래서 신하들은 유리구두의 주인을 찾아 온 나라를 돌아다녔어요.\n",
        "드디어 신데렐라의 집에까지 신하들이 도착했어요.\n",
        "언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리구두는 너무 작았어요.\n",
        "그때 신데렐라가 조용히 다가와 자기도 한번 신어보게 해달라고 부탁했어요.\n",
        "신데렐라는 신하에게서 받은 유리구두를 신었어요.\n",
        "유리구두는 신데렐라의 발에 꼭 맞았어요.\n",
        "신하들은 신데렐라를 왕궁으로 데리고 갔어요.\n",
        "그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\n",
        "\"\"\""
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "h7mVuIMzsEdN",
        "outputId": "ce0dc932-e2a5-4c75-9b41-a6663b65ecf0"
      },
      "source": [
        "txt = \"\"\"\n",
        "황금마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼어요. \n",
        "그러니까 반드시 밤 열두시가 되기 전에 돌아와야 해요.\n",
        "\"\"\"\n",
        "hist,st = sam_wgan2('',txt.strip(),comp_rate=0.4,method=2, display= True)\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "황금마차는 호박으로 흰말은 생쥐로 마부는 도마뱀으로 변하게 돼어요. 그러니까 반드시 밤 열두시가 되기 전에 돌아와야 해요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   60/60 epochs, grammar loss:-1.1750  similarity loss:-0.8354\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "황금마차는 호박으로 도마뱀으로 변하게 전에 돌아와야 해요. 3.668661594390869 0.475898054942236\n",
            "--------------------------------------------------\n",
            "황금마차는 호박으로 흰말은 생쥐로 마부는 도마뱀으로 변하게 돼어요. 그러니까 반드시 밤 열두시가 되기 전에 돌아와야 해요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   60/60 epochs, grammar loss:-0.7334  similarity loss:-0.8593\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAssAAAFlCAYAAAAd9qXYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXzV1b3/+9dHBlMBKSo92kIFey0QAiQQwIpBcaRI0SoOdaj81HrEWnvbU+9BbdFD633YAw/1OBfqUK2tIBYOp2KlDhRwKkGBSpAiJRXUg4iKDKUyrPtHYm6AfAHJDpvA6/l48HB/v9+11/rsZUjeWXyHSCkhSZIkaXsH5LsASZIkaW9lWJYkSZIyGJYlSZKkDIZlSZIkKYNhWZIkScpgWJYkSZIyNM13AVkOO+yw1KFDh3yXIUmSpH3cnDlz3k8pta3r2F4bljt06EB5eXm+y5AkSdI+LiL+nnXM0zAkSZKkDIZlSZIkKYNhWZIkScpgWJYkSZIyGJYlSZKkDIZlSZIkKYNhWZIkScpgWJYkSZIy5CQsR8QDEfFeRLyecTwi4o6IeDMi5kdEz1yMK0mSJDWkXK0sPwQM3MHxrwNHV/+5Arg3R+NKkiRJDSYnYTmlNAP4YAdNzgAeTlVeBj4fEUfkYmxJkiSpoeypc5a/BCyrtb28et9WIuKKiCiPiPKVK1fuodIkSZKkujXNdwG1pZTGAmMBSktLU57L0X6kw4gnd3i8suCCnXdy0+ocVSNJyoWdfW+HXfj+7vf2/d6eCstvA+1rbber3ifpM6h3qPebvhpAYwkk/lIs1c/++jNoT52GMQX4dvVdMY4BVqeU3t1DY0uSJEm7JScryxHxW+AE4LCIWA7cCDQDSCndB0wFBgFvAuuB/5OLcSVJkqSGlJOwnFL61k6OJ+C7uRhLkiRJ2lN8gp8kSZKUwbAsSZIkZTAsS5IkSRkMy5IkSVIGw7IkSZKUwbAsSZIkZTAsS5IkSRkMy5IkSVIGw7IkSZKUwbAsSZIkZTAsS5IkSRkMy5IkSVIGw7IkSZKUwbAsSZIkZTAsS5IkSRkMy5IkSVIGw7IkSZKUwbAsSZIkZTAsS5IkSRkMy5IkSVIGw7IkSZKUwbAsSZIkZTAsS5IkSRkMy5IkSVIGw7IkSZKUwbAsSZIkZTAsS5IkSRkMy5IkSVIGw7IkSZKUwbAsSZIkZTAsS5IkSRkMy5IkSVIGw7IkSZKUwbAsSZIkZTAsS5IkSRkMy5IkSVIGw7IkSZKUISdhOSIGRsSiiHgzIkbUcfzLEfF8RLwWEfMjYlAuxpUkSZIaUr3DckQ0Ae4Gvg4UAt+KiMJtmv0YmJBSKgHOB+6p77iSJElSQ8vFynIf4M2U0t9SSp8AjwFnbNMmAQdXv24NvJODcSVJkqQG1TQHfXwJWFZreznQd5s2NwHTIuJ7QAvg5ByMK0mSJDWoPXWB37eAh1JK7YBBwCMRsd3YEXFFRJRHRPnKlSv3UGmSJElS3XIRlt8G2tfable9r7bLgAkAKaWXgALgsG07SimNTSmVppRK27Ztm4PSJEmSpN2Xi7A8Gzg6IjpGRHOqLuCbsk2bt4CTACKiC1Vh2aVjSZIk7dXqHZZTSpuAq4GngYVU3fViQUSMiogh1c3+DfhORMwDfgsMSyml+o4tSZIkNaRcXOBHSmkqMHWbfSNrva4A+uViLEmSJGlP8Ql+kiRJUgbDsiRJkpTBsCxJkiRlMCxLkiRJGQzLkiRJUgbDsiRJkpTBsCxJkiRlMCxLkiRJGQzLkiRJUgbDsiRJkpTBsCxJkiRlMCxLkiRJGQzLkiRJUgbDsiRJkpTBsCxJkiRlMCxLkiRJGQzLkiRJUgbDsiRJkpTBsCxJkiRlMCxLkiRJGQzLkiRJUgbDsiRJkpTBsCxJkiRlMCxLkiRJGQzLkiRJUgbDsiRJkpTBsCxJkiRlMCxLkiRJGQzLkiRJUgbDsiRJkpTBsCxJkiRlMCxLkiRJGQzLkiRJUgbDsiRJkpTBsCxJkiRlMCxLkiRJGQzLkiRJUgbDsiRJkpQhJ2E5IgZGxKKIeDMiRmS0OTciKiJiQUT8JhfjSpIkSQ2paX07iIgmwN3AKcByYHZETEkpVdRqczRwHdAvpfRhRHyhvuNKkiRJDS0XK8t9gDdTSn9LKX0CPAacsU2b7wB3p5Q+BEgpvZeDcSVJkqQGlYuw/CVgWa3t5dX7avsq8NWIeCEiXo6IgXV1FBFXRER5RJSvXLkyB6VJkiRJu29PXeDXFDgaOAH4FjAuIj6/baOU0tiUUmlKqbRt27Z7qDRJkiSpbrkIy28D7Wttt6veV9tyYEpKaWNKaSnwV6rCsyRJkrTXykVYng0cHREdI6I5cD4wZZs2k6laVSYiDqPqtIy/5WBsSZIkqcHUOyynlDYBVwNPAwuBCSmlBRExKiKGVDd7GlgVERXA88C1KaVV9R1bkiRJakj1vnUcQEppKjB1m30ja71OwA+r/0iSJEmNgk/wkyRJkjIYliVJkqQMhmVJkiQpg2FZkiRJymBYliRJkjIYliVJkqQMhmVJkiQpg2FZkiRJymBYliRJkjIYliVJkqQMhmVJkiQpg2FZkiRJymBYliRJkjIYliVJkqQMhmVJkiQpg2FZkiRJymBYliRJkjIYliVJkqQMhmVJkiQpg2FZkiRJymBYliRJkjIYliVJkqQMhmVJkiQpg2FZkiRJymBYliRJkjIYliVJkqQMhmVJkiQpg2FZkiRJymBYliRJkjIYliVJkqQMhmVJkiQpg2FZkiRJymBYliRJkjIYliVJkqQMhmVJkiQpg2FZkiRJymBYliRJkjLkJCxHxMCIWBQRb0bEiB20OzsiUkSU5mJcSZIkqSHVOyxHRBPgbuDrQCHwrYgorKNdK+D7wCv1HVOSJEnaE3KxstwHeDOl9LeU0ifAY8AZdbT7KfBzYEMOxpQkSZIaXC7C8peAZbW2l1fvqxERPYH2KaUnd9RRRFwREeURUb5y5coclCZJkiTtvga/wC8iDgBuBf5tZ21TSmNTSqUppdK2bds2dGmSJEnSDuUiLL8NtK+13a5636daAUXA9IioBI4BpniRnyRJkvZ2uQjLs4GjI6JjRDQHzgemfHowpbQ6pXRYSqlDSqkD8DIwJKVUnoOxJUmSpAZT77CcUtoEXA08DSwEJqSUFkTEqIgYUt/+JUmSpHxpmotOUkpTganb7BuZ0faEXIwpSZIkNTSf4CdJkiRlMCxLkiRJGQzLkiRJUgbDsiRJkpTBsCxJkiRlMCxLkiRJGQzLkiRJUgbDsiRJkpTBsCxJkiRlMCxLkiRJGQzLkiRJUgbDsiRJkpTBsCxJkiRlMCxLkiRJGQzLkiRJUgbDsiRJkpTBsCxJkiRlMCxLkiRJGQzLkiRJUgbDsiRJkpTBsCxJkiRlMCxLkiRJGQzLkiRJUgbDsiRJkpTBsCxJkiRlMCxLkiRJGQzLkiRJUgbDsiRJkpTBsCxJkiRlMCxLkiRJGQzLkiRJUgbDsiRJkpTBsCxJkiRlMCxLkiRJGQzLkiRJUgbDsiRJkpTBsCxJkiRlyElYjoiBEbEoIt6MiBF1HP9hRFRExPyIeDYijszFuJIkSVJDqndYjogmwN3A14FC4FsRUbhNs9eA0pRSd2Ai8J/1HVeSJElqaLlYWe4DvJlS+ltK6RPgMeCM2g1SSs+nlNZXb74MtMvBuJIkSVKDykVY/hKwrNb28up9WS4DnsrBuJIkSVKDaronB4uIi4BS4PiM41cAVwB8+ctf3oOVSZIkSdvLxcry20D7WtvtqvdtJSJOBm4AhqSU/llXRymlsSml0pRSadu2bXNQmiRJkrT7chGWZwNHR0THiGgOnA9Mqd0gIkqAX1AVlN/LwZiSJElSg6t3WE4pbQKuBp4GFgITUkoLImJURAypbjYaaAk8HhFzI2JKRneSJEnSXiMn5yynlKYCU7fZN7LW65NzMY4kSZK0J/kEP0mSJCmDYVmSJEnKYFiWJEmSMhiWJUmSpAyGZUmSJCnDHn2CnyRJ2jd0GPHkDo9XFlyw4w5uWp3DaqSG48qyJEmSlMGwLEmSJGUwLEuSJEkZDMuSJElSBsOyJEmSlMGwLEmSJGUwLEuSJEkZDMuSJElSBsOyJEmSlMGwLEmSJGUwLEuSJEkZDMuSJElSBsOyJEmSlKFpvguQtG/pMOLJnbapLLhgxw1uWp2jaiRJqh9XliVJkqQMhmVJkiQpg2FZkiRJymBYliRJkjIYliVJkqQMhmVJkiQpg2FZkiRJymBYliRJkjIYliVJkqQMhmVJkiQpg2FZkiRJymBYliRJkjIYliVJkqQMhmVJkiQpg2FZkiRJymBYliRJkjIYliVJkqQMhmVJkiQpQ07CckQMjIhFEfFmRIyo4/iBETG++vgrEdEhF+NKkiRJDalpfTuIiCbA3cApwHJgdkRMSSlV1Gp2GfBhSun/iojzgZ8D59V37P1dhxFP7vB4ZcEFO+7gptU5rEaSJGnfk4uV5T7Amymlv6WUPgEeA87Yps0ZwK+qX08EToqIyMHYkiRJUoOJlFL9OogYCgxMKV1evX0x0DeldHWtNq9Xt1levb2kus372/R1BXAFwJe//OVef//73+tV2+6q94otuGpbbWdzCa6AKz8ay99z/wVp/9NYvjaVO/4/z7+ImJNSKq3r2F51gV9KaWxKqTSlVNq2bdt8lyNJkqT9XC7C8ttA+1rb7ar31dkmIpoCrYFVORhbkiRJajC5CMuzgaMjomNENAfOB6Zs02YKcEn166HAc6m+539IkiRJDazed8NIKW2KiKuBp4EmwAMppQURMQooTylNAe4HHomIN4EPqArUkiRJ0l6t3mEZIKU0FZi6zb6RtV5vAM7JxViSJEnSnrJXXeAnSZIk7U1ysrIsSWoYlbecvpMW3i5KkhqSK8uSJElSBsOyJEmSlMGwLEmSJGUwLEuSJEkZDMuSJElSBsOyJEmSlMGwLEmSJGUwLEuSJEkZfCiJGtTOH6gAPlRBkiTtrVxZliRJkjIYliVJkqQMhmVJkiQpg2FZkiRJymBYliRJkjIYliVJkqQMhmVJkiQpg2FZkiRJymBYliRJkjIYliVJkqQMhmVJkiQpQ9N8FyBJn9q4cSPLly9nw4YNDT7WU5cctcPjC3l5550sXJijarQ3KSgooF27djRr1izfpUjaCxiWJe01li9fTqtWrejQoQMRke9ytB9KKbFq1SqWL19Ox44d812OpL2Ap2FI2mts2LCBQw891KCsvIkIDj300D3yrxuSGgdXliXtVQzKyje/BrWnVd5y+k5arN4jdahurixLUrXKykqKioo+03veeOMNiouLKSkpYcmSJQ1U2c7NnTuXqVOn1mxPmTKFW265Zbf6mjx5MhUVFTXbI0eO5Jlnnql3jfVx7LHH7rRNhw4deP/997fbP336dF588cWGKEvSfsCVZUl7rQ4jnsxpfztfvfnsJk+ezNChQ/nxj3+8S+1TSqSUOOCA3K5VzJ07l/LycgYNGgTAkCFDGDJkyG71NXnyZAYPHkxhYSEAo0aNylmdu6s+YXf69Om0bNlylwK3JG3LlWVJqmXTpk1ceOGFdOnShaFDh7J+/XoA5syZw/HHH0+vXr047bTTePfdd5k6dSq333479957LwMGDADg1ltvpaioiKKiIm6//XagasW6U6dOfPvb36aoqIhly5YxevRoevfuTffu3bnxxhvrrGX48OGUlpbStWvXrdrMnj2bY489lh49etCnTx9Wr17NyJEjGT9+PMXFxYwfP56HHnqIq6++mtWrV3PkkUeyZcsWANatW0f79u3ZuHEj48aNo3fv3vTo0YOzzz6b9evX8+KLLzJlyhSuvfZaiouLWbJkCcOGDWPixIkAPPvss5SUlNCtWzcuvfRS/vnPfwJVq7o33ngjPXv2pFu3brzxxhvbfZ7TTz+d+fPnA1BSUlITwkeOHMm4ceMAMuelZcuWAGzZsoWrrrqKzp07c8oppzBo0KCa2gDuvPPOrWqorKzkvvvu47bbbqO4uJiZM2fy+OOPU1RURI8ePejfv/9n+vqQtP8xLEtSLYsWLeKqq65i4cKFHHzwwdxzzz1s3LiR733ve0ycOJE5c+Zw6aWXcsMNNzBo0CCuvPJKfvCDH/D8888zZ84cHnzwQV555RVefvllxo0bx2uvvQbA4sWLueqqq1iwYAGLFi1i8eLF/PnPf2bu3LnMmTOHGTNmbFfLzTffTHl5OfPnz+dPf/oT8+fP55NPPuG8887jv/7rv5g3bx7PPPMMLVq0YNSoUZx33nnMnTuX8847r6aP1q1bU1xczJ/+9CcAfv/733PaaafRrFkzzjrrLGbPns28efPo0qUL999/P8ceeyxDhgxh9OjRzJ07l6985Ss1fW3YsIFhw4Yxfvx4/vKXv7Bp0ybuvffemuOHHXYYr776KsOHD2fMmDHbfZ6ysjJmzpzJ6tWradq0KS+88AIAM2fOpH///kybNm2n8/K73/2OyspKKioqeOSRR3jppZe2Or5tDR06dKj5fzR37lzKysoYNWoUTz/9NPPmzWPKlCmf9UtE0n7GsCxJtbRv355+/foBcNFFFzFr1iwWLVrE66+/zimnnEJxcTE/+9nPWL58+XbvnTVrFt/85jdp0aIFLVu25KyzzmLmzJkAHHnkkRxzzDEATJs2jWnTplFSUkLPnj154403WLx48Xb9TZgwgZ49e1JSUsKCBQuoqKhg0aJFHHHEEfTu3RuAgw8+mKZNd3xG3Xnnncf48eMBeOyxx2rC9Ouvv05ZWRndunXj0UcfZcGCBTvsZ9GiRXTs2JGvfvWrAFxyySVbhdmzzjoLgF69elFZWbnd+8vKypgxYwYvvPACp59+OmvXrmX9+vUsXbqUTp067dK8zJo1i3POOYcDDjiAww8/vGZFf1drAOjXrx/Dhg1j3LhxbN68eYefWZI8Z1mSatn2TggRQUqJrl27breK+Vm0aNGi5nVKieuuu45//dd/zWy/dOlSxowZw+zZs2nTpg3Dhg3b7duZDRkyhOuvv54PPviAOXPmcOKJJwIwbNgwJk+eTI8ePXjooYeYPn36bvX/qQMPPBCAJk2asGnTpu2O9+7dm/Lyco466ihOOeUU3n//fcaNG0evXr2AXZuX+tYAcN999/HKK6/w5JNP0qtXL+bMmcOhhx6622NK2re5sixJtbz11ls1ofg3v/kNxx13HJ06dWLlypU1+zdu3FjnKmxZWRmTJ09m/fr1rFu3jkmTJlFWVrZdu9NOO40HHniAtWvXAvD222/z3nvvbdXm448/pkWLFrRu3ZoVK1bw1FNPAdCpUyfeffddZs+eDcCaNWvYtGkTrVq1Ys2aNXV+ppYtW9K7d2++//3vM3jwYJo0aVLz3iOOOIKNGzfy6KOP1rTP6qtTp05UVlby5ptvAvDII49w/PHH72A2t9a8eXPat2/P448/zte+9jXKysoYM2ZMzXnDuzIv/fr144knnmDLli2sWLFilwL+tp9nyZIl9O3bl1GjRtG2bVuWLVu2y59B0v7HsCxJtXTq1Im7776bLl268OGHHzJ8+HCaN2/OxIkT+fd//3d69OhBcXFxnXdn6NmzJ8OGDaNPnz707duXyy+/nJKSku3anXrqqVxwwQV87Wtfo1u3bgwdOnS7cNqjRw9KSkro3LkzF1xwQc2pIc2bN2f8+PF873vfo0ePHpxyyils2LCBAQMGUFFRUXOB37bOO+88fv3rX291PvNPf/pT+vbtS79+/ejcuXPN/vPPP5/Ro0dvdzu8goICHnzwQc455xy6devGAQccwJVXXvmZ5resrIwvfOELfO5zn6OsrIzly5fX/EKxK/Ny9tln065dOwoLC7nooovo2bMnrVu33uGY3/jGN5g0aVLNBX7XXnst3bp1o6ioqOZCSUnKEimlfNdQp9LS0lReXp6XsXd2u6rKggt23slN3kBc+qwWLlxIly5d8l2G9nJr166lZcuWrFq1ij59+vDCCy9w+OGH53SMfH4t+jNI2vMiYk5KqbSuY56zLElqVAYPHsxHH33EJ598wk9+8pOcB2VJqs2wLElqVOp7IaIkfRb1CssRcQgwHugAVALnppQ+3KZNMXAvcDCwGbg5pbT9CXWSJGkXnjTpKRbSnlTfC/xGAM+mlI4Gnq3e3tZ64Nsppa7AQOD2iPh8PceVJEmSGlx9w/IZwK+qX/8KOHPbBimlv6aUFle/fgd4D2hbz3ElSZKkBlffsPwvKaV3q1//L/AvO2ocEX2A5sCSHbWTJEmS9gY7DcsR8UxEvF7HnzNqt0tV96DLvA9dRBwBPAL8n5TSlow2V0REeUSUr1y58jN+FElqOI8//jhdunRhwIABTJ8+vc77LDekyZMnU1FRUbM9cuRInnnmmd3q6/bbb2f9+vU124MGDeKjjz6qd427q7y8nGuuuWaHbSorKykqKqrz2EMPPcQ777zTEKVJ0s4v8EspnZx1LCJWRMQRKaV3q8PwexntDgaeBG5IKb28g7HGAmOh6j7LO6tN0j7uph0/bOKz97f7F0bdf//9jBs3juOOO46bbrqJli1bcuyxx+7y+zdt2kTTprt/TfXkyZMZPHgwhYWFAIwaNWq3+7r99tu56KKLOOiggwCYOnXqbveVC6WlpZSW1nl7013y0EMPUVRUxBe/+MUcViVJVep7GsYU4JLq15cA/71tg4hoDkwCHk4pTazneJLUoM4880x69epF165dGTt2LFAVTGfNmsVll13GOeecw3333cdtt91W80S4lStXcvbZZ9O7d2969+7NCy+8AMBNN93ExRdfTL9+/bj44ou3Gmft2rWcdNJJ9OzZk27duvHf//3/f/t8+OGH6d69Oz169ODiiy/mxRdfZMqUKVx77bUUFxezZMkShg0bxsSJE/nDH/7AOeecU/Pe6dOnM3jwYACGDx9OaWkpXbt25cYbbwTgjjvu4J133mHAgAEMGDAAgA4dOvD+++8DcOutt1JUVERRURG33347ULWq26VLF77zne/QtWtXTj31VP7xj39s9Xk2b95Mx44dSSnx0Ucf0aRJE2bMmAFA//79Wbx4MevWrePSSy+lT58+lJSU1Hzm2jWvXLmSU045ha5du3L55Zdz5JFH1tS2efPm7WqYOHEi5eXlXHjhhRQXF/OPf/yDESNGUFhYSPfu3fnRj35Ur68HSarvfZZvASZExGXA34FzASKiFLgypXR59b7+wKERMaz6fcNSSnPrObYk5dwDDzzAIYccwj/+8Q969+7N2WefzciRI3nuuecYM2YMpaWlNSvLnwaxCy64gB/84Accd9xxvPXWW5x22mksXLgQgIqKCmbNmsXnPve5rcYpKChg0qRJHHzwwbz//vscc8wxDBkyhIqKCn72s5/x4osvcthhh/HBBx9wyCGHMGTIEAYPHszQoUO36ufkk0/miiuuYN26dbRo0YLx48dz/vnnA3DzzTdzyCGHsHnzZk466STmz5/PNddcw6233srzzz/PYYcdtlVfc+bM4cEHH+SVV14hpUTfvn05/vjjadOmDYsXL+a3v/0t48aN49xzz+WJJ57goosuqnlvkyZN6NSpExUVFSxdupSePXsyc+ZM+vbty7Jlyzj66KO5/vrrOfHEE3nggQf46KOP6NOnDyefvPU/Xv7Hf/wHJ554Itdddx1/+MMfuP/++2uOZdVw11131fy/WbVqFZMmTeKNN94gIvJ6eomkfUO9wnJKaRVwUh37y4HLq1//Gvh1fcaRpD3ljjvuYNKkSQAsW7aMxYsXc+ihh+7wPc8888xW5xN//PHHrF27FoAhQ4ZsF5QBUkpcf/31zJgxgwMOOIC3336bFStW8Nxzz3HOOefUBNlDDjlkh2M3bdqUgQMH8j//8z8MHTqUJ598kv/8z/8EYMKECYwdO5ZNmzbx7rvvUlFRQffu3TP7mjVrFt/85jdp0aIFAGeddRYzZ85kyJAhdOzYkeLiYgB69epFZWXldu8vKytjxowZLF26lOuuu45x48Zx/PHH07t3bwCmTZvGlClTGDNmDAAbNmzgrbfe2q6GT+d/4MCBtGnTpubYrtTQunVrCgoKuOyyyxg8eHDNirUk7S6f4CdJ1aZPn84zzzzDSy+9xEEHHcQJJ5zAhg0bdvq+LVu28PLLL1NQULDdsU+D57YeffRRVq5cyZw5c2jWrBkdOnTYpbHqcv7553PXXXdxyCGHUFpaSqtWrVi6dCljxoxh9uzZtGnThmHDhu12/wAHHnhgzesmTZpsdxoGVJ1uce+99/LOO+8watQoRo8ezfTp0ykrKwOqfkF44okn6NSp01bvW7FiRc5qaNq0KX/+85959tlnmThxInfddRfPPffcLvUvSXWp7znLkrTPWL16NW3atOGggw7ijTfe4OWX674euVWrVqxZs6Zm+9RTT+XOO++s2Z47d+dnma1evZovfOELNGvWjOeff56///3vAJx44ok8/vjjrFq1CoAPPvigzjFrO/7443n11VcZN25czSkYH3/8MS1atKB169asWLGCp556KrP+T5WVlTF58mTWr1/PunXrmDRpUk3Q3RV9+vThxRdf5IADDqCgoIDi4mJ+8Ytf0L9/fwBOO+007rzzTqpungSvvfbadn3069ePCRMmAFUr0R9++OF2bbZV+/OsXbuW1atXM2jQIG677TbmzZu3y/VLUl0My5JUbeDAgWzatIkuXbowYsQIjjnmmDrbfeMb32DSpEk1F/jdcccdlJeX0717dwoLC7nvvvt2OtaFF15IeXk53bp14+GHH6Zz584AdO3alRtuuIHjjz+eHj168MMf/hCoWj0ePXo0JSUlLFmy9a3qmzRpwuDBg3nqqadqTjvo0aMHJSUldO7cmQsuuIB+/frVtL/iiisYOHBgzQV+n+rZsyfDhg2jT58+9O3bl8svv5ySkpJdnr8DDzyQ9u3b18xbWVkZa9asoVu3bgD85Cc/YePGjXTv3p2uXbvyk5/8ZLs+brzxRqZNm0ZRURGPP/44hx9+OK1atdrhuMOGDePKK6+kuLiYNWvWMHjwYLp3785xxx3Hrbfeusv1S1Jd4tPf8CqKVUQAAAqbSURBVPc2paWlqby8PC9jdxjx5A6PVxZcsPNO6nGLKml/tXDhQrp06ZLvMpRH//znP2nSpAlNmzblpZdeYvjw4bu0Up9rfi1K+5eImJNSqvMelp6zLEnaa7z11luce+65bNmyhebNmzNu3Lh8lyRpP2dYliTtNY4++ug6z2WWpHzxnGVJkiQpg2FZkiRJymBYliRJkjIYliVJkqQMhmVJqlZZWUlRUVGdxy6//PKtHmm9J73zzjsMHTp0p+1atmxZ5/7JkyfnrXZJauy8G4akvVa3X3XLaX9/ueQvu/3eX/7ylzms5LP54he/yMSJE3f7/ZMnT2bw4MEUFhbmsCpJ2j+4sixJtWzatIkLL7yQLl26MHToUNavXw/ACSecwKcPSho+fDilpaV07dqVG2+8sea9I0aMoLCwkO7du/OjH/1ou767devGRx99REqJQw89lIcffhiAb3/72/zxj39k8+bNXHvttfTu3Zvu3bvzi1/8Ath6xXv9+vWce+65FBYW8s1vfpO+fftS+wFON9xwAz169OCYY45hxYoVvPjii0yZMoVrr72W4uJilixZwh133FFT56ePx5Yk1c2wLEm1LFq0iKuuuoqFCxdy8MEHc88992zX5uabb6a8vJz58+fzpz/9ifnz57Nq1SomTZrEggULmD9/Pj/+8Y+3e1+/fv144YUXWLBgAUcddRQzZ84E4KWXXuLYY4/l/vvvp3Xr1syePZvZs2czbtw4li5dulUf99xzD23atKGiooKf/vSnzJkzp+bYunXrOOaYY5g3bx79+/dn3LhxHHvssQwZMoTRo0czd+5cvvKVr3DLLbfw2muvMX/+/F16NLck7c88DaMOlbecvpMWPspa2le1b9+efv36AXDRRRdxxx13bLdKPGHCBMaOHcumTZt49913qaiooLCwkIKCAi677DIGDx7M4MGDt+u7rKyMGTNmcOSRRzJ8+HDGjh3L22+/TZs2bWjRogXTpk1j/vz5NadcrF69msWLF/PVr361po9Zs2bx/e9/H4CioiK6d+9ec6x58+Y14/bq1Ys//vGPdX7G7t27c+GFF3LmmWdy5pln1mO2JGnf58qyJNUSETvcXrp0KWPGjOHZZ59l/vz5nH766WzYsIGmTZvy5z//maFDh/L73/+egQMHbtd3//79mTlzJjNnzuSEE06gbdu2TJw4kbKyMgBSStx5553MnTuXuXPnsnTpUk499dRdrr1Zs2Y19TZp0oRNmzbV2e7JJ5/ku9/9Lq+++iq9e/fObCdJMixL0lbeeustXnrpJQB+85vfcNxxx211/OOPP6ZFixa0bt2aFStW8NRTTwGwdu1aVq9ezaBBg7jtttuYN2/edn23b9+e999/n8WLF3PUUUdx3HHHMWbMGPr37w/Aaaedxr333svGjRsB+Otf/8q6deu26qNfv35MmDABgIqKCv7yl51ftNiqVSvWrFkDwJYtW1i2bBkDBgzg5z//OatXr2bt2rWfZYokab/iaRiSVEunTp24++67ufTSSyksLGT48OFbHe/RowclJSV07tx5q1M21qxZwxlnnMGGDRtIKXHrrbfW2X/fvn3ZvHkzUHVaxnXXXVcTyC+//HIqKyvp2bMnKSXatm3L5MmTt3r/VVddxSWXXEJhYSGdO3ema9eutG7deoef6fzzz+c73/kOd9xxB4899hiXXXYZq1evJqXENddcw+c///ndmitJ2h9ESinfNdSptLQ01b7CW9K+b+HChXTp0iXfZezVNm/ezMaNGykoKGDJkiWcfPLJLFq0iObNm+e7tH2KX4vS/iUi5qSUSus65sqyJDUi69evZ8CAAWzcuJGUEvfcc49BWZIakGFZkhqRVq1a4b+6SdKe4wV+kiRJUgbDsqS9yt56HYX2H34NSqrNsCxpr1FQUMCqVasMK8qblBKrVq2ioKAg36VI2kt4zrKkvUa7du1Yvnw5K1euzHcp2o8VFBTQrl27fJchaS9hWJa012jWrBkdO3bMdxmSJNXwNAxJkiQpg2FZkiRJymBYliRJkjLstY+7joiVwN/zXUe1w4D3813EPsK5zC3nM3ecy9xyPnPHucwd5zK39qX5PDKl1LauA3ttWN6bRER51vPC9dk4l7nlfOaOc5lbzmfuOJe541zm1v4yn56GIUmSJGUwLEuSJEkZDMu7Zmy+C9iHOJe55XzmjnOZW85n7jiXueNc5tZ+MZ+esyxJkiRlcGVZkiRJymBY3omIGBgRiyLizYgYke96GquIaB8Rz0dERUQsiIjv57umxi4imkTEaxHx+3zX0thFxOcjYmJEvBERCyPia/muqbGKiB9U/x1/PSJ+GxEF+a6pMYmIByLivYh4vda+QyLijxGxuPq/bfJZY2ORMZejq/+ez4+ISRHx+XzW2JjUNZ+1jv1bRKSIOCwftTU0w/IOREQT4G7g60Ah8K2IKMxvVY3WJuDfUkqFwDHAd53Levs+sDDfRewj/gv4Q0qpM9AD53W3RMSXgGuA0pRSEdAEOD+/VTU6DwEDt9k3Ang2pXQ08Gz1tnbuIbafyz8CRSml7sBfgev2dFGN2ENsP59ERHvgVOCtPV3QnmJY3rE+wJsppb+llD4BHgPOyHNNjVJK6d2U0qvVr9dQFUa+lN+qGq+IaAecDvwy37U0dhHRGugP3A+QUvokpfRRfqtq1JoCn4uIpsBBwDt5rqdRSSnNAD7YZvcZwK+qX/8KOHOPFtVI1TWXKaVpKaVN1ZsvA+32eGGNVMbXJsBtwP8D7LMXwRmWd+xLwLJa28sx4NVbRHQASoBX8ltJo3Y7Vd+ctuS7kH1AR2Al8GD1aS2/jIgW+S6qMUopvQ2MoWqF6V1gdUppWn6r2if8S0rp3erX/wv8Sz6L2YdcCjyV7yIas4g4A3g7pTQv37U0JMOy9qiIaAk8AfzfKaWP811PYxQRg4H3Ukpz8l3LPqIp0BO4N6VUAqzDf+beLdXn0p5B1S8gXwRaRMRF+a1q35KqbmG1z67g7SkRcQNVpwc+mu9aGquIOAi4HhiZ71oammF5x94G2tfable9T7shIppRFZQfTSn9Lt/1NGL9gCERUUnVqUEnRsSv81tSo7YcWJ5S+vRfOiZSFZ712Z0MLE0prUwpbQR+Bxyb55r2BSsi4giA6v++l+d6GrWIGAYMBi5M3j+3Pr5C1S/G86p/HrUDXo2Iw/NaVQMwLO/YbODoiOgYEc2pulBlSp5rapQiIqg6J3RhSunWfNfTmKWUrksptUspdaDqa/K5lJKrd7sppfS/wLKI6FS96ySgIo8lNWZvAcdExEHVf+dPwoslc2EKcEn160uA/85jLY1aRAyk6hS2ISml9fmupzFLKf0lpfSFlFKH6p9Hy4Ge1d9T9ymG5R2ovgjgauBpqr7hT0gpLchvVY1WP+BiqlZB51b/GZTvoqRq3wMejYj5QDHw/+a5nkapenV+IvAq8BeqfsbsF0/4ypWI+C3wEtApIpZHxGXALcApEbGYqtX7W/JZY2ORMZd3Aa2AP1b/HLovr0U2IhnzuV/wCX6SJElSBleWJUmSpAyGZUmSJCmDYVmSJEnKYFiWJEmSMhiWJUmSpAyGZUmSJCmDYVmSJEnKYFiWJEmSMvx/xGtwlvvsycoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtEAAAFlCAYAAAAterT5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3xfVf3H8df9juzV7DZJm4500z1oSwd7iCAbFQFZIiqIAxFEERURBzjABQWUHzIUFZDV0g2UDrr3Stqk2Xt/1/n9cb8ZTQctTfrNeD8f3sfd935StbxzOPccyxiDiIiIiIgcP0eoCxARERER6WkUokVERERETpBCtIiIiIjICVKIFhERERE5QQrRIiIiIiInSCFaREREROQEuUJdwKeRnJxssrOzQ12GiIiIiPRya9euLTPGpHQ83iNDdHZ2NmvWrAl1GSIiIiLSy1mWlXek4+rOISIiIiJyghSiRUREREROkEK0iIiIiMgJUogWERERETlBCtEiIiIiIidIIVpERERE5AQpRIuIiIiInCCFaBERERGRE6QQLSIiIiJyghSiRUREREROkEK0iIiIiMgJcoW6ABERETl1jDFgDAQCYMzh+wEDBI/ZN7Stg9um5ViH4y3brecNhz+r3XbbY8ynfs6htRzykx5ef7v1oc9ud/7wP7EjHDrOZx7r3mMda7dvDjt3lDKP5z3HU08o330MVngE0dOnHff1p4JCtIiIhITx+zE+H8brxXi94PPZ+z4fxuvD+ILHW67z+exr/H77vN9nn2vZ9vkw/gAE/Bif31637PsDwesDmIAf2h0/6trvx5iAfa0J2OEyELy/ZbvlfMt26zX2Mfu4aXtWoCWsBp9nzOH3tmy3nMMcsn9I8G29hrb9dos5wjGRnsidlcWwBe+GuoxDKESLiPRyJhDANDURaG7GNDcfsh1oasI0ezCelv3mtu1mD6apCeNpJuDxtIVdbzD4tgvAh253uMbXtk2760MW6CwLnE4sh+OT1w4HOB1YDic4HFgOC6zgecuyzzsc9rbTCQ4Ly3JguVxYTod9rcNhH3c4wbLajltW6/Wt17Qed4CFXcMh11pt+5Zl14PVum8vHHYdlv1ztx53tNzvsO9v/z4OfZb9R2a1/dnRfptDnt96bcfrjnietne1u+6Qdx3hOYefp+2a9nW1P9z+3R1rb39hu1sP+9/MYYc+6VlHf6Z1hOcd9o5D9o917hg6492HXd/F7z7a48LCju+9p5BCtIhICBhj7KDa0ECgoRHT1Eig0V5MYyOBxiZ7u6llu8EOv03NbcE2uB3wNGOamgk0N2GaWgJw8LrmZjv4ngQrLMxe3O62xeXCCnOD69BjjoiIdte5wOUKngve0/64y2Ufb3le6/G2fcvttgNryzGXs92+E8vlAmfwWofDvr/9un0odjrbwq+IyElSiBYROQ7G5yNQX0+grg5/fb29Xd9AoK4uuF1PoKG+bbu+vjUgBxoa7IDc0GCH4Xp7/4RbYt1uO6SGh+MID8eKiMAKD8MRHoEVEY47Pt4+FxGOFTzmCLe3HRHhWGHh9rGICPt8eJi9HdZyT3AJC2t9j+V2B1snRUSkPYVoEenVjDGYpib8NbUEamvw19YSqK1t26+pJVBXax+vq28NyoH6evz1da1B2TQ1Hd8L3W6c0dE4oqJwREdjRUXiiIrC3a8fjkh72xEViRUVhSMyqu1YZARWZKS9HxmJFRGJIzLC3o6MtEOtS39li4h0F/obWUS6JRMI2C23tbV2629t3VHDbqChHn9ri3DDoUG4rg4+oTuD5XbjiI3FERODIzoaZ3Q0rpQUwgYPxhEdHTwehTN4vnXpuB8djaMb9tsTEZHOpxB9nIzfT9P27USOGRPqUkR6BOP1BkNsfWuLr7+mmkBNLf7aGgI1HY/V2sfq6uzgXF9/XN0dWlp82wdb94ABwf0onLFxOOJiccbG4oiNxRkXZ2+3WzvCw0/Bn4iIiPQmCtHHqerllyl66Cf0+/y1pNx9N87Y2FCXJNJlAs3NdqBtCbbtukDYAbgWf12wRbiu7tDuD8Fjprn52C+xrNZQa4fcOMKyB+GIicURG2O3+nbYdsbGHNJi7IiKUn9dEREJCYXo4xT32Uto3rePyr8/T+3C90j7wf3EnnuuvvKWbsUYE+zDGwy97btA1NYRqK9r6w5RW2u3+gZbfv31dfY1tbWfPJqDy2UH29jYYOtvu+4PLQE3Jrqt+0NsLM64eJxxsTha1jExCsAiItJjWYfNRtMDTJkyxaxZsyYk727ctInCB35I8/btxJx1FukP/AB3//4hqUV6h9a+vy2Btq7O7gLREBzdob7DuuOxYPBtCcQEAsd+ocNhh9pguHXEtLT0xrS1+sbFB1t944KBN9ZeB/etiAj9AikiIn2CZVlrjTFTDjuuEH3ijM9HxXN/o/T3v8dyOEj55l30++IX7TFIpVezA28jgYZ6TEugbRm+LDhsWaCxwR7nt+N+QwOB+oZDujy0dIM4Lg5HW//f9uvY2LZW4dgYnC1dIGJjO3SBsLetyEgFYBERkeOkEN0FPPn5FP34IeqXLyfitNPo/9CPiRg1KtRl9XnG6w2G1yZ7ooqmJjvENjW1m8ii0Q7DLSG3oW2ii0P22wflBjsMnwgrIiI4fFkkjqhIHFHBER1iYtp1d+iw337Eh3ZhWa2/IiIip55CdBcxxlDz5psUP/xz/FVVJN54Aylf+xqOqKhQlxZyxhiMx2MvLVMMt9tvnW64OTgFccuMay3Hmpo6zMLWNltboHXq4iZMY7v9pibw+U6sUMs6ZHxeR2SkPbZvZLvW3o5LdNu21Tr2b/sxf+2xfdXnV0REpGc7WojutA8LLcu6APgt4ASeMsY80uF8OPA3YDJQDlxjjMkNnvs+cDPgB+40xrzTWXV1NcuyiP/MZ4iZNYuSX/+aiqfnU/v2O8TMnYszKRFXUnLr2pWchDMxCUd0VKe0KBq/H+PzYbw+8HntbZ8P4/VivC1rL8brsT8U87U/Flw8nkPWgQ77dghud22HJeD1tLvWa4flltB8klMNY1l2S25wZjZHeLgdWFv2k5Nwt87KFmFPYBGcoMKKiMDRut3+WMvkFfaEF/akFmrhFRERkRPTKS3RlmU5gZ3AuUA+sBr4vDFma7tr7gDGGWNutyzrWuAyY8w1lmWNBv4BTAMGAAuB4cYY/9HeF4qW6OKaJp5YvJt7LxxJVNjRf/doWL2a4l/9Ck9uHoHq6iNeY0VE4EpMxJmcjCMyEuPz2QHX78f4/fZ2cL/1ePAY3raw/IkfkH1aloUVFmYvbre9tOyHhWGFuXG4wzocCy7hYTjCwuzphVumDw5vOR/eel37KYYPmca4JSCHhYHbrXArIiIiIdXVLdHTgN3GmL3Bl70IXApsbXfNpcCDwe1/An+w7IR0KfCiMaYZ2GdZ1u7g8z7spNo6xfu7y/j7yjxW7avgz1+azKCk6CNeFzV1KoNfegkA4/Hgq6zEV1aGv7wcX3kF/vIyfOUV+MrL8JeVE/A02yE1IgJcTiyny/5A0eXEcrnbtp0uLFdwcbvA5bLvc7kPPeZyYbnbhV+3G8vt6rAfvKclJHdca2phERERkWPqrLSUARxot58PTD/aNcYYn2VZ1UBS8PjKDvdmdHyBZVm3AbcBDBw4sJPKPn6XT8okKSacO/+xjs/+fgW/vXYiZ45MPeY9VlgY7rQ03Glpp6hKERERETkVesxXT8aYvxhjphhjpqSkpISkhrnDU3jjG2eQ2S+Km55bzeMLdxII9LwPM0VERETk5HRWiC4AstrtZwaPHfEay7JcQDz2B4bHc2+3kZUYxb++OpPLJmTw+MJd3Pq3NVQ3nuQHdCIiIiLSo3RWiF4N5FiWNdiyrDDgWuC1Dte8BtwQ3L4SWGTsrxpfA661LCvcsqzBQA6wqpPq6hKRYU5+ffV4Hrp0DEt3lnLJH1awvagm1GWJiIiIyCnSKSHaGOMDvg68A2wDXjbGbLEs6yHLsi4JXvY0kBT8cPBbwL3Be7cAL2N/hPg28LVjjczRXViWxfUzsnnxttNp9Pi57IkP+O/6btuALiIiIiKdSJOtdIKSmia+9sLHrM6t5OYzBnPvhSNxO3tMd3MREREROYqjDXGnpNcJUuMieOHW07lxZjZPr9jHF5/6iA/3lFPffIIz54mIiIhIj6ABgTuJ2+ngwUvGMD4rnu+/uonP/3UlDguGp8UycWACE7ISmJDVj2GpMTgdmkBEREREpCdTd44uUNXgYd3+KtYdqGL9gSrW76+kpslulY4JdzEuMz4YqhMYPSCO/vGRCtYiIiIi3VBXz1jY69V763ljzxtcOfxKnA5n2wljoKkKagqh1l4S6oo50x3Nmf1TICeNQFQ2ec3RfFxsWJ9fzboDlfxl2V58wTGmw5wOMvtFMjApioGJ9jIoKZpBSVFk9YsiMsx5lKpEREREJBQUoo/T6xue5mdb/sqbHz/JT50ZZNVXQu1BqC0CX9Mx73UAg4HBzjCuiE6FmBT8o1OpsOIpNMnsIYPNzWmsqUlibW4ltR36UqfGhpOdFM2wtBiGp8aQkxZLTloMKTHh2DOni4iIiMippBB9nK5xpxNZWs4jSQGusCr4rtWPKwdMxorrD3EDIDYdYoPrmDTwNkJdMdSXQF1pcF3cuu2sO0hK3QZS6ooZh+EyAMuBScrG228YlVGDOejKYndgABua49leafjfxkJeaDexS3ykm+FpMQxLjWV4WgzD02LJSY0hJVbhWkRERKQrqU/08fI0gN9Doa+BBz78IR8VfsQZGWfw45k/JjUq9eSeW74bynZC6Q57XbbTPub3tF0Xk47pP46GxDHsD89hc2AQ62ri2FVSx87iukNmTUyKDmP0gDhG949jVP84Rg+IY0hyNC4NuyciIiJyQo7WJ1oh+lMImAAvbn+Rx9Y+RpgzjB+c/gMuHHxh577E74OqvLZwXbINijZC6XYwAfuaiAToPx7Tfzw1CaPY4xzGhsYkthfVs7Wwhh1FtXj89rXhLgcj0mMZHQzVLQE7Olz/MkJERETkaBSiu0BudS73v38/G0s3cn72+fxg+g9IiEjo2pd6G6F4KxSuh8IN9lKyta3VOiwGBkyEzKn4BkwmN3IMm6rcbD1Yw9bCGrYcrKGqwW61tiwYlhLDuMwExmfFMy4zgVH9Ywl36UNGEREREVCI7jK+gI9nNj/DkxueJCE8gR/P/DFzMuec2iL8XruFunADHFwPBWugaBMEgh8oJgyCrGmQORWTMYWiqGFsKWpiU0E1mwqq2ZhfRVmdHcLdTouR6XGMy4xnfGYC47LiyUmN1RB8IiIi0icpRHexHRU7+P6K77OrchdX5FzBHRPuOLm+0ifL22iH6vzVcGCVva4ttM+5IqD/BDtYD5qFyZrOQU8EGw9UsSHfDtWb8qtbRwmJCnMyLjOeyYP6MWlgPyYO7EdidFjofjYRERGRU0Qh+hTw+D08uf5JntnyDAETYGzSWOZlzePMgWeSk5AT+hEzqgvsMN2yHFwX7AZiQdpYGDSzdQlEpbCvvJ5N+dWsP1DFx/sr2XKwBn9wbOshydFMHNiPSYMSmDyon1qrRUREpFdSiD6FcqtzWbh/IYsPLGZj6UYAMmIy7ECddSaT0ibhdrhDXCV2a3XBWsj7APLet1usvQ32uaScYKCeBdmzID6TRo+fjflVfLy/irV5lazbX0l5vd0NJCbcxYSsBKZmJzJ9SCITshKIcKtvtYiIiPRsCtEhUtZYxtIDS1l8YDErC1fS7G8mNiyW2RmzOTPrTGZlzCI2LDbUZdr8XrsLSN77wWD9ITRX2+cSh8DguTBkLmTPgegkjDHsr2hgbV4lH++vZE1uJTuKazHGnoVxfFY80wcnMW1wIpMH9dNIICIiItLjKER3Aw3eBj4s/JDF+xezLH8Zlc2VuCwXk9MmMztzNnMz55Idnx3qMtsE/PbIH/uWw76lkPs+eGoBC9LHBkP1mTBoBoRFA1Dd4GVNXgUf7bOXzQXV+AMGp8NibEY80wcnMi07kWlDEomL6Aat8SIiIiLHoBDdzfgDfjaUbmBp/lKW5S9jd9VuAAbGDmRO5hzmZM5hStoU3M5uFDT9Xrsf9d6ldqg+8JHdp9rhhsypdiv10LMhYxI47K4c9c0+1uZVsmpfBav2VbD+QBUefwCnw2JCVgJzclKYMzyZcZkJ6lMtIiIi3Y5CdDdXUFfAsvxlLMtfxqrCVXgCHqJcUcwcMJM5mXOYnTmb5MjkUJd5KE8D7P/QDtR7l9pdQTAQmQhDz4Kcc+1QHZPSekuT18+6/VW8v7uMZbtK2VRQjTGQEOVm1rBk5uakMHt4Mv3jI0P3c4mIiIgEKUT3IA3eBlYVrWJZ/jKW5i+lpKEEC4vp/afz2aGf5eyBZxPtjg51mYdrqIA9i2DXAti9EBrKAMue/CXnXBh27iGt1AAV9R6W7ypl2c4ylu8qpaS2GYCc1BjmDE9h7vAUpg1O1EeKIiIiEhIK0T2UMYadlTtZkLeA/+39H/l1+UQ4Izhz4Jl8dshnmTFgBi5HN/xgLxCwZ1XcvdAO1QVr7OnKW1qpR1wIOedBRFzrLcYYdhTXsmynHapX5Vbg8QWIdDuZNSyZM0emcNbIVLVSi4iIyCmjEN0LGGPYULqB1/e8ztu5b1PjqSExIpELB1/IxUMuZkzSmNCPRX00La3UuxfaS30pOMPsQD3qEjtURyUeckujx8+He8tYvL2URdtLKKhqBGBkeixnjUzlrJGpTMhKwOV0hOInEhERkT5AIbqX8fq9LC9Yzht732DJgSV4A16y47K5eMjFXDrsUtKj00Nd4tEFApC/Cra+Btteg+oD4HBB9mwYfQmMvBhiDp3t0RjDrpI6Fm0vYfH2EtbkVeIPGBKi3MzJsVuozxyZSnxkN/oQU0RERHo8heherMZTw4LcBby+93XWFq/FYTmYmzmXq0dczcwBM3FY3bil1hh7xI9tr9mhumIPYNkTvYy6BEZ9FuIzDrututHL8l2lLN5eypIdJZTXe3A7LWYOTeai09I5d3S6piYXERGRk6YQ3Ufk1+bzr13/4tVdr1LRVEFGTAZXDr+Szw37XPcb3aMjY+xxqVtaqEu22scHzoAxl8PoSyE27bDbAgHD+vwq3tlcxJubCzlQ0YjTYTF9cCIXjk3n/DHppMZFnOIfRkRERHoDheg+xuv38t7+93h558usLlqNy+HinIHncPWIq5mSNqX79p1ur2w3bP03bP43lGwBywHZZ8DYK+xW6g59qMHu9rHlYA1vby7irc2F7Cmtx7Jg8sB+XHhafy4Ym05Ggj5MFBERkeOjEN2H7a3eyys7XuG/e/5LraeWwfGDuWr4VVwy9BLiw+NDXd7xKdkGm1+Fzf+yu3w4XPZsiWOvgJEXQcSRf45dxbW8tbmItzYXsa2wBoCJAxO4dPwAPjNuACmx4afypxAREZEeRiFaaPI18U7uO7y882U2lm4k0hXJZcMu47rR15EVmxXq8o6PMVC00Q7Tm/8N1fvtUT5yzoPx10LO+eA6cl/o3LJ63tpcxGsbDrKtsAanw2LWsGQuHT+A88akEatpyEVERKQDhWg5xPaK7fx96995c9+bBEyAsweezfWjr2dC6oRQl3b8jIH8NbAl2EJdVwxRSTDuWpj4RUgbc9RbdxbX8tr6g/x3QwEHKhoJdzk4Z1Qal04YwNwRKYS7NLmLiIiIKETLUZQ0lPDCthd4eefL1HpqGZ8ynhvG3MBZWWfhdPSgIOn32eNQr/s77HgLAl7oPwEmXgenXQmR/Y54mzGGj/dX8dr6At7YWEh5vYe4CBcXndafz03MYPrgxJ7Rf1xERES6hEK0HFODt4F/7/43z299nvy6fDJjMrlu9HVcNuwyotxRoS7vxNSXw6aXYd3zULwZnOEw6mKY8EUYMu+Qacfb8/oDvL+7jNfWH+SdLUXUe/wMSori6ilZXDEpk/R4jfAhIiLS1yhEy3HxB/wsOrCI57Y8x4bSDcSGxXLNiGu4btR1JEUmhbq8E2MMFG6A9f8HG1+GpiqIy4CJX4LJN0DcgKPe2ujx8/aWQl5afYCVeytwWDBvRCpXT8ni7FGpuDVLooiISJ+gEC0nbH3Jev629W8szFtIuDOcK4dfyY1jbiQt+vCxmrs9bxPseNNund6zyB4ub+RnYOotMHgOHKPLRm5ZPS+vOcA/1+ZTUttMckwYl0/K5OopWQxLjTmFP4SIiIicagrR8qntq97HU5ue4n97/4fDcnDZsMu4+bSbGRBz9Jbcbq1iL6x5xg7UjRWQlANTb4bxn4fIhKPe5vMHWLqzlJdWH+C97SX4A4bJg/pxzZQsLhrXn5hw1yn8IURERORUUIiWk5Zfm8/Tm5/mP7v/AwYuHnoxt5x2C4PiBoW6tE/H2wRb/wOrn4L81eCKhHFXwZSbYcCxRykpqW3i1Y8LeHn1AfaW1RPhdnDe6HQum5jB7JxkXOruISIi0isoREunKaov4tktz/LPnf/EG/ByQfYF3HrarQzrNyzUpX16hRtg9dOw6RXwNkDGFJh2qz3d+FHGnYaW0T0q+fc6e3SPqgYvyTFhXDxuAJdPyuC0jHiN7iEiItKDKURLpytrLOO5Lc/x0o6XaPQ1cu6gc7l9/O0M7zc81KV9eo1VsOFFu3W6fBfEDoDTb4fJNx51VsQWHl+AJTtK+Pe6At7bVoLHH2BISjSXT8zg0gkZZCX2sFFOREREpGtCtGVZicBLQDaQC1xtjKnscM0E4I9AHOAHfmaMeSl47llgLlAdvPxGY8z6T3qvQnT3UtlUyfPbnueFbS9Q763nM0M+wx0T7ug5syAeiTGw+z344HewbymExdojeky/HRI++eeqbvTy1qZCXl1XwKp9FQBMy07k2mlZXDxuAGEudfcQERHpCboqRD8KVBhjHrEs616gnzHmex2uGQ4YY8wuy7IGAGuBUcaYqmCIfsMY888Tea9CdPdU1VTF/M3zeWH7C/iNn6uGX8Vt424jOTI51KWdnMIN8MEf7FkRAcZeATO/Dv3HH9ft+ZUN/Hf9Qf71cT57S+tJiwvnhpnZfHHaIOKjNNW4iIhId9ZVIXoHMM8YU2hZVn9giTFmxCfcswG4Mhiqn0Uhutcpri/mTxv/xL93/ZswZxjXjbqOG8feSFxYXKhLOzlVB+CjP8Ha58BTC4Pnwsw7YdjZxxwir4UxhqU7S3lq+T5W7C4jKszJ1VOyuGnWYAYmqauHiIhId9RVIbrKGJMQ3LaAypb9o1w/DXgOGGOMCQRD9AygGXgPuNcY03yUe28DbgMYOHDg5Ly8vE9dt5waeTV5PLHuCd7KfYu4sDhuPu1mPj/y80S6IkNd2slpqoa1z8LKP0HtQUgdDdO/AqddBWHRx/WIrQdreHrFPl7bUIA/YDh/TDq3zB7C5EFHnp5cREREQuNTh2jLshYC6Uc4dT/wXPvQbFlWpTHmiCmgpaUauMEYs7LdsSIgDPgLsMcY89An/TBqie5ZtpVv43frfseKghWkRqbylfFf4bKcy3A7enhXBp8Htrxqd/Uo3mR/eDjxS/YELomDj+sRxTVNPPdBLv/30X6qG71MHJjArbOHcP6YdJwOjeohIiISaiHtzmFZVhx2gH74aF03LMuaB3zHGHPxJ71XIbpnWlO0ht9+/FvWl65ncPxg7p9+P9P7Tw91WSfPGNi/Elb9Bba9BgE/5JwH02+DIWeB45M/Imzw+HhlTT5Pr9jH/ooGMhIi+fy0LK6ekkVqXMQp+CFERETkSLoqRP8SKG/3YWGiMeaeDteEAW8BrxtjHu9wrn8wgFvAY0CTMebeT3qvQnTPZYxhyYElPLr6UfLr8rkg+wK+M+U7PXMq8SOpOWh39VjzDNSXQOJQe7zpCV/4xCHyAPwBw4KtRfx9ZR7v7y7H6bA4Z1QqX5g+iNnDknGodVpEROSU6qoQnQS8DAwE8rCHuKuwLGsKcLsx5hbLsq4DngG2tLv1RmPMesuyFgEpgAWsD95T90nvVYju+Zr9zczfPJ+nNz2N03Ly1fFf5Yujv9jzu3i08Hlg639h1Z/t2RDd0TD+WpjxNUgaelyP2FdWz4ur9vPK2nwq6j1k9ovk89MGctWUTFJj1TotIiJyKmiyFemWDtQe4NFVj7IkfwlD44dy3/T7mNZ/WqjL6lwFH8Oqv9pD5AV8MPE6mPs9iM84rtubfX7e3VLMCx/t58O95bgcFueMSuML0wdyhlqnRUREupRCtHRrSw4s4ZFVj1BQV8CF2Rfy7Snf7j1dPFrUFsPyX8Oa+WA57G4eZ9wN0cc/jvbe0jr+sWo//1ybT2WDl4GJUdw6ZwhXTc4kwu3swuJFRET6JoVo6faafE2tXTxcDhd3TLiDL4z6Qu/p4tGiMg+W/gI2/APcUTDj63Y3j4jjH0e7yevnnS1FPPN+LusPVJEaG85tc4bwhekDiQpzdWHxIiIifYtCtPQYB2oO8MjqR1iWv4xhCcP44YwfMjF1YqjL6nylO2DRT+0RPSITYfa37OHx3Mc/jrYxhg/2lPP7RbtYubeCxOgwbj5jMNfPGERsRC/75UNERCQEFKKlx1m8fzE/X/VzCusLuWbENdw16S5iw2JDXVbnK/gYFv0E9iyC2AEw9x6737TzxELwmtwK/rB4N0t2lBIX4eLGWYP58sxs+kWHdVHhIiIivZ9CtPRIDd4Gfr/u97yw/QWSI5K5b/p9nD3o7FCX1TX2LYf3HoL8VdAv255SfMIXwX1iI3Fsyq/mD4t38c6WYqLDnFw3YxC3nDGElNjwrqlbRESkF1OIlh5tc9lmHvzgQXZU7uCsrLO4b/p9ve/DQ7Anbtn5Dix7FArWQnQqzLgDptx0XONMt7ejqJYnFu/mjY0HcTsdXDs1i1tmDyErMaqLihcREel9FKKlx/MGvPx96995cv2TuBwuvjnpm1w94moc1ifPCNjjGAO5y2H5b2DvYgiPs/tLn/5ViEk9oUftLa3jj0v28J/1BQQMXDJ+AF+ZO4SR6cf/IaOIiEhfpRAtvcaBmgM8tPIhVhauZHzKeB6c8SDD+g0LdVld5+A6WPG4PXmLM8zuLz3rTrvLxwkorP8Da9oAACAASURBVG7k6eX7eGHVfho8fs4emcrt84YyNTuxa+oWERHpBRSipVcxxvDG3jd4dPWj1HnruGnsTXxl3FcIc/bij+jKdsMHv4X1/wATgLGXw6xvQvrYE3pMVYOHv32YxzPv76OywcuUQf346ryhnDUyFcvSxC0iIiLtKURLr1TZVMmv1vyK1/a8xoh+I3hk9iO9u1UaoOYgfPgErH0WPHUw7ho4+0fHPQNiiwaPj5dXH+Cvy/dRUNXIiLRYbp83hIvHDcDt7IVdZERERD4FhWjp1ZblL+OB9x+g3lvP3ZPv5gsjv9D7W1UbKuCD39uB2nLYXTxm3QVh0Sf0GK8/wOsbDvKnpXvYWVxHZr9I7jwrh8smZShMi4hIn6cQLb1eWWMZP/rgRyzLX8asAbP4yayfkBKVEuqyul5lHix8ELa8CrH94ewfwrhrwXFiATgQMCzaXsLvFu1iY341g5Ki+MZZOXxuwgBcCtMiItJHKURLn2CM4ZWdr/DL1b8kwhXBgzMf5OyBvXRc6Y72r4S3vw8HP4YBE+H8h2HQzBN+jDGG97aV8NjCnWw5WMPg5GjuPHsYl4zPwOno5a37IiIiHShES5+yt3ov9y67l20V27gi5wrumXoPUe4+MD5yIACbXoH3fgw1BTD6Ujj3oRMeyQPsMP3u1mIeW7CT7UW1DE2J5s6zc7h43ACFaRER6TMUoqXP8fq9PLnhSZ7e9DRZsVk8MvsRTks5LdRlnRqeBru/9PuPQ8Bnjy89+9snPGEL2N083tlSxGMLd7KzuI6c1BjuOieHi8b2x6EwLSIivZxCtPRZa4rWcN+K+yhpKOH28bdzy2m34HK4Ql3WqVFzEN77CWx4AaKSYO73YPKXwXXiQwEGAoY3Nxfy+MJd7C6pY0RaLLfOGcJnx/cn3OXsguJFRERCTyFa+rQaTw0/W/kz3tz3JpPTJvPLOb/sGx8dtji4HhY8APuWQeIQe0i80ZfCpxjBxB8wvLHxIE8s3s3O4jpSYsO5/vRBfPH0QSRG9+JxukVEpE9SiBYBXt/zOj9Z+ROiXFE8OudRpvWfFuqSTh1jYNcCWPBDKN0GmVPhvJ/CwNM/5eMMy3eV8dSKfSzbWUq4y8HlkzK5+YxshqXGdnLxIiIioaEQLRK0p2oPdy+5m7yaPL4x8RvcNPYmHFYfGsIt4If1/weLfgZ1RTDyYjjnQUjO+dSP3Flcy/wV+3h1XQEeX4B5I1K4+YzBnDEsufeP1y0iIr2aQrRIOw3eBh784EHeyn2LOZlzePiMh4kPP/GP7no0Tz18+KT98aG3ESbfCPPuhZjUT/3Isrpm/m/lfv6+MpeyOg8j02O5adZgLp04QP2mRUSkR1KIFunAGMNLO17iF6t/QWpkKr+Z9xvGJI8JdVmnXl0JLP0FrHkG3JFwxjdhxjfAHfGpH9ns8/Pa+oM8vWIf24tqyUiI5FvnDudzEzXWtIiI9CwK0SJHsblsM99a8i3KGsv43tTvcfWIq/tmF4SyXfbMh9vfsMeVvuAXMOKCk3qkMYalO0v59bs72VRQzfC0GL593gjOG53WN/+MRUSkx1GIFjmGqqYq7ltxH8sLlnPR4Iv40Ywf9Y3JWY5kz2J46x4o2wk558EFj0DS0JN6pDGGtzYX8at3drC3rJ4JWQncc8EIZg5N7qSiRUREuoZCtMgnCJgA8zfP5/frfk92XDa/mfcbhiacXHjssXweWPVnWPIL8DfDzG/Yk7WERZ/cY/0B/vVxPo8v3EVhdROzc5K55/yRnJbZx/qji4hIj6EQLXKcPir8iHuW3UOTr4lH5zzK3Ky5oS4pdGqLYMGPYOOLEJcB5/8MRn/uU40v3V6T18/zK/N4YvFuKhu8fOa0/nzrvOEMTYnppMJFREQ6h0K0yAkori/mzsV3sq18G9+e8m2uH3193+7Du38lvPkdKNoE2bPhol9C6qiTfmxtk5e/Lt/HU8v30uwLcPnEDL46byhDFKZFRKSbUIgWOUGNvkbuX3E/C/IWcEXOFdw//X7cTneoywqdgB/WPmNPI95cC9O/AnO+C1GJJ/3osrpmnli8mxc+2o/HH+DCsencMW8YYzPUzUNEREJLIVrkUwiYAH9Y9wf+uumvTEufxm/m/abvjSfdUX05LHoI1j4H4bEw42tw+h0QEXfSjy6ra+aZ9/fxtw/yqG32MWd4CnfMG8r0wYl9+98EiIhIyChEi5yE1/e8zo8++BEDYgbwh7P+QHZ8dqhLCr2SbbD4Z7DtdYhMtMeXnnorhJ38qCY1TV6eX5nH/BX7KKvzMGlgAnfMG8bZo1IVpkVE5JRSiBY5SetK1vHNxd/EG/Dym3m/4fT+p4e6pO7h4DpY9FPYvRBi0mD2d2DyDeAKP+lHN3n9vLLmAH9aupeCqkZGpMVyx5lD+cxp/XE5+9BU7SIiEjIK0SKdoKCugK+/93Vyq3O57/T7uGr4VaEuqfvI+8DuL73/A4jPgrnfg/GfB6frpB/t9Qd4fcNB/rhkD7tK6shKjOSGGdlcNSWL+Mg+3E9dRES6nEK0SCep89Tx3WXfZUXBCq4bdR3fmfIdnA5nqMvqHoyBPe/ZLdMH10HSMJj3fRhzOThOvuU4EDAs3FbMX5fvZXVuJZFuJ5dPyuDGmdnkpMV2wg8gIiJyKIVokU7kC/j49Zpf8/y255mdMZtfzPkFsWEKca2Mge3/s/tMl2yFzKlw8WOQflqnvWJzQTV/+zCX/6w/iMcXYObQJG6cmc3Zo9JwOtRvWkREOodCtEgXeHnHy/z8o5+TGZvJY/MeY1i/YaEuqXsJ+GHDi7DgAWisgtO/ardMh3feONAV9R5eXL2f5z/M42B1E5n9IvnS6YO4ZmoWCVFhnfYeERHpmxSiRbrI2uK1fHvJt2nwNfDQzIe4YPAFoS6p+2mogIU/go//BnGZ9mQtIy/q1Ff4/AEWbC3m2Q9y+WhfBRFuB5+bkMFtc4Zo8hYREfnUuixEW5aVCLwEZAO5wNXGmMojXOcHNgV39xtjLgkeHwy8CCQBa4EvGWM8x3qnQrR0NyUNJXx7ybdZX7qeL43+EndPvhu3Qx+8HSbvQ3jjbijdBiM+Axc9CvGZnf6abYU1/O3DXP69rgCPL8BlEzO58+xhDEqK7vR3iYhI79aVIfpRoMIY84hlWfcC/Ywx3zvCdXXGmMOagyzLehl41RjzomVZfwI2GGP+eKx3KkRLd+T1e/nlml/yj+3/YEraFH4595ckRyaHuqzux+eBlU/Akl+A5YAz74Ppt3fKKB4dldU186cle/j7yjz8AcOVkzP5+lnDyOx38mNZi4hI39CVIXoHMM8YU2hZVn9giTFmxBGuOyxEW/asCaVAujHGZ1nWDOBBY8z5x3qnQrR0Z6/veZ2HPnyIuLA4fnPmbxifMj7UJXVPlbnw5ndh17uQdhp89nHIPOzvqE5RXNPEH5fs4YWP9mMwXDM1i6+fmUN6fESXvE9ERHqPrgzRVcaYhOC2BVS27He4zgesB3zAI8aY/1iWlQysNMYMC16TBbxljBl7hPtvA24DGDhw4OS8vLyTqlukK+2o2ME3F3+TooYi7p16L1ePuFoz7R2JMbD1v/D2vVBbZE/Scub9EJPaJa87WNXIE4t38/KaA1iWxRemDeSOM4eSGqswLSIiR3ZSIdqyrIVA+hFO3Q881z40W5ZVaYzpd4RnZBhjCizLGgIsAs4GqjnOEN2eWqKlJ6hurub7y7/P8oLlXDL0Eh44/QEiXAprR9RUA4sfhlV/AXekPYX46V/rlCnEj+RARQN/WLSbf36cj9tpcf2MbG6bM4TkmJOfZVFERHqXkHfn6HDPs8AbwL9Qdw7pxQImwJ83/Jk/bvgjIxJH8PiZj5MRkxHqsrqvsl2w8EHY/gbEDoCzH4Bx13bKRC1HkltWz+8W7eI/6wpwOR18bsIAvjxrMKP6x3XJ+0REpOfpyhD9S6C83YeFicaYezpc0w9oMMY0B7twfAhcaozZalnWK8C/2n1YuNEY8+Sx3qkQLT3Nsvxl3Lv8XiJdkTx93tNkx2eHuqTuLfd9ePd+e9bD9NPgvJ/CkHld9ro9pXXMX7GPVz8uoNHr5/QhiXx51mDO0cQtIiJ9XleG6CTgZWAgkIc9xF2FZVlTgNuNMbdYljUT+DMQABzA48aYp4P3D8Ee4i4RWAdcZ4xpPtY7FaKlJ9pZuZNb370Vh+XgqfOeYmjC0FCX1L0FArDlVVj4Y6jeDznnwbk/gdSRXfbK6gYvL67ez98+zKOgqpGsxEhumJHN1VOziIvQkIUiIn2RJlsR6Qb2VO3hlndvIWAC/OXcvzAi8Zg9nwTA2wSr/gzLfg2eWph0gz3rYWxal72yZeKWZ97PZVVuBVFhTq6cnMmNM7M1cYuISB+jEC3STeRW53LzuzfT7G/mL+f+hdFJo0NdUs9QXw7LHoXVT4EzHKbfBjO+AdFJXfrazQXVzH9/H29sKMTjDzBvRAo3zMxmbk4KDnX1EBHp9RSiRbqRA7UHuOWdW6j11vLnc/7MaSmnhbqknqN8Dyz+GWx+FdxRpyxMl9Y288JH+3n+ozxKa5sZnBzNl04fxJVTMtXVQ0SkF1OIFulmDtYd5OZ3bqayuZI/nvNHJqZODHVJPUvpDlj6KGz+1ykN0x5fgLc2F/LcB7l8vL+KqDAnV0zK5PoZg8hJi+3Sd4uIyKmnEC3SDRXXF3PLu7dQ3FDME2c/wdT0qaEuqecJUZgG2JRfzbMf5PL6xoN4fAFmDUvihhnZnK1RPUREeg2FaJFuqqyxjFveuYWCugJ+d9bvmDFgRqhL6plCGKbL65p5cfUBnl+ZR2F1ExkJkXxpxiCunJypCVxERHo4hWiRbqyiqYJb372V3OpcHjvzMeZkzgl1ST1XxzB9xt0w8xvg7vrZIltG9Xj2g1w+2leB02Exb3gKl0/K5OxRqUS4nV1eg4iIdC6FaJFurqqpitsW3Mauql38au6vOHvg2aEuqWcr3QGLfgrbXoN+g+HCR2H4eafs9TuLa3n14wL+s66AopomYiNcXDxuAJdPymDKoH5Ylrp7iIj0BArRIj1AjaeGry74KpvLN3P/9Pu5esTVoS6p59uzGN78LpTvghEXwQU/h37Zp+z1/oDhwz3lvPpxPm9tLqLR62dgYhSXTczg8kkZDEqKPmW1iIjIiVOIFukhGrwNfGfpd1hesJybxt7EXZPuwmE5Ql1Wz+bzwMon7W4exm938Zh1F7gjT2kZ9c0+3t5cxKvr8vlgTznGwJRB/bh0YgYXjk1X/2kRkW5IIVqkB/EFfDz80cO8svMVLsy+kJ+c8RPCnQpYJ626AN79gT2deL9suOARGHFhSEoprG7kP+sO8q+P89ldUofTYTFjSBIXj+vP+WPS6RcdFpK6RETkUArRIj2MMYZntjzDY2sfY1LqJH575m9JiEgIdVm9w96ldhePsh2Qcz5c+AgkDglJKcYYthfV8sbGg7yxsZC88gZcDotZw5K5eFx/zhuTTnykJnMREQkVhWiRHurtfW9z34r7yIjJ4MlzniQrNivUJfUOfi989CdY8oi9PeNr9igeUYkhK8kYw5aDNby+8SD/21hIfmUjbqfFnJwULh7fn3NGpRGr2RFFRE4phWiRHmxt8VruXHQnLoeLP5z1B00T3plqCmHBA7DpFQiPg9PvgNO/CpGhbfU3xrAhv5o3Nhzkf5sKKaxuwu20mJqdyJzhKcwdnsLI9FiN8iEi0sUUokV6uH3V+/jqwq9S3ljOI3Me0RB4na14Cyz5OWx7HSLi7Vbp6bdDeOin8g4EDOsOVPLOlmKW7Sxle1EtAGlx4czJSWHO8BRm5ySTEKV+1CIinU0hWqQXKG8s5xuLvsHmss3cM/Uerht9XahL6n0KN9hdPHa8CZH97FE8pt4K4TGhrqxVUXUTy3aVsnRnKSt2lVHd6MVhwfisBOYOt0P1+MwETT0uItIJFKJFeolGXyP3LruXRQcWcd2o6/ju1O9qCLyuUPCx3TK9612ISoYzvglTboawqFBXdgh/wLAhv4qlO0pZtquU9QeqMAYSotycMSy5tetHWlzXz9goItIbKUSL9CL+gJ9frfkVz297nmtGXMP90+9X39iucmA1LHkY9iyC6FR7jOkpXz7lY0wfr8p6Dyt2l7Fsp91SXVLbDMDI9NjWVuop2f0Id2kKchGR46EQLdLLGGN47OPHeGbzM3x5zJe5e/LdCtJdKe9DWPwzyF0OMWkw65vdOkxD2/B5S3eWsmxnKatzK/D6DZFuJzOGJjE32Jd6cHK0/rcjInIUCtEivZAxhp999DNe2vESX5vwNW4ff3uoS+r9clfYfaZbw/RdMPnL3a6bx5HUN/v4cE85y3aVsmRHKfsrGgDISIhk1rAkZg1LZtawZM2cKCLSjkK0SC8VMAEeeP8BXtvzGt+d8l2uH3N9qEvqG3Lfh6WPwL5ldjePWXfBlJt6RJhukVtWz4rdZazYVcYHe8qoafIBMKp/HGcEQ/W0wYlEhblCXKmISOgoRIv0Yr6Aj3uW3cOCvAX8cMYPuWr4VaEuqe/I+8Bumd63FKJT2oXp6FBXdkL8AcPmgurWUL02rxKPP0CY08GkQQlMH5zE6AFxjEqPI7NfJA6N/CEifYRCtEgv5/V7uWvxXawoWMHDsx/m4iEXh7qkviXvQ7tleu8SezSPWXfaYbobjDP9aTR6/KzOreD93WWs2F3G1sIaWv5xER3mZER6LCP7xzEyPZaR6XGMSI/V9OQi0ispRIv0AU2+Jr723tdYW7yWX8/7tSZkCYX9K+2W6b2LISwWJl4H026FpKGhruyk1Df72Flcy46iWrYX1bKtsIbtRbVUN3pbr8lIiGREeqy9pMUyPC2WoanRGglERHo0hWiRPqLeW89tC25jW/k2fn/W75mVMSvUJfVN+Wvhoz/Bln9DwAc558H0r8DQs6CXjIRhjKGopontRbVsL6xle1EN2wtr2VtWh9dv/7PF6bAYnBzdGqpbQvbAxChNBiMiPYJCtEgfUt1czS3v3kJudS5/POePTEk/7P/7cqrUFsGaZ2DNfKgvgeThMO02GP/5bjULYmfy+ALkltezo6i2tfV6R3Et+ysaWruEhLscDEuNISc1hpy02NbtgYlRuJyaPEhEug+FaJE+pryxnC+/82VKGkp46rynGJs8NtQl9W2+ZtjyH/joj3BwHYTHB7t63AKJQ0Jd3SnR4PGxu6TODtVFtewqqWN3SR0FVY2t14Q5HQxJiQ6G6lhy0mIYFgzXEW51CxGRU08hWqQPKq4v5oa3b6DWU8v88+czInFEqEsSYyB/jd3VY+t/IOCHnHNh8o2Qcz44+95wcnXNPvaU1LGrpI5dJbXsKrbXByrawrVlwYD4SLKToxiUFM3gpGiyk6MZnBxFVmKU+l2LSJdRiBbpo/Jr87nh7RvwBXzMP38+QxN69gduvUpNod3N4+O/QV0RxPaHCV+ESddDv0Ghri7kGjw+9pbWs6e0jn1l9eSW1bOvvIHcsvpDPmh0WDAgIZLspGjS4yNIj4sgLS6ctLgI0uIiSI+PICk6TN1ERORTUYgW6cNyq3O56Z2bCJgA8y+Yz5D4vtF9oMfw+2DXO7D2Odi9wG6tHnoWTL4BRlwETg0d11FVg8cO1uX17Cuzg3VeRQPF1U2U1jXjDxz6zzaHBckx4aTHR5AaG0FqXDipseH2dmx4cD+C5BiFbRE5lEK0SB+3t3ovN719Ew7LwTMXPMOgOLV0dkvV+bDuebt1uqbAng1xwhfs1ukePkzeqeIPGMrrmimuaaa4pomimiZKguuWYyW1zVTUew6717IgKTqMlGC4To4JJyHKTUKkm4ToMHsd5SYhMsxeR7mJCXdh9ZIRV0TkcArRIsLuyt3c/O7NuBwunj3/WbLiskJdkhxNwA+734O1z8LOt8H4IXs2jLkMRn0WYlJDXWGP5/EFKKtrpqS2mZJgsC6pbaa0tomSGnu7vK6ZqkYvDR7/UZ/jdFht4TrKDtrxUW76RbWF7vjgdr+oMOIiXcRFuImNcKnVW6QHUIgWEQB2Vu7k5nduJsIVwTPnP0NmbGaoS5JPUlMI65+HDS9C+W7AgkGzYPSldqCO6x/qCnu9Zp+f6kYv1Q1eKhu8VDV4qAruVzV6qGxo265q8AYXD/XHCN8AUWFO4iLcxEW6iI1wExfhIi7STVyEm5gIF1FuJ1HhLqLDnESGOYkOcxEV7iQqrO1YVJiLCLeDCJdT07GLdAGFaBFptb1iOze/czMx7hieueAZBsQMCHVJcjyMgZJtsPW/9lK6DbAga7odqEdfAvH6pag78fgCdvgOhuvKBi81jV5qmrzUNvkO3W7yUtPoo7bJS02TvW6ZtOZ4uZ0WES4n4W4H4S4nER3W0eFOosNdRIe7iAl3ER3mIjrcSUy4i5iItuMRLicup4XLYeFyOoJrC6fDwuVwtJ1zOHA7LXVnkV5NIVpEDrG1fCu3vHsLcWFxPHvBs6RHp4e6JDlRpTtg62t2oC7eZB/LmNIWqPtlh7Q8OXlef4AGj58Gj4/6Znvdfr/R46fe46PZF6DJ629dN3kDNPv8NAfXTV77eL3HT32zj/pmH3XN9n2dIczlINzpwO1yEOZ0EOYKLh22XU4Lt9MO3m6nA5fDQZirJYy3HQ9zte2Ht263HQ9z2dc5LQuHww73DssO9i3bToeF0wFOh/1LgMPRdr5l7XS0/WLgsNAvA3JECtEicpjNZZu59d1bSYxIZP7580mLTgt1SfJple9pa6EuXG8f6z8hGKgv1UeJckRef4CGZj91nrZgXd/so8kbwB8I4PUb/AGD1x+w1wGD3x/AFzD24g/g8QVoDq49vgDelu2Wc8HjvuBzvP6WdQCf3+DxB/AFj3uCx0MVTRwWOCwLKxioLdr2W49Dh0DuOCyY2632wfAefFbLs9s/r+19bdtOR9v5lm1n8D6Ho62GlrxvYbXbPvQXgda6gzVbwYtajlkd3t/yS0lLbc4O23T4M2l5hoW90/L+1uNWh/ran6Pjz9J+v8MzLIsot5PpQ5K69L//o+mSEG1ZViLwEpAN5AJXG2MqO1xzJvBYu0MjgWuNMf+xLOtZYC5QHTx3ozFm/Se9VyFapPNsKN3AVxZ8hZTIFOafP5+UqJRQlyQnq2IfbHvdDtQFwb8r005rC9Qpw0Nbn8gn8AdMaxBvCdxtAb0tbPsDhkDA4Dd22G9ZAsbgDxA8HrC3A3aQDwR/AfC3W/uDvxAEDBiMvTZgjMEAgUBwbQzGEHz+oc+w1/YvBi37vkCg9fpA8HmBw/btWlu2W88H7G1/wGCM/TO23NMS3Qz2ftt28HiwboLPajkXOMLxQLvndWcDE6NYds+ZIXl3V4XoR4EKY8wjlmXdC/QzxnzvGNcnAruBTGNMQzBEv2GM+eeJvFchWqRzrStZx1cWfIX06HTmnz+f5MjkUJcknaXqQFugPrDSPpYyqi1Qp45qa9ISkT4r0PLLhzEEArRum+AvI4Hggv2f1vDdEuTbArz9i8gh52gJ+O2Pt7uu3fbRnuF2OhibEX/q/2DouhC9A5hnjCm0LKs/sMQYc9R5hS3Lug2Ya4z5YnD/WRSiRbqFNUVruOO9O8iIyeDZC54lPjw0f1lJF6o5CNvesAN13vuAgaRhMPIzMPJiuz+1Q0OuiYi011UhusoYkxDctoDKlv2jXL8I+I0x5o3g/rPADKAZeA+41xjTfJR7bwNuAxg4cODkvLy8T123iBzZysKV3LHwDsYmj+XP5/6ZSFdkqEuSrlJbDNtfh+3/g33LIOCzJ3YZeZEdqAfPAVd4qKsUEQm5Tx2iLctaCBzps/37gefah2bLsiqNMf2O8pz+wEZggDHG2+5YERAG/AXYY4x56JN+GLVEi3Sdd3Pf5TtLv8OczDk8fubjuByuUJckXa2xCnYvhO1vwK4F4KmDsBjIORdGfMZeRx61fUREpFcLeXcOy7LuAsYYY247yvl5wHeMMRd/0nsVokW61kvbX+KnH/2Uzw37HA/NfEjDPvUlvma7ZXr7G7D9TagvAYcLBs6AzKmQMQkyJkOcxhYXkb7haCH6ZJuYXgNuAB4Jrv97jGs/D3y/Q1H9gwHcAj4HbD7JekSkE1wz8hrKmsr404Y/kRiRyN2T7w51SXKquMLtluecc+Ezj9mje2z/H+xdDB/8zu72ARCTbofpjEn2MmAiRB7xX0SKiPRKJxuiHwFetizrZiAPuBrAsqwpwO3GmFuC+9lAFrC0w/3/Z1lWCvawgOuB20+yHhHpJHeMv4PyxnLmb55PUkQS14+5PtQlyanmcEDWNHvhx+BtgqJNcPBjKFhrLzv+13Z94lA7WA+YaC/pp0F4TMjKFxHpSppsRUSOyh/w891l32VB3gIePuNhPjv0s6EuSbqbxio4uC4YrIPhurbQPmc5IHl4W6geMBHSxkJYVGhrFhE5AV3VnUNEejGnw8nPZ/+cquYqfvj+D0kIT2B25uxQlyX/396dx1VR738cf33P4QCyCC6oueSWicqmiWuulGnmUraZprZpmln3llez3VtdUytv97pUZlpZaZuV2tUsykwz1B/uJqmYu2CKICJwmN8f53ACdxI5iO9nj3nMzHdmvvMZvnT8nC/fmSlNyoVC/U6uKV/6ftib6E6u/w9++xbWfujaZuwQFu5KqK+Idk3VIsA30Dvxi4j8ReqJFpFzysjO4J5F97Dz6E6md5lOVFiUt0OSS4lluXqn85Pq/CnzkHsH43pe9RXRcEUUVItyLQdU9GrYIiJwkZ7O4S1KokVKXurxVO5eeDfpOem82/Vd6oXW83ZIcimzLDi6B/atg/3rXPN9a+Ho7j/3CanlSqirRUKVcKjSGCrWA7vDe3GLyGVHSbSIXLBdR3dx99d347A7eK/be1QLPN0j5EUuwLFDrqR6vzup3rcODv2G6+W/gM0BlRu4TN0w8QAAIABJREFUXlce1siVXIc1gop1wWb3augiUjYpiRaRYrH50GbuWXQPVQOqMuOGGVQqV8nbIUlZl50JqVshZQsc3OyaUjbDkd//3MfHHyo1gAq1XT3YobUgpKZ7+UoIqAR63rmI/AVKokWk2CTsT2DYkmFUD6rOW13eokpAFW+HJJejExmQ+isc3AIHN7kS7SO7IG2X662LBTkCCiTV7gS7fE0IqeFerqHXnIvIaSmJFpFilbA/gYe+fYjK5Sozvct0qgfpDXZSSlgWHD/sSqbzk+ojuyDt9z/XPTc1FhAY9mdCHVLTNQVfAYGVIaCye15JY7JFLjNKokWk2CUeTGTYkmEE+Qbxdpe3qVW+lrdDEjk/2ZlwdK/rRsa0Pa6bHNN2FVjefWpvdj7/0MKJdaHlMFeiHRimpFukjFASLSIXxaZDmxjyzRB8bb68dcNb1AvRUzukDLAsyEpzPfM6MxWOpbrnh+BYyp9l+eWZh8DKO31d+Ul3foIdUMn1ivSAiq55ufx5gTINLREpNZREi8hFk3Q4iQcWP4CFxZvXv0nDig29HZJIycrLcw0hyUx1JdmepDt/SnEl2sdS3Pv9AXk5Z67PEQj+IeAX7J6C3PPyrrlvUIFtweAo554CTj/3Ked6jbuIFJmSaBG5qHak7eD+xfdzwnmCN657gyaVm3g7JJHSy7Ig+xgc/8OVVOcn1vnLxw+7Xqmene66gfJEumvKzoATR13LZ+r5PhMff1fy7R8C/uVdCbl/eXeyflKZIwDsvq6hKDYf19zu63rEoN3HPXdv9/F39Zznz/UUFCljlESLyEW3K30XDyx+gLQTaUy9bioxVWK8HZJI2WRZkHP8z8Q6J9O1fsq8wHL2Mde+WWmQddSVjBdcPtMY8KKy+52aWPv4g4+vO/F2Tz5+7uTcr8C2/GU/93F+7n39T1p272PzcU+2Ass+rmeGG/upZflfCmwO9czLeVMSLSIlYv+x/dy/+H4OZh7kv53/S4srWng7JBE5H85cdy/3UdeNl3k5rrK8HHDmgDMb8nJdywXLck+4p6yzzLP+3NeZ7V52z50nCiy79znbUJdiY/7sZS+YeBtbgckUnmP+3OY5Lr+O/F56n8I99sbmrrtgnSef56R9PF8CCsRksxU4zrhjOWkOhcs8ywWvxXb6slPO6z5f/nk9sRSsP78uCtfJac59crzGdlK8tjPUbf5sq0DvvJdASbSIlJjU46k8sPgBdqXvYlKnSVxb41pvhyQil5K8PHdCfVLynZ+Y55dZTshzupJ7z+Q8aZ5TeB9nzknb3Ps6c1z1WZZrqIyV51qm4Lq7rND58ussMC/4BeSUY50nlTld11to3emOpYhDdsqyCnXgkbVeOfWZkmgfbwQjImVb5XKVmXHDDAZ/M5iHv3uYf3f6N+1rtvd2WCJyqbDZwOYPDn9vR+JdBRP6gom1J7kvOC9wTKFteX/Ww0lfEArNCyTveflfJk4ucxaol8L155/Tc54zlRWM8VzHFtjuG1wiP/KiUE+0iFw0aSfSeGDxAyQfTeadru/QpJJuNhQRkUvLmXqiNapeRC6aEL8Qplw3hQp+FRj+7XD2Zuz1dkgiIiLFQkm0iFxUlctVZsp1UziRe4JhS4ZxNPuot0MSERG5YEqiReSiqx9an0mdJrEzfSd/i/8bOc6SuPNeRETk4lESLSIlosUVLRjbZiy/7P+FZ5c/y6V4P4aIiEg+PZ1DREpMj/o92JOxh8mJk6kRXIOHYh7ydkgiIiJ/iZJoESlRQ6KGsDdjL9PWTqN6YHVubnCzt0MSEREpMiXRIlKijDE83fpp9h/bz9gVY6kWWI3W1Vt7OywREZEi0ZhoESlxDpuDVzq+Qt3Quvz9+7+z9fBWb4ckIiJSJEqiRcQrgn2DmRI3hQCfAIYtGcaBYwe8HZKIiMh5UxItIl5TLbAak6+bTHp2OsO/G86xnGPeDklEROS8KIkWEa8KrxjOKx1fIelwEn///u9kO7O9HZKIiMg5KYkWEa+7tsa1PNv6WZbvXc5j3z+ml7GIiEippyRaREqFmxvczJMtn+T73d8z6sdR5OblejskERGRM1ISLSKlxp3hd/KP2H/wzc5vGPPjGJx5Tm+HJCIiclp6TrSIlCp3N76bbGc2k9ZMwmF38M+2/8Rm9H1fRERKFyXRIlLq3Bd5H9l52UxJnILD5uCZ1s8okRYRkVJFSbSIlEoPRj1ItjOb6eun42v35YkWT2CM8XZYIiIigJJoESmljDGMaDqCbGc27256F1+bL481f0yJtIiIlApKokWk1DLG8Hjzx8nJy2HWpln42n15uOnDSqRFRMTrLniQoTHmNmPMRmNMnjGm+Vn262qM+dUY85sxZnSB8rrGmJXu8jnGGN8LjUlEyg5jDKNbjKZPgz68tf4tpq2b5u2QREREiuURdxuAW4ClZ9rBGGMHJgPdgMZAX2NMY/fml4HXLMu6CjgM3FcMMYlIGWIzNp5p/Qw96/dkSuIU3l7/trdDEhGRy9wFJ9GWZW22LOvXc+zWAvjNsqztlmVlAx8BvYzrb7KdgU/c+80Cel9oTCJS9tiMjbFtxtKtbjcmrZnElMQpWJbl7bBEROQyVVJjomsAuwqs7wZaApWAI5Zl5RYor3G6Cowxg4HBAFdeeeXFi1RESi27zc5L176Er82XqWunknYijVEtRunxdyIiUuLOK4k2xiwBqp1m05OWZX1RvCGdnmVZbwJvAjRv3lzdTyKXKR+bD2PbjiXEL4R3N73L0eyjjG07FofN4e3QRETkMnJeSbRlWddd4Hn2ALUKrNd0lx0CQo0xPu7e6PxyEZEzshkbjzd/nFC/UF7/v9dJz05nYoeJ+Pv4ezs0ERG5TJTU30ATgAbuJ3H4AncCX1quAY3xwK3u/QYCJdKzLSKXNmMMD0Q9wNOtnmbp7qUM+WYI6dnp3g5LREQuE8XxiLubjTG7gdbAAmPMInd5dWPMQgB3L/NwYBGwGZhrWdZGdxWjgL8bY37DNUZat92LyHm7veHtjG8/nnUp67h30b2kHk/1dkgiInIZMJfi3e3Nmze3Vq1a5e0wRKQUWbZnGX///u9UCajCm9e/SfWg6t4OSUREygBjzGrLsk55F4puaReRMuHaGtfy5vVv8kfWH9z99d1sO7LN2yGJiEgZpiRaRMqMmCoxvHPDO+RZeQz63yDWp6z3dkgiIlJGKYkWkTKlYcWGvNvtXYIcQdy3+D5WH1jt7ZBERKQMUhItImVOreBavNvtXaoFVuPhbx9m6+Gt3g5JRETKGCXRIlImhQWE8eb1b1LOUY6h3wxlb8Zeb4ckIiJliJJoESmzqgVW443r3uC48zhDvhnC4azD3g5JRETKCCXRIlKmXVXhKv7b+b/sO7aP4d8OJzMn09shiYhIGaAkWkTKvGZVmzG+/Xg2HNrA4z88Tk5ejrdDEhGRS5ySaBG5LHS+sjNPtXqKH/f8yHPLn+NSfNGUiIiUHj7eDkBEpKTcdvVtpGamMmXtFMLKhfHoNY96OyQREblEKYkWkcvKg9EPknI8hbc3vE1YQBj9GvXzdkgiInIJUhItIpcVYwxPtnySP7L+4OVfXqaSfyW61u3q7bBEROQSozHRInLZsdvsvNz+ZZpWacoTy57g530/ezskERG5xCiJFpHLkp/dj//E/Yc65evwaPyjbDq0ydshiYjIJURJtIhctsr7lmfaddMo71uewd8MZkPqBm+HJCIilwgl0SJyWasaWJW3b3ibIEcQ9y++n4T9Cd4OSURELgFKokXkslcruBazus6iakBVhi4ZytLdS70dkoiIlHJKokVEcPVIz+w6k/qh9Xnku0f4X/L/vB2SiIiUYkqiRUTcKvhXYHqX6USFRTFq6Sg+S/rM2yGJiEgppSRaRKSAYN9gpl0/jdbVW/Ps8md5d+O73g5JRERKISXRIiInKedTjv90+g/X176eCasmMCVxCpZleTssEREpRZREi4ichsPuYHz78fSq34upa6cyPmG8EmkREfHQa79FRM7Ax+bD2LZjCfIN4v3N75OZm8kzrZ7BbrN7OzQREfEyJdEiImdhMzZGxY4iyBHEG+veICM7g3HtxuGwO7wdmoiIeJGSaBGRczDGMLzpcIJ9g5m4aiIZORm81vE1AhwB3g5NRES8RGOiRUTO08AmAxnbZiw/7/uZB755gLQTad4OSUREvERJtIhIEdzc4GZe7fAqmw9tZtD/BnHg2AFvhyQiIl6gJFpEpIjiascx7bpp7Du2j4H/G8jOozu9HZKIiJQwJdEiIn9Biyta8PYNb5OZk8mArwew+dBmb4ckIiIlSEm0iMhf1KRSE2Z1m4Wv3Zd7F93Lqv2rvB2SiIiUECXRIiIXoG5IXd7r9h5hAWE8uORBvt/1vbdDEhGREqAkWkTkAlULrMasrrNoENqAR+Mf5cttX3o7JBERuciURIuIFIMK/hWYfsN0mldrzpPLnuS9Te95OyQREbmIlESLiBSTQEcgU+KmcH3t6xmfMJ6Xf3mZnLwcb4clIiIXgZJoEZFi5Gv3ZUL7CfRv1J/3N7/P4MWDOXT8kLfDEhGRYnZBSbQx5jZjzEZjTJ4xpvkZ9qlljIk3xmxy7/tIgW3PGWP2GGMS3dONFxKPiEhpYLfZGdViFC9d+xLrU9dzx/w72Ji60dthiYhIMbrQnugNwC3A0rPskws8ZllWY6AV8JAxpnGB7a9ZlhXjnhZeYDwiIqVGj/o9eK/be9iNnQFfD2Deb/O8HZKIiBSTC0qiLcvabFnWr+fYZ59lWWvcy+nAZqDGhZxXRORS0ahSIz666SOaVm3K0z89zYs/v0iOU+OkRUQudSU6JtoYUwdoCqwsUDzcGLPOGDPDGFOhJOMRESkJFfwrMO26aQxqMoiPfv2I+xffT+rxVG+HJSIiF+CcSbQxZokxZsNppl5FOZExJgj4FHjUsqyj7uKpQH0gBtgHvHKW4wcbY1YZY1alpKQU5dQiIl7nY/PhseaPMb79eDYd2sQdX93B2pS13g5LRET+ImNZ1oVXYsz3wOOWZZ32nbfGGAcwH1hkWdarZ9inDjDfsqyIc52vefPm1qpVer2uiFyafv3jVx6Jf4SDmQcZ03IMt159q7dDEhGRMzDGrLYs65QHaFz04RzGGAO8DWw+OYE2xlxRYPVmXDcqioiUaQ0rNmTOTXOIrRbL8yue59nlz3I897i3wxIRkSK40Efc3WyM2Q20BhYYYxa5y6sbY/KftNEWuBvofJpH2Y03xqw3xqwDOgF/u5B4REQuFSF+IUyJm8IDkQ/wWdJn9J3fl6TDSd4OS0REzlOxDOcoaRrOISJlyfI9y3li2RMcyznG6Baj6dOgD64/4omIiLd5bTiHiIicXZsabfi056c0rdKU51c8z8ilI0nPTvd2WCIichZKokVESoHK5SrzxvVv8EizR1iycwm3fXUb61PWezssERE5gzIznCMnJ4fdu3eTlZXlpahE/jp/f39q1qyJw+HwdihSCiQeTGTU0lEczDzII80eYUCTAdiM+jxERLzhTMM5ykwSvWPHDoKDg6lUqZLGEsolxbIsDh06RHp6OnXr1vV2OFJKpJ1I47nlz7Hk9yW0rdGWF9u+SKVylbwdlojIZafMj4nOyspSAi2XJGMMlSpV0l9RpJAQvxBe7fgqT7V8ioR9Cdz21W2s3Lfy3AeKiEiJKDNJNKAEWi5Z+t2V0zHGcEf4HXzQ/QOCfIO4f/H9PP3T0/yR9Ye3QxMRueyVqSRa/jRp0iQyMzO9HYbXzZw5k+HDh3s7DJEL0rBiQz7q/hH3RtzL/G3z6fF5Dz7e+jF5Vp63QxMRuWwpib5EWZZFXt6Z/wH9K0m00+kschy5ublFPuZiKm3xiBSXAEcAf7vmb3zS8xOurnA1Y1eM5e6Fd7Pp0CZvhyYicllSEl2M/vnPf9KwYUOuvfZa+vbty8SJEwHYtm0bXbt25ZprrqFdu3Zs2bIFgEGDBjFixAjatGlDvXr1+OSTTzx1TZgwgdjYWKKionj22WcBSE5OpmHDhgwYMICIiAh27drF0KFDad68OU2aNPHs9/rrr7N37146depEp06dAPjwww+JjIwkIiKCUaNGec4TFBTEY489RnR0NCtWrCh0PQkJCURFRRETE8PIkSOJiIgAXL27PXv2pHPnzsTFxZGRkUFcXBzNmjUjMjKSL774whNveHg4gwYN4uqrr6Zfv34sWbKEtm3b0qBBA3755RcAnnvuOQYOHEi7du2oXbs2n332Gf/4xz+IjIyka9eu5OTkADB27FhiY2OJiIhg8ODB5N8U27FjRx599FGaN2/Ov//97zO2T3JyMp07dyYqKoq4uDh+//13AD7++GMiIiKIjo6mffv2AGzcuJEWLVoQExNDVFQUSUl6k5yUDvVD6zPjhhm8dO1L7M7YTd8FffnXyn/pudIiIiXMx9sBXAzPf7WRTXuPFmudjauX59keTc64PSEhgU8//ZS1a9eSk5NDs2bNuOaaawAYPHgw06ZNo0GDBqxcuZJhw4bx3XffAbBv3z6WLVvGli1b6NmzJ7feeiuLFy8mKSmJX375Bcuy6NmzJ0uXLuXKK68kKSmJWbNm0apVKwBefPFFKlasiNPpJC4ujnXr1jFixAheffVV4uPjqVy5Mnv37mXUqFGsXr2aChUq0KVLF+bNm0fv3r05duwYLVu25JVXXjnlmu655x7eeustWrduzejRowttW7NmDevWraNixYrk5uby+eefU758eVJTU2nVqhU9e/YE4LfffuPjjz9mxowZxMbG8sEHH7Bs2TK+/PJLXnrpJebNmwe4vmjEx8ezadMmWrduzaeffsr48eO5+eabWbBgAb1792b48OE888wzANx9993Mnz+fHj16AJCdnc253mL58MMPM3DgQAYOHMiMGTMYMWIE8+bNY+zYsSxatIgaNWpw5MgRAKZNm8YjjzxCv379yM7O/ku99CIXizGGHvV70KFWB/77f//lo18/YlHyIh6PfZzudbtrjL2ISAlQT3Qx+emnn+jVqxf+/v4EBwd7kruMjAyWL1/ObbfdRkxMDEOGDGHfvn2e43r37o3NZqNx48YcOHAAgMWLF7N48WKaNm1Ks2bN2LJli6cntHbt2p4EGmDu3Lk0a9aMpk2bsnHjRjZtOvVPuwkJCXTs2JGwsDB8fHzo168fS5cuBcBut9OnT59Tjjly5Ajp6em0bt0agLvuuqvQ9uuvv56KFSsCrqElY8aMISoqiuuuu449e/Z4rqVu3bpERkZis9lo0qQJcXFxGGOIjIwkOTnZU1+3bt1wOBxERkbidDrp2rUrQKH94uPjadmyJZGRkXz33Xds3LjRc/wdd9xxriZixYoVnuu4++67WbZsGQBt27Zl0KBBvPXWW55kuXXr1rz00ku8/PLL7Ny5k3Llyp2zfpGSVt63PGNajuGD7h9QPag6T/z4BPctvo/tR7Z7OzQRkTKvTPZEn63HuKTl5eURGhpKYmLiabf7+fl5lvOHJ1iWxRNPPMGQIUMK7ZucnExgYKBnfceOHUycOJGEhAQqVKjAoEGDivyYNH9/f+x2e5GOAQrFMXv2bFJSUli9ejUOh4M6dep44ih4fTabzbNus9kKjV8uWO5wODw9afn7ZWVlMWzYMFatWkWtWrV47rnnCl1rwXiKatq0aaxcuZIFCxZwzTXXsHr1au666y5atmzJggULuPHGG3njjTfo3LnzXz6HyMXUpFIT3r/xfT7Z+gn/XvNv+nzZhwFNBjAkaggBjgBvhyciUiapJ7qYtG3blq+++oqsrCwyMjKYP38+AOXLl6du3bp8/PHHgCtBXrt27VnruuGGG5gxYwYZGRkA7Nmzh4MHD56y39GjRwkMDCQkJIQDBw7w9ddfe7YFBweTnu4aI9miRQt++OEHUlNTcTqdfPjhh3To0OGsMYSGhhIcHMzKla7n0n700Udn3DctLY0qVargcDiIj49n586dZ637r8hPmCtXrkxGRkah8ePnq02bNp7rmD17Nu3atQNcQ0latmzJ2LFjCQsLY9euXWzfvp169eoxYsQIevXqxbp164rvYkQuApuxcXvD2/nq5q/oXq87MzbMoPcXvfn292+5FF+qJSJS2pXJnmhviI2NpWfPnkRFRVG1alUiIyMJCQkBXAnb0KFDeeGFF8jJyeHOO+8kOjr6jHV16dKFzZs3e4ZSBAUF8f7775/SYxwdHU3Tpk0JDw+nVq1atG3b1rNt8ODBdO3alerVqxMfH8+4cePo1KkTlmXRvXt3evXqdc5revvtt3nggQew2Wx06NDBcz0n69evHz169CAyMpLmzZsTHh5+zrqLKjQ0lAceeICIiAiqVatGbGxskev4z3/+wz333MOECRMICwvjnXfeAWDkyJEkJSVhWRZxcXFER0fz8ssv89577+FwOKhWrRpjxowp7ksSuSgq+lfkhWtf4JYGt/DCyhd4NP5R2tVoxxMtn6BWcC1vhyciUmaUmdd+b968mUaNGnkpIpeMjAyCgoLIzMykffv2vPnmmzRr1syrMV2I/OsBGDduHPv27Tvr0y/kwpSG32EpW3Lzcvlg8wdMTpxMbl4u90fdz70R9+Jn9zv3wSIiAlwGr/0uDQYPHkxMTAzNmjWjT58+l3QCDbBgwQJiYmKIiIjgxx9/5KmnnvJ2SCJSBD42HwY0GcCXvb+k85WdmZI4hVu+uIWf9vzk7dBERC556okWKSX0OywX24q9K3hp5UskH03m+trX84/Yf1AtsJq3wxIRKdXUEy0icplrXb01n/b8lBFNR7B091J6zuvJjA0zyHZmezs0EZFLjpJoEZHLiK/dlweiHmBer3m0vKIlr61+jZu/uJnvd32vp3iIiBSBkmgRkctQzeCa/Kfzf5h23TR8bD48/N3DPLjkQbYd2ebt0ERELglKokVELmNta7Tlk56fMCp2FOtT1tPnyz68/MvLpJ1I83ZoIiKlmpLoMmrSpElkZmYWe71ffvkl48aNK9IxN954I0eOHAHwPDLvrxx/5MgRpkyZUqRjk5OTiYiIKPI5RS4nDpuD/o37M/+W+fRp0IcPtnzATZ/fxNxf5+LMc3o7PBGRUklJ9CXKsizy8vLOuP2vJNFO57n/sezZsyejR48uUr0LFy4kNDS0SMfAn9eYf/xfSaJF5PxV9K/I062fZs5Nc7gq9Cr++fM/uX3+7STsT/B2aCIipY6S6GL0z3/+k4YNG3LttdfSt29fJk6cCLheK921a1euueYa2rVrx5YtWwAYNGgQI0aMoE2bNtSrV6/Qq6wnTJhAbGwsUVFRPPvss4CrV7Vhw4YMGDCAiIgIdu3axdChQ2nevDlNmjTx7Pf666+zd+9eOnXqRKdOnQD48MMPiYyMJCIiglGjRnnOExQUxGOPPUZ0dDQrVqwodD2vv/46jRs3JioqijvvvBOAmTNnMnz4cE/8Q4cOpVWrVtSrV4/vv/+ee++9l0aNGjFo0CBPPXXq1CE1NbVQ3RkZGcTFxdGsWTMiIyP54osvzniN+cePHj2abdu2ERMTw8iRIxkwYADz5s3z1NmvXz9PPaeTlZXFPffcQ2RkJE2bNiU+Ph6AjRs30qJFC2JiYoiKiiIpKYljx47RvXt3oqOjiYiIYM6cOWdte5GyJLxiODNumMErHV4hIzuDexfdy4jvRrB873LyrDN/eRcRuZyUzdd+fz0a9q8v3jqrRUK3Mw9jSEhI4NNPP2Xt2rXk5OTQrFkzrrnmGsD1EpZp06bRoEEDVq5cybBhw/juu+8A2LdvH8uWLWPLli307NmTW2+9lcWLF5OUlMQvv/yCZVn07NmTpUuXcuWVV5KUlMSsWbNo1aoVAC+++CIVK1bE6XQSFxfHunXrGDFiBK+++irx8fFUrlyZvXv3MmrUKFavXk2FChXo0qUL8+bNo3fv3hw7doyWLVvyyiuvnHJN48aNY8eOHfj5+XmGY5zs8OHDrFixgi+//JKePXvy008/MX36dGJjY0lMTCQmJua0x/n7+/P5559Tvnx5UlNTadWqFT179gQ45RoLxrNhwwYSExMB+OGHH3jttdfo3bs3aWlpLF++nFmzZp2xjSZPnowxhvXr17Nlyxa6dOnC1q1bmTZtGo888gj9+vUjOzsbp9PJwoULqV69OgsWLAAgLU3jQ+XyYoyhS50utK/ZnpkbZ/L+5veJ3xVP9cDq9L6qN72v6s0VQVd4O0wREa9RT3Qx+emnn+jVqxf+/v4EBwfTo0cPwNXjunz5cm677TZiYmIYMmQI+/bt8xzXu3dvbDYbjRs35sCBAwAsXryYxYsX07RpU5o1a8aWLVtISkoCoHbt2oWSy7lz59KsWTOaNm3Kxo0b2bRp0ymxJSQk0LFjR8LCwvDx8aFfv34sXboUALvdTp8+fU57TVFRUfTr14/3338fH5/Tf9/q0aMHxhgiIyOpWrUqkZGR2Gw2mjRpQnJy8hl/XpZlMWbMGKKiorjuuuvYs2eP5/pPvsYz6dChA0lJSaSkpPDhhx/Sp0+fM8YJsGzZMvr37w9AeHg4tWvXZuvWrbRu3ZqXXnqJl19+mZ07d1KuXDkiIyP55ptvGDVqFD/++CMhISHnjEekLPL38efB6Af59rZvGd9+PFeWv5Ipa6dww6c38OA3D7I4eTE5zhxvhykiUuLKZk/0WXqMS1peXh6hoaGe3tOT+fn5eZbzn9FqWRZPPPEEQ4YMKbRvcnIygYGBnvUdO3YwceJEEhISqFChAoMGDSIrK6tI8fn7+2O320+7bcGCBSxdupSvvvqKF198kfXrT+3dz4/fZrMVuhabzUZubu4Zzzt79mxSUlJYvXo1DoeDOnXqeGIveI3nMmDAAN5//30++ugj3nnnnfM+rqC77rqLli1bsmDBAm688UbeeOMNOnfuzJo1a1i4cCFPPfUUcXFxPPPMM3+pfpGSJz7TAAAdKUlEQVSywM/uR7e63ehWtxu703cz77d5zPttHo/98BgV/CrQo34PbmlwC/VD63s7VBGREqGe6GLStm1bvvrqK7KyssjIyGD+/PkAlC9fnrp16/Lxxx8DrgR57dq1Z63rhhtuYMaMGWRkZACwZ88eDh48eMp+R48eJTAwkJCQEA4cOMDXX3/t2RYcHEx6ejoALVq04IcffiA1NRWn08mHH35Ihw4dzhpDXl4eu3btolOnTrz88sukpaV54ikOaWlpVKlSBYfDQXx8PDt37jznMQWvKd+gQYOYNGkSAI0bNz7r8e3atWP27NkAbN26ld9//52GDRuyfft26tWrx4gRI+jVqxfr1q1j7969BAQE0L9/f0aOHMmaNWv+4pWKlD01g2syvOlwFvVZxJS4KTSv1pwPtnxA7y96029hPz7Z+gnp2ennrkhE5BJWNnuivSA2NpaePXsSFRXlGdaQPwRg9uzZDB06lBdeeIGcnBzuvPNOoqOjz1hXly5d2Lx5M61btwZcN/+9//77p/QYR0dH07RpU8LDw6lVqxZt27b1bBs8eDBdu3alevXqxMfHM27cODp16oRlWXTv3p1evXqd9XqcTif9+/cnLS0Ny7IYMWLEX3rCxpn069ePHj16EBkZSfPmzQkPDz/nMZUqVaJt27ZERETQrVs3JkyYQNWqVWnUqBG9e/c+5/HDhg1j6NChREZG4uPjw8yZM/Hz82Pu3Lm89957OBwOqlWrxpgxY0hISGDkyJHYbDYcDgdTp04tjssWKVPsNjvtarajXc12HDp+iPnb5/NZ0mc8v+J5xv0yjrgr4+h1VS9aVmuJ3Xb6v3iJiFyqzKX4mtfmzZtbq1atKlS2efNmGjVq5KWIXDIyMggKCiIzM5P27dvz5ptv0qxZM6/GVNZlZmYSGRnJmjVrLvlxy6Xhd1jkQlmWxcZDG5n32zwW7lhIenY6VQKq0LN+T3rW70ndkLreDlFEpEiMMasty2p+crl6oovR4MGD2bRpE1lZWQwcOFAJ9EW2ZMkS7rvvPv72t79d8gm0SFlhjCGicgQRlSMYGTuS73d9zxe/fcGMDTOYvn460WHR9LqqF13rdCXYN9jb4YqI/GXqiRYpJfQ7LGVZSmYK87fPZ95v89ieth0/ux+danXipno30aZGGxw2h7dDFBE5LfVEi4iI14QFhHFPxD0MajLIM9xjUfIi/pf8P0L9Qrmhzg3cVO8mosOiMcZ4O1wRkXNSEi0iIiWm4HCPUbGj+GnvTyzYvoB5v81jzq9zqBFUg+71utO9XnfqhdTzdrgiImekJFpERLzCYXfQsVZHOtbqSEZ2Bt/+/i0Lti9g+vrpvLnuTRpVbET3et3pVrcbVQKqeDtcEZFCLiiJNsbcBjwHNAJaWJa16gz7JQPpgBPIzR9XYoypCMwB6gDJwO2WZR2+kJhEROTSE+QbRK+retHrql6kZKbw9Y6vWbBjARNXTWTiqolEhUXRuVZn4q6Mo05IHW+HKyJywS9b2QDcAiw9j307WZYVc9LA7NHAt5ZlNQC+da9fko4cOcKUKVPOuV9ycjIffPDBee0XERFx3uUiImVFWEAYA5oMYM5Nc/ii9xcMjxlOjjOHSWsm0WNeD3rN68W/1/yb9SnrybPyvB2uiFymLiiJtixrs2VZv15AFb2AWe7lWcC535hRShV3Ei0iIlAvpB5Doocwt8dcFvdZzOgWowkrF8Y7G97hroV3cf3H1/PCzy+wfM9ycpw53g5XRC4jJfXabwtYbIxZbYwZXKC8qmVZ+9zL+4GqJRRPsRs9ejTbtm0jJiaGkSNHYlkWI0eOJCIigsjISObMmePZ78cffyQmJobXXnuN5ORk2rVrR7NmzWjWrBnLly8/73NmZWVxzz33EBkZSdOmTYmPjwdg48aNtGjRgpiYGKKiokhKSuLYsWN0796d6OhoIiIiPPGIiFwqrgi6gn6N+jH9hun8cMcPvHTtS0SFRfHlti8ZsmQI7ea048FvHmRq4lSW71muV4+LyEV1zjHRxpglQLXTbHrSsqwvzvM811qWtccYUwX4xhizxbKsQkNALMuyjDFnfGi1O/keDHDllVee9WQv//IyW/7Ycp6hnZ/wiuGMajHqjNvHjRvHhg0bSExMBODTTz8lMTGRtWvXkpqaSmxsLO3bt2fcuHFMnDiR+fPnA6437n3zzTf4+/uTlJRE3759OfkZ2GcyefJkjDGsX7+eLVu20KVLF7Zu3cq0adN45JFH6NevH9nZ2TidThYuXEj16tVZsGABAGlpaRf4ExER8Z4QvxB61O9Bj/o9yMrN4ud9P/PD7h9IPJjI1LVTsbAwGOqH1iemSgzRYdFEh0VTp3wdPUJPRIrFOZNoy7Kuu9CTWJa1xz0/aIz5HGiBaxz1AWPMFZZl7TPGXAEcPEsdbwJvgutlKxca08W2bNky+vbti91up2rVqnTo0IGEhATKly9faL+cnByGDx9OYmIidrudrVu3FukcDz/8MADh4eHUrl2brVu30rp1a1588UV2797NLbfcQoMGDYiMjOSxxx5j1KhR3HTTTbRr165Yr1dExFv8ffw9T/kASM9OZ33qetYeXMvalLUs2rGIT7Z+AkCoXyhRYVFEVY4isnIkTSo3IcRPbzwVkaK76I+4M8YEAjbLstLdy12Ase7NXwIDgXHu+fn2bJ/V2XqMS5vXXnuNqlWrsnbtWvLy8vD397/gOu+66y5atmzJggULuPHGG3njjTfo3Lkza9asYeHChTz11FPExcXxzDPPFMMViIiULsG+wbSp3oY21dsAkGflsSNtB4kHE1mb4kqsl+7+84+htcvXJqJyBJGVI4msHEnDig3xs/t5K3wRuURc6CPubgb+A4QBC4wxiZZl3WCMqQ5MtyzrRlzjnD93//nMB/jAsqz/uasYB8w1xtwH7ARuv5B4vCk4OJj09D/H37Vr14433niDgQMH8scff7B06VImTJjAnj17Cu2XlpZGzZo1sdlszJo1C6fTed7nbNeuHbNnz6Zz585s3bqV33//nYYNG7J9+3bq1avHiBEj+P3331m3bh3h4eFUrFiR/v37ExoayvTp04v1+kVESiubsVE/tD71Q+vT5+o+gKu3euOhjWxI3cD6lPX8su8XFmx3DXfzsfnQsEJDIipH0LhSY+qG1KVu+bqE+od68zJEpJS5oCTasqzPgc9PU74XuNG9vB2IPsPxh4C4C4mhtKhUqRJt27YlIiKCbt26MX78eFasWEF0tOsVtuPHj6datWpUqlQJu91OdHQ0gwYNYtiwYfTp04d3332Xrl27EhgYeN7nHDZsGEOHDiUyMhIfHx9mzpyJn58fc+fO5b333sPhcFCtWjXGjBlDQkICI0eOxGaz4XA4mDp16kX8aYiIlG7BvsG0uqIVra5o5Sk7cOyAK6lOXc/61PXM3z6fOb/+eRN2qF8odUPqUqd8nULzGsE1cNgc3rgMEfEiY1mlfnjxKZo3b26dfPPd5s2badSokZciErlw+h0WKV2ceU72Zuxlx9Ed7EjbQfLRZNc8LZlDWYc8+/kYH2qVr0Xd8nWpE+JKrPOTbI23Frn0GWNWn/SeE0Cv/RYRETktu81OrfK1qFW+Fu1rti+07Wj2UZLTkk9JrpfuWUpuXq5nv4r+FQv1WtcNqcuVwVdSPag6vnbfkr4kESlGSqJFRESKqLxveddTPsKiCpXn5uWyJ2OPJ8HecdSVXH/3+3ccPnHYs5/BUDWwKjWDalIzuKZnXiu4FjWDa1LBr4IexSdSyimJFhERKSY+Nh9ql69N7fK16VCrQ6FtR7KOkHw0mV3pu9idvts1z9jNT3t+IuV4SqF9A3wCqB5UncrlKhNWLoywgDDCyoVROaAyVcpV8SyX8ylXkpcnIgUoiRYRESkBof6hxPjHEFMl5pRtx3OPszdjrye53pW+i/3H9pN6PJVVR1eRcjyl0DCRfMGOYCqVq0SoXyihfqGU9ytPqF8oIX4hhdd9XeuVy1XGYddNkCLFQUm0iIiIl5XzKed5DN/p5Fl5pJ1II+V4CimZKaQcTyH1eCoHMw+SejyVoyeOsu/YPrYc3kLaiTSO5x4/47kq+VeiWmA1qgZUpWpgVc+ypyygqhJtkfOgJFpERKSUsxkbFfwrUMG/AldXuPqc+59wniDtRBppJ9I4cuIIR08c5fCJw6RkpnAg8wD7j+3n9/TfSdifQHpO+inHBzuC8ffxx8/uh7+PP+V8yp122d/uT7BvMEGOIIJ8gzzzYEewa+7e5mf30xhvKXOURF9kkyZNYvDgwQQEBBRLfXXq1GHVqlVUrlz5Lx0/c+ZMVq1axX//+9+Ldp42bdqwfPnyM24/cuQIH3zwAcOGDQNg7969jBgxgk8++aTI5zpfP/74Iw8++CAOh4MVK1ZQrtyf4wiDgoLIyMi4aOcWESlpfnY/qgRUoUpAlXPueyznGAeOHWB/5n7X/Nh+0rLTyMrNIsuZVWh+OOsw+537OZ57nKzcLI7nHiczN/Oc5/Cx+RDoCCTAJ4AAnwACHYGUc5RzrTsCCPQJJMAR8Oe6I5AgRxCBjsBCU36ZesqlNFASfZFNmjSJ/v37F1sSXVROpxO73V6i5zxbAg2uJHrKlCmeJLp69eoXNYEGmD17Nk888QT9+/e/qOcREbnUBDoCqRdaj3qh9f7S8c48Jxk5Ga4p+895ek66Zz09O53MnEwyczNdiXdOJsdyjnE463Ch8rMNQynI1+ZLgCMAX5svDrsDX7svvjZffO2+OGzudXeZw+7Az+6Hn90PX7sv/nZ/fO2+nvWC23xsPtiN/c/JZsdmbJ7l/HIfm4/nOD+7H34+rrnN2P7Sz1AuTUqii8mxY8e4/fbb2b17N06nk6effpoDBw6wd+9eOnXqROXKlYmPj2fo0KEkJCRw/Phxbr31Vp5//nnA1fM7cOBAvvrqK3Jycvj4448JDw/n0KFD9O3blz179tC6dWsKvhynd+/e7Nq1i6ysLB555BEGDx4MuHpWhwwZwpIlS5g8eTJJSUn861//IjQ0lOjoaPz8/E6J/2znef/993n99dfJzs6mZcuWTJkyhbfeeott27YxYcIEoHAPd37PbkZGBr169eLw4cPk5OTwwgsv0KtXL0aPHs22bduIiYnh+uuv56GHHuKmm25iw4YNZGVlMXToUFatWoWPjw+vvvoqnTp1YubMmXz55ZdkZmaybds2br75ZsaPH3/KdXz77bc8/vjj5ObmEhsby9SpU3nvvfeYO3cuixYt4uuvv2b27NmnbUPLsvjHP/7B119/jTGGp556ijvuuIN9+/Zxxx13cPToUXJzc5k6dSpt2rThvvvuY9WqVRhjuPfee/nb3/7Gtm3beOihh0hJSSEgIIC33nqL8PBwPv74Y55//nnsdjshISEsXbr0r/+yiYiUInabnRC/kGJ5sYwzz8nx3ONk5GSQmZNJRk4Gx3KOcSznWKHl/CknL4dsZ7Zryssmx5lDdl42x3OPk3YijZy8HE44T3j2yV/OtU69SbM4OGwO/O3+nqTaz+7nScw9CbrNtexjfDyJef42h83h2mbzOesyuIb45CftNmPDhg1jDMYYz3L+F4D8uTGm0Hr+PL/uc53Xx+aDwbi+XLjPkV/v5ahMJtH7X3qJE5u3FGudfo3CqTZmzBm3/+9//6N69eosWLAAgLS0NEJCQnj11VeJj4/3DIt48cUXqVixIk6nk7i4ONatW0dUlOs5o5UrV2bNmjVMmTKFiRMnMn36dJ5//nmuvfZannnmGRYsWMDbb7/tOeeMGTOoWLEix48fJzY2lj59+lCpUiWOHTtGy5YteeWVV9i3bx933XUXq1evJiQkhE6dOtG0adNT4j/TeTZv3sycOXP46aefcDgcDBs2jNmzZ9OnTx9at27tSaLnzJnDk08+WahOf39/Pv/8c8qXL09qaiqtWrWiZ8+ejBs3jg0bNpCYmAhAcnKy55jJkydjjGH9+vVs2bKFLl26sHXrVgASExP5v//7P/z8/GjYsCEPP/wwtWrV8hyblZXFoEGD+Pbbb7n66qsZMGAAU6dO5dFHH2XZsmXcdNNN3HrrrWdsw88++4zExETWrl1LamoqsbGxtG/fng8++IAbbriBJ598EqfTSWZmJomJiezZs4cNGzYArt51gMGDBzNt2jQaNGjAypUrGTZsGN999x1jx45l0aJF1KhRw7OviIgUZrfZXWOrfYMu6nly83JPSayznFnk5uWSZ+WRa7nmzjwnTss95TldZZbTk7xnObNc89wsTjhPeKaC6/l15Oblkmvl4sxzku3M5njecc96bl6up96cvBzXvnm5nuWcvJyL+vMoDvlJvQ3bn8umcDJvMGcsO+WYk+qpFliNVzu+6u3LLKRMJtHeEBkZyWOPPcaoUaO46aabaNeu3Wn3mzt3Lm+++Sa5ubns27ePTZs2eZLoW265BYBrrrmGzz77DIClS5d6lrt3706FChU8db3++ut8/vnnAOzatYukpCQqVaqE3W6nT58+AKxcuZKOHTsSFhYGwB133OFJSgs603m+/fZbVq9eTWxsLADHjx+nSpUqhIWFUa9ePX7++WcaNGjAli1baNu2baE6LctizJgxLF26FJvNxp49ezhw4MBZf47Lli3j4YcfBiA8PJzatWt74o2LiyMkxNXT0bhxY3bu3Fkoif7111+pW7cuV1/tuulm4MCBTJ48mUcfffSs5yx47r59+2K326latSodOnQgISGB2NhY7r33XnJycujduzcxMTHUq1eP7du38/DDD9O9e3e6dOlCRkYGy5cv57bbbvPUeeLECQDatm3LoEGDuP322z3tLCIi3pHfuxrg8M5Qy6KyLOvPRDwvFyv/P8siz8rDwj23rELLTuvPxD/PyvNMJ5cXTNjPtpybl0seeYXqtyyLPFxfOPLPXXCysM64rWDsJ++fR57n+vKsvFL5TPQymUSfrcf4Yrn66qtZs2YNCxcu5KmnniIuLo5nnnmm0D47duxg4sSJJCQkUKFCBQYNGkRWVpZne/4wC7vdTm7u2f/U9P3337NkyRJWrFhBQEAAHTt29NTl7+9fbOOgLcti4MCB/Otf/zpl25133sncuXMJDw/n5ptvPuXPObNnzyYlJYXVq1fjcDioU6dOoestqoLDUM7nZ1Rc2rdvz9KlS1mwYAGDBg3i73//OwMGDGDt2rUsWrSIadOmMXfuXCZNmkRoaKinh72gadOmsXLlShYsWMA111zD6tWrqVSpUonELyIilzZjDD7GlfhL6aER8MVk7969BAQE0L9/f0aOHMmaNWsACA4OJj3d9figo0ePEhgYSEhICAcOHODrr78+Z735wwkAvv76aw4fdr02Ni0tjQoVKhAQEMCWLVv4+eefT3t8y5Yt+eGHHzh06JBnrHVRzhMXF8cnn3zCwYMHAfjjjz/YuXMnADfffDNffPEFH374IXfeeecpdaalpVGlShUcDgfx8fGe4wr+TE7Wrl07z5jlrVu38vvvv9OwYcNz/pwAGjZsSHJyMr/99hsA7733Hh06dDjHUYXPPWfOHJxOJykpKSxdupQWLVqwc+dOqlatygMPPMD999/PmjVrSE1NJS8vjz59+vDCCy+wZs0aypcvT926dT0/Y8uyWLt2LQDbtm2jZcuWjB07lrCwMHbt2nXecYmIiEjpo680xWT9+vWMHDkSm82Gw+Fg6tSpgGuMbNeuXalevTrx8fE0bdqU8PBwatWqdcrwh9N59tln6du3L02aNKFNmzZceeWVAHTt2pVp06bRqFEjGjZsSKtWrU57/BVXXMFzzz1H69atCQ0NJSbm1Ddlne08jRs35oUXXqBLly7k5eXhcDiYPHkytWvXpkKFCjRq1IhNmzbRokWLU+rs168fPXr0IDIykubNmxMeHg5ApUqVaNu2LREREXTr1o2HHnrIc8ywYcMYOnQokZGR+Pj4MHPmzNPeCHk6/v7+vPPOO9x2222eGwsffPDB8zoWXF8KVqxYQXR0NMYYxo8fT7Vq1Zg1axYTJkzA4XAQFBTEu+++y549e7jnnnvIy8sD8PTUz549m6FDh/LCCy+Qk5PDnXfeSXR0NCNHjiQpKQnLsoiLiyM6Ovq84xIREZHSxxR8CsOlonnz5taqVasKlW3evJlGjRp5KSKRC6ffYRERkdLHGLPasqzmJ5drOIeIiIiISBEpiRYRERERKSIl0SIiIiIiRVSmkuhLcXy3COh3V0RE5FJTZpJof39/Dh06pGRELjmWZXHo0CH8/f29HYqIiIicpzLziLuaNWuye/duUlJSvB2KSJH5+/tTs2ZNb4chIiIi56nMJNEOh4O6det6OwwRERERuQyUmeEcIiIiIiIlRUm0iIiIiEgRKYkWERERESmiS/K138aYFGCnF05dGUj1wnnl/Kh9Si+1Temltim91Dall9qm9LoYbVPbsqywkwsvySTaW4wxq0737nQpHdQ+pZfapvRS25ReapvSS21TepVk22g4h4iIiIhIESmJFhEREREpIiXRRfOmtwOQs1L7lF5qm9JLbVN6qW1KL7VN6VVibaMx0SIiIiIiRaSeaBERERGRIlISfZ6MMV2NMb8aY34zxoz2djyXM2PMDGPMQWPMhgJlFY0x3xhjktzzCt6M8XJljKlljIk3xmwyxmw0xjziLlf7eJkxxt8Y84sxZq27bZ53l9c1xqx0f7bNMcb4ejvWy5Uxxm6M+T9jzHz3utqmlDDGJBtj1htjEo0xq9xl+lwrBYwxocaYT4wxW4wxm40xrUuqbZREnwdjjB2YDHQDGgN9jTGNvRvVZW0m0PWkstHAt5ZlNQC+da9LycsFHrMsqzHQCnjI/f+K2sf7TgCdLcuKBmKArsaYVsDLwGuWZV0FHAbu82KMl7tHgM0F1tU2pUsny7JiCjw+TZ9rpcO/gf9ZlhUOROP6f6hE2kZJ9PlpAfxmWdZ2y7KygY+AXl6O6bJlWdZS4I+TinsBs9zLs4DeJRqUAGBZ1j7Lsta4l9NxfZjVQO3jdZZLhnvV4Z4soDPwibtcbeMlxpiaQHdgunvdoLYp7fS55mXGmBCgPfA2gGVZ2ZZlHaGE2kZJ9PmpAewqsL7bXSalR1XLsva5l/cDVb0ZjIAxpg7QFFiJ2qdUcA8XSAQOAt8A24AjlmXlunfRZ5v3TAL+AeS51yuhtilNLGCxMWa1MWawu0yfa95XF0gB3nEPhZpujAmkhNpGSbSUOZbrkTN67IwXGWOCgE+BRy3LOlpwm9rHeyzLclqWFQPUxPUXtnAvhySAMeYm4KBlWau9HYuc0bWWZTXDNazzIWNM+4Ib9bnmNT5AM2CqZVlNgWOcNHTjYraNkujzsweoVWC9prtMSo8DxpgrANzzg16O57JljHHgSqBnW5b1mbtY7VOKuP/cGQ+0BkKNMT7uTfps8462QE9jTDKu4YKdcY3zVNuUEpZl7XHPDwKf4/oSqs8179sN7LYsa6V7/RNcSXWJtI2S6POTADRw3yntC9wJfOnlmKSwL4GB7uWBwBdejOWy5R7H+Taw2bKsVwtsUvt4mTEmzBgT6l4uB1yPa8x6PHCreze1jRdYlvWEZVk1Lcuqg+vfl+8sy+qH2qZUMMYEGmOC85eBLsAG9LnmdZZl7Qd2GWMauovigE2UUNvoZSvnyRhzI64xa3ZghmVZL3o5pMuWMeZDoCNQGTgAPAvMA+YCVwI7gdstyzr55kO5yIwx1wI/Auv5c2znGFzjotU+XmSMicJ1g40dVwfKXMuyxhpj6uHq/awI/B/Q37KsE96L9PJmjOkIPG5Z1k1qm9LB3Q6fu1d9gA8sy3rRGFMJfa55nTEmBtcNub7AduAe3J9xXOS2URItIiIiIlJEGs4hIiIiIlJESqJFRERERIpISbSIiIiISBEpiRYRERERKSIl0SIiIiIiRaQkWkRERESkiJREi4iIiIgUkZJoEREREZEi+n/Vch2lrfZY+AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-069b6772bddb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0m그러니까\u001b[0m \u001b[0m반드시\u001b[0m \u001b[0m밤\u001b[0m \u001b[0m열두시가\u001b[0m \u001b[0m되기\u001b[0m \u001b[0m전에\u001b[0m \u001b[0m돌아와야\u001b[0m \u001b[0m해요\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \"\"\"\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mhist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msam_wgan2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcomp_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-61-89f6093b078d>\u001b[0m in \u001b[0;36msam_wgan2\u001b[0;34m(g_summ, text, comp_rate, method, display)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m#print('-'*50)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2.9\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msim\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.65\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msam_wgan2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_summ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcomp_rate\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;31m#df,arr = evaluate('SAM+WGAN',summary_text,g_summ,org_text[0],org_text[1],org_text[2])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-89f6093b078d>\u001b[0m in \u001b[0;36msam_wgan2\u001b[0;34m(g_summ, text, comp_rate, method, display)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0msummarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mframe_expansion_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0msummary_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m#print('-'*50)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m#print('gold summary:')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-e896e1ac7f70>\u001b[0m in \u001b[0;36mget_summary\u001b[0;34m(self, count)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0;31m#print(text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_discriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransfer_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_for\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0msim_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_discriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg_text_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-c2a1d7c569e1>\u001b[0m in \u001b[0;36mtransfer_learning\u001b[0;34m(self, sentences, train_for)\u001b[0m\n\u001b[1;32m    506\u001b[0m                             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m                             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_masks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m                                 labels=b_labels)\n\u001b[0m\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;31m#print(outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1529\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1531\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1532\u001b[0m         )\n\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m         )\n\u001b[1;32m    991\u001b[0m         encoder_outputs = self.encoder(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"absolute\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "YjLQHlXbz4El",
        "outputId": "c07ee3b7-915f-4cfe-b3a2-3a3712bfd876"
      },
      "source": [
        "org_text = summary(full_text)\n",
        "print('-'*50)\n",
        "for txt in np.array(nltk.sent_tokenize(org_text.strip())):\n",
        "    print(txt)\n",
        "\n",
        "org_text = summary(org_text)\n",
        "print('-'*50)\n",
        "for txt in np.array(nltk.sent_tokenize(org_text.strip())):\n",
        "    print(txt)\n",
        "\n",
        "org_text = summary(org_text)\n",
        "print('-'*50)\n",
        "for txt in np.array(nltk.sent_tokenize(org_text.strip())):\n",
        "    print(txt)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   60/60 epochs, grammar loss:-1.1372  similarity loss:-1.5604\n",
            "--------------------------------------------------\n",
            "귀여운 여자 아기는 자라서 예쁘고 마음씨 소녀가 되었어요. 3.236619472503662 0.8181120490295744\n",
            "--------------------------------------------------\n",
            "그러던 어느날 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요. 소녀의 아버지는 홀로 남은 소녀가 걱정되었어요.\n",
            "--------------------------------------------------\n",
            "Train... |||..................| 10.0%   6/60 epochs, grammar loss:0.0070  similarity loss:-0.0130"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-14a0f31e8fdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0morg_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtxt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morg_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-b8a31f1c1d0d>\u001b[0m in \u001b[0;36msummary\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morg_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mtxt\u001b[0m \u001b[0;34m+=\u001b[0m  \u001b[0;34m' '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morg_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mhist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msam_wgan2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcomp_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0msummary_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-89f6093b078d>\u001b[0m in \u001b[0;36msam_wgan2\u001b[0;34m(g_summ, text, comp_rate, method, display)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msummarizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSAM_Summarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_discriminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms_discriminator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0msummarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mframe_expansion_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0msummary_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#print('-'*50)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-e896e1ac7f70>\u001b[0m in \u001b[0;36msummarize\u001b[0;34m(self, epochs, batch_size, frame_expansion_ratio, init_bias, learning_rate, method, display)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mframe_expansion_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minit_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_expansion_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe_expansion_ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minit_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-e896e1ac7f70>\u001b[0m in \u001b[0;36m__train\u001b[0;34m(self, epochs, batch_size, init_bias, learning_rate, method, display)\u001b[0m\n\u001b[1;32m    210\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                     \u001b[0;31m#fake_gmr_out, fake_com_out, fake_sim_out, D_z_loss, D_x_loss = self.__discrete_gradient(sw,gen_length,beta)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                     \u001b[0mfake_gmr_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_sim_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__discrete_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgen_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m                     \u001b[0;31m#print(D_z_loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                 '''\n",
            "\u001b[0;32m<ipython-input-40-e896e1ac7f70>\u001b[0m in \u001b[0;36m__discrete_gradient\u001b[0;34m(self, weights, gen_length, use_gpu, verbose)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mo_sim_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfake_text\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfake_outs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mo_sim_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_discriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg_text_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-fe99af6fcd77>\u001b[0m in \u001b[0;36msimilarity\u001b[0;34m(self, query_text, org_text_emb)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morg_text_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mqueries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mquery_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embedder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;31m#query_embeddings = self._embedder.encode(queries,show_progress_bar=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m#print(queries)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m                 \u001b[0mout_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0moutput_value\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'token_embeddings'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sentence_transformers/models/Transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mtrans_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_type_ids'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_type_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0moutput_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrans_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0moutput_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m         )\n\u001b[1;32m    844\u001b[0m         encoder_outputs = self.encoder(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0membeddings\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mposition_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         return F.layer_norm(\n\u001b[0;32m--> 174\u001b[0;31m             input, self.normalized_shape, self.weight, self.bias, self.eps)\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2344\u001b[0m             \u001b[0mlayer_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2345\u001b[0m         )\n\u001b[0;32m-> 2346\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiByPXpKSMam"
      },
      "source": [
        "\n",
        "for i in range(3):\n",
        "    org_text = summary(full_text,full_text)\n",
        "    print('-'*50)\n",
        "    for txt in np.array(nltk.sent_tokenize(org_text.strip())):\n",
        "        print(txt)\n",
        "\n",
        "    org_text = summary(full_text,org_text)\n",
        "    print('-'*50)\n",
        "    for txt in np.array(nltk.sent_tokenize(org_text.strip())):\n",
        "        print(txt)\n",
        "\n",
        "    org_text = summary(full_text,org_text)\n",
        "    print('-'*50)\n",
        "    for txt in np.array(nltk.sent_tokenize(org_text.strip())):\n",
        "        print(txt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aX5yhTzMpwP"
      },
      "source": [
        "## Building the Model\n",
        "\n",
        "Next, we'll build the model. Like previous notebooks it is made up of an *encoder* and a *decoder*, with the encoder *encoding* the input/source sentence (in German) into *context vector* and the decoder then *decoding* this context vector to output our output/target sentence (in English). \n",
        "\n",
        "### Encoder\n",
        "\n",
        "Similar to the ConvSeq2Seq model, the Transformer's encoder does not attempt to compress the entire source sentence, $X = (x_1, ... ,x_n)$, into a single context vector, $z$. Instead it produces a sequence of context vectors, $Z = (z_1, ... , z_n)$. So, if our input sequence was 5 tokens long we would have $Z = (z_1, z_2, z_3, z_4, z_5)$. Why do we call this a sequence of context vectors and not a sequence of hidden states? A hidden state at time $t$ in an RNN has only seen tokens $x_t$ and all the tokens before it. However, each context vector here has seen all tokens at all positions within the input sequence.\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/blob/master/assets/transformer-encoder.png?raw=1)\n",
        "\n",
        "First, the tokens are passed through a standard embedding layer. Next, as the model has no recurrent it has no idea about the order of the tokens within the sequence. We solve this by using a second embedding layer called a *positional embedding layer*. This is a standard embedding layer where the input is not the token itself but the position of the token within the sequence, starting with the first token, the `<sos>` (start of sequence) token, in position 0. The position embedding has a \"vocabulary\" size of 100, which means our model can accept sentences up to 100 tokens long. This can be increased if we want to handle longer sentences.\n",
        "\n",
        "The original Transformer implementation from the Attention is All You Need paper does not learn positional embeddings. Instead it uses a fixed static embedding. Modern Transformer architectures, like BERT, use positional embeddings instead, hence we have decided to use them in these tutorials. Check out [this](http://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding) section to read more about the positional embeddings used in the original Transformer model.\n",
        "\n",
        "Next, the token and positional embeddings are elementwise summed together to get a vector which contains information about the token and also its position with in the sequence. However, before they are summed, the token embeddings are multiplied by a scaling factor which is $\\sqrt{d_{model}}$, where $d_{model}$ is the hidden dimension size, `hid_dim`. This supposedly reduces variance in the embeddings and the model is difficult to train reliably without this scaling factor. Dropout is then applied to the combined embeddings.\n",
        "\n",
        "The combined embeddings are then passed through $N$ *encoder layers* to get $Z$, which is then output and can be used by the decoder.\n",
        "\n",
        "The source mask, `src_mask`, is simply the same shape as the source sentence but has a value of 1 when the token in the source sentence is not a `<pad>` token and 0 when it is a `<pad>` token. This is used in the encoder layers to mask the multi-head attention mechanisms, which are used to calculate and apply attention over the source sentence, so the model does not pay attention to `<pad>` tokens, which contain no useful information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D36jLeOKMrRk"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim,\n",
        "                 dropout, \n",
        "                 device,\n",
        "                 max_length = 100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim,\n",
        "                                                  dropout, \n",
        "                                                  device) \n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        \n",
        "        batch_size = src.shape[0]\n",
        "        src_len = src.shape[1]\n",
        "        \n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        \n",
        "        #pos = [batch size, src len]\n",
        "        \n",
        "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "            \n",
        "        #src = [batch size, src len, hid dim]\n",
        "            \n",
        "        return src"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V92yOkpQMzrc"
      },
      "source": [
        "### Encoder Layer\n",
        "\n",
        "The encoder layers are where all of the \"meat\" of the encoder is contained. We first pass the source sentence and its mask into the *multi-head attention layer*, then perform dropout on it, apply a residual connection and pass it through a [Layer Normalization](https://arxiv.org/abs/1607.06450) layer. We then pass it through a *position-wise feedforward* layer and then, again, apply dropout, a residual connection and then layer normalization to get the output of this layer which is fed into the next layer. The parameters are not shared between layers. \n",
        "\n",
        "The mutli head attention layer is used by the encoder layer to attend to the source sentence, i.e. it is calculating and applying attention over itself instead of another sequence, hence we call it *self attention*.\n",
        "\n",
        "[This](https://mlexplained.com/2018/01/13/weight-normalization-and-layer-normalization-explained-normalization-in-deep-learning-part-2/) article goes into more detail about layer normalization, but the gist is that it normalizes the values of the features, i.e. across the hidden dimension, so each feature has a mean of 0 and a standard deviation of 1. This allows neural networks with a larger number of layers, like the Transformer, to be trained easier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1SN1XHqM0Q_"
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim,  \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
        "                                                                     pf_dim, \n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        #src_mask = [batch size, 1, 1, src len] \n",
        "                \n",
        "        #self attention\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        #positionwise feedforward\n",
        "        _src = self.positionwise_feedforward(src)\n",
        "        \n",
        "        #dropout, residual and layer norm\n",
        "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        return src"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMup-v5jM37K"
      },
      "source": [
        "### Mutli Head Attention Layer\n",
        "\n",
        "One of the key, novel concepts introduced by the Transformer paper is the *multi-head attention layer*. \n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/blob/master/assets/transformer-attention.png?raw=1)\n",
        "\n",
        "Attention can be though of as *queries*, *keys* and *values* - where the query is used with the key to get an attention vector (usually the output of a *softmax* operation and has all values between 0 and 1 which sum to 1) which is then used to get a weighted sum of the values.\n",
        "\n",
        "The Transformer uses *scaled dot-product attention*, where the query and key are combined by taking the dot product between them, then applying the softmax operation and scaling by $d_k$ before finally then multiplying by the value. $d_k$ is the *head dimension*, `head_dim`, which we will shortly explain further.\n",
        "\n",
        "$$ \\text{Attention}(Q, K, V) = \\text{Softmax} \\big( \\frac{QK^T}{\\sqrt{d_k}} \\big)V $$ \n",
        "\n",
        "This is similar to standard *dot product attention* but is scaled by $d_k$, which the paper states is used to stop the results of the dot products growing large, causing gradients to become too small.\n",
        "\n",
        "However, the scaled dot-product attention isn't simply applied to the queries, keys and values. Instead of doing a single attention application the queries, keys and values have their `hid_dim` split into $h$ *heads* and the scaled dot-product attention is calculated over all heads in parallel. This means instead of paying attention to one concept per attention application, we pay attention to $h$. We then re-combine the heads into their `hid_dim` shape, thus each `hid_dim` is potentially paying attention to $h$ different concepts.\n",
        "\n",
        "$$ \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1,...,\\text{head}_h)W^O $$\n",
        "\n",
        "$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) $$\n",
        "\n",
        "$W^O$ is the linear layer applied at the end of the multi-head attention layer, `fc`. $W^Q, W^K, W^V$ are the linear layers `fc_q`, `fc_k` and `fc_v`.\n",
        "\n",
        "Walking through the module, first we calculate $QW^Q$, $KW^K$ and $VW^V$ with the linear layers, `fc_q`, `fc_k` and `fc_v`, to give us `Q`, `K` and `V`. Next, we split the `hid_dim` of the query, key and value into `n_heads` using `.view` and correctly permute them so they can be multiplied together. We then calculate the `energy` (the un-normalized attention) by multiplying `Q` and `K` together and scaling it by the square root of `head_dim`, which is calulated as `hid_dim // n_heads`. We then mask the energy so we do not pay attention over any elements of the sequeuence we shouldn't, then apply the softmax and dropout. We then apply the attention to the value heads, `V`, before combining the `n_heads` together. Finally, we multiply this $W^O$, represented by `fc_o`. \n",
        "\n",
        "Note that in our implementation the lengths of the keys and values are always the same, thus when matrix multiplying the output of the softmax, `attention`, with `V` we will always have valid dimension sizes for matrix multiplication. This multiplication is carried out using `torch.matmul` which, when both tensors are >2-dimensional, does a batched matrix multiplication over the last two dimensions of each tensor. This will be a **[query len, key len] x [value len, head dim]** batched matrix multiplication over the batch size and each head which provides the **[batch size, n heads, query len, head dim]** result.\n",
        "\n",
        "One thing that looks strange at first is that dropout is applied directly to the attention. This means that our attention vector will most probably not sum to 1 and we may pay full attention to a token but the attention over that token is set to 0 by dropout. This is never explained, or even mentioned, in the paper however is used by the [official implementation](https://github.com/tensorflow/tensor2tensor/) and every Transformer implementation since, [including BERT](https://github.com/google-research/bert/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBNy91EuM6ZA"
      },
      "source": [
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        assert hid_dim % n_heads == 0\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = hid_dim // n_heads\n",
        "        \n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "        \n",
        "    def forward(self, query, key, value, mask = None):\n",
        "        \n",
        "        batch_size = query.shape[0]\n",
        "        \n",
        "        #query = [batch size, query len, hid dim]\n",
        "        #key = [batch size, key len, hid dim]\n",
        "        #value = [batch size, value len, hid dim]\n",
        "                \n",
        "        Q = self.fc_q(query)\n",
        "        K = self.fc_k(key)\n",
        "        V = self.fc_v(value)\n",
        "        \n",
        "        #Q = [batch size, query len, hid dim]\n",
        "        #K = [batch size, key len, hid dim]\n",
        "        #V = [batch size, value len, hid dim]\n",
        "                \n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        \n",
        "        #Q = [batch size, n heads, query len, head dim]\n",
        "        #K = [batch size, n heads, key len, head dim]\n",
        "        #V = [batch size, n heads, value len, head dim]\n",
        "                \n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "        \n",
        "        #energy = [batch size, n heads, query len, key len]\n",
        "        \n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\n",
        "        \n",
        "        attention = torch.softmax(energy, dim = -1)\n",
        "                \n",
        "        #attention = [batch size, n heads, query len, key len]\n",
        "                \n",
        "        x = torch.matmul(self.dropout(attention), V)\n",
        "        \n",
        "        #x = [batch size, n heads, query len, head dim]\n",
        "        \n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        \n",
        "        #x = [batch size, query len, n heads, head dim]\n",
        "        \n",
        "        x = x.view(batch_size, -1, self.hid_dim)\n",
        "        \n",
        "        #x = [batch size, query len, hid dim]\n",
        "        \n",
        "        x = self.fc_o(x)\n",
        "        \n",
        "        #x = [batch size, query len, hid dim]\n",
        "        \n",
        "        return x, attention"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lNtAnGwM-HF"
      },
      "source": [
        "### Position-wise Feedforward Layer\n",
        "\n",
        "The other main block inside the encoder layer is the *position-wise feedforward layer* This is relatively simple compared to the multi-head attention layer. The input is transformed from `hid_dim` to `pf_dim`, where `pf_dim` is usually a lot larger than `hid_dim`. The original Transformer used a `hid_dim` of 512 and a `pf_dim` of 2048. The ReLU activation function and dropout are applied before it is transformed back into a `hid_dim` representation. \n",
        "\n",
        "Why is this used? Unfortunately, it is never explained in the paper.\n",
        "\n",
        "BERT uses the [GELU](https://arxiv.org/abs/1606.08415) activation function, which can be used by simply switching `torch.relu` for `F.gelu`. Why did they use GELU? Again, it is never explained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NovvsMMTNAz7"
      },
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
        "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "        \n",
        "        #x = [batch size, seq len, pf dim]\n",
        "        \n",
        "        x = self.fc_2(x)\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzXU89vdNDAc"
      },
      "source": [
        "### Decoder\n",
        "\n",
        "The objective of the decoder is to take the encoded representation of the source sentence, $Z$, and convert it into predicted tokens in the target sentence, $\\hat{Y}$. We then compare $\\hat{Y}$ with the actual tokens in the target sentence, $Y$, to calculate our loss, which will be used to calculate the gradients of our parameters and then use our optimizer to update our weights in order to improve our predictions. \n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/blob/master/assets/transformer-decoder.png?raw=1)\n",
        "\n",
        "The decoder is similar to encoder, however it now has two multi-head attention layers. A *masked multi-head attention layer* over the target sequence, and a multi-head attention layer which uses the decoder representation as the query and the encoder representation as the key and value.\n",
        "\n",
        "The decoder uses positional embeddings and combines - via an elementwise sum - them with the scaled embedded target tokens, followed by dropout. Again, our positional encodings have a \"vocabulary\" of 100, which means they can accept sequences up to 100 tokens long. This can be increased if desired.\n",
        "\n",
        "The combined embeddings are then passed through the $N$ decoder layers, along with the encoded source, `enc_src`, and the source and target masks. Note that the number of layers in the encoder does not have to be equal to the number of layers in the decoder, even though they are both denoted by $N$.\n",
        "\n",
        "The decoder representation after the $N^{th}$ layer is then passed through a linear layer, `fc_out`. In PyTorch, the softmax operation is contained within our loss function, so we do not explicitly need to use a softmax layer here.\n",
        "\n",
        "As well as using the source mask, as we did in the encoder to prevent our model attending to `<pad>` tokens, we also use a target mask. This will be explained further in the `Seq2Seq` model which encapsulates both the encoder and decoder, but the gist of it is that it performs a similar operation as the decoder padding in the convolutional sequence-to-sequence model. As we are processing all of the target tokens at once in parallel we need a method of stopping the decoder from \"cheating\" by simply \"looking\" at what the next token in the target sequence is and outputting it. \n",
        "\n",
        "Our decoder layer also outputs the normalized attention values so we can later plot them to see what our model is actually paying attention to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6VBqriTNFfD"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 output_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 dropout, \n",
        "                 device,\n",
        "                 max_length = 100):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim, \n",
        "                                                  dropout, \n",
        "                                                  device)\n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch size, trg len]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "                \n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "                            \n",
        "        #pos = [batch size, trg len]\n",
        "            \n",
        "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
        "                \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        output = self.fc_out(trg)\n",
        "        \n",
        "        #output = [batch size, trg len, output dim]\n",
        "            \n",
        "        return output, attention"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUg6kYKONJ6d"
      },
      "source": [
        "### Decoder Layer\n",
        "\n",
        "As mentioned previously, the decoder layer is similar to the encoder layer except that it now has two multi-head attention layers, `self_attention` and `encoder_attention`. \n",
        "\n",
        "The first performs self-attention, as in the encoder, by using the decoder representation so far as the query, key and value. This is followed by dropout, residual connection and layer normalization. This `self_attention` layer uses the target sequence mask, `trg_mask`, in order to prevent the decoder from \"cheating\" by paying attention to tokens that are \"ahead\" of the one it is currently processing as it processes all tokens in the target sentence in parallel.\n",
        "\n",
        "The second is how we actually feed the encoded source sentence, `enc_src`, into our decoder. In this multi-head attention layer the queries are the decoder representations and the keys and values are the encoder representations. Here, the source mask, `src_mask` is used to prevent the multi-head attention layer from attending to `<pad>` tokens within the source sentence. This is then followed by the dropout, residual connection and layer normalization layers. \n",
        "\n",
        "Finally, we pass this through the position-wise feedforward layer and yet another sequence of dropout, residual connection and layer normalization.\n",
        "\n",
        "The decoder layer isn't introducing any new concepts, just using the same set of layers as the encoder in a slightly different way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrZFQxJzNKdt"
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
        "                                                                     pf_dim, \n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        \n",
        "        #self attention\n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
        "            \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "            \n",
        "        #encoder attention\n",
        "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
        "                    \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        \n",
        "        #positionwise feedforward\n",
        "        _trg = self.positionwise_feedforward(trg)\n",
        "        \n",
        "        #dropout, residual and layer norm\n",
        "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return trg, attention"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyCDCCjVNMhC"
      },
      "source": [
        "### Seq2Seq\n",
        "\n",
        "Finally, we have the `Seq2Seq` module which encapsulates the encoder and decoder, as well as handling the creation of the masks.\n",
        "\n",
        "The source mask is created by checking where the source sequence is not equal to a `<pad>` token. It is 1 where the token is not a `<pad>` token and 0 when it is. It is then unsqueezed so it can be correctly broadcast when applying the mask to the `energy`, which of shape **_[batch size, n heads, seq len, seq len]_**.\n",
        "\n",
        "The target mask is slightly more complicated. First, we create a mask for the `<pad>` tokens, as we did for the source mask. Next, we create a \"subsequent\" mask, `trg_sub_mask`, using `torch.tril`. This creates a diagonal matrix where the elements above the diagonal will be zero and the elements below the diagonal will be set to whatever the input tensor is. In this case, the input tensor will be a tensor filled with ones. So this means our `trg_sub_mask` will look something like this (for a target with 5 tokens):\n",
        "\n",
        "$$\\begin{matrix}\n",
        "1 & 0 & 0 & 0 & 0\\\\\n",
        "1 & 1 & 0 & 0 & 0\\\\\n",
        "1 & 1 & 1 & 0 & 0\\\\\n",
        "1 & 1 & 1 & 1 & 0\\\\\n",
        "1 & 1 & 1 & 1 & 1\\\\\n",
        "\\end{matrix}$$\n",
        "\n",
        "This shows what each target token (row) is allowed to look at (column). The first target token has a mask of **_[1, 0, 0, 0, 0]_** which means it can only look at the first target token. The second target token has a mask of **_[1, 1, 0, 0, 0]_** which it means it can look at both the first and second target tokens. \n",
        "\n",
        "The \"subsequent\" mask is then logically anded with the padding mask, this combines the two masks ensuring both the subsequent tokens and the padding tokens cannot be attended to. For example if the last two tokens were `<pad>` tokens the mask would look like:\n",
        "\n",
        "$$\\begin{matrix}\n",
        "1 & 0 & 0 & 0 & 0\\\\\n",
        "1 & 1 & 0 & 0 & 0\\\\\n",
        "1 & 1 & 1 & 0 & 0\\\\\n",
        "1 & 1 & 1 & 0 & 0\\\\\n",
        "1 & 1 & 1 & 0 & 0\\\\\n",
        "\\end{matrix}$$\n",
        "\n",
        "After the masks are created, they used with the encoder and decoder along with the source and target sentences to get our predicted target sentence, `output`, along with the decoder's attention over the source sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uu0Q85mENQI2"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, \n",
        "                 encoder, \n",
        "                 decoder, \n",
        "                 src_pad_idx, \n",
        "                 trg_pad_idx, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "        \n",
        "    def make_src_mask(self, src):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        \n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "\n",
        "        return src_mask\n",
        "    \n",
        "    def make_trg_mask(self, trg):\n",
        "        \n",
        "        #trg = [batch size, trg len]\n",
        "        \n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        \n",
        "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
        "        \n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
        "        \n",
        "        #trg_sub_mask = [trg len, trg len]\n",
        "            \n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "        \n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        \n",
        "        return trg_mask\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #trg = [batch size, trg len]\n",
        "                \n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        \n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        \n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        \n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "                \n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        #output = [batch size, trg len, output dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return output, attention"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfbu36-9NTLx"
      },
      "source": [
        "## Training the Seq2Seq Model\n",
        "\n",
        "We can now define our encoder and decoders. This model is significantly smaller than Transformers used in research today, but is able to be run on a single GPU quickly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNq0TGgZNWwc"
      },
      "source": [
        "#INPUT_DIM = len(SRC.vocab)\n",
        "#OUTPUT_DIM = len(TRG.vocab)\n",
        "INPUT_DIM = g_discriminator.tokenizer.vocab_size\n",
        "OUTPUT_DIM = g_discriminator.tokenizer.vocab_size\n",
        "HID_DIM = 256\n",
        "ENC_LAYERS = 3\n",
        "DEC_LAYERS = 3\n",
        "ENC_HEADS = 8\n",
        "DEC_HEADS = 8\n",
        "ENC_PF_DIM = 512\n",
        "DEC_PF_DIM = 512\n",
        "ENC_DROPOUT = 0.1\n",
        "DEC_DROPOUT = 0.1\n",
        "\n",
        "enc = Encoder(INPUT_DIM, \n",
        "              HID_DIM, \n",
        "              ENC_LAYERS, \n",
        "              ENC_HEADS, \n",
        "              ENC_PF_DIM, \n",
        "              ENC_DROPOUT, \n",
        "              device)\n",
        "\n",
        "dec = Decoder(OUTPUT_DIM, \n",
        "              HID_DIM, \n",
        "              DEC_LAYERS, \n",
        "              DEC_HEADS, \n",
        "              DEC_PF_DIM, \n",
        "              DEC_DROPOUT, \n",
        "              device)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Acf7EyDTNZxV"
      },
      "source": [
        "#SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
        "#TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "SRC_PAD_IDX = g_discriminator.tokenizer.pad_token_id\n",
        "TRG_PAD_IDX = g_discriminator.tokenizer.pad_token_id\n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iut5ELoUNebL",
        "outputId": "1ee0f85a-5aec-4b82-b4ba-47f782e136d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 10,158,402 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87ZPuXTENi8m"
      },
      "source": [
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMY99qESNlLT"
      },
      "source": [
        "model.apply(initialize_weights);"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMJUzyXUNmsG"
      },
      "source": [
        "LEARNING_RATE = 0.0005\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Izr0cUh6NodJ"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4T9eiriNqTJ"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "        rwd = batch.rwd\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output, _ = model(src, trg[:,:-1])\n",
        "                \n",
        "        #output = [batch size, trg len - 1, output dim]\n",
        "        #trg = [batch size, trg len]\n",
        "            \n",
        "        output_dim = output.shape[-1]\n",
        "            \n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "                \n",
        "        #output = [batch size * trg len - 1, output dim]\n",
        "        #trg = [batch size * trg len - 1]\n",
        "            \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U79np8MYNwCY"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output, _ = model(src, trg[:,:-1])\n",
        "            \n",
        "            #output = [batch size, trg len - 1, output dim]\n",
        "            #trg = [batch size, trg len]\n",
        "            \n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "            \n",
        "            #output = [batch size * trg len - 1, output dim]\n",
        "            #trg = [batch size * trg len - 1]\n",
        "            \n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ie3cYfWgNyJD"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4_LSAVfUuCO"
      },
      "source": [
        "# dataset 만들기..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1SOW6LEUwHk",
        "outputId": "3254cffe-dd9a-4620-b80b-71354cdfd3dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "class Batch:\n",
        "  src = []\n",
        "  trg = []\n",
        "  rwd = []\n",
        "\n",
        "max_length = 128\n",
        "batch_size = 64\n",
        "dataset_iterator = []\n",
        "batch_counter = 0\n",
        "\n",
        "\n",
        "org_sentences = np.array(nltk.sent_tokenize(full_text.strip()))\n",
        "summary_text = []\n",
        "for i in range(0,len(org_sentences),2):\n",
        "    txt = org_sentences[i]\n",
        "    if i < len(org_sentences)-1:\n",
        "        txt +=  ' [SEP] ' + org_sentences[i+1]\n",
        "\n",
        "    source = Source(txt)\n",
        "    comp_rate = 0.4\n",
        "    source.set_key_rate(s_discriminator,comp_rate=comp_rate)\n",
        "    summarizer = SAM_Summarizer(g_discriminator,s_discriminator)\n",
        "    summarizer.ready(source)\n",
        "    hist = summarizer.summarize(epochs=50,batch_size=4,frame_expansion_ratio = 0.0, init_bias=0.0,learning_rate=5e-3,method=2,display=False)\n",
        "    samples = summarizer.get_samples(batch_size)\n",
        "\n",
        "\n",
        "    nSRC = []\n",
        "    nTRG = []\n",
        "    nRWD = []\n",
        "    src_max = 0\n",
        "    trg_max = 0\n",
        "    batch = Batch()\n",
        "    batch.src = []\n",
        "    batch.trg = []\n",
        "    batch.rwd = []\n",
        "\n",
        "    percent = (\"{0:.2f}\").format(100 * ((i+2) / float(len(org_sentences))))\n",
        "    print(f'{percent}% {i+2}/{str(len(org_sentences))}')\n",
        "\n",
        "    for (combined_txt,g,s) in samples:\n",
        "        srct = g_discriminator.tokenizer.convert_tokens_to_ids(['[CLS]']+g_discriminator.tokenizer.tokenize(txt.strip())+['[SEP]'])\n",
        "        trgt = g_discriminator.tokenizer.convert_tokens_to_ids(['[CLS]']+g_discriminator.tokenizer.tokenize(combined_txt.strip())+['[SEP]'])\n",
        "        rwd = int(((g - 2.9) + s * 4 - 2.4) * 5) # (2.9 - 2.9) + (0.6*4-2.4) = 0\n",
        "        if len(srct) < max_length and len(trgt) < max_length:\n",
        "            #if len(trgt) > trg_max:\n",
        "            #    trg_max = len(trgt)            \n",
        "            nTRG.append(trgt)\n",
        "            #if len(srct) > src_max:\n",
        "            #    src_max = len(srct)\n",
        "            nSRC.append(srct)\n",
        "            nRWD.append(rwd)\n",
        "\n",
        "    for s in nSRC:\n",
        "        ss = s\n",
        "        if len(s) < max_length:\n",
        "            ss += [1 for i in range(max_length-len(s))]\n",
        "        batch.src.append(ss)\n",
        "\n",
        "    for t in nTRG:\n",
        "        tt = t\n",
        "        if len(t) < max_length:        \n",
        "            tt += [1 for i in range(max_length-len(t))]\n",
        "        batch.trg.append(tt)\n",
        "\n",
        "    #print(len(batch.src),len(batch.src[0]))\n",
        "    #print(len(batch.trg),len(batch.trg[0]))\n",
        "\n",
        "    batch.src = torch.tensor(batch.src).to(device)\n",
        "    batch.trg = torch.tensor(batch.trg).to(device)\n",
        "    batch.rwd = torch.tensor(nRWD).to(device)\n",
        "    \n",
        "    dataset_iterator.append(batch)\n",
        "\n"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "옛날 어느 집에 귀여운 여자 아기가 태어났어요. [SEP] 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs, grammar loss:-0.0910  similarity loss:-0.1042\n",
            "4.26% 2/47\n",
            "--------------------------------------------------\n",
            "그러던 어느날 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요. [SEP] 소녀의 아버지는 홀로 남은 소녀가 걱정되었어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs, grammar loss:-1.0006  similarity loss:-1.2395\n",
            "8.51% 4/47\n",
            "--------------------------------------------------\n",
            "그래서 얼마 지나서 새어머니를 맞이했어요. [SEP] 새어머니는 소녀보다 나이가 위인 두명의 딸을 데리고 왔어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs, grammar loss:-0.3771  similarity loss:-0.6739\n",
            "12.77% 6/47\n",
            "--------------------------------------------------\n",
            "그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요. [SEP] 새어머니는 소녀가 자기 딸들보다 예쁘고 착한게 못마땅했어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs, grammar loss:-1.1364  similarity loss:-1.3633\n",
            "17.02% 8/47\n",
            "--------------------------------------------------\n",
            "그런데 이번에는 아버지마저 돌아가셨어요. [SEP] 소녀는 쓸고 닦고 하녀처럼 하루 종일 집안일을 도맡아 했어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs, grammar loss:-1.1276  similarity loss:-0.8598\n",
            "21.28% 10/47\n",
            "--------------------------------------------------\n",
            "집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시 쉬곤 했어요. [SEP] 그러던 어느날 왕궁에서 무도회가 열렸어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs, grammar loss:0.0038  similarity loss:-0.0028\n",
            "25.53% 12/47\n",
            "--------------------------------------------------\n",
            "신데렐라의 집에도 무도회 초대장이 왔어요. [SEP] 새어머니는 언니들을 데리고 무도회장으로 떠났어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs, grammar loss:-0.2521  similarity loss:-0.2879\n",
            "29.79% 14/47\n",
            "--------------------------------------------------\n",
            "신데렐라도 무도회에 가고 싶었어요. [SEP] 혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs, grammar loss:-0.6709  similarity loss:-0.4740\n",
            "34.04% 16/47\n",
            "--------------------------------------------------\n",
            "그때 어디선가 마법사 할머니가 나타났어요. [SEP] 신데렐라가 고개를 들어보니 마법사 할머니가 빙그레 웃고 있었어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs, grammar loss:-0.6611  similarity loss:-0.7869\n",
            "38.30% 18/47\n",
            "--------------------------------------------------\n",
            "할머니는 소녀를 무도회에 보내줄테니 호박 한개와 생쥐 두마리 도마뱀을 가지고 오라 했어요. [SEP] 마법사 할머니가 이것들을 보면서 주문을 외웠어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs, grammar loss:-0.0029  similarity loss:0.0022\n",
            "42.55% 20/47\n",
            "--------------------------------------------------\n",
            "그리고 지팡이로 호박을 건드리자 호박이 화려한 황금마차로 변했어요. [SEP] 이번에는 생쥐와 도마뱀을 건드렸어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs, grammar loss:-0.9605  similarity loss:-0.7603\n",
            "46.81% 22/47\n",
            "--------------------------------------------------\n",
            "그랬더니 생쥐는 흰말로 도마뱀은 멋진 마부로 변했어요. [SEP] 신데렐라의 낡은 옷은 구슬 장식이 반짝이는 예쁜 드레스로 바뀌었어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs, grammar loss:0.0021  similarity loss:-0.0023\n",
            "51.06% 24/47\n",
            "--------------------------------------------------\n",
            "할머니는 신데렐라에게 반짝반짝 빛나는 유리구두를 신겨 주었어요. [SEP] 그리고 밤 열두시가 되면 모든게 처음대로 돌아간다고 알려주었어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs, grammar loss:0.0026  similarity loss:-0.0027\n",
            "55.32% 26/47\n",
            "--------------------------------------------------\n",
            "황금마차는 호박으로 흰말은 생쥐로 마부는 도마뱀으로 변하게 돼어요. [SEP] 그러니까 반드시 밤 열두시가 되기 전에 돌아와야 해요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs, grammar loss:-0.0659  similarity loss:-0.0445\n",
            "59.57% 28/47\n",
            "--------------------------------------------------\n",
            "신데렐라는 황금마차를 타고 왕궁 무도회장으로 가서 멋진 왕자님을 만났어요. [SEP] 왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs, grammar loss:-0.0090  similarity loss:-0.0185\n",
            "63.83% 30/47\n",
            "--------------------------------------------------\n",
            "왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고 신데렐라하고만 춤을 추었어요. [SEP] 신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs, grammar loss:-1.0000  similarity loss:-1.4964\n",
            "68.09% 32/47\n",
            "--------------------------------------------------\n",
            "어느덧 시간이 흘러 열두시가 되었어요. [SEP] 벽시계의 열두시를 알리는 종소리에 신데렐라는 화들짝 놀랐어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs, grammar loss:-0.0036  similarity loss:0.0012\n",
            "72.34% 34/47\n",
            "--------------------------------------------------\n",
            "신데렐라가 허둥지둥 왕궁을 빠져나가는데 유리구두 한짝이 벗겨졌어요. [SEP] 하지만 구두를 주울 시간이 없었어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs, grammar loss:-0.0015  similarity loss:0.0024\n",
            "76.60% 36/47\n",
            "--------------------------------------------------\n",
            "신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리구두 한짝을 주웠어요. [SEP] 왕자님은 유리구두를 가지고 임금님께 가서 말했어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs, grammar loss:-1.0476  similarity loss:-1.3353\n",
            "80.85% 38/47\n",
            "--------------------------------------------------\n",
            "이 유리구두의 주인과 결혼하겠어요. [SEP] 그래서 신하들은 유리구두의 주인을 찾아 온 나라를 돌아다녔어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs, grammar loss:-0.0014  similarity loss:0.0016\n",
            "85.11% 40/47\n",
            "--------------------------------------------------\n",
            "드디어 신데렐라의 집에까지 신하들이 도착했어요. [SEP] 언니들은 발을 오므려도 보고 구두를 늘려도 보았지만 한눈에 보기에도 유리구두는 너무 작았어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs, grammar loss:-1.1575  similarity loss:-1.0026\n",
            "89.36% 42/47\n",
            "--------------------------------------------------\n",
            "그때 신데렐라가 조용히 다가와 자기도 한번 신어보게 해달라고 부탁했어요. [SEP] 신데렐라는 신하에게서 받은 유리구두를 신었어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs, grammar loss:-1.1855  similarity loss:-1.6224\n",
            "93.62% 44/47\n",
            "--------------------------------------------------\n",
            "유리구두는 신데렐라의 발에 꼭 맞았어요. [SEP] 신하들은 신데렐라를 왕궁으로 데리고 갔어요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs, grammar loss:-0.0003  similarity loss:0.0005\n",
            "97.87% 46/47\n",
            "--------------------------------------------------\n",
            "그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs, grammar loss:-1.0754  similarity loss:-1.3532\n",
            "102.13% 48/47\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPPxsSzdiAK9",
        "outputId": "4cceaf0f-51d1-461f-922c-fdace5e2960a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dataset_iterator[6].rwd"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-10,   0, -10, -10, -10, -10, -10, -10,  -2, -10, -10, -10, -10, -10,\n",
              "          5,   0, -10, -10, -10, -10, -10, -10, -10, -10, -10, -10, -10, -10,\n",
              "          0, -10,   0, -10,   0,  -2, -10,   0, -10,   2, -10,  -2, -10,  -2,\n",
              "          0,   4, -10,   0, -10, -10,  -2, -10,   0, -10,   0, -10, -10,  -2,\n",
              "        -10, -10, -10, -10, -10,   0, -10,  -2], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjf1dkILOa9s"
      },
      "source": [
        "# Train !!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIA-I7YGN3HS",
        "outputId": "4808e049-f8d8-4c17-c014-0b094ac06958",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "N_EPOCHS = 100\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, dataset_iterator, optimizer, criterion, CLIP)\n",
        "    #valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    '''\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut6-model.pt')\n",
        "    '''\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    #print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-138-10284a5b4603>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m#valid_loss = evaluate(model, valid_iterator, criterion)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-104-574e1240b758>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m#output = [batch size, trg len - 1, output dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-f2328c072cab>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m#enc_src = [batch size, src len, hid dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_src\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m#output = [batch size, trg len, output dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-4aa4dec5117c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, trg, enc_src, trg_mask, src_mask)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mtrg_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m#pos = [batch size, trg len]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJOaw-5KOsbl"
      },
      "source": [
        "def complete_sentence(sentence, model, device, max_len = 50):\n",
        "    \n",
        "    model.eval()\n",
        "    '''\n",
        "    if isinstance(sentence, str):\n",
        "        nlp = spacy.load('de_core_news_sm')\n",
        "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "    '''\n",
        "    tokens = ['[CLS]']+g_discriminator.tokenizer.tokenize(sentence.strip())+['[SEP]']\n",
        "    #tokens = [g_discriminator.tokenizer.bos_token] + tokens + [g_discriminator.tokenizer.eos_token]\n",
        "    \n",
        "\n",
        "    src_indexes = [ g_discriminator.tokenizer.convert_tokens_to_ids(token) for token in tokens]\n",
        "    #src_indexes = get_stoi(tk,sentence)\n",
        "    print(src_indexes)\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "    print(src_tensor)\n",
        "    src_mask = model.make_src_mask(src_tensor)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\n",
        "\n",
        "    trg_indexes = [g_discriminator.tokenizer.bos_token_id]\n",
        "    \n",
        "\n",
        "    for i in range(max_len):\n",
        "        print('trg_indexes',trg_indexes)\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "        trg_mask = model.make_trg_mask(trg_tensor)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        pred_token = output.argmax(2)[:,-1].item()\n",
        "        \n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == g_discriminator.tokenizer.eos_token_id: #tk.stoi['<eos>']: #.eos_token_id: # trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "            break\n",
        "    \n",
        "    trg_tokens = g_discriminator.tokenizer.convert_ids_to_tokens(trg_indexes) # :[tk.itos[i] for i in trg_indexes]\n",
        "    \n",
        "    return trg_tokens[1:], attention"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzaoxF3ZnC1n",
        "outputId": "cb700041-00af-4fee-a8ee-a98c7198abf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "trg, _ = complete_sentence('옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.',model,device)\n",
        "print(' '.join(trg))"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 3416, 5664, 3222, 4384, 6896, 1176, 3318, 3093, 5562, 4720, 6855, 5671, 6857, 54, 3093, 5568, 2095, 6039, 6228, 6039, 3886, 6003, 6553, 3403, 5439, 1917, 6785, 993, 7010, 2826, 5330, 517, 5895, 6857, 54, 0]\n",
            "tensor([[   0, 3416, 5664, 3222, 4384, 6896, 1176, 3318, 3093, 5562, 4720, 6855,\n",
            "         5671, 6857,   54, 3093, 5568, 2095, 6039, 6228, 6039, 3886, 6003, 6553,\n",
            "         3403, 5439, 1917, 6785,  993, 7010, 2826, 5330,  517, 5895, 6857,   54,\n",
            "            0]], device='cuda:0')\n",
            "trg_indexes [None]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-122-289c26ea3d63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomplete_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-121-c8cd747431d4>\u001b[0m in \u001b[0;36mcomplete_sentence\u001b[0;34m(sentence, model, device, max_len)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trg_indexes'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrg_indexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mtrg_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg_indexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mtrg_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_trg_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: an integer is required (got type NoneType)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDcoc0CmsEWS",
        "outputId": "a414ff55-e2bc-4201-c857-ed6b072c0398",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(g_discriminator.tokenizer.get_vocab())"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'[UNK]': 0, '[PAD]': 1, '[CLS]': 2, '[SEP]': 3, '[MASK]': 4, '!': 5, \"!'\": 6, '!”': 7, '\"': 8, '#': 9, '$': 10, '%': 11, '%)': 12, '&': 13, '&#34;': 14, \"'\": 15, \"'(\": 16, \"',\": 17, '(': 18, '(0': 19, '(1': 20, '(10': 21, '(12': 22, '(15': 23, '(17': 24, '(18': 25, '(19': 26, '(2': 27, '(20': 28, '(23': 29, '(24': 30, '(25': 31, '(3': 32, '(4': 33, '(5': 34, '(6': 35, '(7': 36, '(8': 37, '(9': 38, '(?)': 39, ')': 40, \")'\": 41, '),': 42, ')’': 43, '*': 44, '+': 45, ',': 46, '-': 47, '----------------': 48, '-1': 49, '-2': 50, '-20': 51, '-3': 52, '-4': 53, '.': 54, '...': 55, '...\"': 56, \"...'\": 57, '...”': 58, '/': 59, '0': 60, '0%': 61, '0%)': 62, '0.0': 63, '0.00': 64, '0.1': 65, '0.1%': 66, '0.2%': 67, '0.3': 68, '0.3%': 69, '0.4%': 70, '0.5': 71, '0.5%': 72, '0.6': 73, '0.6%': 74, '0.7': 75, '0.7%': 76, '0.8': 77, '0.8%': 78, '00': 79, '000.0': 80, '00000': 81, '01': 82, '02': 83, '02-': 84, '03': 85, '04': 86, '05': 87, '06': 88, '07': 89, '08': 90, '09': 91, '0:00:00': 92, '1': 93, '1%': 94, '1%)': 95, '1)': 96, '1,000': 97, '1.3%': 98, '1.4%': 99, '1.5%': 100, '1.6': 101, '1.6%': 102, '1.7%': 103, '1.8': 104, '10': 105, '100': 106, '1000': 107, '11': 108, '12': 109, '13': 110, '14': 111, '15': 112, '16': 113, '17': 114, '18': 115, '19': 116, '1⁄2': 117, '1⁄4': 118, '2': 119, '2%': 120, '2)': 121, '2.0': 122, '2.3%': 123, '2.5': 124, '2.5%': 125, '2.8%': 126, '20': 127, '200': 128, '2000': 129, '2011': 130, '2012': 131, '2013': 132, '21': 133, '22': 134, '23': 135, '24': 136, '25': 137, '26': 138, '27': 139, '28': 140, '29': 141, '3': 142, '3%': 143, '30': 144, '300': 145, '3000': 146, '31': 147, '32': 148, '33': 149, '34': 150, '35': 151, '36': 152, '37': 153, '38': 154, '39': 155, '3⁄4': 156, '4': 157, '4%': 158, '4%)': 159, '40': 160, '400': 161, '4000': 162, '41': 163, '42': 164, '43': 165, '44': 166, '45': 167, '46': 168, '47': 169, '48': 170, '49': 171, '5': 172, '5%': 173, '5%)': 174, '5,000': 175, '50': 176, '500': 177, '5000': 178, '51': 179, '52': 180, '53': 181, '54': 182, '55': 183, '56': 184, '57': 185, '58': 186, '59': 187, '6': 188, '6%': 189, '60': 190, '600': 191, '6000': 192, '61': 193, '62': 194, '63': 195, '64': 196, '65': 197, '66': 198, '67': 199, '68': 200, '69': 201, '7': 202, '7%': 203, '7%)': 204, '7.0': 205, '7.5%': 206, '70': 207, '700': 208, '7000': 209, '71': 210, '72': 211, '73': 212, '74': 213, '75': 214, '76': 215, '77': 216, '78': 217, '79': 218, '8': 219, '8%': 220, '8%)': 221, '80': 222, '800': 223, '8000': 224, '81': 225, '82': 226, '83': 227, '84': 228, '85': 229, '86': 230, '87': 231, '88': 232, '89': 233, '9': 234, '9%': 235, '9%)': 236, '90': 237, '900': 238, '9000': 239, '91': 240, '92': 241, '93': 242, '94': 243, '95': 244, '96': 245, '97': 246, '98': 247, '99': 248, ':': 249, '://': 250, ':00': 251, ';': 252, '<': 253, '=': 254, '=\"\"': 255, '=\"\">': 256, '>': 257, '?': 258, '?\"': 259, '??': 260, '???': 261, '????': 262, '?”': 263, 'A': 264, 'AM': 265, 'AP': 266, 'AR': 267, 'AS': 268, 'AT': 269, 'B': 270, 'BC': 271, 'BO': 272, 'BS': 273, 'C': 274, 'CC': 275, 'CD': 276, 'CI': 277, 'D': 278, 'DB': 279, 'DC': 280, 'DI': 281, 'E': 282, 'ER': 283, 'EU': 284, 'F': 285, 'FC': 286, 'FI': 287, 'FIFA': 288, 'FTA': 289, 'G': 290, 'GB': 291, 'GDP': 292, 'GM': 293, 'H': 294, 'HD': 295, 'I': 296, 'IA': 297, 'IB': 298, 'IC': 299, 'II': 300, 'IM': 301, 'IN': 302, 'IP': 303, 'IS': 304, 'IT': 305, 'J': 306, 'K': 307, 'KB': 308, 'KBS': 309, 'KT': 310, 'L': 311, 'LED': 312, 'LG': 313, 'LPGA': 314, 'LS': 315, 'M': 316, 'MBC': 317, 'MBN': 318, 'MC': 319, 'MOU': 320, 'MS': 321, 'N': 322, 'NA': 323, 'NE': 324, 'NLL': 325, 'NS': 326, 'NTSB': 327, 'New': 328, 'O': 329, 'OC': 330, 'OS': 331, 'OSEN': 332, 'P': 333, 'PC': 334, 'PD': 335, 'PGA': 336, 'PI': 337, 'PM': 338, 'POP': 339, 'PS': 340, 'Q': 341, 'R': 342, 'S': 343, 'SBS': 344, 'SI': 345, 'SK': 346, 'SNS': 347, 'SP': 348, 'SS': 349, 'ST': 350, 'T': 351, 'TI': 352, 'TS': 353, 'TV': 354, 'The': 355, 'U': 356, 'V': 357, 'W': 358, 'X': 359, 'Y': 360, 'Z': 361, '[': 362, ']': 363, '^': 364, '_': 365, '`': 366, 'a': 367, 'ab': 368, 'ac': 369, 'ad': 370, 'al': 371, 'all': 372, 'am': 373, 'an': 374, 'ar': 375, 'as': 376, 'at': 377, 'ation': 378, 'ay': 379, 'b': 380, 'bp': 381, 'c': 382, 'ch': 383, 'cm': 384, 'co': 385, 'com': 386, 'ct': 387, 'd': 388, 'e': 389, 'ed': 390, 'el': 391, 'en': 392, 'ent': 393, 'er': 394, 'es': 395, 'est': 396, 'et': 397, 'f': 398, 'g': 399, 'go': 400, 'h': 401, 'ha': 402, 'ho': 403, 'http': 404, 'i': 405, 'ic': 406, 'id': 407, 'il': 408, 'in': 409, 'ing': 410, 'ir': 411, 'is': 412, 'it': 413, 'j': 414, 'k': 415, 'kW': 416, 'kg': 417, 'km': 418, 'kr': 419, 'l': 420, 'le': 421, 'lo': 422, 'm': 423, 'mm': 424, 'n': 425, 'net': 426, 'o': 427, 'ol': 428, 'on': 429, 'or': 430, 'ow': 431, 'p': 432, 'q': 433, 'quot': 434, 'r': 435, 'ra': 436, 're': 437, 'ri': 438, 'ro': 439, 's': 440, 'st': 441, 't': 442, 'ter': 443, 'th': 444, 'tion': 445, 'u': 446, 'ul': 447, 'um': 448, 'un': 449, 'ur': 450, 'us': 451, 'ut': 452, 'v': 453, 'ver': 454, 'w': 455, 'www': 456, 'x': 457, 'y': 458, 'z': 459, '{': 460, '|': 461, '}': 462, '~': 463, '~1': 464, '~20': 465, '~3': 466, '~5': 467, '~6': 468, '~8': 469, '¡': 470, '¤': 471, '§': 472, '\\xad': 473, '®': 474, '°': 475, '±': 476, '¶': 477, '·': 478, '¿': 479, 'Æ': 480, '×': 481, 'Ø': 482, 'ß': 483, 'æ': 484, '÷': 485, 'ø': 486, '́': 487, '̧': 488, 'μ': 489, 'ᄀ': 490, 'ᄋ': 491, 'ᄏ': 492, 'ᄒ': 493, 'ᅵ': 494, 'ᆞ': 495, '―': 496, '‘': 497, '’': 498, '’(': 499, '’,': 500, '“': 501, '”': 502, '”,': 503, '′': 504, '※': 505, '⁄': 506, '↑': 507, '→': 508, '↓': 509, '↓]': 510, '∼': 511, '─': 512, '───': 513, '│': 514, '├': 515, '┼': 516, '▁': 517, '▁\"': 518, '▁&': 519, \"▁'\": 520, \"▁'2013\": 521, '▁(': 522, '▁*': 523, '▁-': 524, '▁/': 525, '▁0': 526, '▁0.2': 527, '▁00:0': 528, '▁1': 529, '▁1%': 530, '▁1,2': 531, '▁1.5': 532, '▁10': 533, '▁10%': 534, '▁100': 535, '▁100%': 536, '▁1000': 537, '▁11': 538, '▁12': 539, '▁120': 540, '▁13': 541, '▁14': 542, '▁15': 543, '▁150': 544, '▁16': 545, '▁17': 546, '▁18': 547, '▁19': 548, '▁1990': 549, '▁1997': 550, '▁1998': 551, '▁1~2': 552, '▁2': 553, '▁20': 554, '▁20%': 555, '▁200': 556, '▁2000': 557, '▁2001': 558, '▁2002': 559, '▁2003': 560, '▁2004': 561, '▁2005': 562, '▁2006': 563, '▁2007': 564, '▁2008': 565, '▁2009': 566, '▁2010': 567, '▁2011': 568, '▁2012': 569, '▁2013': 570, '▁2013-0': 571, '▁2013.0': 572, '▁2013.07.1': 573, '▁2013.07.2': 574, '▁2014': 575, '▁2015': 576, '▁2016': 577, '▁2017': 578, '▁21': 579, '▁22': 580, '▁23': 581, '▁24': 582, '▁25': 583, '▁26': 584, '▁27': 585, '▁28': 586, '▁29': 587, '▁2~3': 588, '▁3': 589, '▁3.0': 590, '▁3.3': 591, '▁30': 592, '▁30%': 593, '▁300': 594, '▁3000': 595, '▁31': 596, '▁32': 597, '▁33': 598, '▁34': 599, '▁35': 600, '▁36': 601, '▁37': 602, '▁38': 603, '▁39': 604, '▁4': 605, '▁40': 606, '▁40%': 607, '▁400': 608, '▁45': 609, '▁48': 610, '▁5': 611, '▁50': 612, '▁50%': 613, '▁500': 614, '▁5000': 615, '▁55': 616, '▁6': 617, '▁60': 618, '▁60%': 619, '▁65': 620, '▁7': 621, '▁70': 622, '▁70%': 623, '▁8': 624, '▁80': 625, '▁80%': 626, '▁9': 627, '▁90': 628, '▁:': 629, '▁<': 630, '▁=': 631, '▁>': 632, '▁?': 633, '▁??': 634, '▁A': 635, '▁APP': 636, '▁B': 637, '▁C': 638, '▁CCTV': 639, '▁CEO': 640, '▁CES': 641, '▁CGV': 642, '▁CJ': 643, '▁D': 644, '▁E': 645, '▁ELS': 646, '▁ETF': 647, '▁F': 648, '▁FA': 649, '▁G': 650, '▁GS': 651, '▁H': 652, '▁HOT': 653, '▁Home': 654, '▁I': 655, '▁IT': 656, '▁J': 657, '▁JTBC': 658, '▁K': 659, '▁KB': 660, '▁KBO': 661, '▁KBS': 662, '▁KDB': 663, '▁KIA': 664, '▁KT': 665, '▁L': 666, '▁LA': 667, '▁LG': 668, '▁LIG': 669, '▁LTE': 670, '▁M': 671, '▁MBC': 672, '▁MC': 673, '▁Mnet': 674, '▁N': 675, '▁NC': 676, '▁NH': 677, '▁NHN': 678, '▁NLL': 679, '▁O': 680, '▁P': 681, '▁PC': 682, '▁PD': 683, '▁Q': 684, '▁QPR': 685, '▁R': 686, '▁S': 687, '▁SBS': 688, '▁SK': 689, '▁SM': 690, '▁SNS': 691, '▁STX': 692, '▁T': 693, '▁TV': 694, '▁U': 695, '▁US': 696, '▁V': 697, '▁VIP': 698, '▁W': 699, '▁XML': 700, '▁YG': 701, '▁[': 702, '▁`': 703, '▁and': 704, '▁c': 705, '▁for': 706, '▁of': 707, '▁the': 708, '▁to': 709, '▁tvN': 710, '▁|': 711, '▁‘': 712, '▁‘2013': 713, '▁“': 714, '▁“‘': 715, '▁│': 716, '▁■': 717, '▁▦': 718, '▁▲': 719, '▁△': 720, '▁▷': 721, '▁◀': 722, '▁◆': 723, '▁◇': 724, '▁【': 725, '▁中': 726, '▁美': 727, '▁가격': 728, '▁가격은': 729, '▁가격이': 730, '▁가계': 731, '▁가계부채': 732, '▁가구': 733, '▁가까운': 734, '▁가까이': 735, '▁가는': 736, '▁가능': 737, '▁가능성': 738, '▁가능성도': 739, '▁가능성을': 740, '▁가능성이': 741, '▁가능하다': 742, '▁가능한': 743, '▁가동': 744, '▁가득': 745, '▁가량': 746, '▁가려': 747, '▁가로': 748, '▁가르': 749, '▁가리': 750, '▁가맹점': 751, '▁가방': 752, '▁가수': 753, '▁가스': 754, '▁가슴': 755, '▁가시': 756, '▁가운데': 757, '▁가입': 758, '▁가입자': 759, '▁가장': 760, '▁가전': 761, '▁가정': 762, '▁가져': 763, '▁가졌다': 764, '▁가족': 765, '▁가족들': 766, '▁가지': 767, '▁가지고': 768, '▁가진': 769, '▁가짜': 770, '▁가치': 771, '▁가치를': 772, '▁각': 773, '▁각각': 774, '▁각자': 775, '▁각종': 776, '▁간': 777, '▁간담회': 778, '▁간부': 779, '▁간사': 780, '▁갈': 781, '▁갈등': 782, '▁갈수록': 783, '▁감': 784, '▁감각': 785, '▁감독': 786, '▁감독님': 787, '▁감독은': 788, '▁감독의': 789, '▁감독이': 790, '▁감동': 791, '▁감면': 792, '▁감사': 793, '▁감사원': 794, '▁감성': 795, '▁감소': 796, '▁감소한': 797, '▁감소했다': 798, '▁감시': 799, '▁감안': 800, '▁감안하면': 801, '▁감염': 802, '▁감정': 803, '▁감추지': 804, '▁감축': 805, '▁갑자기': 806, '▁강': 807, '▁강남': 808, '▁강남구': 809, '▁강력': 810, '▁강력한': 811, '▁강렬한': 812, '▁강릉': 813, '▁강세': 814, '▁강원': 815, '▁강원도': 816, '▁강제': 817, '▁강조': 818, '▁강조했다': 819, '▁강하게': 820, '▁강한': 821, '▁강행': 822, '▁강호동': 823, '▁강화': 824, '▁갖': 825, '▁갖고': 826, '▁갖추': 827, '▁갖춘': 828, '▁갖춰': 829, '▁같': 830, '▁같다': 831, '▁같아': 832, '▁같은': 833, '▁같이': 834, '▁개': 835, '▁개그맨': 836, '▁개념': 837, '▁개막': 838, '▁개발': 839, '▁개방': 840, '▁개별': 841, '▁개봉': 842, '▁개선': 843, '▁개설': 844, '▁개성': 845, '▁개성공단': 846, '▁개인': 847, '▁개인정보': 848, '▁개입': 849, '▁개장': 850, '▁개정': 851, '▁개정안': 852, '▁개척': 853, '▁개최': 854, '▁개최한다': 855, '▁개통': 856, '▁개편': 857, '▁개편안': 858, '▁개혁': 859, '▁객관적': 860, '▁갤럭시': 861, '▁거': 862, '▁거두': 863, '▁거둔': 864, '▁거뒀다': 865, '▁거듭': 866, '▁거래': 867, '▁거래되고': 868, '▁거래량': 869, '▁거론': 870, '▁거리': 871, '▁거부': 872, '▁거의': 873, '▁거절': 874, '▁거주': 875, '▁거짓말': 876, '▁거쳐': 877, '▁거치': 878, '▁거친': 879, '▁걱정': 880, '▁건': 881, '▁건강': 882, '▁건립': 883, '▁건물': 884, '▁건설': 885, '▁건설사': 886, '▁건축': 887, '▁걷': 888, '▁걸': 889, '▁걸그룹': 890, '▁걸린': 891, '▁걸스데이': 892, '▁걸어': 893, '▁걸쳐': 894, '▁검': 895, '▁검거': 896, '▁검사': 897, '▁검색': 898, '▁검증': 899, '▁검찰': 900, '▁검찰에': 901, '▁검찰은': 902, '▁검토': 903, '▁겁니다': 904, '▁것': 905, '▁것과': 906, '▁것도': 907, '▁것에': 908, '▁것으로': 909, '▁것은': 910, '▁것을': 911, '▁것이': 912, '▁것이다': 913, '▁것이라고': 914, '▁것이라는': 915, '▁것이란': 916, '▁것인가': 917, '▁것인지': 918, '▁것입니다': 919, '▁것처럼': 920, '▁게': 921, '▁게다가': 922, '▁게스트': 923, '▁게시물': 924, '▁게시판에': 925, '▁게임': 926, '▁게재': 927, '▁게재했다': 928, '▁겨냥': 929, '▁겨울': 930, '▁격': 931, '▁격려': 932, '▁격차': 933, '▁겪': 934, '▁겪고': 935, '▁견': 936, '▁결': 937, '▁결과': 938, '▁결과를': 939, '▁결국': 940, '▁결론': 941, '▁결별': 942, '▁결승': 943, '▁결심': 944, '▁결정': 945, '▁결정했다': 946, '▁결제': 947, '▁결코': 948, '▁결합': 949, '▁결혼': 950, '▁결혼식': 951, '▁겸': 952, '▁경': 953, '▁경계': 954, '▁경고': 955, '▁경기': 956, '▁경기가': 957, '▁경기도': 958, '▁경기를': 959, '▁경기에서': 960, '▁경기침체': 961, '▁경남': 962, '▁경력': 963, '▁경매': 964, '▁경북': 965, '▁경비': 966, '▁경영': 967, '▁경우': 968, '▁경우가': 969, '▁경쟁': 970, '▁경쟁력': 971, '▁경제': 972, '▁경제성장률': 973, '▁경제적': 974, '▁경찰': 975, '▁경찰관': 976, '▁경찰에': 977, '▁경찰은': 978, '▁경험': 979, '▁계': 980, '▁계기가': 981, '▁계기로': 982, '▁계산': 983, '▁계속': 984, '▁계약': 985, '▁계약을': 986, '▁계열사': 987, '▁계절': 988, '▁계좌': 989, '▁계획': 990, '▁계획을': 991, '▁계획이다': 992, '▁고': 993, '▁고객': 994, '▁고객들': 995, '▁고객에게': 996, '▁고교': 997, '▁고급': 998, '▁고려': 999, '▁고려해': 1000, '▁고령': 1001, '▁고민': 1002, '▁고발': 1003, '▁고백': 1004, '▁고백했다': 1005, '▁고소': 1006, '▁고스란히': 1007, '▁고양': 1008, '▁고영욱': 1009, '▁고용': 1010, '▁고위': 1011, '▁고통': 1012, '▁곡': 1013, '▁곧': 1014, '▁곧바로': 1015, '▁골': 1016, '▁골든': 1017, '▁골키퍼': 1018, '▁골프': 1019, '▁골프장': 1020, '▁곳': 1021, '▁곳이': 1022, '▁공': 1023, '▁공간': 1024, '▁공감': 1025, '▁공개': 1026, '▁공개됐다': 1027, '▁공개된': 1028, '▁공개했다': 1029, '▁공격': 1030, '▁공격수': 1031, '▁공공': 1032, '▁공공기관': 1033, '▁공급': 1034, '▁공단': 1035, '▁공동': 1036, '▁공략': 1037, '▁공모': 1038, '▁공무원': 1039, '▁공방': 1040, '▁공백': 1041, '▁공부': 1042, '▁공사': 1043, '▁공시': 1044, '▁공시했다': 1045, '▁공식': 1046, '▁공약': 1047, '▁공연': 1048, '▁공연을': 1049, '▁공유': 1050, '▁공장': 1051, '▁공정': 1052, '▁공정위': 1053, '▁공포': 1054, '▁공항': 1055, '▁공화당': 1056, '▁과감': 1057, '▁과거': 1058, '▁과도한': 1059, '▁과세': 1060, '▁과시': 1061, '▁과시했다': 1062, '▁과연': 1063, '▁과장': 1064, '▁과정': 1065, '▁과정에서': 1066, '▁과정을': 1067, '▁과제': 1068, '▁과징금': 1069, '▁과태료': 1070, '▁과학': 1071, '▁곽': 1072, '▁관': 1073, '▁관객': 1074, '▁관객들': 1075, '▁관계': 1076, '▁관계자': 1077, '▁관계자는': 1078, '▁관계자들': 1079, '▁관광': 1080, '▁관광객': 1081, '▁관람': 1082, '▁관련': 1083, '▁관련된': 1084, '▁관련사진': 1085, '▁관련한': 1086, '▁관련해': 1087, '▁관리': 1088, '▁관심': 1089, '▁관심을': 1090, '▁관심이': 1091, '▁관중': 1092, '▁관측': 1093, '▁관한': 1094, '▁관행': 1095, '▁광': 1096, '▁광고': 1097, '▁광주': 1098, '▁광주시': 1099, '▁괜찮': 1100, '▁괴': 1101, '▁굉장히': 1102, '▁교': 1103, '▁교과부': 1104, '▁교류': 1105, '▁교사': 1106, '▁교수': 1107, '▁교수는': 1108, '▁교육': 1109, '▁교육부': 1110, '▁교체': 1111, '▁교통': 1112, '▁교통사고': 1113, '▁교환': 1114, '▁구': 1115, '▁구간': 1116, '▁구글': 1117, '▁구단': 1118, '▁구매': 1119, '▁구성': 1120, '▁구성된': 1121, '▁구속': 1122, '▁구속기소': 1123, '▁구입': 1124, '▁구자철': 1125, '▁구조': 1126, '▁구조조정': 1127, '▁구체적으로': 1128, '▁구체적인': 1129, '▁구축': 1130, '▁구현': 1131, '▁국': 1132, '▁국가': 1133, '▁국가기록원': 1134, '▁국가대표': 1135, '▁국가안보': 1136, '▁국가정보원': 1137, '▁국내': 1138, '▁국내에서': 1139, '▁국내외': 1140, '▁국립': 1141, '▁국무총리': 1142, '▁국무회의': 1143, '▁국민': 1144, '▁국민들': 1145, '▁국민연금': 1146, '▁국민은행': 1147, '▁국민의': 1148, '▁국방': 1149, '▁국방부': 1150, '▁국세청': 1151, '▁국정': 1152, '▁국정원': 1153, '▁국정조사': 1154, '▁국제': 1155, '▁국제사회': 1156, '▁국조': 1157, '▁국채': 1158, '▁국토교통부': 1159, '▁국토부': 1160, '▁국토해양부': 1161, '▁국회': 1162, '▁국회에서': 1163, '▁국회의원': 1164, '▁군': 1165, '▁군부': 1166, '▁군사': 1167, '▁굴': 1168, '▁궁금': 1169, '▁권': 1170, '▁권력': 1171, '▁권리': 1172, '▁권한': 1173, '▁귀': 1174, '▁귀국': 1175, '▁귀여운': 1176, '▁귀엽': 1177, '▁규명': 1178, '▁규모': 1179, '▁규모로': 1180, '▁규모의': 1181, '▁규정': 1182, '▁규제': 1183, '▁균형': 1184, '▁그': 1185, '▁그가': 1186, '▁그간': 1187, '▁그것이': 1188, '▁그냥': 1189, '▁그녀': 1190, '▁그는': 1191, '▁그대로': 1192, '▁그동안': 1193, '▁그때': 1194, '▁그래서': 1195, '▁그래픽': 1196, '▁그랬': 1197, '▁그러': 1198, '▁그러나': 1199, '▁그러면서': 1200, '▁그런': 1201, '▁그런데': 1202, '▁그럼에도': 1203, '▁그렇게': 1204, '▁그렇지': 1205, '▁그려': 1206, '▁그려졌다': 1207, '▁그룹': 1208, '▁그리': 1209, '▁그리고': 1210, '▁그린': 1211, '▁그림': 1212, '▁그만큼': 1213, '▁그의': 1214, '▁그쳤다': 1215, '▁극': 1216, '▁극대화': 1217, '▁극복': 1218, '▁극중': 1219, '▁극찬': 1220, '▁근': 1221, '▁근거': 1222, '▁근로': 1223, '▁근로자': 1224, '▁근무': 1225, '▁근본': 1226, '▁근육': 1227, '▁근절': 1228, '▁근처': 1229, '▁근황': 1230, '▁글': 1231, '▁글과': 1232, '▁글로벌': 1233, '▁글을': 1234, '▁금': 1235, '▁금감원': 1236, '▁금리': 1237, '▁금메달': 1238, '▁금액': 1239, '▁금융': 1240, '▁금융감독원': 1241, '▁금융권': 1242, '▁금융기관': 1243, '▁금융당국': 1244, '▁금융위기': 1245, '▁금융투자': 1246, '▁금융회사': 1247, '▁금지': 1248, '▁금품을': 1249, '▁급': 1250, '▁급격': 1251, '▁급등': 1252, '▁급락': 1253, '▁급여': 1254, '▁급증': 1255, '▁긍정적': 1256, '▁긍정적인': 1257, '▁기': 1258, '▁기간': 1259, '▁기계': 1260, '▁기관': 1261, '▁기기': 1262, '▁기념': 1263, '▁기능': 1264, '▁기능을': 1265, '▁기다리': 1266, '▁기대': 1267, '▁기대감': 1268, '▁기대된다': 1269, '▁기대를': 1270, '▁기대하고': 1271, '▁기대한다': 1272, '▁기록': 1273, '▁기록하고': 1274, '▁기록하며': 1275, '▁기록한': 1276, '▁기록했다': 1277, '▁기반': 1278, '▁기반으로': 1279, '▁기본': 1280, '▁기부': 1281, '▁기분': 1282, '▁기사': 1283, '▁기사본문': 1284, '▁기상': 1285, '▁기성용': 1286, '▁기소': 1287, '▁기소된': 1288, '▁기술': 1289, '▁기아차': 1290, '▁기억': 1291, '▁기업': 1292, '▁기업들': 1293, '▁기업의': 1294, '▁기여': 1295, '▁기온': 1296, '▁기울': 1297, '▁기재부': 1298, '▁기조': 1299, '▁기존': 1300, '▁기준': 1301, '▁기준금리': 1302, '▁기준으로': 1303, '▁기초': 1304, '▁기초자산': 1305, '▁기타': 1306, '▁기회': 1307, '▁기회를': 1308, '▁기획': 1309, '▁기획재정부': 1310, '▁긴': 1311, '▁긴급': 1312, '▁긴장': 1313, '▁긴장감': 1314, '▁길': 1315, '▁김': 1316, '▁김기': 1317, '▁김동': 1318, '▁김모': 1319, '▁김민': 1320, '▁김성': 1321, '▁김씨': 1322, '▁김씨는': 1323, '▁김연경': 1324, '▁김연아': 1325, '▁김영': 1326, '▁김용': 1327, '▁김용준': 1328, '▁김재': 1329, '▁김정': 1330, '▁김종': 1331, '▁김종학': 1332, '▁김진': 1333, '▁김태': 1334, '▁김태희': 1335, '▁김한길': 1336, '▁김현': 1337, '▁깊은': 1338, '▁깊이': 1339, '▁깔': 1340, '▁깜짝': 1341, '▁깨': 1342, '▁깨끗': 1343, '▁꺼': 1344, '▁꺾고': 1345, '▁꼬리': 1346, '▁꼭': 1347, '▁꼽았다': 1348, '▁꼽히': 1349, '▁꼽힌다': 1350, '▁꽃': 1351, '▁꽃미남': 1352, '▁꾸': 1353, '▁꾸준한': 1354, '▁꾸준히': 1355, '▁꿈': 1356, '▁꿈꾸': 1357, '▁끈다': 1358, '▁끊임없': 1359, '▁끌고': 1360, '▁끌어': 1361, '▁끌었다': 1362, '▁끝': 1363, '▁끝까지': 1364, '▁끝나': 1365, '▁끝난': 1366, '▁끝내': 1367, '▁끝에': 1368, '▁끼': 1369, '▁나': 1370, '▁나가': 1371, '▁나갈': 1372, '▁나누': 1373, '▁나눠': 1374, '▁나는': 1375, '▁나라': 1376, '▁나란히': 1377, '▁나로호': 1378, '▁나머지': 1379, '▁나쁜': 1380, '▁나서': 1381, '▁나선': 1382, '▁나선다': 1383, '▁나설': 1384, '▁나섰': 1385, '▁나섰다': 1386, '▁나아가': 1387, '▁나오': 1388, '▁나오고': 1389, '▁나오는': 1390, '▁나온': 1391, '▁나온다': 1392, '▁나올': 1393, '▁나와': 1394, '▁나왔': 1395, '▁나왔다': 1396, '▁나이': 1397, '▁나중에': 1398, '▁나타나': 1399, '▁나타난': 1400, '▁나타났다': 1401, '▁나타내': 1402, '▁나타냈다': 1403, '▁낙': 1404, '▁낙찰': 1405, '▁난': 1406, '▁날': 1407, '▁날씨': 1408, '▁남': 1409, '▁남겨': 1410, '▁남겼다': 1411, '▁남긴': 1412, '▁남녀': 1413, '▁남다른': 1414, '▁남부': 1415, '▁남북': 1416, '▁남북관계': 1417, '▁남북정상회담': 1418, '▁남성': 1419, '▁남아': 1420, '▁남아있': 1421, '▁남은': 1422, '▁남자': 1423, '▁남자친구': 1424, '▁남편': 1425, '▁납부': 1426, '▁납치': 1427, '▁납품': 1428, '▁낮': 1429, '▁낮아': 1430, '▁낮은': 1431, '▁낮추': 1432, '▁낳': 1433, '▁내': 1434, '▁내가': 1435, '▁내내': 1436, '▁내년': 1437, '▁내놓': 1438, '▁내놨다': 1439, '▁내다봤다': 1440, '▁내달': 1441, '▁내려': 1442, '▁내렸다': 1443, '▁내리': 1444, '▁내린': 1445, '▁내부': 1446, '▁내수': 1447, '▁내야': 1448, '▁내용': 1449, '▁내용은': 1450, '▁내용을': 1451, '▁내용의': 1452, '▁내용이': 1453, '▁낸': 1454, '▁냈다': 1455, '▁냉': 1456, '▁너': 1457, '▁너무': 1458, '▁넓': 1459, '▁넘': 1460, '▁넘게': 1461, '▁넘겨': 1462, '▁넘기': 1463, '▁넘는': 1464, '▁넘어': 1465, '▁넘치는': 1466, '▁넣': 1467, '▁넣어': 1468, '▁네': 1469, '▁네이버': 1470, '▁네트워크': 1471, '▁네티즌': 1472, '▁네티즌들': 1473, '▁네티즌들은': 1474, '▁넥센': 1475, '▁노': 1476, '▁노동': 1477, '▁노동자': 1478, '▁노래': 1479, '▁노량진': 1480, '▁노력': 1481, '▁노력을': 1482, '▁노력하겠다': 1483, '▁노무현': 1484, '▁노사': 1485, '▁노선': 1486, '▁노인': 1487, '▁노조': 1488, '▁노출': 1489, '▁노트북': 1490, '▁노하우': 1491, '▁노홍철': 1492, '▁노후': 1493, '▁녹': 1494, '▁녹색': 1495, '▁녹음': 1496, '▁녹화': 1497, '▁논': 1498, '▁논란': 1499, '▁논란이': 1500, '▁논리': 1501, '▁논의': 1502, '▁논쟁': 1503, '▁놀': 1504, '▁놀라': 1505, '▁놀라게': 1506, '▁농': 1507, '▁농업': 1508, '▁농촌': 1509, '▁농협': 1510, '▁높': 1511, '▁높게': 1512, '▁높다': 1513, '▁높아': 1514, '▁높아졌다': 1515, '▁높아지고': 1516, '▁높았다': 1517, '▁높여': 1518, '▁높였다': 1519, '▁높은': 1520, '▁높이': 1521, '▁놓': 1522, '▁놓고': 1523, '▁뇌': 1524, '▁뇌물': 1525, '▁누': 1526, '▁누가': 1527, '▁누구': 1528, '▁누구나': 1529, '▁누군가': 1530, '▁누리꾼들': 1531, '▁누리꾼들은': 1532, '▁누적': 1533, '▁누출': 1534, '▁눈': 1535, '▁눈길을': 1536, '▁눈물': 1537, '▁눈물을': 1538, '▁눈빛': 1539, '▁눈에': 1540, '▁뉴': 1541, '▁뉴스': 1542, '▁뉴욕': 1543, '▁느껴': 1544, '▁느꼈': 1545, '▁느끼': 1546, '▁느낀': 1547, '▁느낄': 1548, '▁느낌': 1549, '▁늘': 1550, '▁늘려': 1551, '▁늘리': 1552, '▁늘어': 1553, '▁늘어나': 1554, '▁늘어난': 1555, '▁늘어날': 1556, '▁늘어났다': 1557, '▁늘었다': 1558, '▁능력': 1559, '▁능력을': 1560, '▁늦어': 1561, '▁다': 1562, '▁다녀': 1563, '▁다니': 1564, '▁다룬': 1565, '▁다르다': 1566, '▁다른': 1567, '▁다리': 1568, '▁다만': 1569, '▁다문화': 1570, '▁다섯': 1571, '▁다소': 1572, '▁다수': 1573, '▁다시': 1574, '▁다양한': 1575, '▁다운로드': 1576, '▁다음': 1577, '▁다음날': 1578, '▁다음달': 1579, '▁다이어트': 1580, '▁다저스': 1581, '▁다저스는': 1582, '▁다짐': 1583, '▁다하겠다': 1584, '▁단': 1585, '▁단계': 1586, '▁단기': 1587, '▁단독': 1588, '▁단말기': 1589, '▁단속': 1590, '▁단순': 1591, '▁단장': 1592, '▁단지': 1593, '▁단체': 1594, '▁단축': 1595, '▁단행': 1596, '▁달': 1597, '▁달라': 1598, '▁달러': 1599, '▁달려': 1600, '▁달리': 1601, '▁달성': 1602, '▁달아': 1603, '▁달하는': 1604, '▁달한다': 1605, '▁닮은': 1606, '▁담': 1607, '▁담겨': 1608, '▁담긴': 1609, '▁담당': 1610, '▁담배': 1611, '▁담보': 1612, '▁담아': 1613, '▁담은': 1614, '▁답': 1615, '▁답변': 1616, '▁답했다': 1617, '▁당': 1618, '▁당국': 1619, '▁당기순이익': 1620, '▁당부했다': 1621, '▁당분간': 1622, '▁당사자': 1623, '▁당선': 1624, '▁당선인': 1625, '▁당시': 1626, '▁당연히': 1627, '▁당장': 1628, '▁당첨': 1629, '▁당초': 1630, '▁당했다': 1631, '▁당황': 1632, '▁대': 1633, '▁대거': 1634, '▁대결': 1635, '▁대구': 1636, '▁대규모': 1637, '▁대기': 1638, '▁대기업': 1639, '▁대다수': 1640, '▁대단': 1641, '▁대답': 1642, '▁대리점': 1643, '▁대만': 1644, '▁대법원': 1645, '▁대변인': 1646, '▁대부분': 1647, '▁대북': 1648, '▁대비': 1649, '▁대사': 1650, '▁대상': 1651, '▁대상으로': 1652, '▁대상자': 1653, '▁대선': 1654, '▁대신': 1655, '▁대안': 1656, '▁대외': 1657, '▁대우': 1658, '▁대응': 1659, '▁대전': 1660, '▁대중': 1661, '▁대중교통': 1662, '▁대책': 1663, '▁대처': 1664, '▁대체': 1665, '▁대출': 1666, '▁대통령': 1667, '▁대통령은': 1668, '▁대통령의': 1669, '▁대통령이': 1670, '▁대통령직': 1671, '▁대통령직인수위원회': 1672, '▁대폭': 1673, '▁대표': 1674, '▁대표는': 1675, '▁대표단': 1676, '▁대표이사': 1677, '▁대표적인': 1678, '▁대표팀': 1679, '▁대학': 1680, '▁대학생': 1681, '▁대한': 1682, '▁대한민국': 1683, '▁대한항공': 1684, '▁대해': 1685, '▁대해서': 1686, '▁대해서는': 1687, '▁대해서도': 1688, '▁대해선': 1689, '▁대형': 1690, '▁대형마트': 1691, '▁대화': 1692, '▁대화록': 1693, '▁대회': 1694, '▁대회에서': 1695, '▁댄스': 1696, '▁댓글': 1697, '▁더': 1698, '▁더불어': 1699, '▁더욱': 1700, '▁덕': 1701, '▁덕분에': 1702, '▁던지': 1703, '▁덜': 1704, '▁덧붙였다': 1705, '▁데': 1706, '▁데뷔': 1707, '▁데이터': 1708, '▁데이트': 1709, '▁도내': 1710, '▁도로': 1711, '▁도발': 1712, '▁도시': 1713, '▁도심': 1714, '▁도약': 1715, '▁도와': 1716, '▁도움': 1717, '▁도움을': 1718, '▁도움이': 1719, '▁도입': 1720, '▁도전': 1721, '▁도중': 1722, '▁도착': 1723, '▁도쿄': 1724, '▁독': 1725, '▁독립': 1726, '▁독일': 1727, '▁독자': 1728, '▁독특한': 1729, '▁돈': 1730, '▁돈을': 1731, '▁돌': 1732, '▁돌려': 1733, '▁돌아': 1734, '▁돌아가': 1735, '▁돌아오': 1736, '▁돌아온': 1737, '▁돌입': 1738, '▁돌파': 1739, '▁돕는': 1740, '▁동': 1741, '▁동결': 1742, '▁동기': 1743, '▁동남아': 1744, '▁동료': 1745, '▁동물': 1746, '▁동반': 1747, '▁동반성장': 1748, '▁동부': 1749, '▁동생': 1750, '▁동시에': 1751, '▁동아시안컵': 1752, '▁동아제약': 1753, '▁동안': 1754, '▁동양': 1755, '▁동영상': 1756, '▁동원': 1757, '▁동의': 1758, '▁동작': 1759, '▁동참': 1760, '▁돼': 1761, '▁됐다': 1762, '▁되': 1763, '▁되고': 1764, '▁되는': 1765, '▁되면': 1766, '▁되어': 1767, '▁되지': 1768, '▁되찾': 1769, '▁된': 1770, '▁된다': 1771, '▁될': 1772, '▁두': 1773, '▁두고': 1774, '▁두드러': 1775, '▁두산': 1776, '▁둔': 1777, '▁둔화': 1778, '▁둘': 1779, '▁둘러': 1780, '▁둘러싼': 1781, '▁둘째': 1782, '▁뒤': 1783, '▁뒤늦게': 1784, '▁뒤집': 1785, '▁뒷': 1786, '▁뒷받침': 1787, '▁드': 1788, '▁드라마': 1789, '▁드러나': 1790, '▁드러난': 1791, '▁드러났다': 1792, '▁드러내': 1793, '▁드러낸': 1794, '▁드러냈다': 1795, '▁드레스': 1796, '▁득점': 1797, '▁든다': 1798, '▁듣': 1799, '▁듣고': 1800, '▁들': 1801, '▁들고': 1802, '▁들려': 1803, '▁들어': 1804, '▁들어가': 1805, '▁들어간': 1806, '▁들어갈': 1807, '▁들어갔다': 1808, '▁들어서': 1809, '▁들어오': 1810, '▁들었다': 1811, '▁들여': 1812, '▁듯': 1813, '▁듯한': 1814, '▁등': 1815, '▁등과': 1816, '▁등도': 1817, '▁등록': 1818, '▁등록금': 1819, '▁등에': 1820, '▁등에서': 1821, '▁등으로': 1822, '▁등은': 1823, '▁등을': 1824, '▁등의': 1825, '▁등이': 1826, '▁등이다': 1827, '▁등장': 1828, '▁등판': 1829, '▁디': 1830, '▁디자인': 1831, '▁디지털': 1832, '▁따': 1833, '▁따뜻한': 1834, '▁따라': 1835, '▁따라서': 1836, '▁따로': 1837, '▁따르면': 1838, '▁따른': 1839, '▁딸': 1840, '▁땅': 1841, '▁땅볼': 1842, '▁땅에': 1843, '▁때': 1844, '▁때까지': 1845, '▁때는': 1846, '▁때마다': 1847, '▁때문': 1848, '▁때문에': 1849, '▁때문이다': 1850, '▁때부터': 1851, '▁떠': 1852, '▁떠나': 1853, '▁떠난': 1854, '▁떠오르': 1855, '▁떨': 1856, '▁떨어져': 1857, '▁떨어졌다': 1858, '▁떨어지': 1859, '▁떨어진': 1860, '▁또': 1861, '▁또는': 1862, '▁또다시': 1863, '▁또한': 1864, '▁똑같': 1865, '▁뚜렷': 1866, '▁뛰': 1867, '▁뛰어': 1868, '▁뛰어난': 1869, '▁뛰어넘': 1870, '▁뜨거운': 1871, '▁뜻': 1872, '▁뜻을': 1873, '▁띠고': 1874, '▁라': 1875, '▁라디오': 1876, '▁라이벌': 1877, '▁라이브': 1878, '▁라인업': 1879, '▁랭킹': 1880, '▁러시아': 1881, '▁런던': 1882, '▁런던올림픽': 1883, '▁레': 1884, '▁레드카펫': 1885, '▁레알': 1886, '▁레이': 1887, '▁레전드': 1888, '▁로그인': 1889, '▁로비': 1890, '▁로이킴': 1891, '▁로켓': 1892, '▁롯데': 1893, '▁롯데마트': 1894, '▁루': 1895, '▁루머': 1896, '▁류': 1897, '▁류현진': 1898, '▁류현진은': 1899, '▁리': 1900, '▁리그': 1901, '▁리더': 1902, '▁리더십': 1903, '▁리베이트': 1904, '▁리스크': 1905, '▁리포트': 1906, '▁마': 1907, '▁마감': 1908, '▁마감했다': 1909, '▁마드리드': 1910, '▁마련': 1911, '▁마련했다': 1912, '▁마리': 1913, '▁마무리': 1914, '▁마운드': 1915, '▁마을': 1916, '▁마음': 1917, '▁마음을': 1918, '▁마지막': 1919, '▁마지막으로': 1920, '▁마찬가지': 1921, '▁마쳤다': 1922, '▁마치': 1923, '▁마치고': 1924, '▁마친': 1925, '▁마케팅': 1926, '▁막': 1927, '▁막기': 1928, '▁막아': 1929, '▁막판': 1930, '▁만': 1931, '▁만기': 1932, '▁만나': 1933, '▁만난': 1934, '▁만날': 1935, '▁만남': 1936, '▁만났다': 1937, '▁만드는': 1938, '▁만든': 1939, '▁만들': 1940, '▁만들고': 1941, '▁만들기': 1942, '▁만들어': 1943, '▁만들었다': 1944, '▁만약': 1945, '▁만에': 1946, '▁만족': 1947, '▁만큼': 1948, '▁만한': 1949, '▁많고': 1950, '▁많다': 1951, '▁많아': 1952, '▁많았': 1953, '▁많았다': 1954, '▁많은': 1955, '▁많이': 1956, '▁많지': 1957, '▁말': 1958, '▁말까지': 1959, '▁말레이시아': 1960, '▁말씀': 1961, '▁말을': 1962, '▁말이': 1963, '▁말한다': 1964, '▁말해': 1965, '▁말했다': 1966, '▁맛': 1967, '▁망': 1968, '▁망명': 1969, '▁맞': 1970, '▁맞는': 1971, '▁맞대결': 1972, '▁맞서': 1973, '▁맞아': 1974, '▁맞았다': 1975, '▁맞추': 1976, '▁맞춘': 1977, '▁맞춤형': 1978, '▁맞춰': 1979, '▁맡': 1980, '▁맡고': 1981, '▁맡아': 1982, '▁맡았': 1983, '▁맡았다': 1984, '▁맡은': 1985, '▁매': 1986, '▁매각': 1987, '▁매년': 1988, '▁매달': 1989, '▁매도': 1990, '▁매력': 1991, '▁매력을': 1992, '▁매매': 1993, '▁매수': 1994, '▁매우': 1995, '▁매일': 1996, '▁매입': 1997, '▁매장': 1998, '▁매주': 1999, '▁매체': 2000, '▁매출': 2001, '▁매출액': 2002, '▁매출이': 2003, '▁맨': 2004, '▁맨유': 2005, '▁맺': 2006, '▁머': 2007, '▁머리': 2008, '▁머물': 2009, '▁먹': 2010, '▁먹고': 2011, '▁먼저': 2012, '▁멀티': 2013, '▁멀티미디어': 2014, '▁멋진': 2015, '▁메': 2016, '▁메뉴': 2017, '▁메시지': 2018, '▁메시지를': 2019, '▁메이저': 2020, '▁메이저리그': 2021, '▁메이크업': 2022, '▁메인': 2023, '▁멕시코': 2024, '▁멘토': 2025, '▁멜로': 2026, '▁멤버': 2027, '▁멤버들': 2028, '▁면': 2029, '▁면담': 2030, '▁면모를': 2031, '▁면접': 2032, '▁면제': 2033, '▁명': 2034, '▁명단': 2035, '▁명령': 2036, '▁명예': 2037, '▁명의': 2038, '▁명이': 2039, '▁명칭': 2040, '▁명품': 2041, '▁명확': 2042, '▁몇': 2043, '▁모': 2044, '▁모니터링': 2045, '▁모델': 2046, '▁모두': 2047, '▁모든': 2048, '▁모르': 2049, '▁모르겠다': 2050, '▁모른다': 2051, '▁모멘텀': 2052, '▁모바일': 2053, '▁모비스': 2054, '▁모색': 2055, '▁모습': 2056, '▁모습으로': 2057, '▁모습을': 2058, '▁모습이': 2059, '▁모습이다': 2060, '▁모아': 2061, '▁모았다': 2062, '▁모양': 2063, '▁모여': 2064, '▁모으고': 2065, '▁모자': 2066, '▁모집': 2067, '▁목': 2068, '▁목격': 2069, '▁목동': 2070, '▁목록': 2071, '▁목사': 2072, '▁목소리': 2073, '▁목소리가': 2074, '▁목숨': 2075, '▁목적': 2076, '▁목적으로': 2077, '▁목표': 2078, '▁목표로': 2079, '▁목표주가': 2080, '▁몰': 2081, '▁몰려': 2082, '▁몰아': 2083, '▁몸': 2084, '▁몸매': 2085, '▁못': 2086, '▁못하고': 2087, '▁못하는': 2088, '▁못한': 2089, '▁못한다': 2090, '▁못할': 2091, '▁못해': 2092, '▁못했다': 2093, '▁묘': 2094, '▁무': 2095, '▁무게': 2096, '▁무기': 2097, '▁무대': 2098, '▁무대를': 2099, '▁무대에': 2100, '▁무더위': 2101, '▁무려': 2102, '▁무료': 2103, '▁무료로': 2104, '▁무르시': 2105, '▁무릎': 2106, '▁무리': 2107, '▁무산': 2108, '▁무상': 2109, '▁무슨': 2110, '▁무승부': 2111, '▁무실점': 2112, '▁무엇보다': 2113, '▁무역': 2114, '▁무이자': 2115, '▁무제한': 2116, '▁무조건': 2117, '▁무죄': 2118, '▁묶': 2119, '▁문': 2120, '▁문서': 2121, '▁문의': 2122, '▁문자': 2123, '▁문재인': 2124, '▁문제': 2125, '▁문제가': 2126, '▁문제는': 2127, '▁문제로': 2128, '▁문제를': 2129, '▁문제에': 2130, '▁문제점': 2131, '▁문화': 2132, '▁문화체육관광부': 2133, '▁묻': 2134, '▁물': 2135, '▁물가': 2136, '▁물건': 2137, '▁물놀이': 2138, '▁물량': 2139, '▁물론': 2140, '▁물류': 2141, '▁물리': 2142, '▁물어': 2143, '▁물품': 2144, '▁뭐': 2145, '▁뭔가': 2146, '▁뮤지컬': 2147, '▁뮤직비디오': 2148, '▁미': 2149, '▁미국': 2150, '▁미국의': 2151, '▁미니': 2152, '▁미드필더': 2153, '▁미디어': 2154, '▁미래': 2155, '▁미래부': 2156, '▁미래창조과학부': 2157, '▁미뤄': 2158, '▁미리': 2159, '▁미만': 2160, '▁미모': 2161, '▁미사일': 2162, '▁미소': 2163, '▁미얀마': 2164, '▁미치는': 2165, '▁미치지': 2166, '▁미칠': 2167, '▁미투데이': 2168, '▁민': 2169, '▁민간': 2170, '▁민낯': 2171, '▁민생': 2172, '▁민원': 2173, '▁민족': 2174, '▁민주': 2175, '▁민주당': 2176, '▁민주당은': 2177, '▁민주주의': 2178, '▁민주통합당': 2179, '▁믿고': 2180, '▁밀': 2181, '▁밀려': 2182, '▁밀어': 2183, '▁및': 2184, '▁밑': 2185, '▁바': 2186, '▁바꾸': 2187, '▁바꿔': 2188, '▁바뀌': 2189, '▁바뀐': 2190, '▁바다': 2191, '▁바닥': 2192, '▁바라': 2193, '▁바란다': 2194, '▁바람': 2195, '▁바로': 2196, '▁바이러스': 2197, '▁바탕으로': 2198, '▁박': 2199, '▁박근혜': 2200, '▁박명수': 2201, '▁박수': 2202, '▁박인비': 2203, '▁박지성': 2204, '▁밖에': 2205, '▁밖으로': 2206, '▁반': 2207, '▁반대': 2208, '▁반도체': 2209, '▁반드시': 2210, '▁반등': 2211, '▁반면': 2212, '▁반박했다': 2213, '▁반발': 2214, '▁반복': 2215, '▁반영': 2216, '▁반응': 2217, '▁반응을': 2218, '▁반전': 2219, '▁받': 2220, '▁받게': 2221, '▁받고': 2222, '▁받기': 2223, '▁받는': 2224, '▁받는다': 2225, '▁받아': 2226, '▁받아들여': 2227, '▁받아들이': 2228, '▁받았': 2229, '▁받았다': 2230, '▁받으며': 2231, '▁받은': 2232, '▁받을': 2233, '▁받지': 2234, '▁발': 2235, '▁발견': 2236, '▁발견됐다': 2237, '▁발굴': 2238, '▁발급': 2239, '▁발매': 2240, '▁발목': 2241, '▁발사': 2242, '▁발생': 2243, '▁발생한': 2244, '▁발생할': 2245, '▁발생했다': 2246, '▁발언': 2247, '▁발언을': 2248, '▁발전': 2249, '▁발탁': 2250, '▁발표': 2251, '▁발표한': 2252, '▁발표했다': 2253, '▁발행': 2254, '▁발효': 2255, '▁발휘': 2256, '▁밝은': 2257, '▁밝혀': 2258, '▁밝혀졌다': 2259, '▁밝혔': 2260, '▁밝혔다': 2261, '▁밝혔습니다': 2262, '▁밝히': 2263, '▁밝힌': 2264, '▁밤': 2265, '▁밥': 2266, '▁방': 2267, '▁방문': 2268, '▁방문해': 2269, '▁방법': 2270, '▁방북': 2271, '▁방송': 2272, '▁방송되는': 2273, '▁방송된': 2274, '▁방송된다': 2275, '▁방송에서': 2276, '▁방식': 2277, '▁방식으로': 2278, '▁방안': 2279, '▁방안을': 2280, '▁방지': 2281, '▁방침': 2282, '▁방침이다': 2283, '▁방통위': 2284, '▁방향': 2285, '▁방향으로': 2286, '▁배': 2287, '▁배경': 2288, '▁배경으로': 2289, '▁배당': 2290, '▁배려': 2291, '▁배우': 2292, '▁배우들': 2293, '▁배제': 2294, '▁배출': 2295, '▁배치': 2296, '▁배터리': 2297, '▁백': 2298, '▁백악관': 2299, '▁백지영': 2300, '▁백화점': 2301, '▁버': 2302, '▁버냉키': 2303, '▁버디': 2304, '▁버스': 2305, '▁버전': 2306, '▁번': 2307, '▁번째': 2308, '▁벌': 2309, '▁벌금': 2310, '▁벌써': 2311, '▁벌어지': 2312, '▁벌어진': 2313, '▁벌였다': 2314, '▁벌이고': 2315, '▁벌이는': 2316, '▁벌인': 2317, '▁범': 2318, '▁범위': 2319, '▁범죄': 2320, '▁범행': 2321, '▁법': 2322, '▁법률': 2323, '▁법무부': 2324, '▁법안': 2325, '▁법원': 2326, '▁법인': 2327, '▁법적': 2328, '▁법정': 2329, '▁법칙': 2330, '▁벗': 2331, '▁벗어나': 2332, '▁베': 2333, '▁베이징': 2334, '▁베트남': 2335, '▁벤': 2336, '▁벤처': 2337, '▁벽': 2338, '▁변': 2339, '▁변경': 2340, '▁변동': 2341, '▁변동성': 2342, '▁변수': 2343, '▁변신': 2344, '▁변호사': 2345, '▁변화': 2346, '▁변화를': 2347, '▁별': 2348, '▁별도로': 2349, '▁별도의': 2350, '▁병': 2351, '▁병역': 2352, '▁병원': 2353, '▁병행': 2354, '▁보': 2355, '▁보건': 2356, '▁보건복지부': 2357, '▁보고': 2358, '▁보고서': 2359, '▁보관': 2360, '▁보급': 2361, '▁보기': 2362, '▁보내': 2363, '▁보낸': 2364, '▁보냈다': 2365, '▁보는': 2366, '▁보니': 2367, '▁보다': 2368, '▁보도': 2369, '▁보도자료': 2370, '▁보도했다': 2371, '▁보면': 2372, '▁보상': 2373, '▁보수': 2374, '▁보안': 2375, '▁보여': 2376, '▁보여주': 2377, '▁보여준': 2378, '▁보여줄': 2379, '▁보여줬다': 2380, '▁보였다': 2381, '▁보완': 2382, '▁보유': 2383, '▁보유하고': 2384, '▁보유한': 2385, '▁보육': 2386, '▁보이': 2387, '▁보이고': 2388, '▁보이는': 2389, '▁보이며': 2390, '▁보이지': 2391, '▁보인다': 2392, '▁보장': 2393, '▁보조금': 2394, '▁보증': 2395, '▁보통': 2396, '▁보험': 2397, '▁보험료': 2398, '▁보험사': 2399, '▁보호': 2400, '▁복': 2401, '▁복귀': 2402, '▁복무': 2403, '▁복수': 2404, '▁복잡': 2405, '▁복지': 2406, '▁복합': 2407, '▁본': 2408, '▁본격': 2409, '▁본격적으로': 2410, '▁본격적인': 2411, '▁본격화': 2412, '▁본다': 2413, '▁본사': 2414, '▁본인': 2415, '▁본회의': 2416, '▁볼': 2417, '▁봉': 2418, '▁봉사활동': 2419, '▁봐': 2420, '▁봤': 2421, '▁봤다': 2422, '▁부': 2423, '▁부각': 2424, '▁부과': 2425, '▁부담': 2426, '▁부담을': 2427, '▁부담이': 2428, '▁부당': 2429, '▁부대': 2430, '▁부동산': 2431, '▁부르': 2432, '▁부모': 2433, '▁부모님': 2434, '▁부문': 2435, '▁부부': 2436, '▁부분': 2437, '▁부분이': 2438, '▁부사장': 2439, '▁부산': 2440, '▁부산시': 2441, '▁부상': 2442, '▁부서': 2443, '▁부실': 2444, '▁부여': 2445, '▁부위원장': 2446, '▁부인': 2447, '▁부작용': 2448, '▁부정': 2449, '▁부정적': 2450, '▁부족': 2451, '▁부족한': 2452, '▁부지': 2453, '▁부진': 2454, '▁부채': 2455, '▁부처': 2456, '▁부총리': 2457, '▁부탁': 2458, '▁부품': 2459, '▁부활': 2460, '▁부회장': 2461, '▁북': 2462, '▁북미': 2463, '▁북측': 2464, '▁북한': 2465, '▁북한의': 2466, '▁북한이': 2467, '▁분': 2468, '▁분노': 2469, '▁분당': 2470, '▁분들': 2471, '▁분류': 2472, '▁분리': 2473, '▁분명': 2474, '▁분명히': 2475, '▁분석': 2476, '▁분석이다': 2477, '▁분석했다': 2478, '▁분야': 2479, '▁분야에서': 2480, '▁분양': 2481, '▁분위기': 2482, '▁분위기를': 2483, '▁분쟁': 2484, '▁불': 2485, '▁불가능': 2486, '▁불가피': 2487, '▁불공정': 2488, '▁불과': 2489, '▁불과하다': 2490, '▁불구속': 2491, '▁불구하고': 2492, '▁불러': 2493, '▁불리는': 2494, '▁불만을': 2495, '▁불법': 2496, '▁불안': 2497, '▁불투명': 2498, '▁불편': 2499, '▁불확실성': 2500, '▁불황': 2501, '▁붕괴': 2502, '▁붙': 2503, '▁붙잡': 2504, '▁브': 2505, '▁브라운': 2506, '▁브라질': 2507, '▁브랜드': 2508, '▁브리핑': 2509, '▁블랙': 2510, '▁블랙박스': 2511, '▁블로그': 2512, '▁블루': 2513, '▁비': 2514, '▁비가': 2515, '▁비공개': 2516, '▁비과세': 2517, '▁비교': 2518, '▁비교적': 2519, '▁비교해': 2520, '▁비난': 2521, '▁비대위': 2522, '▁비대위원장': 2523, '▁비롯': 2524, '▁비롯한': 2525, '▁비롯해': 2526, '▁비리': 2527, '▁비밀': 2528, '▁비상': 2529, '▁비서실': 2530, '▁비서실장': 2531, '▁비스트': 2532, '▁비슷': 2533, '▁비슷한': 2534, '▁비용': 2535, '▁비율': 2536, '▁비자금': 2537, '▁비정규직': 2538, '▁비중': 2539, '▁비중이': 2540, '▁비즈니스': 2541, '▁비판': 2542, '▁비판했다': 2543, '▁비해': 2544, '▁비행기': 2545, '▁빅': 2546, '▁빈': 2547, '▁빈소': 2548, '▁빌': 2549, '▁빌려': 2550, '▁빙': 2551, '▁빚': 2552, '▁빛': 2553, '▁빠': 2554, '▁빠르게': 2555, '▁빠르고': 2556, '▁빠른': 2557, '▁빠져': 2558, '▁빠졌다': 2559, '▁빠지': 2560, '▁빠진': 2561, '▁빨': 2562, '▁빨리': 2563, '▁빼': 2564, '▁빼앗': 2565, '▁뽐내': 2566, '▁뽐냈다': 2567, '▁뽑': 2568, '▁뽑아': 2569, '▁뿌리': 2570, '▁뿐': 2571, '▁뿐만': 2572, '▁사': 2573, '▁사건': 2574, '▁사건을': 2575, '▁사고': 2576, '▁사고가': 2577, '▁사고로': 2578, '▁사과': 2579, '▁사기': 2580, '▁사나이': 2581, '▁사는': 2582, '▁사라지': 2583, '▁사람': 2584, '▁사람들': 2585, '▁사람들이': 2586, '▁사람은': 2587, '▁사람의': 2588, '▁사람이': 2589, '▁사랑': 2590, '▁사랑을': 2591, '▁사례': 2592, '▁사로잡': 2593, '▁사로잡았다': 2594, '▁사망': 2595, '▁사망자': 2596, '▁사면': 2597, '▁사명': 2598, '▁사무': 2599, '▁사무실': 2600, '▁사무총장': 2601, '▁사법': 2602, '▁사상': 2603, '▁사실': 2604, '▁사실상': 2605, '▁사실을': 2606, '▁사실이': 2607, '▁사안': 2608, '▁사업': 2609, '▁사업을': 2610, '▁사업자': 2611, '▁사연': 2612, '▁사용': 2613, '▁사용자': 2614, '▁사용하는': 2615, '▁사용할': 2616, '▁사유': 2617, '▁사이': 2618, '▁사이버': 2619, '▁사이에': 2620, '▁사이에서': 2621, '▁사이트': 2622, '▁사장': 2623, '▁사장은': 2624, '▁사전': 2625, '▁사정': 2626, '▁사진': 2627, '▁사진을': 2628, '▁사진이': 2629, '▁사태': 2630, '▁사퇴': 2631, '▁사항': 2632, '▁사회': 2633, '▁사회공헌': 2634, '▁사회복지': 2635, '▁사회적': 2636, '▁사흘': 2637, '▁삭감': 2638, '▁삭제': 2639, '▁산': 2640, '▁산업': 2641, '▁산하': 2642, '▁살': 2643, '▁살아': 2644, '▁살인': 2645, '▁살펴': 2646, '▁살펴보면': 2647, '▁살해': 2648, '▁삶': 2649, '▁삼': 2650, '▁삼성': 2651, '▁삼성생명': 2652, '▁삼성전자': 2653, '▁삼성전자는': 2654, '▁삼성화재': 2655, '▁삼진': 2656, '▁삼청동': 2657, '▁상': 2658, '▁상담': 2659, '▁상당': 2660, '▁상당수': 2661, '▁상당의': 2662, '▁상당한': 2663, '▁상대': 2664, '▁상대로': 2665, '▁상대적으로': 2666, '▁상무': 2667, '▁상반기': 2668, '▁상생': 2669, '▁상승': 2670, '▁상승세': 2671, '▁상승세를': 2672, '▁상승한': 2673, '▁상승했다': 2674, '▁상위': 2675, '▁상임위': 2676, '▁상장': 2677, '▁상징': 2678, '▁상처': 2679, '▁상태': 2680, '▁상태다': 2681, '▁상태에서': 2682, '▁상품': 2683, '▁상하이': 2684, '▁상한가': 2685, '▁상향': 2686, '▁상호': 2687, '▁상환': 2688, '▁상황': 2689, '▁상황에': 2690, '▁상황에서': 2691, '▁상황을': 2692, '▁상황이': 2693, '▁상황이다': 2694, '▁새': 2695, '▁새누리당': 2696, '▁새누리당은': 2697, '▁새로': 2698, '▁새로운': 2699, '▁새롭게': 2700, '▁새벽': 2701, '▁새해': 2702, '▁샌프란시스코': 2703, '▁생': 2704, '▁생각': 2705, '▁생각을': 2706, '▁생각이': 2707, '▁생각한다': 2708, '▁생각해': 2709, '▁생기': 2710, '▁생긴': 2711, '▁생명': 2712, '▁생방송': 2713, '▁생산': 2714, '▁생존': 2715, '▁생태계': 2716, '▁생활': 2717, '▁서': 2718, '▁서귀포': 2719, '▁서로': 2720, '▁서류': 2721, '▁서민': 2722, '▁서부': 2723, '▁서비스': 2724, '▁서비스를': 2725, '▁서울': 2726, '▁서울대': 2727, '▁서울시': 2728, '▁서울중앙지검': 2729, '▁서울중앙지법': 2730, '▁서초구': 2731, '▁석': 2732, '▁석유': 2733, '▁선': 2734, '▁선거': 2735, '▁선고': 2736, '▁선고받': 2737, '▁선고했다': 2738, '▁선두': 2739, '▁선물': 2740, '▁선박': 2741, '▁선발': 2742, '▁선발투수': 2743, '▁선배': 2744, '▁선보여': 2745, '▁선보였다': 2746, '▁선보이며': 2747, '▁선보인': 2748, '▁선보인다': 2749, '▁선보일': 2750, '▁선사': 2751, '▁선생님': 2752, '▁선수': 2753, '▁선수가': 2754, '▁선수단': 2755, '▁선수들': 2756, '▁선수들이': 2757, '▁선언': 2758, '▁선예': 2759, '▁선임': 2760, '▁선정': 2761, '▁선정됐다': 2762, '▁선정된': 2763, '▁선제골': 2764, '▁선진국': 2765, '▁선출': 2766, '▁선택': 2767, '▁선호': 2768, '▁설': 2769, '▁설계': 2770, '▁설득': 2771, '▁설립': 2772, '▁설명': 2773, '▁설명이다': 2774, '▁설명했다': 2775, '▁설문조사': 2776, '▁설비': 2777, '▁설정': 2778, '▁설치': 2779, '▁섬': 2780, '▁성': 2781, '▁성격': 2782, '▁성공': 2783, '▁성공했다': 2784, '▁성과': 2785, '▁성과를': 2786, '▁성남': 2787, '▁성능': 2788, '▁성동일': 2789, '▁성매매': 2790, '▁성명을': 2791, '▁성장': 2792, '▁성장률': 2793, '▁성장세': 2794, '▁성적': 2795, '▁성추행': 2796, '▁성폭력': 2797, '▁성폭행': 2798, '▁성향': 2799, '▁성형': 2800, '▁세': 2801, '▁세계': 2802, '▁세계적인': 2803, '▁세금': 2804, '▁세대': 2805, '▁세력': 2806, '▁세무조사': 2807, '▁세미나': 2808, '▁세부': 2809, '▁세븐': 2810, '▁세상': 2811, '▁세상을': 2812, '▁세우': 2813, '▁세워': 2814, '▁세웠다': 2815, '▁세종': 2816, '▁세종시': 2817, '▁세트': 2818, '▁섹시': 2819, '▁셀카': 2820, '▁셈이다': 2821, '▁소': 2822, '▁소감을': 2823, '▁소개': 2824, '▁소개했다': 2825, '▁소녀': 2826, '▁소녀시대': 2827, '▁소득': 2828, '▁소리': 2829, '▁소방': 2830, '▁소방당국': 2831, '▁소비': 2832, '▁소비자': 2833, '▁소비자들': 2834, '▁소설': 2835, '▁소셜': 2836, '▁소속': 2837, '▁소속사': 2838, '▁소송': 2839, '▁소식에': 2840, '▁소식을': 2841, '▁소식통': 2842, '▁소요': 2843, '▁소유': 2844, '▁소장': 2845, '▁소재': 2846, '▁소중한': 2847, '▁소지섭': 2848, '▁소집': 2849, '▁소통': 2850, '▁소폭': 2851, '▁소프트웨어': 2852, '▁소형': 2853, '▁소화': 2854, '▁소환': 2855, '▁속': 2856, '▁속도': 2857, '▁속에': 2858, '▁속에서': 2859, '▁손': 2860, '▁손님': 2861, '▁손실': 2862, '▁손연재': 2863, '▁손을': 2864, '▁손해배상': 2865, '▁손흥민': 2866, '▁솔로': 2867, '▁솔루션': 2868, '▁송': 2869, '▁쇼': 2870, '▁쇼핑': 2871, '▁수': 2872, '▁수급': 2873, '▁수도': 2874, '▁수도권': 2875, '▁수립': 2876, '▁수많은': 2877, '▁수목드라마': 2878, '▁수밖에': 2879, '▁수비': 2880, '▁수비수': 2881, '▁수사': 2882, '▁수사를': 2883, '▁수상': 2884, '▁수석': 2885, '▁수석대표': 2886, '▁수수료': 2887, '▁수술': 2888, '▁수십': 2889, '▁수업': 2890, '▁수영': 2891, '▁수요': 2892, '▁수요가': 2893, '▁수용': 2894, '▁수원': 2895, '▁수익': 2896, '▁수익률': 2897, '▁수익성': 2898, '▁수익을': 2899, '▁수입': 2900, '▁수정': 2901, '▁수주': 2902, '▁수준': 2903, '▁수준으로': 2904, '▁수준이다': 2905, '▁수출': 2906, '▁수치': 2907, '▁수치다': 2908, '▁수행': 2909, '▁수혜': 2910, '▁숙': 2911, '▁순': 2912, '▁순간': 2913, '▁순매도': 2914, '▁순매수': 2915, '▁순위': 2916, '▁순이익': 2917, '▁술': 2918, '▁숨': 2919, '▁숨졌다': 2920, '▁숨진': 2921, '▁숫자': 2922, '▁쉬': 2923, '▁쉽게': 2924, '▁쉽지': 2925, '▁슈': 2926, '▁슈팅': 2927, '▁슈퍼': 2928, '▁스': 2929, '▁스노든': 2930, '▁스마트': 2931, '▁스마트폰': 2932, '▁스몰캡': 2933, '▁스스로': 2934, '▁스위스': 2935, '▁스케줄': 2936, '▁스크린': 2937, '▁스타': 2938, '▁스타일': 2939, '▁스태프': 2940, '▁스토리': 2941, '▁스트레스': 2942, '▁스페셜올림픽': 2943, '▁스페인': 2944, '▁스포츠': 2945, '▁스포츠조선닷컴': 2946, '▁스피드': 2947, '▁슬': 2948, '▁승': 2949, '▁승객': 2950, '▁승리': 2951, '▁승리를': 2952, '▁승무원': 2953, '▁승부': 2954, '▁승부주': 2955, '▁승용차': 2956, '▁승인': 2957, '▁승진': 2958, '▁시': 2959, '▁시가총액': 2960, '▁시각': 2961, '▁시간': 2962, '▁시간을': 2963, '▁시간이': 2964, '▁시기': 2965, '▁시나리오': 2966, '▁시너지': 2967, '▁시는': 2968, '▁시달리': 2969, '▁시대': 2970, '▁시도': 2971, '▁시리아': 2972, '▁시리즈': 2973, '▁시민': 2974, '▁시민단체': 2975, '▁시민들': 2976, '▁시범': 2977, '▁시사': 2978, '▁시상식': 2979, '▁시선을': 2980, '▁시설': 2981, '▁시스템': 2982, '▁시스템을': 2983, '▁시신': 2984, '▁시위': 2985, '▁시작': 2986, '▁시작된': 2987, '▁시작으로': 2988, '▁시작한': 2989, '▁시작했다': 2990, '▁시장': 2991, '▁시장에': 2992, '▁시장에서': 2993, '▁시장은': 2994, '▁시장의': 2995, '▁시절': 2996, '▁시점': 2997, '▁시즌': 2998, '▁시청률': 2999, '▁시청자': 3000, '▁시청자들': 3001, '▁시청자들의': 3002, '▁시카고': 3003, '▁시행': 3004, '▁시험': 3005, '▁식': 3006, '▁식당': 3007, '▁식사': 3008, '▁식품': 3009, '▁신': 3010, '▁신경': 3011, '▁신고': 3012, '▁신곡': 3013, '▁신규': 3014, '▁신동엽': 3015, '▁신뢰': 3016, '▁신문': 3017, '▁신분': 3018, '▁신설': 3019, '▁신세계': 3020, '▁신속': 3021, '▁신시내티': 3022, '▁신용': 3023, '▁신용등급': 3024, '▁신용카드': 3025, '▁신인': 3026, '▁신임': 3027, '▁신제품': 3028, '▁신중': 3029, '▁신청': 3030, '▁신한금융투자': 3031, '▁신한은행': 3032, '▁신호': 3033, '▁신화': 3034, '▁신흥국': 3035, '▁실': 3036, '▁실내': 3037, '▁실력': 3038, '▁실망': 3039, '▁실무': 3040, '▁실무회담': 3041, '▁실수': 3042, '▁실시': 3043, '▁실시간': 3044, '▁실시한': 3045, '▁실시한다': 3046, '▁실적': 3047, '▁실적이': 3048, '▁실제': 3049, '▁실제로': 3050, '▁실종': 3051, '▁실질적인': 3052, '▁실천': 3053, '▁실태': 3054, '▁실패': 3055, '▁실행': 3056, '▁실험': 3057, '▁실현': 3058, '▁싫어': 3059, '▁심': 3060, '▁심각': 3061, '▁심각한': 3062, '▁심경': 3063, '▁심리': 3064, '▁심사': 3065, '▁심사위원': 3066, '▁심지어': 3067, '▁심판': 3068, '▁심화': 3069, '▁싱가포르': 3070, '▁싱글': 3071, '▁싶다': 3072, '▁싶어': 3073, '▁싶었': 3074, '▁싶은': 3075, '▁싸': 3076, '▁싸이': 3077, '▁쌍용차': 3078, '▁쌓': 3079, '▁써': 3080, '▁써니': 3081, '▁썼다': 3082, '▁쏟아': 3083, '▁쓰': 3084, '▁쓰레기': 3085, '▁쓴': 3086, '▁쓸': 3087, '▁씨': 3088, '▁씨가': 3089, '▁씨는': 3090, '▁씨스타': 3091, '▁씨엔블루': 3092, '▁아': 3093, '▁아끼': 3094, '▁아나운서': 3095, '▁아내': 3096, '▁아니': 3097, '▁아니냐': 3098, '▁아니냐는': 3099, '▁아니다': 3100, '▁아니라': 3101, '▁아니면': 3102, '▁아니었다': 3103, '▁아니지만': 3104, '▁아닌': 3105, '▁아동': 3106, '▁아들': 3107, '▁아래': 3108, '▁아름': 3109, '▁아름다운': 3110, '▁아무': 3111, '▁아버지': 3112, '▁아베': 3113, '▁아빠': 3114, '▁아쉬움': 3115, '▁아시아': 3116, '▁아시아나': 3117, '▁아시아나항공': 3118, '▁아예': 3119, '▁아울러': 3120, '▁아이': 3121, '▁아이돌': 3122, '▁아이들': 3123, '▁아이디어': 3124, '▁아이유': 3125, '▁아이템': 3126, '▁아이폰': 3127, '▁아주': 3128, '▁아직': 3129, '▁아침': 3130, '▁아파트': 3131, '▁아픔': 3132, '▁악': 3133, '▁악화': 3134, '▁안': 3135, '▁안내': 3136, '▁안보': 3137, '▁안에': 3138, '▁안전': 3139, '▁안정': 3140, '▁안정적인': 3141, '▁안타': 3142, '▁안타를': 3143, '▁안팎': 3144, '▁앉아': 3145, '▁않': 3146, '▁않게': 3147, '▁않겠다': 3148, '▁않고': 3149, '▁않기': 3150, '▁않는': 3151, '▁않는다': 3152, '▁않다': 3153, '▁않도록': 3154, '▁않아': 3155, '▁않았': 3156, '▁않았다': 3157, '▁않았던': 3158, '▁않았지만': 3159, '▁않으': 3160, '▁않으면': 3161, '▁않은': 3162, '▁않을': 3163, '▁않을까': 3164, '▁않지만': 3165, '▁알': 3166, '▁알게': 3167, '▁알고': 3168, '▁알려': 3169, '▁알려져': 3170, '▁알려졌다': 3171, '▁알려진': 3172, '▁알렸다': 3173, '▁알리': 3174, '▁알아': 3175, '▁알제리': 3176, '▁암': 3177, '▁압': 3178, '▁압도적': 3179, '▁압력': 3180, '▁압류': 3181, '▁압박': 3182, '▁압수수색': 3183, '▁앞': 3184, '▁앞두고': 3185, '▁앞둔': 3186, '▁앞서': 3187, '▁앞선': 3188, '▁앞세워': 3189, '▁앞에': 3190, '▁앞에서': 3191, '▁앞으로': 3192, '▁앞장서': 3193, '▁애': 3194, '▁애널리스트': 3195, '▁애니메이션': 3196, '▁애리조나': 3197, '▁애정': 3198, '▁애초': 3199, '▁애플': 3200, '▁애플리케이션': 3201, '▁액션': 3202, '▁앤': 3203, '▁앨범': 3204, '▁앱': 3205, '▁야': 3206, '▁야간': 3207, '▁야구': 3208, '▁야당': 3209, '▁야외': 3210, '▁약': 3211, '▁약세': 3212, '▁약속': 3213, '▁양': 3214, '▁양국': 3215, '▁양성': 3216, '▁양적완화': 3217, '▁양측': 3218, '▁얘기': 3219, '▁어': 3220, '▁어깨': 3221, '▁어느': 3222, '▁어디': 3223, '▁어떤': 3224, '▁어떻게': 3225, '▁어려운': 3226, '▁어려울': 3227, '▁어려움': 3228, '▁어려움을': 3229, '▁어려워': 3230, '▁어렵': 3231, '▁어렵다': 3232, '▁어린': 3233, '▁어린이': 3234, '▁어린이집': 3235, '▁어머니': 3236, '▁어울리': 3237, '▁어제': 3238, '▁억울': 3239, '▁억제': 3240, '▁언': 3241, '▁언급': 3242, '▁언급했다': 3243, '▁언론': 3244, '▁언제': 3245, '▁얻고': 3246, '▁얻어': 3247, '▁얻었다': 3248, '▁얻은': 3249, '▁얻을': 3250, '▁얼굴': 3251, '▁얼마': 3252, '▁얼마나': 3253, '▁얼음': 3254, '▁엄': 3255, '▁엄격': 3256, '▁엄마': 3257, '▁엄청난': 3258, '▁엄태웅': 3259, '▁업': 3260, '▁업계': 3261, '▁업그레이드': 3262, '▁업데이트': 3263, '▁업무': 3264, '▁업무를': 3265, '▁업무보고': 3266, '▁업종': 3267, '▁업체': 3268, '▁업체들': 3269, '▁없': 3270, '▁없고': 3271, '▁없는': 3272, '▁없다': 3273, '▁없다고': 3274, '▁없다는': 3275, '▁없도록': 3276, '▁없애': 3277, '▁없어': 3278, '▁없었': 3279, '▁없었다': 3280, '▁없었던': 3281, '▁없을': 3282, '▁없이': 3283, '▁없지만': 3284, '▁에너지': 3285, '▁에릭': 3286, '▁에스': 3287, '▁에어컨': 3288, '▁에이': 3289, '▁에이스': 3290, '▁에피소드': 3291, '▁엔': 3292, '▁엔진': 3293, '▁엔화': 3294, '▁엘': 3295, '▁엠': 3296, '▁엠넷': 3297, '▁여': 3298, '▁여객기': 3299, '▁여건': 3300, '▁여기': 3301, '▁여기에': 3302, '▁여당': 3303, '▁여러': 3304, '▁여러분': 3305, '▁여론': 3306, '▁여름': 3307, '▁여름철': 3308, '▁여배우': 3309, '▁여부': 3310, '▁여부를': 3311, '▁여성': 3312, '▁여성들': 3313, '▁여신': 3314, '▁여야': 3315, '▁여유': 3316, '▁여의도': 3317, '▁여자': 3318, '▁여자친구': 3319, '▁여전히': 3320, '▁여행': 3321, '▁역': 3322, '▁역대': 3323, '▁역량': 3324, '▁역사': 3325, '▁역사적': 3326, '▁역시': 3327, '▁역을': 3328, '▁역전': 3329, '▁역할': 3330, '▁역할을': 3331, '▁연': 3332, '▁연간': 3333, '▁연결': 3334, '▁연계': 3335, '▁연구': 3336, '▁연구개발': 3337, '▁연구원은': 3338, '▁연극': 3339, '▁연금': 3340, '▁연기': 3341, '▁연기를': 3342, '▁연락': 3343, '▁연령': 3344, '▁연말': 3345, '▁연방': 3346, '▁연봉': 3347, '▁연세대': 3348, '▁연속': 3349, '▁연습': 3350, '▁연애': 3351, '▁연예': 3352, '▁연예병사': 3353, '▁연예인': 3354, '▁연인': 3355, '▁연장': 3356, '▁연출': 3357, '▁열': 3358, '▁열고': 3359, '▁열람': 3360, '▁열렸다': 3361, '▁열리': 3362, '▁열리는': 3363, '▁열린': 3364, '▁열린다': 3365, '▁열릴': 3366, '▁열심히': 3367, '▁열애': 3368, '▁열애설': 3369, '▁열어': 3370, '▁열었다': 3371, '▁열정': 3372, '▁열차': 3373, '▁염': 3374, '▁염두에': 3375, '▁영': 3376, '▁영광': 3377, '▁영국': 3378, '▁영등포': 3379, '▁영상': 3380, '▁영어': 3381, '▁영업': 3382, '▁영업이익': 3383, '▁영업이익은': 3384, '▁영업익': 3385, '▁영업정지': 3386, '▁영역': 3387, '▁영입': 3388, '▁영하': 3389, '▁영향': 3390, '▁영향력': 3391, '▁영향으로': 3392, '▁영향을': 3393, '▁영화': 3394, '▁옆': 3395, '▁예': 3396, '▁예고': 3397, '▁예금': 3398, '▁예능': 3399, '▁예능프로그램': 3400, '▁예방': 3401, '▁예비': 3402, '▁예쁘': 3403, '▁예산': 3404, '▁예상': 3405, '▁예상되는': 3406, '▁예상된다': 3407, '▁예상치': 3408, '▁예상했다': 3409, '▁예술': 3410, '▁예약': 3411, '▁예전': 3412, '▁예정': 3413, '▁예정이다': 3414, '▁예측': 3415, '▁옛': 3416, '▁오': 3417, '▁오는': 3418, '▁오늘': 3419, '▁오늘의': 3420, '▁오디션': 3421, '▁오래': 3422, '▁오랜': 3423, '▁오랫동안': 3424, '▁오르': 3425, '▁오른': 3426, '▁오름세': 3427, '▁오바마': 3428, '▁오빠': 3429, '▁오연서': 3430, '▁오전': 3431, '▁오픈': 3432, '▁오피스텔': 3433, '▁오후': 3434, '▁오히려': 3435, '▁옥': 3436, '▁온': 3437, '▁온라인': 3438, '▁올': 3439, '▁올라': 3440, '▁올랐': 3441, '▁올랐다': 3442, '▁올려': 3443, '▁올렸다': 3444, '▁올리': 3445, '▁올린': 3446, '▁올림픽': 3447, '▁올스타': 3448, '▁올스타전': 3449, '▁올해': 3450, '▁올해부터': 3451, '▁옮겨': 3452, '▁옮기': 3453, '▁옷': 3454, '▁완': 3455, '▁완공': 3456, '▁완료': 3457, '▁완벽': 3458, '▁완벽한': 3459, '▁완성': 3460, '▁완전': 3461, '▁완전히': 3462, '▁완화': 3463, '▁왔다': 3464, '▁왕': 3465, '▁왜': 3466, '▁왜곡': 3467, '▁외': 3468, '▁외교': 3469, '▁외교부': 3470, '▁외국': 3471, '▁외국인': 3472, '▁외모': 3473, '▁외부': 3474, '▁외에': 3475, '▁외에도': 3476, '▁외환': 3477, '▁외환은행': 3478, '▁왼쪽': 3479, '▁요': 3480, '▁요구': 3481, '▁요구하는': 3482, '▁요구했다': 3483, '▁요금': 3484, '▁요금제': 3485, '▁요리': 3486, '▁요소': 3487, '▁요인': 3488, '▁요즘': 3489, '▁요청': 3490, '▁요청했다': 3491, '▁욕': 3492, '▁욕심': 3493, '▁용': 3494, '▁용산': 3495, '▁용의자': 3496, '▁용인': 3497, '▁우': 3498, '▁우려': 3499, '▁우려가': 3500, '▁우리': 3501, '▁우리가': 3502, '▁우리나라': 3503, '▁우리는': 3504, '▁우리은행': 3505, '▁우리투자증권': 3506, '▁우선': 3507, '▁우수': 3508, '▁우승': 3509, '▁우승을': 3510, '▁우주': 3511, '▁우즈': 3512, '▁운': 3513, '▁운동': 3514, '▁운명': 3515, '▁운송': 3516, '▁운영': 3517, '▁운영하는': 3518, '▁운용': 3519, '▁운전': 3520, '▁운전자': 3521, '▁운항': 3522, '▁운행': 3523, '▁울': 3524, '▁울산': 3525, '▁움직이': 3526, '▁움직임': 3527, '▁웃': 3528, '▁웃음을': 3529, '▁워': 3530, '▁워낙': 3531, '▁워싱턴': 3532, '▁원': 3533, '▁원내대변인': 3534, '▁원내대표': 3535, '▁원내대표는': 3536, '▁원래': 3537, '▁원인': 3538, '▁원장': 3539, '▁원전': 3540, '▁원정': 3541, '▁원정경기': 3542, '▁원칙': 3543, '▁원하는': 3544, '▁원화': 3545, '▁원활': 3546, '▁월드': 3547, '▁월드컵': 3548, '▁월화드라마': 3549, '▁웨딩': 3550, '▁웹': 3551, '▁위': 3552, '▁위기': 3553, '▁위로': 3554, '▁위반': 3555, '▁위안부': 3556, '▁위원': 3557, '▁위원장': 3558, '▁위원장은': 3559, '▁위조': 3560, '▁위촉': 3561, '▁위축': 3562, '▁위치': 3563, '▁위치한': 3564, '▁위탁': 3565, '▁위한': 3566, '▁위해': 3567, '▁위해서': 3568, '▁위해서는': 3569, '▁위헌': 3570, '▁위험': 3571, '▁위협': 3572, '▁윌리엄': 3573, '▁유': 3574, '▁유가증권시장': 3575, '▁유감': 3576, '▁유나이티드': 3577, '▁유니폼': 3578, '▁유도': 3579, '▁유동성': 3580, '▁유럽': 3581, '▁유력': 3582, '▁유로존': 3583, '▁유리': 3584, '▁유명': 3585, '▁유사': 3586, '▁유엔': 3587, '▁유일': 3588, '▁유입': 3589, '▁유재석': 3590, '▁유족': 3591, '▁유지': 3592, '▁유출': 3593, '▁유치': 3594, '▁유통': 3595, '▁유튜브': 3596, '▁육': 3597, '▁육군': 3598, '▁육박': 3599, '▁육성': 3600, '▁윤': 3601, '▁윤씨': 3602, '▁윤창중': 3603, '▁은퇴': 3604, '▁은행': 3605, '▁음': 3606, '▁음반': 3607, '▁음성': 3608, '▁음식': 3609, '▁음악': 3610, '▁음원': 3611, '▁응': 3612, '▁응급': 3613, '▁응답': 3614, '▁응답자': 3615, '▁응시': 3616, '▁응원': 3617, '▁의견': 3618, '▁의견을': 3619, '▁의견이': 3620, '▁의결': 3621, '▁의도': 3622, '▁의뢰': 3623, '▁의료': 3624, '▁의류': 3625, '▁의무': 3626, '▁의문': 3627, '▁의미': 3628, '▁의사': 3629, '▁의상': 3630, '▁의식': 3631, '▁의심': 3632, '▁의약품': 3633, '▁의원': 3634, '▁의원들': 3635, '▁의원은': 3636, '▁의원이': 3637, '▁의장': 3638, '▁의존': 3639, '▁의지': 3640, '▁의지를': 3641, '▁의한': 3642, '▁의해': 3643, '▁의혹': 3644, '▁의혹을': 3645, '▁의회': 3646, '▁이': 3647, '▁이같은': 3648, '▁이같이': 3649, '▁이것': 3650, '▁이곳': 3651, '▁이끄는': 3652, '▁이끌': 3653, '▁이끌어': 3654, '▁이끌었다': 3655, '▁이날': 3656, '▁이내': 3657, '▁이는': 3658, '▁이달': 3659, '▁이대호': 3660, '▁이데일리': 3661, '▁이동': 3662, '▁이동통신': 3663, '▁이동흡': 3664, '▁이들': 3665, '▁이들은': 3666, '▁이들의': 3667, '▁이들이': 3668, '▁이라크': 3669, '▁이래': 3670, '▁이러한': 3671, '▁이런': 3672, '▁이렇게': 3673, '▁이례적': 3674, '▁이로써': 3675, '▁이루': 3676, '▁이뤄': 3677, '▁이뤄졌다': 3678, '▁이뤄지': 3679, '▁이뤄진': 3680, '▁이뤄질': 3681, '▁이르는': 3682, '▁이르면': 3683, '▁이른다': 3684, '▁이른바': 3685, '▁이를': 3686, '▁이름': 3687, '▁이름을': 3688, '▁이마트': 3689, '▁이메일': 3690, '▁이명박': 3691, '▁이미': 3692, '▁이미지': 3693, '▁이미지를': 3694, '▁이민': 3695, '▁이밖에': 3696, '▁이번': 3697, '▁이번에': 3698, '▁이벤트': 3699, '▁이벤트를': 3700, '▁이병헌': 3701, '▁이사장': 3702, '▁이사회': 3703, '▁이상': 3704, '▁이상의': 3705, '▁이서진': 3706, '▁이수': 3707, '▁이슈': 3708, '▁이스라엘': 3709, '▁이슬람': 3710, '▁이승': 3711, '▁이승엽': 3712, '▁이시영': 3713, '▁이야기': 3714, '▁이야기를': 3715, '▁이어': 3716, '▁이어가고': 3717, '▁이어갔다': 3718, '▁이어졌다': 3719, '▁이어지고': 3720, '▁이어지는': 3721, '▁이어진': 3722, '▁이어질': 3723, '▁이에': 3724, '▁이와': 3725, '▁이용': 3726, '▁이용자': 3727, '▁이용한': 3728, '▁이용할': 3729, '▁이용해': 3730, '▁이웃': 3731, '▁이유': 3732, '▁이유는': 3733, '▁이유로': 3734, '▁이유를': 3735, '▁이익': 3736, '▁이재': 3737, '▁이적': 3738, '▁이전': 3739, '▁이정': 3740, '▁이정현': 3741, '▁이제': 3742, '▁이종': 3743, '▁이종석': 3744, '▁이준': 3745, '▁이중': 3746, '▁이집트': 3747, '▁이처럼': 3748, '▁이탈리아': 3749, '▁이틀': 3750, '▁이하': 3751, '▁이해': 3752, '▁이행': 3753, '▁이혼': 3754, '▁이효리': 3755, '▁이후': 3756, '▁익': 3757, '▁인': 3758, '▁인간': 3759, '▁인구': 3760, '▁인권': 3761, '▁인근': 3762, '▁인기': 3763, '▁인기를': 3764, '▁인도': 3765, '▁인도네시아': 3766, '▁인력': 3767, '▁인물': 3768, '▁인사': 3769, '▁인사들': 3770, '▁인사를': 3771, '▁인사청문회': 3772, '▁인상': 3773, '▁인생': 3774, '▁인선': 3775, '▁인쇄': 3776, '▁인수': 3777, '▁인수위': 3778, '▁인수위원': 3779, '▁인수위원회': 3780, '▁인식': 3781, '▁인연': 3782, '▁인원': 3783, '▁인재': 3784, '▁인정': 3785, '▁인정받': 3786, '▁인증': 3787, '▁인증샷': 3788, '▁인질': 3789, '▁인천': 3790, '▁인천공항': 3791, '▁인천시': 3792, '▁인터': 3793, '▁인터넷': 3794, '▁인터뷰': 3795, '▁인터뷰에서': 3796, '▁인턴기자': 3797, '▁인프라': 3798, '▁인피니트': 3799, '▁인하': 3800, '▁인한': 3801, '▁인해': 3802, '▁일': 3803, '▁일각에서는': 3804, '▁일단': 3805, '▁일대': 3806, '▁일반': 3807, '▁일방적': 3808, '▁일본': 3809, '▁일본의': 3810, '▁일부': 3811, '▁일상': 3812, '▁일어나': 3813, '▁일어난': 3814, '▁일으키': 3815, '▁일으킨': 3816, '▁일을': 3817, '▁일이': 3818, '▁일자리': 3819, '▁일정': 3820, '▁일제히': 3821, '▁일주일': 3822, '▁일환으로': 3823, '▁읽': 3824, '▁잃은': 3825, '▁임': 3826, '▁임금': 3827, '▁임기': 3828, '▁임대': 3829, '▁임명': 3830, '▁임시': 3831, '▁임시국회': 3832, '▁임신': 3833, '▁임원': 3834, '▁임직원': 3835, '▁입': 3836, '▁입건': 3837, '▁입고': 3838, '▁입단': 3839, '▁입력': 3840, '▁입법': 3841, '▁입원': 3842, '▁입은': 3843, '▁입장': 3844, '▁입장을': 3845, '▁입장이다': 3846, '▁입주': 3847, '▁입증': 3848, '▁입지': 3849, '▁입찰': 3850, '▁입학': 3851, '▁잇': 3852, '▁잇따라': 3853, '▁있': 3854, '▁있게': 3855, '▁있겠': 3856, '▁있고': 3857, '▁있기': 3858, '▁있느냐': 3859, '▁있는': 3860, '▁있는데': 3861, '▁있다': 3862, '▁있다고': 3863, '▁있다는': 3864, '▁있던': 3865, '▁있도록': 3866, '▁있습니다': 3867, '▁있어': 3868, '▁있어서': 3869, '▁있어야': 3870, '▁있었': 3871, '▁있었는데': 3872, '▁있었다': 3873, '▁있었던': 3874, '▁있었지만': 3875, '▁있으나': 3876, '▁있으니': 3877, '▁있으며': 3878, '▁있으면': 3879, '▁있을': 3880, '▁있을까': 3881, '▁있을지': 3882, '▁있음': 3883, '▁있지': 3884, '▁있지만': 3885, '▁자': 3886, '▁자격': 3887, '▁자극': 3888, '▁자금': 3889, '▁자기': 3890, '▁자녀': 3891, '▁자동': 3892, '▁자동차': 3893, '▁자랑': 3894, '▁자료': 3895, '▁자료를': 3896, '▁자리': 3897, '▁자리를': 3898, '▁자리에': 3899, '▁자리에서': 3900, '▁자발적': 3901, '▁자본': 3902, '▁자본시장': 3903, '▁자사': 3904, '▁자산': 3905, '▁자살': 3906, '▁자세': 3907, '▁자세한': 3908, '▁자신': 3909, '▁자신감': 3910, '▁자신들': 3911, '▁자신을': 3912, '▁자신의': 3913, '▁자신이': 3914, '▁자아냈다': 3915, '▁자연': 3916, '▁자연스럽게': 3917, '▁자원': 3918, '▁자원봉사': 3919, '▁자유': 3920, '▁자율': 3921, '▁자전거': 3922, '▁자존심': 3923, '▁자주': 3924, '▁자체': 3925, '▁자체가': 3926, '▁자칫': 3927, '▁자택': 3928, '▁자회사': 3929, '▁작': 3930, '▁작가': 3931, '▁작곡': 3932, '▁작년': 3933, '▁작동': 3934, '▁작성': 3935, '▁작업': 3936, '▁작업을': 3937, '▁작용': 3938, '▁작은': 3939, '▁작품': 3940, '▁잔': 3941, '▁잘': 3942, '▁잘못': 3943, '▁잘못된': 3944, '▁잠': 3945, '▁잠시': 3946, '▁잠실': 3947, '▁잠재': 3948, '▁잠정': 3949, '▁잡': 3950, '▁잡고': 3951, '▁잡아': 3952, '▁잡았다': 3953, '▁장': 3954, '▁장관': 3955, '▁장관은': 3956, '▁장기': 3957, '▁장난': 3958, '▁장르': 3959, '▁장면': 3960, '▁장비': 3961, '▁장소': 3962, '▁장애': 3963, '▁장애인': 3964, '▁장윤정': 3965, '▁장점': 3966, '▁장착': 3967, '▁장학금': 3968, '▁재': 3969, '▁재가동': 3970, '▁재개': 3971, '▁재건축': 3972, '▁재계약': 3973, '▁재난': 3974, '▁재능': 3975, '▁재무': 3976, '▁재미': 3977, '▁재미있': 3978, '▁재발': 3979, '▁재발방지': 3980, '▁재벌': 3981, '▁재산': 3982, '▁재원': 3983, '▁재정': 3984, '▁재정절벽': 3985, '▁재판': 3986, '▁재판부는': 3987, '▁재활': 3988, '▁쟁점': 3989, '▁저': 3990, '▁저녁': 3991, '▁저렴': 3992, '▁저소득층': 3993, '▁저장': 3994, '▁저지른': 3995, '▁적': 3996, '▁적극': 3997, '▁적극적으로': 3998, '▁적극적인': 3999, '▁적립': 4000, '▁적발': 4001, '▁적시타': 4002, '▁적어': 4003, '▁적용': 4004, '▁적은': 4005, '▁적응': 4006, '▁적이': 4007, '▁적자': 4008, '▁적절': 4009, '▁적지': 4010, '▁적합': 4011, '▁전': 4012, '▁전개': 4013, '▁전국': 4014, '▁전기': 4015, '▁전날': 4016, '▁전날보다': 4017, '▁전남': 4018, '▁전년': 4019, '▁전년대비': 4020, '▁전년동기': 4021, '▁전달': 4022, '▁전담': 4023, '▁전두환': 4024, '▁전략': 4025, '▁전력': 4026, '▁전망': 4027, '▁전망된다': 4028, '▁전망이다': 4029, '▁전망치': 4030, '▁전망했다': 4031, '▁전면': 4032, '▁전문': 4033, '▁전문가': 4034, '▁전문가들': 4035, '▁전문가들은': 4036, '▁전반': 4037, '▁전반기': 4038, '▁전부터': 4039, '▁전북': 4040, '▁전세': 4041, '▁전세계': 4042, '▁전시': 4043, '▁전액': 4044, '▁전에': 4045, '▁전역': 4046, '▁전용': 4047, '▁전원': 4048, '▁전자': 4049, '▁전쟁': 4050, '▁전주': 4051, '▁전지현': 4052, '▁전지훈련': 4053, '▁전체': 4054, '▁전체회의': 4055, '▁전통': 4056, '▁전투': 4057, '▁전파': 4058, '▁전해': 4059, '▁전해졌다': 4060, '▁전했다': 4061, '▁전혀': 4062, '▁전형': 4063, '▁전화': 4064, '▁전환': 4065, '▁전후': 4066, '▁절': 4067, '▁절감': 4068, '▁절대': 4069, '▁절반': 4070, '▁절차': 4071, '▁절차를': 4072, '▁젊은': 4073, '▁점': 4074, '▁점검': 4075, '▁점도': 4076, '▁점수': 4077, '▁점에서': 4078, '▁점유율': 4079, '▁점을': 4080, '▁점이': 4081, '▁점이다': 4082, '▁점점': 4083, '▁점차': 4084, '▁점포': 4085, '▁접': 4086, '▁접근': 4087, '▁접속': 4088, '▁접수': 4089, '▁접촉': 4090, '▁접한': 4091, '▁정': 4092, '▁정권': 4093, '▁정규': 4094, '▁정규직': 4095, '▁정기': 4096, '▁정당': 4097, '▁정당공천': 4098, '▁정도': 4099, '▁정도로': 4100, '▁정리': 4101, '▁정말': 4102, '▁정보': 4103, '▁정보를': 4104, '▁정보통신': 4105, '▁정부': 4106, '▁정부가': 4107, '▁정부는': 4108, '▁정부와': 4109, '▁정부의': 4110, '▁정부조직': 4111, '▁정비': 4112, '▁정상': 4113, '▁정상화': 4114, '▁정상회담': 4115, '▁정식': 4116, '▁정신': 4117, '▁정착': 4118, '▁정책': 4119, '▁정책을': 4120, '▁정체': 4121, '▁정치': 4122, '▁정치권': 4123, '▁정치적': 4124, '▁정확한': 4125, '▁정확히': 4126, '▁정황': 4127, '▁제': 4128, '▁제거': 4129, '▁제공': 4130, '▁제공하고': 4131, '▁제공하는': 4132, '▁제공한다': 4133, '▁제기': 4134, '▁제기되고': 4135, '▁제대로': 4136, '▁제도': 4137, '▁제목으로': 4138, '▁제시': 4139, '▁제시한': 4140, '▁제시했다': 4141, '▁제안': 4142, '▁제약': 4143, '▁제외': 4144, '▁제외한': 4145, '▁제임스': 4146, '▁제작': 4147, '▁제작발표회': 4148, '▁제작진': 4149, '▁제재': 4150, '▁제조': 4151, '▁제조업': 4152, '▁제조업체': 4153, '▁제주': 4154, '▁제주도': 4155, '▁제출': 4156, '▁제치고': 4157, '▁제품': 4158, '▁제품을': 4159, '▁제한': 4160, '▁제휴': 4161, '▁조': 4162, '▁조건': 4163, '▁조금': 4164, '▁조금씩': 4165, '▁조기': 4166, '▁조달': 4167, '▁조례': 4168, '▁조만간': 4169, '▁조명': 4170, '▁조사': 4171, '▁조사결과': 4172, '▁조사됐다': 4173, '▁조사를': 4174, '▁조사하고': 4175, '▁조선': 4176, '▁조성': 4177, '▁조성민': 4178, '▁조심': 4179, '▁조언': 4180, '▁조율': 4181, '▁조작': 4182, '▁조절': 4183, '▁조정': 4184, '▁조종사': 4185, '▁조직': 4186, '▁조직개편': 4187, '▁조치': 4188, '▁조치를': 4189, '▁조합원': 4190, '▁조항': 4191, '▁존': 4192, '▁존재': 4193, '▁존중': 4194, '▁졸업': 4195, '▁좀': 4196, '▁종': 4197, '▁종교': 4198, '▁종로구': 4199, '▁종료': 4200, '▁종류': 4201, '▁종목': 4202, '▁종합': 4203, '▁좋': 4204, '▁좋겠다': 4205, '▁좋다': 4206, '▁좋아': 4207, '▁좋았': 4208, '▁좋은': 4209, '▁좋지': 4210, '▁좌': 4211, '▁죄': 4212, '▁주': 4213, '▁주가': 4214, '▁주가가': 4215, '▁주거': 4216, '▁주고': 4217, '▁주관': 4218, '▁주는': 4219, '▁주도': 4220, '▁주력': 4221, '▁주로': 4222, '▁주말': 4223, '▁주말드라마': 4224, '▁주목': 4225, '▁주목된다': 4226, '▁주문': 4227, '▁주민': 4228, '▁주변': 4229, '▁주식': 4230, '▁주식시장': 4231, '▁주연': 4232, '▁주요': 4233, '▁주요뉴스': 4234, '▁주인공': 4235, '▁주장': 4236, '▁주장했다': 4237, '▁주재': 4238, '▁주제': 4239, '▁주제로': 4240, '▁주주': 4241, '▁주최': 4242, '▁주택': 4243, '▁죽': 4244, '▁죽음': 4245, '▁준': 4246, '▁준공': 4247, '▁준다': 4248, '▁준비': 4249, '▁준수': 4250, '▁준우승': 4251, '▁줄': 4252, '▁줄어든': 4253, '▁줄어들': 4254, '▁줄었다': 4255, '▁줄여': 4256, '▁중': 4257, '▁중간': 4258, '▁중국': 4259, '▁중국의': 4260, '▁중단': 4261, '▁중반': 4262, '▁중소': 4263, '▁중소기업': 4264, '▁중소형주': 4265, '▁중순': 4266, '▁중심': 4267, '▁중심으로': 4268, '▁중앙': 4269, '▁중요': 4270, '▁중요성': 4271, '▁중요하다': 4272, '▁중요한': 4273, '▁중이다': 4274, '▁중인': 4275, '▁중장기': 4276, '▁중점': 4277, '▁중흥': 4278, '▁즉': 4279, '▁즉각': 4280, '▁즉시': 4281, '▁즐거운': 4282, '▁즐기': 4283, '▁즐길': 4284, '▁증': 4285, '▁증가': 4286, '▁증가율': 4287, '▁증가한': 4288, '▁증가했다': 4289, '▁증거': 4290, '▁증권': 4291, '▁증권사': 4292, '▁증명': 4293, '▁증시': 4294, '▁증액': 4295, '▁증인': 4296, '▁지': 4297, '▁지구': 4298, '▁지금': 4299, '▁지금까지': 4300, '▁지금은': 4301, '▁지급': 4302, '▁지나': 4303, '▁지난': 4304, '▁지난달': 4305, '▁지난해': 4306, '▁지내': 4307, '▁지낸': 4308, '▁지닌': 4309, '▁지도': 4310, '▁지도부': 4311, '▁지도자': 4312, '▁지동원': 4313, '▁지명': 4314, '▁지목': 4315, '▁지방': 4316, '▁지방자치단체': 4317, '▁지배': 4318, '▁지분': 4319, '▁지사': 4320, '▁지속': 4321, '▁지속될': 4322, '▁지속적으로': 4323, '▁지속적인': 4324, '▁지수': 4325, '▁지시': 4326, '▁지식': 4327, '▁지식경제부': 4328, '▁지역': 4329, '▁지연': 4330, '▁지원': 4331, '▁지원을': 4332, '▁지원하는': 4333, '▁지원한다': 4334, '▁지자체': 4335, '▁지적': 4336, '▁지적했다': 4337, '▁지점': 4338, '▁지정': 4339, '▁지지': 4340, '▁지출': 4341, '▁지켜': 4342, '▁지켜보': 4343, '▁지키': 4344, '▁지표': 4345, '▁지하': 4346, '▁지하철': 4347, '▁지휘': 4348, '▁직': 4349, '▁직구': 4350, '▁직무': 4351, '▁직업': 4352, '▁직원': 4353, '▁직원들': 4354, '▁직장': 4355, '▁직장인': 4356, '▁직전': 4357, '▁직접': 4358, '▁직후': 4359, '▁진': 4360, '▁진단': 4361, '▁진료': 4362, '▁진보': 4363, '▁진술': 4364, '▁진실': 4365, '▁진입': 4366, '▁진주의료원': 4367, '▁진짜': 4368, '▁진출': 4369, '▁진행': 4370, '▁진행됐다': 4371, '▁진행되는': 4372, '▁진행된': 4373, '▁진행된다': 4374, '▁진행될': 4375, '▁진행하고': 4376, '▁진행한다': 4377, '▁진화': 4378, '▁질': 4379, '▁질문': 4380, '▁질문에': 4381, '▁질병': 4382, '▁짐': 4383, '▁집': 4384, '▁집계': 4385, '▁집계됐다': 4386, '▁집권': 4387, '▁집단': 4388, '▁집중': 4389, '▁집행': 4390, '▁집행유예': 4391, '▁집회': 4392, '▁짓': 4393, '▁징계': 4394, '▁징역': 4395, '▁짜': 4396, '▁짧은': 4397, '▁쪽': 4398, '▁쪽으로': 4399, '▁찍': 4400, '▁찍은': 4401, '▁차': 4402, '▁차관': 4403, '▁차기': 4404, '▁차단': 4405, '▁차량': 4406, '▁차례': 4407, '▁차별': 4408, '▁차별화된': 4409, '▁차세대': 4410, '▁차원에서': 4411, '▁차원의': 4412, '▁차이': 4413, '▁차이가': 4414, '▁차지': 4415, '▁차지하는': 4416, '▁차지한': 4417, '▁차지했다': 4418, '▁차질': 4419, '▁착': 4420, '▁착공': 4421, '▁착륙': 4422, '▁착수': 4423, '▁착용': 4424, '▁찬': 4425, '▁찬성': 4426, '▁참': 4427, '▁참가': 4428, '▁참가자들': 4429, '▁참고': 4430, '▁참석': 4431, '▁참석한': 4432, '▁참석해': 4433, '▁참석했다': 4434, '▁참여': 4435, '▁참여한': 4436, '▁참의원': 4437, '▁창': 4438, '▁창단': 4439, '▁창업': 4440, '▁창원': 4441, '▁창조': 4442, '▁창출': 4443, '▁찾기': 4444, '▁찾는': 4445, '▁찾아': 4446, '▁찾았다': 4447, '▁찾은': 4448, '▁찾을': 4449, '▁찾지': 4450, '▁채': 4451, '▁채권': 4452, '▁채널': 4453, '▁채무': 4454, '▁채용': 4455, '▁채택': 4456, '▁책': 4457, '▁책임': 4458, '▁책임을': 4459, '▁챔피언십': 4460, '▁챙겨': 4461, '▁챙기': 4462, '▁챙긴': 4463, '▁처': 4464, '▁처리': 4465, '▁처벌': 4466, '▁처분': 4467, '▁처음': 4468, '▁처음으로': 4469, '▁처음이다': 4470, '▁천': 4471, '▁천안': 4472, '▁철': 4473, '▁철강': 4474, '▁철거': 4475, '▁철저한': 4476, '▁철저히': 4477, '▁철학': 4478, '▁철회': 4479, '▁첨단': 4480, '▁첫': 4481, '▁첫날': 4482, '▁청': 4483, '▁청구': 4484, '▁청년': 4485, '▁청문회': 4486, '▁청소년': 4487, '▁청약': 4488, '▁청와대': 4489, '▁청주': 4490, '▁체': 4491, '▁체결': 4492, '▁체결했다': 4493, '▁체계적': 4494, '▁체력': 4495, '▁체제': 4496, '▁체크': 4497, '▁체포': 4498, '▁체험': 4499, '▁첼시': 4500, '▁초': 4501, '▁초과': 4502, '▁초기': 4503, '▁초대': 4504, '▁초등학교': 4505, '▁초반': 4506, '▁초점을': 4507, '▁초청': 4508, '▁촉구': 4509, '▁촉구했다': 4510, '▁촉진': 4511, '▁총': 4512, '▁총괄': 4513, '▁총기': 4514, '▁총리': 4515, '▁총장': 4516, '▁총재': 4517, '▁촬영': 4518, '▁최': 4519, '▁최강': 4520, '▁최강희': 4521, '▁최고': 4522, '▁최고위원': 4523, '▁최고의': 4524, '▁최근': 4525, '▁최다': 4526, '▁최대': 4527, '▁최대주주': 4528, '▁최대한': 4529, '▁최선을': 4530, '▁최소': 4531, '▁최소화': 4532, '▁최신': 4533, '▁최악': 4534, '▁최우선': 4535, '▁최우수': 4536, '▁최저': 4537, '▁최종': 4538, '▁최초': 4539, '▁최초로': 4540, '▁추': 4541, '▁추가': 4542, '▁추가로': 4543, '▁추격': 4544, '▁추구': 4545, '▁추락': 4546, '▁추산': 4547, '▁추세': 4548, '▁추신수': 4549, '▁추신수는': 4550, '▁추억': 4551, '▁추적': 4552, '▁추정': 4553, '▁추정된다': 4554, '▁추진': 4555, '▁추진하고': 4556, '▁추진할': 4557, '▁추징금': 4558, '▁추천': 4559, '▁추첨': 4560, '▁축': 4561, '▁축구': 4562, '▁축구협회': 4563, '▁축소': 4564, '▁축제': 4565, '▁축하': 4566, '▁춘천': 4567, '▁출': 4568, '▁출구전략': 4569, '▁출국': 4570, '▁출근': 4571, '▁출동': 4572, '▁출발': 4573, '▁출범': 4574, '▁출산': 4575, '▁출석': 4576, '▁출시': 4577, '▁출신': 4578, '▁출연': 4579, '▁출연한': 4580, '▁출연해': 4581, '▁출연했다': 4582, '▁출입': 4583, '▁출장': 4584, '▁출전': 4585, '▁출처': 4586, '▁출판': 4587, '▁충': 4588, '▁충격': 4589, '▁충남': 4590, '▁충돌': 4591, '▁충북': 4592, '▁충분': 4593, '▁충분히': 4594, '▁충실': 4595, '▁충족': 4596, '▁충청': 4597, '▁취': 4598, '▁취급': 4599, '▁취득': 4600, '▁취득세': 4601, '▁취소': 4602, '▁취약': 4603, '▁취업': 4604, '▁취임': 4605, '▁취임식': 4606, '▁취재': 4607, '▁취재진': 4608, '▁취지': 4609, '▁취하고': 4610, '▁측': 4611, '▁측근': 4612, '▁측면': 4613, '▁측면에서': 4614, '▁측은': 4615, '▁측정': 4616, '▁치': 4617, '▁치러': 4618, '▁치료': 4619, '▁치료를': 4620, '▁치르': 4621, '▁치른': 4622, '▁치솟': 4623, '▁치열': 4624, '▁치열한': 4625, '▁친': 4626, '▁친구': 4627, '▁친구들': 4628, '▁친환경': 4629, '▁침': 4630, '▁침수': 4631, '▁침체': 4632, '▁침해': 4633, '▁칭찬': 4634, '▁카': 4635, '▁카드': 4636, '▁카드사': 4637, '▁카리스마': 4638, '▁카메라': 4639, '▁카카오': 4640, '▁카페': 4641, '▁칼': 4642, '▁캐': 4643, '▁캐나다': 4644, '▁캐릭터': 4645, '▁캐스팅': 4646, '▁캘리포니아': 4647, '▁캠페인': 4648, '▁캠프': 4649, '▁캠핑': 4650, '▁캡처': 4651, '▁커': 4652, '▁커뮤니티': 4653, '▁커지고': 4654, '▁커플': 4655, '▁커피': 4656, '▁컨트롤': 4657, '▁컬러': 4658, '▁컴백': 4659, '▁컴퓨터': 4660, '▁컸다': 4661, '▁케': 4662, '▁케이': 4663, '▁케이블': 4664, '▁코': 4665, '▁코너': 4666, '▁코넥스': 4667, '▁코리아': 4668, '▁코미디': 4669, '▁코스': 4670, '▁코스닥': 4671, '▁코스피': 4672, '▁코스피지수': 4673, '▁코치': 4674, '▁콘': 4675, '▁콘서트': 4676, '▁콘셉트': 4677, '▁콘텐츠': 4678, '▁콜': 4679, '▁쿠': 4680, '▁크': 4681, '▁크게': 4682, '▁크기': 4683, '▁크다': 4684, '▁크로스': 4685, '▁크루즈': 4686, '▁크리스': 4687, '▁큰': 4688, '▁클': 4689, '▁클라라': 4690, '▁클래식': 4691, '▁클럽': 4692, '▁키': 4693, '▁키스': 4694, '▁키우': 4695, '▁키워': 4696, '▁키워드': 4697, '▁타': 4698, '▁타격': 4699, '▁타고': 4700, '▁타선': 4701, '▁타율': 4702, '▁타이': 4703, '▁타이틀': 4704, '▁타이틀곡': 4705, '▁타자': 4706, '▁탄': 4707, '▁탄력': 4708, '▁탄생': 4709, '▁탄탄한': 4710, '▁탈': 4711, '▁탈락': 4712, '▁탈출': 4713, '▁탐': 4714, '▁탑승': 4715, '▁탑재': 4716, '▁탓': 4717, '▁탓에': 4718, '▁탔다': 4719, '▁태': 4720, '▁태국': 4721, '▁태도': 4722, '▁태블릿': 4723, '▁태양': 4724, '▁태양광': 4725, '▁태어났다': 4726, '▁택시': 4727, '▁터': 4728, '▁터치': 4729, '▁터키': 4730, '▁털어놓': 4731, '▁털어놨다': 4732, '▁테': 4733, '▁테러': 4734, '▁테마': 4735, '▁테스트': 4736, '▁토': 4737, '▁토대로': 4738, '▁토론회': 4739, '▁토지': 4740, '▁토크쇼': 4741, '▁톱스타': 4742, '▁통': 4743, '▁통계': 4744, '▁통과': 4745, '▁통보': 4746, '▁통산': 4747, '▁통상': 4748, '▁통신': 4749, '▁통일': 4750, '▁통일부': 4751, '▁통제': 4752, '▁통증': 4753, '▁통한': 4754, '▁통합': 4755, '▁통해': 4756, '▁통행': 4757, '▁통화': 4758, '▁통화정책': 4759, '▁퇴': 4760, '▁퇴직': 4761, '▁투': 4762, '▁투구': 4763, '▁투명': 4764, '▁투수': 4765, '▁투어': 4766, '▁투입': 4767, '▁투자': 4768, '▁투자의견': 4769, '▁투자자': 4770, '▁투자자들': 4771, '▁투표': 4772, '▁트': 4773, '▁트렌드': 4774, '▁트위터': 4775, '▁트위터에': 4776, '▁특': 4777, '▁특별': 4778, '▁특별사면': 4779, '▁특별한': 4780, '▁특사': 4781, '▁특성': 4782, '▁특수': 4783, '▁특위': 4784, '▁특유': 4785, '▁특정': 4786, '▁특집': 4787, '▁특징': 4788, '▁특징이다': 4789, '▁특허': 4790, '▁특혜': 4791, '▁특히': 4792, '▁틀': 4793, '▁티아라': 4794, '▁티저': 4795, '▁티켓': 4796, '▁팀': 4797, '▁팀의': 4798, '▁파': 4799, '▁파견': 4800, '▁파괴': 4801, '▁파악': 4802, '▁파워': 4803, '▁파트너': 4804, '▁판': 4805, '▁판결': 4806, '▁판단': 4807, '▁판단했다': 4808, '▁판매': 4809, '▁판매량': 4810, '▁판문점': 4811, '▁판사': 4812, '▁판정': 4813, '▁팔': 4814, '▁패': 4815, '▁패널': 4816, '▁패배': 4817, '▁패션': 4818, '▁패스': 4819, '▁패키지': 4820, '▁팬': 4821, '▁팬들': 4822, '▁팬들에게': 4823, '▁팬들의': 4824, '▁팽팽': 4825, '▁퍼': 4826, '▁퍼포먼스': 4827, '▁펀드': 4828, '▁페': 4829, '▁페널티': 4830, '▁페이스북': 4831, '▁편': 4832, '▁편리하게': 4833, '▁편성': 4834, '▁편의점': 4835, '▁편집': 4836, '▁펼쳐': 4837, '▁펼쳤다': 4838, '▁펼치고': 4839, '▁펼칠': 4840, '▁평': 4841, '▁평가': 4842, '▁평가를': 4843, '▁평가했다': 4844, '▁평균': 4845, '▁평균자책점': 4846, '▁평생': 4847, '▁평소': 4848, '▁평창': 4849, '▁평택': 4850, '▁평화': 4851, '▁폐': 4852, '▁폐기': 4853, '▁폐쇄': 4854, '▁폐지': 4855, '▁포': 4856, '▁포기': 4857, '▁포스코': 4858, '▁포인트': 4859, '▁포즈를': 4860, '▁포착': 4861, '▁포털': 4862, '▁포토': 4863, '▁포함': 4864, '▁포함돼': 4865, '▁포함됐다': 4866, '▁포함된': 4867, '▁포함한': 4868, '▁포함해': 4869, '▁포항': 4870, '▁폭': 4871, '▁폭력': 4872, '▁폭로': 4873, '▁폭발': 4874, '▁폭염': 4875, '▁폭우': 4876, '▁폭풍': 4877, '▁폭행': 4878, '▁폴': 4879, '▁표': 4880, '▁표명': 4881, '▁표시': 4882, '▁표정': 4883, '▁표준': 4884, '▁표현': 4885, '▁푸': 4886, '▁푸이그': 4887, '▁풀': 4888, '▁풀어': 4889, '▁풀이된다': 4890, '▁품': 4891, '▁품목': 4892, '▁품질': 4893, '▁풍': 4894, '▁풍부한': 4895, '▁프랑스': 4896, '▁프로': 4897, '▁프로그램': 4898, '▁프로그램을': 4899, '▁프로모션': 4900, '▁프로야구': 4901, '▁프로젝트': 4902, '▁프로축구': 4903, '▁프리': 4904, '▁프리미어리그': 4905, '▁프리미엄': 4906, '▁플랫폼': 4907, '▁플레이': 4908, '▁피': 4909, '▁피부': 4910, '▁피의자': 4911, '▁피해': 4912, '▁피해가': 4913, '▁피해를': 4914, '▁피해자': 4915, '▁필': 4916, '▁필리핀': 4917, '▁필수': 4918, '▁필요': 4919, '▁필요가': 4920, '▁필요성': 4921, '▁필요하다': 4922, '▁필요한': 4923, '▁하': 4924, '▁하겠다': 4925, '▁하고': 4926, '▁하기': 4927, '▁하나': 4928, '▁하나로': 4929, '▁하는': 4930, '▁하는데': 4931, '▁하다': 4932, '▁하락': 4933, '▁하락세': 4934, '▁하락한': 4935, '▁하락했다': 4936, '▁하루': 4937, '▁하며': 4938, '▁하면': 4939, '▁하면서': 4940, '▁하반기': 4941, '▁하이': 4942, '▁하자': 4943, '▁하정우': 4944, '▁하지': 4945, '▁하지만': 4946, '▁하향': 4947, '▁학': 4948, '▁학교': 4949, '▁학교폭력': 4950, '▁학부모': 4951, '▁학생': 4952, '▁학생들': 4953, '▁학습': 4954, '▁한': 4955, '▁한강': 4956, '▁한계': 4957, '▁한국': 4958, '▁한국거래소': 4959, '▁한국은': 4960, '▁한국은행': 4961, '▁한국의': 4962, '▁한국인': 4963, '▁한국전력': 4964, '▁한다': 4965, '▁한다고': 4966, '▁한다는': 4967, '▁한때': 4968, '▁한마디': 4969, '▁한반도': 4970, '▁한번': 4971, '▁한층': 4972, '▁한파': 4973, '▁한편': 4974, '▁한혜진': 4975, '▁한화': 4976, '▁할': 4977, '▁할리우드': 4978, '▁할머니': 4979, '▁할부': 4980, '▁할인': 4981, '▁함': 4982, '▁함께': 4983, '▁합': 4984, '▁합격': 4985, '▁합니다': 4986, '▁합동': 4987, '▁합류': 4988, '▁합리적': 4989, '▁합병': 4990, '▁합의': 4991, '▁항': 4992, '▁항공': 4993, '▁항공기': 4994, '▁항목': 4995, '▁항상': 4996, '▁항소심': 4997, '▁해': 4998, '▁해결': 4999, '▁해당': 5000, '▁해당하는': 5001, '▁해도': 5002, '▁해명': 5003, '▁해명했다': 5004, '▁해병대': 5005, '▁해상': 5006, '▁해서': 5007, '▁해석': 5008, '▁해소': 5009, '▁해야': 5010, '▁해양': 5011, '▁해외': 5012, '▁해운대': 5013, '▁핵': 5014, '▁핵실험': 5015, '▁핵심': 5016, '▁했': 5017, '▁했는데': 5018, '▁했다': 5019, '▁했던': 5020, '▁했지만': 5021, '▁행': 5022, '▁행동': 5023, '▁행보': 5024, '▁행복': 5025, '▁행사': 5026, '▁행사를': 5027, '▁행사에': 5028, '▁행위': 5029, '▁행정': 5030, '▁행진': 5031, '▁향': 5032, '▁향상': 5033, '▁향한': 5034, '▁향해': 5035, '▁향후': 5036, '▁허': 5037, '▁허가': 5038, '▁허리': 5039, '▁허용': 5040, '▁허위': 5041, '▁헌': 5042, '▁헌법': 5043, '▁헌법재판소장': 5044, '▁헌재': 5045, '▁헤': 5046, '▁헤어': 5047, '▁혁신': 5048, '▁현': 5049, '▁현금': 5050, '▁현대': 5051, '▁현대건설': 5052, '▁현대모비스': 5053, '▁현대자동차': 5054, '▁현대중공업': 5055, '▁현대차': 5056, '▁현대캐피탈': 5057, '▁현상': 5058, '▁현실': 5059, '▁현안': 5060, '▁현역': 5061, '▁현장': 5062, '▁현장에서': 5063, '▁현재': 5064, '▁현재까지': 5065, '▁현지': 5066, '▁현행': 5067, '▁현황': 5068, '▁혈': 5069, '▁혐의': 5070, '▁혐의로': 5071, '▁혐의를': 5072, '▁협력': 5073, '▁협력업체': 5074, '▁협박': 5075, '▁협상': 5076, '▁협약': 5077, '▁협업': 5078, '▁협의': 5079, '▁협조': 5080, '▁형': 5081, '▁형사': 5082, '▁형성': 5083, '▁형식': 5084, '▁형제': 5085, '▁형태': 5086, '▁형태로': 5087, '▁혜택': 5088, '▁혜택을': 5089, '▁호': 5090, '▁호소': 5091, '▁호응': 5092, '▁호조': 5093, '▁호주': 5094, '▁호텔': 5095, '▁호투': 5096, '▁호평': 5097, '▁호흡': 5098, '▁혹은': 5099, '▁혼': 5100, '▁혼란': 5101, '▁혼자': 5102, '▁홀': 5103, '▁홈': 5104, '▁홈경기': 5105, '▁홈런': 5106, '▁홈페이지': 5107, '▁홍': 5108, '▁홍명보': 5109, '▁홍보': 5110, '▁홍콩': 5111, '▁화': 5112, '▁화려한': 5113, '▁화면': 5114, '▁화보': 5115, '▁화성': 5116, '▁화이트': 5117, '▁화장실': 5118, '▁화장품': 5119, '▁화재': 5120, '▁화제': 5121, '▁화제다': 5122, '▁화학': 5123, '▁확': 5124, '▁확대': 5125, '▁확보': 5126, '▁확산': 5127, '▁확실한': 5128, '▁확실히': 5129, '▁확인': 5130, '▁확인됐다': 5131, '▁확인할': 5132, '▁확장': 5133, '▁확정': 5134, '▁확충': 5135, '▁환': 5136, '▁환경': 5137, '▁환영': 5138, '▁환율': 5139, '▁환자': 5140, '▁활동': 5141, '▁활동을': 5142, '▁활발': 5143, '▁활성화': 5144, '▁활약': 5145, '▁활용': 5146, '▁활용해': 5147, '▁활주로': 5148, '▁황': 5149, '▁황금': 5150, '▁황우여': 5151, '▁회': 5152, '▁회계': 5153, '▁회담': 5154, '▁회복': 5155, '▁회사': 5156, '▁회사채': 5157, '▁회원': 5158, '▁회원가입': 5159, '▁회의': 5160, '▁회의록': 5161, '▁회의를': 5162, '▁회장': 5163, '▁회장은': 5164, '▁회장의': 5165, '▁회장이': 5166, '▁획득': 5167, '▁횡령': 5168, '▁효': 5169, '▁효과': 5170, '▁효과가': 5171, '▁효과를': 5172, '▁효과적': 5173, '▁효율성': 5174, '▁효율적': 5175, '▁후': 5176, '▁후문이다': 5177, '▁후반': 5178, '▁후반기': 5179, '▁후보': 5180, '▁후보자': 5181, '▁후속': 5182, '▁후원': 5183, '▁훈련': 5184, '▁훌륭': 5185, '▁훔친': 5186, '▁훨씬': 5187, '▁훼손': 5188, '▁휘': 5189, '▁휩쓸': 5190, '▁휴': 5191, '▁휴가': 5192, '▁휴대전화': 5193, '▁휴대폰': 5194, '▁휴식': 5195, '▁흐': 5196, '▁흐름': 5197, '▁흑자': 5198, '▁흔들': 5199, '▁흔적': 5200, '▁흘러': 5201, '▁흘리': 5202, '▁흡수': 5203, '▁흡연': 5204, '▁흥국생명': 5205, '▁흥미': 5206, '▁흥행': 5207, '▁희망': 5208, '▁희생': 5209, '▁히트': 5210, '▁힘': 5211, '▁힘든': 5212, '▁힘들': 5213, '▁힘을': 5214, '▁힘입어': 5215, '▁힙합': 5216, '■': 5217, '□': 5218, '▦': 5219, '▲': 5220, '△': 5221, '▶': 5222, '▷': 5223, '▼': 5224, '▽': 5225, '◀': 5226, '◆': 5227, '◇': 5228, '◈': 5229, '○': 5230, '★': 5231, '☎': 5232, '〃': 5233, '〈': 5234, '〉': 5235, '「': 5236, '」': 5237, '『': 5238, '』': 5239, '【': 5240, '】': 5241, '一': 5242, '三': 5243, '上': 5244, '下': 5245, '不': 5246, '中': 5247, '亞': 5248, '京': 5249, '人': 5250, '企': 5251, '倍': 5252, '先': 5253, '公': 5254, '前': 5255, '北': 5256, '南': 5257, '反': 5258, '史': 5259, '國': 5260, '報': 5261, '外': 5262, '大': 5263, '天': 5264, '女': 5265, '子': 5266, '安': 5267, '家': 5268, '對': 5269, '小': 5270, '山': 5271, '島': 5272, '州': 5273, '市': 5274, '平': 5275, '年': 5276, '弗': 5277, '心': 5278, '性': 5279, '故': 5280, '文': 5281, '新': 5282, '日': 5283, '晋': 5284, '朴': 5285, '李': 5286, '東': 5287, '株': 5288, '檢': 5289, '母': 5290, '比': 5291, '民': 5292, '江': 5293, '海': 5294, '無': 5295, '獨': 5296, '王': 5297, '生': 5298, '田': 5299, '甲': 5300, '男': 5301, '百': 5302, '盧': 5303, '知': 5304, '硏': 5305, '社': 5306, '美': 5307, '習': 5308, '胎': 5309, '與': 5310, '英': 5311, '草': 5312, '行': 5313, '西': 5314, '證': 5315, '車': 5316, '軍': 5317, '近': 5318, '道': 5319, '重': 5320, '野': 5321, '金': 5322, '銀': 5323, '電': 5324, '靑': 5325, '非': 5326, '韓': 5327, '高': 5328, '鬼': 5329, '가': 5330, '가격': 5331, '가구': 5332, '가량': 5333, '가족': 5334, '가지': 5335, '각': 5336, '간': 5337, '간다': 5338, '갇': 5339, '갈': 5340, '감': 5341, '감독': 5342, '감사': 5343, '감을': 5344, '갑': 5345, '값': 5346, '갓': 5347, '갔': 5348, '갔다': 5349, '강': 5350, '강남스타일': 5351, '강심장': 5352, '갖': 5353, '같': 5354, '같은': 5355, '갚': 5356, '개': 5357, '개국': 5358, '개그콘서트': 5359, '개로': 5360, '개를': 5361, '개발': 5362, '개사': 5363, '개선': 5364, '개성공단': 5365, '개월': 5366, '개월간': 5367, '개의': 5368, '개혁': 5369, '객': 5370, '갤': 5371, '갭': 5372, '갯': 5373, '갱': 5374, '갸': 5375, '걀': 5376, '거': 5377, '거나': 5378, '거래': 5379, '거래소': 5380, '거래일': 5381, '거리': 5382, '걱': 5383, '건': 5384, '건강': 5385, '건설': 5386, '건으로': 5387, '건전성': 5388, '걷': 5389, '걸': 5390, '걸음': 5391, '검': 5392, '검사': 5393, '검색': 5394, '검증': 5395, '검찰': 5396, '겁': 5397, '것': 5398, '겉': 5399, '게': 5400, '게임': 5401, '겐': 5402, '겔': 5403, '겟': 5404, '겠': 5405, '겠다': 5406, '겠다고': 5407, '겠다는': 5408, '겠습니다': 5409, '겠지만': 5410, '겨': 5411, '격': 5412, '겪': 5413, '견': 5414, '결': 5415, '결과': 5416, '결정': 5417, '결제': 5418, '결혼': 5419, '겸': 5420, '겹': 5421, '겼': 5422, '겼다': 5423, '경': 5424, '경기': 5425, '경기에서': 5426, '경영': 5427, '경쟁': 5428, '경쟁력': 5429, '경제': 5430, '경찰': 5431, '경찰서': 5432, '경찰서는': 5433, '경찰청': 5434, '곁': 5435, '계': 5436, '계약': 5437, '계획': 5438, '고': 5439, '고객': 5440, '고등학교': 5441, '고속도로': 5442, '곡': 5443, '곤': 5444, '곧': 5445, '골': 5446, '골을': 5447, '골프': 5448, '곰': 5449, '곱': 5450, '곳': 5451, '공': 5452, '공간': 5453, '공개': 5454, '공급': 5455, '공단': 5456, '공동체': 5457, '공무원': 5458, '공사': 5459, '공약': 5460, '공업': 5461, '공연': 5462, '공원': 5463, '공장': 5464, '공항': 5465, '공화국': 5466, '곶': 5467, '과': 5468, '과정': 5469, '과정에서': 5470, '과학': 5471, '과학기술': 5472, '곽': 5473, '관': 5474, '관계': 5475, '관광': 5476, '관련': 5477, '관리': 5478, '괄': 5479, '괌': 5480, '광': 5481, '광고': 5482, '광역시': 5483, '괜': 5484, '괴': 5485, '괴물': 5486, '굉': 5487, '교': 5488, '교사': 5489, '교섭': 5490, '교육': 5491, '교육청': 5492, '교통': 5493, '교회': 5494, '구': 5495, '구나': 5496, '구단': 5497, '구를': 5498, '구역': 5499, '구장에서': 5500, '구조': 5501, '구청장': 5502, '국': 5503, '국가': 5504, '국내': 5505, '국내외': 5506, '국민': 5507, '국장': 5508, '국정원': 5509, '국제': 5510, '국회': 5511, '군': 5512, '군은': 5513, '군의': 5514, '굳': 5515, '굴': 5516, '굵': 5517, '굶': 5518, '굽': 5519, '굿': 5520, '궁': 5521, '궂': 5522, '궈': 5523, '권': 5524, '권을': 5525, '궐': 5526, '궜': 5527, '궤': 5528, '귀': 5529, '귀태': 5530, '귄': 5531, '규': 5532, '규모': 5533, '규제': 5534, '규칙': 5535, '균': 5536, '귤': 5537, '그': 5538, '그동안': 5539, '그래': 5540, '그런': 5541, '그룹': 5542, '극': 5543, '극본': 5544, '극장': 5545, '근': 5546, '글': 5547, '글로벌': 5548, '긁': 5549, '금': 5550, '금리': 5551, '금속': 5552, '금액': 5553, '금융': 5554, '금융그룹': 5555, '금융지주': 5556, '금을': 5557, '급': 5558, '긋': 5559, '긍': 5560, '기': 5561, '기가': 5562, '기간': 5563, '기관': 5564, '기구': 5565, '기금': 5566, '기념': 5567, '기는': 5568, '기능': 5569, '기도': 5570, '기로': 5571, '기록': 5572, '기를': 5573, '기사': 5574, '기술': 5575, '기아차': 5576, '기업': 5577, '기업은행': 5578, '기에': 5579, '기자': 5580, '기준': 5581, '기지': 5582, '기획': 5583, '긴': 5584, '길': 5585, '김': 5586, '깁': 5587, '깃': 5588, '깅': 5589, '깊': 5590, '까': 5591, '까지': 5592, '깍': 5593, '깎': 5594, '깐': 5595, '깔': 5596, '깜': 5597, '깜찍': 5598, '깝': 5599, '깡': 5600, '깥': 5601, '깨': 5602, '깬': 5603, '꺼': 5604, '꺾': 5605, '껄': 5606, '껌': 5607, '껍': 5608, '껏': 5609, '껑': 5610, '께': 5611, '께서': 5612, '껴': 5613, '꼈': 5614, '꼬': 5615, '꼭': 5616, '꼴': 5617, '꼼': 5618, '꼽': 5619, '꽁': 5620, '꽂': 5621, '꽃': 5622, '꽃보다': 5623, '꽉': 5624, '꽝': 5625, '꽤': 5626, '꾀': 5627, '꾸': 5628, '꾼': 5629, '꿀': 5630, '꿇': 5631, '꿈': 5632, '꿋': 5633, '꿔': 5634, '꿨': 5635, '꿰': 5636, '뀌': 5637, '뀐': 5638, '뀔': 5639, '끄': 5640, '끄러': 5641, '끈': 5642, '끊': 5643, '끌': 5644, '끓': 5645, '끔': 5646, '끗': 5647, '끝': 5648, '끼': 5649, '끼리': 5650, '끽': 5651, '낀': 5652, '낄': 5653, '낌': 5654, '나': 5655, '나갈': 5656, '나갔다': 5657, '나는': 5658, '나라': 5659, '나무': 5660, '낙': 5661, '낚': 5662, '난': 5663, '날': 5664, '낡': 5665, '남': 5666, '남북': 5667, '남자': 5668, '납': 5669, '낫': 5670, '났': 5671, '났다': 5672, '낭': 5673, '낮': 5674, '낯': 5675, '낱': 5676, '낳': 5677, '내': 5678, '내가': 5679, '내기': 5680, '낵': 5681, '낸': 5682, '낼': 5683, '냄': 5684, '냅': 5685, '냈': 5686, '냈다': 5687, '냉': 5688, '냐': 5689, '냥': 5690, '너': 5691, '너목들': 5692, '넉': 5693, '넋': 5694, '넌': 5695, '널': 5696, '넓': 5697, '넘': 5698, '넛': 5699, '넝': 5700, '넣': 5701, '네': 5702, '네요': 5703, '네트워크': 5704, '넥': 5705, '넨': 5706, '넬': 5707, '넷': 5708, '넸': 5709, '녀': 5710, '녁': 5711, '년': 5712, '년간': 5713, '년까지': 5714, '년대': 5715, '년만에': 5716, '년부터': 5717, '년생': 5718, '년에': 5719, '년에는': 5720, '년째': 5721, '념': 5722, '녔': 5723, '녕': 5724, '노': 5725, '노동': 5726, '노조': 5727, '노컷': 5728, '노트': 5729, '녹': 5730, '논': 5731, '놀': 5732, '놈': 5733, '농': 5734, '농협': 5735, '높': 5736, '놓': 5737, '놓고': 5738, '놔': 5739, '놨': 5740, '뇌': 5741, '뇨': 5742, '누': 5743, '눅': 5744, '눈': 5745, '눌': 5746, '눔': 5747, '눕': 5748, '눠': 5749, '눴': 5750, '뉘': 5751, '뉜': 5752, '뉴': 5753, '뉴스': 5754, '뉴시스': 5755, '늄': 5756, '느': 5757, '느냐': 5758, '늑': 5759, '는': 5760, '는데': 5761, '늘': 5762, '늙': 5763, '늠': 5764, '능': 5765, '능력': 5766, '늦': 5767, '늪': 5768, '늬': 5769, '니': 5770, '니까': 5771, '니다': 5772, '니스': 5773, '니아': 5774, '닉': 5775, '닌': 5776, '닐': 5777, '님': 5778, '닙': 5779, '닛': 5780, '닝': 5781, '다': 5782, '다른': 5783, '다면': 5784, '다운': 5785, '닥': 5786, '닦': 5787, '단': 5788, '단계': 5789, '단지': 5790, '단체': 5791, '닫': 5792, '달': 5793, '달라': 5794, '달러': 5795, '닭': 5796, '닮': 5797, '담': 5798, '담당': 5799, '담보대출': 5800, '답': 5801, '닷': 5802, '닷컴': 5803, '당': 5804, '당국': 5805, '당선인': 5806, '닿': 5807, '대': 5808, '대가': 5809, '대강': 5810, '대교': 5811, '대로': 5812, '대를': 5813, '대비': 5814, '대사관': 5815, '대상': 5816, '대우증권': 5817, '대의': 5818, '대책': 5819, '대출': 5820, '대통령': 5821, '대표': 5822, '대표팀': 5823, '대학': 5824, '대학교': 5825, '대학원': 5826, '대한': 5827, '대화록': 5828, '대회': 5829, '댁': 5830, '댄': 5831, '댈': 5832, '댐': 5833, '댓': 5834, '댔': 5835, '댜': 5836, '더': 5837, '더니': 5838, '더라': 5839, '더라도': 5840, '덕': 5841, '던': 5842, '덜': 5843, '덟': 5844, '덤': 5845, '덥': 5846, '덧': 5847, '덩': 5848, '덮': 5849, '데': 5850, '데다': 5851, '데일리': 5852, '덱': 5853, '덴': 5854, '델': 5855, '뎀': 5856, '뎁': 5857, '뎌': 5858, '도': 5859, '도로': 5860, '도록': 5861, '도서관': 5862, '도시': 5863, '도지사': 5864, '독': 5865, '돈': 5866, '돋': 5867, '돌': 5868, '돔': 5869, '돕': 5870, '돗': 5871, '동': 5872, '동맹': 5873, '동안': 5874, '동향': 5875, '돼': 5876, '돼야': 5877, '됐': 5878, '됐고': 5879, '됐다': 5880, '됐던': 5881, '됐습니다': 5882, '됐으나': 5883, '됐으며': 5884, '됐지만': 5885, '되': 5886, '되고': 5887, '되기': 5888, '되는': 5889, '되도록': 5890, '되며': 5891, '되면': 5892, '되면서': 5893, '되어': 5894, '되었': 5895, '되었다': 5896, '되자': 5897, '되지': 5898, '된': 5899, '된다': 5900, '된다면': 5901, '될': 5902, '됨': 5903, '됨에': 5904, '됩': 5905, '됩니다': 5906, '두': 5907, '둑': 5908, '둔': 5909, '둘': 5910, '둠': 5911, '둡': 5912, '둥': 5913, '둬': 5914, '뒀': 5915, '뒤': 5916, '뒷': 5917, '듀': 5918, '듈': 5919, '드': 5920, '드는': 5921, '드라마': 5922, '드래곤': 5923, '드리': 5924, '드립니다': 5925, '득': 5926, '득점': 5927, '든': 5928, '든지': 5929, '듣': 5930, '들': 5931, '들과': 5932, '들도': 5933, '들어': 5934, '들에': 5935, '들에게': 5936, '들은': 5937, '들을': 5938, '들의': 5939, '들이': 5940, '듬': 5941, '듭': 5942, '듯': 5943, '등': 5944, '등급': 5945, '등록': 5946, '디': 5947, '디스크': 5948, '디스플레이': 5949, '디지털': 5950, '딕': 5951, '딘': 5952, '딛': 5953, '딜': 5954, '딤': 5955, '딧': 5956, '딩': 5957, '딪': 5958, '따': 5959, '딱': 5960, '딴': 5961, '딸': 5962, '땀': 5963, '땅': 5964, '때': 5965, '땐': 5966, '땠': 5967, '땡': 5968, '떠': 5969, '떡': 5970, '떤': 5971, '떨': 5972, '떳': 5973, '떴': 5974, '떻': 5975, '떼': 5976, '뗀': 5977, '뗄': 5978, '뗐': 5979, '또': 5980, '똑': 5981, '똘': 5982, '똥': 5983, '뚜': 5984, '뚝': 5985, '뚫': 5986, '뚱': 5987, '뛰': 5988, '뛴': 5989, '뛸': 5990, '뜨': 5991, '뜩': 5992, '뜬': 5993, '뜯': 5994, '뜰': 5995, '뜸': 5996, '뜻': 5997, '띄': 5998, '띈': 5999, '띔': 6000, '띠': 6001, '띤': 6002, '라': 6003, '라고': 6004, '라는': 6005, '라도': 6006, '라디오스타': 6007, '라며': 6008, '라면': 6009, '라운드': 6010, '라이': 6011, '라이트': 6012, '라이프': 6013, '라인': 6014, '락': 6015, '란': 6016, '랄': 6017, '람': 6018, '랍': 6019, '랏': 6020, '랐': 6021, '랑': 6022, '래': 6023, '랙': 6024, '랜': 6025, '랜드': 6026, '랠': 6027, '램': 6028, '랩': 6029, '랫': 6030, '랬': 6031, '랭': 6032, '랴': 6033, '략': 6034, '량': 6035, '량이': 6036, '러': 6037, '러스': 6038, '럭': 6039, '런': 6040, '런닝맨': 6041, '럴': 6042, '럼': 6043, '럽': 6044, '럿': 6045, '렀': 6046, '렀다': 6047, '렁': 6048, '렇': 6049, '레': 6050, '레드': 6051, '레미제라블': 6052, '레스': 6053, '레이': 6054, '렉': 6055, '렌': 6056, '렐': 6057, '렘': 6058, '렛': 6059, '려': 6060, '려고': 6061, '려는': 6062, '려면': 6063, '력': 6064, '력을': 6065, '력이': 6066, '련': 6067, '렬': 6068, '렴': 6069, '렵': 6070, '렷': 6071, '렸': 6072, '렸고': 6073, '렸다': 6074, '렸던': 6075, '렸지만': 6076, '령': 6077, '례': 6078, '로': 6079, '로부터': 6080, '로서': 6081, '로운': 6082, '록': 6083, '론': 6084, '롤': 6085, '롬': 6086, '롭': 6087, '롭게': 6088, '롯': 6089, '롯데': 6090, '롱': 6091, '뢰': 6092, '료': 6093, '룡': 6094, '루': 6095, '루타': 6096, '룩': 6097, '룬': 6098, '룰': 6099, '룸': 6100, '룹': 6101, '룻': 6102, '룽': 6103, '뤄': 6104, '뤘': 6105, '뤼': 6106, '류': 6107, '륙': 6108, '륜': 6109, '률': 6110, '륨': 6111, '륭': 6112, '르': 6113, '르트': 6114, '른': 6115, '를': 6116, '름': 6117, '릅': 6118, '릇': 6119, '릉': 6120, '릎': 6121, '리': 6122, '리그': 6123, '리는': 6124, '리를': 6125, '리바운드': 6126, '리스': 6127, '리스트': 6128, '리아': 6129, '리조트': 6130, '리지': 6131, '릭': 6132, '린': 6133, '린다': 6134, '릴': 6135, '림': 6136, '립': 6137, '릿': 6138, '링': 6139, '링크': 6140, '마': 6141, '마다': 6142, '마을': 6143, '마음': 6144, '마저': 6145, '마케팅': 6146, '마켓': 6147, '마트': 6148, '막': 6149, '만': 6150, '만달러': 6151, '만명': 6152, '만원': 6153, '만원으로': 6154, '만원을': 6155, '만으로': 6156, '만큼': 6157, '많': 6158, '맏': 6159, '말': 6160, '맑': 6161, '맘': 6162, '맙': 6163, '맛': 6164, '망': 6165, '맞': 6166, '맡': 6167, '매': 6168, '매매': 6169, '매출': 6170, '매치': 6171, '맥': 6172, '맨': 6173, '맴': 6174, '맵': 6175, '맷': 6176, '맹': 6177, '맺': 6178, '머': 6179, '머니': 6180, '머니투데이': 6181, '머리': 6182, '먹': 6183, '먼': 6184, '먼트': 6185, '멀': 6186, '멈': 6187, '멋': 6188, '멍': 6189, '메': 6190, '메이': 6191, '멕': 6192, '멘': 6193, '멜': 6194, '멤': 6195, '멧': 6196, '며': 6197, '면': 6198, '면서': 6199, '면서도': 6200, '면세점': 6201, '멸': 6202, '몄': 6203, '명': 6204, '명과': 6205, '명에게': 6206, '명으로': 6207, '명은': 6208, '명을': 6209, '명의': 6210, '명이': 6211, '몇': 6212, '모': 6213, '모델': 6214, '모바일': 6215, '모씨': 6216, '목': 6217, '목표': 6218, '몫': 6219, '몬': 6220, '몰': 6221, '몸': 6222, '몹': 6223, '못': 6224, '몽': 6225, '뫼': 6226, '묘': 6227, '무': 6228, '무릎팍도사': 6229, '무역': 6230, '무한도전': 6231, '묵': 6232, '묶': 6233, '문': 6234, '문을': 6235, '문제': 6236, '문학': 6237, '문화': 6238, '문화예술': 6239, '묻': 6240, '물': 6241, '물을': 6242, '물질': 6243, '뭄': 6244, '뭇': 6245, '뭉': 6246, '뭐': 6247, '뭔': 6248, '뭘': 6249, '뮌': 6250, '뮤': 6251, '뮤직': 6252, '뮬': 6253, '므': 6254, '미': 6255, '미국': 6256, '미디어': 6257, '미래': 6258, '미술관': 6259, '미스터': 6260, '믹': 6261, '믹스': 6262, '민': 6263, '민주': 6264, '민주당': 6265, '믿': 6266, '밀': 6267, '밋': 6268, '밌': 6269, '밍': 6270, '및': 6271, '밑': 6272, '바': 6273, '바다': 6274, '바람': 6275, '바르셀로나': 6276, '바이': 6277, '바이오': 6278, '바퀴': 6279, '박': 6280, '박근혜': 6281, '박람회': 6282, '박물관': 6283, '밖': 6284, '밖에': 6285, '반': 6286, '반도체': 6287, '받': 6288, '받고': 6289, '받는': 6290, '받아': 6291, '받았다': 6292, '받은': 6293, '받을': 6294, '발': 6295, '발굴': 6296, '발언': 6297, '발전': 6298, '발전소': 6299, '발표': 6300, '밝': 6301, '밟': 6302, '밤': 6303, '밥': 6304, '방': 6305, '방송': 6306, '방식': 6307, '방안': 6308, '방위': 6309, '방향': 6310, '밭': 6311, '배': 6312, '배우': 6313, '백': 6314, '백화점': 6315, '밴': 6316, '밴드': 6317, '밸': 6318, '뱀': 6319, '뱃': 6320, '뱅': 6321, '뱉': 6322, '버': 6323, '버리': 6324, '버린': 6325, '버스': 6326, '벅': 6327, '번': 6328, '번째': 6329, '번호': 6330, '번홀': 6331, '벌': 6332, '범': 6333, '범죄': 6334, '법': 6335, '법원': 6336, '법을': 6337, '법인': 6338, '벗': 6339, '벙': 6340, '베': 6341, '베르': 6342, '베를린': 6343, '베이': 6344, '베이스': 6345, '벡': 6346, '벤': 6347, '벤처': 6348, '벨': 6349, '벨트': 6350, '벳': 6351, '벵': 6352, '벼': 6353, '벽': 6354, '변': 6355, '별': 6356, '별로': 6357, '별로는': 6358, '볍': 6359, '볐': 6360, '병': 6361, '병원': 6362, '볕': 6363, '보': 6364, '보건': 6365, '보고': 6366, '보고서': 6367, '보기': 6368, '보는': 6369, '보니': 6370, '보다': 6371, '보다는': 6372, '보드': 6373, '보이': 6374, '보장': 6375, '보증': 6376, '보험': 6377, '보호': 6378, '복': 6379, '복지': 6380, '복합': 6381, '볶': 6382, '본': 6383, '본부': 6384, '본부장': 6385, '볼': 6386, '볼넷': 6387, '볼륨': 6388, '봄': 6389, '봅': 6390, '봇': 6391, '봉': 6392, '봐': 6393, '봐야': 6394, '봤': 6395, '봤다': 6396, '뵙': 6397, '부': 6398, '부가': 6399, '부는': 6400, '부담': 6401, '부동산': 6402, '부르크': 6403, '부문': 6404, '부산': 6405, '부장': 6406, '부장검사': 6407, '부장판사': 6408, '부처': 6409, '부터': 6410, '부품': 6411, '북': 6412, '북방한계선': 6413, '북부': 6414, '북한': 6415, '분': 6416, '분과': 6417, '분기': 6418, '분기에': 6419, '분께': 6420, '분석': 6421, '분야': 6422, '분쯤': 6423, '불': 6424, '붉': 6425, '붐': 6426, '붓': 6427, '붕': 6428, '붙': 6429, '뷔': 6430, '뷰': 6431, '브': 6432, '브라': 6433, '브랜드': 6434, '브레이크': 6435, '브리': 6436, '븐': 6437, '블': 6438, '블랙': 6439, '블록': 6440, '비': 6441, '비가': 6442, '비를': 6443, '비서관': 6444, '비아': 6445, '비용': 6446, '비율': 6447, '비치': 6448, '빅': 6449, '빈': 6450, '빌': 6451, '빌딩': 6452, '빔': 6453, '빕': 6454, '빗': 6455, '빙': 6456, '빚': 6457, '빛': 6458, '빠': 6459, '빡': 6460, '빨': 6461, '빴': 6462, '빵': 6463, '빼': 6464, '빽': 6465, '뺀': 6466, '뺏': 6467, '뺐': 6468, '뺑': 6469, '뺨': 6470, '뻐': 6471, '뻑': 6472, '뻔': 6473, '뻗': 6474, '뻤': 6475, '뻥': 6476, '뼈': 6477, '뽀': 6478, '뽐': 6479, '뽑': 6480, '뽕': 6481, '뾰': 6482, '뿌': 6483, '뿐': 6484, '뿐만': 6485, '뿔': 6486, '뿜': 6487, '쁘': 6488, '쁜': 6489, '쁠': 6490, '쁨': 6491, '삐': 6492, '사': 6493, '사가': 6494, '사건': 6495, '사고': 6496, '사는': 6497, '사람': 6498, '사랑': 6499, '사를': 6500, '사무소': 6501, '사실': 6502, '사업': 6503, '사업본부': 6504, '사업부': 6505, '사업자': 6506, '사와': 6507, '사의': 6508, '사이드': 6509, '사이트': 6510, '사진': 6511, '사진제공': 6512, '사항': 6513, '사회': 6514, '삭': 6515, '산': 6516, '산업': 6517, '산업단지': 6518, '살': 6519, '삶': 6520, '삼': 6521, '삼성': 6522, '삼성전자': 6523, '삽': 6524, '삿': 6525, '샀': 6526, '상': 6527, '상공회의소': 6528, '상담': 6529, '상당히': 6530, '상을': 6531, '상태': 6532, '상품': 6533, '상황': 6534, '샅': 6535, '새': 6536, '새누리당': 6537, '색': 6538, '샌': 6539, '샐': 6540, '샘': 6541, '생': 6542, '생명': 6543, '생산': 6544, '생활': 6545, '샤': 6546, '샬': 6547, '샴': 6548, '샵': 6549, '샷': 6550, '샹': 6551, '섀': 6552, '서': 6553, '서는': 6554, '서비스': 6555, '서울': 6556, '석': 6557, '섞': 6558, '선': 6559, '선거': 6560, '선물': 6561, '선수': 6562, '선수권대회': 6563, '선을': 6564, '섣': 6565, '설': 6566, '설국열차': 6567, '설명회': 6568, '섬': 6569, '섭': 6570, '섯': 6571, '섰': 6572, '성': 6573, '성과': 6574, '성은': 6575, '성을': 6576, '성이': 6577, '성장': 6578, '세': 6579, '세가': 6580, '세계': 6581, '세계일보': 6582, '세대': 6583, '세력': 6584, '세를': 6585, '세요': 6586, '세이브': 6587, '세트': 6588, '세포': 6589, '섹': 6590, '센': 6591, '센스': 6592, '센터': 6593, '센터에서': 6594, '센트': 6595, '셀': 6596, '셈': 6597, '셉': 6598, '셋': 6599, '셍': 6600, '셔': 6601, '션': 6602, '셜': 6603, '셨': 6604, '셰': 6605, '셸': 6606, '소': 6607, '소득': 6608, '소리': 6609, '소방서': 6610, '소비자': 6611, '소송': 6612, '소연': 6613, '소프트': 6614, '속': 6615, '손': 6616, '손해보험': 6617, '솔': 6618, '솜': 6619, '솟': 6620, '송': 6621, '솥': 6622, '쇄': 6623, '쇠': 6624, '쇼': 6625, '숀': 6626, '숍': 6627, '숏': 6628, '수': 6629, '수가': 6630, '수는': 6631, '수록': 6632, '수를': 6633, '수사': 6634, '수석': 6635, '수수료': 6636, '수술': 6637, '수원': 6638, '수익': 6639, '수익률': 6640, '수지': 6641, '숙': 6642, '순': 6643, '순위': 6644, '술': 6645, '숨': 6646, '숨어있': 6647, '숫': 6648, '숭': 6649, '숱': 6650, '숲': 6651, '쉐': 6652, '쉬': 6653, '쉰': 6654, '쉴': 6655, '쉼': 6656, '쉽': 6657, '슈': 6658, '슈퍼': 6659, '슈퍼스타': 6660, '슐': 6661, '슘': 6662, '슛': 6663, '스': 6664, '스가': 6665, '스는': 6666, '스러운': 6667, '스러워': 6668, '스럽': 6669, '스럽게': 6670, '스럽다': 6671, '스를': 6672, '스마트': 6673, '스와': 6674, '스완지시티': 6675, '스의': 6676, '스카': 6677, '스캔들': 6678, '스케': 6679, '스코': 6680, '스쿨': 6681, '스크': 6682, '스키': 6683, '스타': 6684, '스탠': 6685, '스터': 6686, '스턴': 6687, '스테': 6688, '스토리': 6689, '스토어': 6690, '스트': 6691, '스티': 6692, '스페셜': 6693, '스포츠': 6694, '스포츠조선': 6695, '슨': 6696, '슬': 6697, '슴': 6698, '습': 6699, '습니까': 6700, '습니다': 6701, '슷': 6702, '승': 6703, '승을': 6704, '시': 6705, '시간': 6706, '시께': 6707, '시대': 6708, '시리즈': 6709, '시민': 6710, '시부터': 6711, '시설': 6712, '시스템': 6713, '시아': 6714, '시의회': 6715, '시장': 6716, '시장에서': 6717, '시즌': 6718, '시청': 6719, '시켜': 6720, '시켰다': 6721, '시키': 6722, '시키고': 6723, '시키기': 6724, '시키는': 6725, '시킨': 6726, '시킬': 6727, '시티': 6728, '시험': 6729, '식': 6730, '식을': 6731, '식품': 6732, '신': 6733, '신도시': 6734, '신문': 6735, '신청': 6736, '싣': 6737, '실': 6738, '실리콘': 6739, '실에서': 6740, '실장': 6741, '실적': 6742, '실점': 6743, '싫': 6744, '심': 6745, '심리': 6746, '심사': 6747, '심을': 6748, '십': 6749, '싱': 6750, '싶': 6751, '싸': 6752, '싸움': 6753, '싹': 6754, '싼': 6755, '쌀': 6756, '쌈': 6757, '쌌': 6758, '쌍': 6759, '쌓': 6760, '써': 6761, '썩': 6762, '썬': 6763, '썰': 6764, '썸': 6765, '썹': 6766, '썼': 6767, '썽': 6768, '쎄': 6769, '쏘': 6770, '쏙': 6771, '쏜': 6772, '쏟': 6773, '쏠': 6774, '쏴': 6775, '쐐': 6776, '쑤': 6777, '쑥': 6778, '쓰': 6779, '쓴': 6780, '쓸': 6781, '씀': 6782, '씁': 6783, '씌': 6784, '씨': 6785, '씨가': 6786, '씨는': 6787, '씨를': 6788, '씨에게': 6789, '씨와': 6790, '씨의': 6791, '씩': 6792, '씬': 6793, '씹': 6794, '씻': 6795, '씽': 6796, '아': 6797, '아버지': 6798, '아빠': 6799, '아시아': 6800, '아시아경제': 6801, '아시아투데이': 6802, '아웃': 6803, '아이': 6804, '아일랜드': 6805, '아직': 6806, '아카데미': 6807, '아트': 6808, '아파트': 6809, '아프리카': 6810, '악': 6811, '안': 6812, '안드로이드': 6813, '안보': 6814, '안을': 6815, '안전': 6816, '안정': 6817, '안타': 6818, '앉': 6819, '않': 6820, '알': 6821, '알리미': 6822, '앓': 6823, '암': 6824, '압': 6825, '앗': 6826, '았': 6827, '았다': 6828, '았던': 6829, '았지만': 6830, '앙': 6831, '앞': 6832, '앞으로': 6833, '애': 6834, '액': 6835, '액은': 6836, '앤': 6837, '앨': 6838, '앰': 6839, '앱': 6840, '앳': 6841, '앵': 6842, '앵커멘트': 6843, '야': 6844, '야구': 6845, '약': 6846, '약품': 6847, '얀': 6848, '얄': 6849, '얇': 6850, '얌': 6851, '얏': 6852, '양': 6853, '얘': 6854, '어': 6855, '어야': 6856, '어요': 6857, '억': 6858, '억달러': 6859, '억여원': 6860, '억원': 6861, '억원으로': 6862, '억원을': 6863, '억원의': 6864, '언': 6865, '언더파': 6866, '언론': 6867, '얹': 6868, '얻': 6869, '얼': 6870, '얽': 6871, '엄': 6872, '업': 6873, '업계': 6874, '업무': 6875, '업소': 6876, '업자': 6877, '업종': 6878, '업체': 6879, '업체들': 6880, '없': 6881, '없는': 6882, '없이': 6883, '엇': 6884, '었': 6885, '었고': 6886, '었는데': 6887, '었다': 6888, '었던': 6889, '었습니다': 6890, '었으나': 6891, '었으며': 6892, '었지만': 6893, '엉': 6894, '엎': 6895, '에': 6896, '에게': 6897, '에게는': 6898, '에너지': 6899, '에는': 6900, '에도': 6901, '에만': 6902, '에서': 6903, '에서는': 6904, '에서도': 6905, '에선': 6906, '엑': 6907, '엑스': 6908, '엔': 6909, '엔지니어링': 6910, '엔터테인먼트': 6911, '엘': 6912, '엠': 6913, '엣': 6914, '엥': 6915, '여': 6916, '여개': 6917, '여건': 6918, '여름': 6919, '여명': 6920, '여명의': 6921, '여명이': 6922, '여성': 6923, '여자': 6924, '여행': 6925, '역': 6926, '엮': 6927, '연': 6928, '연구': 6929, '연구소': 6930, '연구원': 6931, '연금': 6932, '연맹': 6933, '연수원': 6934, '연승': 6935, '연예': 6936, '연패': 6937, '연합': 6938, '연합회': 6939, '열': 6940, '염': 6941, '엽': 6942, '엿': 6943, '였': 6944, '였고': 6945, '였다': 6946, '였던': 6947, '였습니다': 6948, '였으며': 6949, '였지만': 6950, '영': 6951, '영상': 6952, '영업': 6953, '영화': 6954, '영화제': 6955, '옆': 6956, '예': 6957, '예방': 6958, '예산': 6959, '예술': 6960, '옌': 6961, '옐': 6962, '옛': 6963, '오': 6964, '오는': 6965, '오늘': 6966, '오른쪽': 6967, '오토': 6968, '오픈': 6969, '옥': 6970, '온': 6971, '온라인': 6972, '올': 6973, '올림픽': 6974, '올해': 6975, '옮': 6976, '옳': 6977, '옴': 6978, '옵': 6979, '옵션': 6980, '옷': 6981, '옹': 6982, '와': 6983, '와의': 6984, '완': 6985, '왈': 6986, '왑': 6987, '왓': 6988, '왔': 6989, '왔다': 6990, '왔던': 6991, '왕': 6992, '왜': 6993, '왠': 6994, '외': 6995, '외국인': 6996, '왼': 6997, '왼쪽': 6998, '요': 6999, '요금': 7000, '욕': 7001, '욘': 7002, '용': 7003, '용품': 7004, '우': 7005, '우는': 7006, '우리': 7007, '우스': 7008, '욱': 7009, '운': 7010, '운동': 7011, '운영': 7012, '울': 7013, '움': 7014, '웃': 7015, '웃음': 7016, '웅': 7017, '워': 7018, '웍': 7019, '원': 7020, '원에': 7021, '원에서': 7022, '원으로': 7023, '원은': 7024, '원을': 7025, '원의': 7026, '원이': 7027, '월': 7028, '월까지': 7029, '월드': 7030, '월드컵': 7031, '월말': 7032, '월부터': 7033, '월에': 7034, '월에는': 7035, '웠': 7036, '웠다': 7037, '웨': 7038, '웨어': 7039, '웨이': 7040, '웬': 7041, '웰': 7042, '웹': 7043, '위': 7044, '위권': 7045, '위기': 7046, '위는': 7047, '위로': 7048, '위를': 7049, '위안': 7050, '위에': 7051, '위원': 7052, '위원장': 7053, '위원회': 7054, '위원회는': 7055, '위험': 7056, '윅': 7057, '윈': 7058, '윌': 7059, '윔': 7060, '윗': 7061, '윙': 7062, '유': 7063, '유럽': 7064, '유통': 7065, '유플러스': 7066, '육': 7067, '윤': 7068, '율': 7069, '율은': 7070, '율을': 7071, '율이': 7072, '융': 7073, '으': 7074, '으나': 7075, '으니': 7076, '으려': 7077, '으로': 7078, '으로부터': 7079, '으로서': 7080, '으로써': 7081, '으며': 7082, '으면': 7083, '으면서': 7084, '윽': 7085, '은': 7086, '은행': 7087, '을': 7088, '음': 7089, '음악': 7090, '음에도': 7091, '음을': 7092, '읍': 7093, '응': 7094, '의': 7095, '이': 7096, '이기도': 7097, '이나': 7098, '이닝': 7099, '이다': 7100, '이라': 7101, '이라고': 7102, '이라는': 7103, '이라도': 7104, '이라며': 7105, '이라면서': 7106, '이란': 7107, '이며': 7108, '이번': 7109, '이상': 7110, '이어서': 7111, '이었다': 7112, '이었던': 7113, '이익': 7114, '이자': 7115, '이지만': 7116, '이하': 7117, '익': 7118, '인': 7119, '인데': 7120, '인사': 7121, '인수위': 7122, '인지': 7123, '인터내셔널': 7124, '인터넷': 7125, '일': 7126, '일간': 7127, '일까지': 7128, '일반': 7129, '일보': 7130, '일본': 7131, '일부터': 7132, '일에는': 7133, '읽': 7134, '잃': 7135, '임': 7136, '임을': 7137, '입': 7138, '입니다': 7139, '잇': 7140, '있': 7141, '있는': 7142, '있다': 7143, '잉': 7144, '잊': 7145, '잎': 7146, '자': 7147, '자가': 7148, '자금': 7149, '자는': 7150, '자동차': 7151, '자들': 7152, '자들은': 7153, '자들의': 7154, '자들이': 7155, '자로': 7156, '자료': 7157, '자를': 7158, '자리': 7159, '자마자': 7160, '자본': 7161, '자산': 7162, '자산운용': 7163, '자에게': 7164, '자와': 7165, '자원': 7166, '자의': 7167, '자인': 7168, '자치': 7169, '작': 7170, '작업': 7171, '잔': 7172, '잖': 7173, '잘': 7174, '잠': 7175, '잡': 7176, '잣': 7177, '장': 7178, '장과': 7179, '장관': 7180, '장비': 7181, '장애': 7182, '장애인': 7183, '장에서': 7184, '장으로': 7185, '장은': 7186, '장을': 7187, '장이': 7188, '장치': 7189, '잦': 7190, '재': 7191, '재단': 7192, '재료': 7193, '재산': 7194, '재정': 7195, '재판': 7196, '잭': 7197, '쟁': 7198, '저': 7199, '저축': 7200, '저축은행': 7201, '적': 7202, '적으로': 7203, '적이고': 7204, '적이다': 7205, '적인': 7206, '전': 7207, '전략': 7208, '전문': 7209, '전에': 7210, '전에서': 7211, '전우치': 7212, '전을': 7213, '전자': 7214, '전쟁': 7215, '전환': 7216, '절': 7217, '절차': 7218, '젊': 7219, '점': 7220, '점검': 7221, '점으로': 7222, '점을': 7223, '점이': 7224, '접': 7225, '젓': 7226, '정': 7227, '정글의': 7228, '정보': 7229, '정부': 7230, '정책': 7231, '정치': 7232, '젖': 7233, '제': 7234, '제도': 7235, '제를': 7236, '제약': 7237, '제주': 7238, '제품': 7239, '젝': 7240, '젠': 7241, '젤': 7242, '젬': 7243, '젯': 7244, '져': 7245, '져야': 7246, '젼': 7247, '졌': 7248, '졌고': 7249, '졌다': 7250, '졌습니다': 7251, '졌지만': 7252, '조': 7253, '조건': 7254, '조사': 7255, '조선': 7256, '조선해양': 7257, '조원': 7258, '조정': 7259, '조직': 7260, '조차': 7261, '조치': 7262, '족': 7263, '존': 7264, '졸': 7265, '좀': 7266, '좁': 7267, '종': 7268, '종목': 7269, '종합': 7270, '좋': 7271, '좋은': 7272, '좌': 7273, '죄': 7274, '죠': 7275, '주': 7276, '주가': 7277, '주기': 7278, '주년': 7279, '주는': 7280, '주민': 7281, '주세요': 7282, '주식': 7283, '주의': 7284, '주의보': 7285, '주택': 7286, '죽': 7287, '준': 7288, '준비': 7289, '준호': 7290, '준희': 7291, '줄': 7292, '줌': 7293, '줍': 7294, '중': 7295, '중견기업': 7296, '중공업': 7297, '중국': 7298, '중소기업': 7299, '중심': 7300, '중앙': 7301, '중인': 7302, '줘': 7303, '줘야': 7304, '줬': 7305, '줬다': 7306, '쥐': 7307, '쥔': 7308, '쥬': 7309, '즈': 7310, '즉': 7311, '즌': 7312, '즐': 7313, '즘': 7314, '즙': 7315, '증': 7316, '증권': 7317, '지': 7318, '지가': 7319, '지검': 7320, '지고': 7321, '지구': 7322, '지금': 7323, '지나치게': 7324, '지난': 7325, '지난해': 7326, '지는': 7327, '지도': 7328, '지를': 7329, '지만': 7330, '지면서': 7331, '지방': 7332, '지방경찰청': 7333, '지법': 7334, '지수': 7335, '지수는': 7336, '지역': 7337, '지원': 7338, '지원센터': 7339, '지지': 7340, '지훈': 7341, '직': 7342, '직원': 7343, '진': 7344, '진다': 7345, '진영': 7346, '진짜': 7347, '진흥': 7348, '진흥원': 7349, '질': 7350, '질서': 7351, '질환': 7352, '짐': 7353, '집': 7354, '집행': 7355, '짓': 7356, '징': 7357, '짖': 7358, '짙': 7359, '짚': 7360, '짜': 7361, '짜리': 7362, '짝': 7363, '짠': 7364, '짤': 7365, '짧': 7366, '짬': 7367, '짱': 7368, '째': 7369, '쨌': 7370, '쩌': 7371, '쩍': 7372, '쩔': 7373, '쩡': 7374, '쪼': 7375, '쪽': 7376, '쫄': 7377, '쫓': 7378, '쭈': 7379, '쭉': 7380, '쯔': 7381, '쯤': 7382, '찌': 7383, '찍': 7384, '찐': 7385, '찔': 7386, '찜': 7387, '찢': 7388, '차': 7389, '차량': 7390, '차례': 7391, '차익': 7392, '차전': 7393, '착': 7394, '찬': 7395, '찮': 7396, '찰': 7397, '참': 7398, '참여': 7399, '찹': 7400, '찼': 7401, '창': 7402, '창업': 7403, '찾': 7404, '채': 7405, '채권': 7406, '채널': 7407, '책': 7408, '책을': 7409, '책임': 7410, '챈': 7411, '챌': 7412, '챔': 7413, '챔피언': 7414, '챙': 7415, '처': 7416, '처럼': 7417, '처리': 7418, '처분': 7419, '처음': 7420, '척': 7421, '천': 7422, '천만': 7423, '천만원': 7424, '천명': 7425, '천억원': 7426, '철': 7427, '첨': 7428, '첩': 7429, '첫': 7430, '청': 7431, '청담동': 7432, '청사': 7433, '청소년': 7434, '청은': 7435, '체': 7436, '체계': 7437, '체육': 7438, '체제': 7439, '체험': 7440, '첸': 7441, '첼': 7442, '쳐': 7443, '쳤': 7444, '쳤다': 7445, '초': 7446, '초등학교': 7447, '촉': 7448, '촌': 7449, '촘': 7450, '촛': 7451, '총': 7452, '총리': 7453, '총회': 7454, '촨': 7455, '촬': 7456, '촬영': 7457, '최': 7458, '최고': 7459, '최근': 7460, '추': 7461, '추진': 7462, '축': 7463, '축구': 7464, '축구연맹': 7465, '축제': 7466, '춘': 7467, '출': 7468, '춤': 7469, '춥': 7470, '춧': 7471, '충': 7472, '춰': 7473, '췄': 7474, '췌': 7475, '취': 7476, '츄': 7477, '츠': 7478, '측': 7479, '측은': 7480, '츰': 7481, '층': 7482, '치': 7483, '치고': 7484, '치는': 7485, '치료': 7486, '치를': 7487, '칙': 7488, '친': 7489, '칠': 7490, '침': 7491, '칩': 7492, '칫': 7493, '칭': 7494, '카': 7495, '카드': 7496, '카페': 7497, '칵': 7498, '칸': 7499, '칼': 7500, '캄': 7501, '캉': 7502, '캐': 7503, '캐피탈': 7504, '캔': 7505, '캘': 7506, '캠': 7507, '캠퍼스': 7508, '캠프': 7509, '캡': 7510, '캣': 7511, '커': 7512, '컥': 7513, '컨': 7514, '컨설팅': 7515, '컫': 7516, '컬': 7517, '컴': 7518, '컴퍼니': 7519, '컵': 7520, '컷': 7521, '컸': 7522, '케': 7523, '케미칼': 7524, '케이': 7525, '켄': 7526, '켈': 7527, '켐': 7528, '켓': 7529, '켜': 7530, '켠': 7531, '켰': 7532, '코': 7533, '코드': 7534, '코리아': 7535, '코스닥': 7536, '코스피': 7537, '콕': 7538, '콘': 7539, '콘서트': 7540, '콘텐츠': 7541, '콜': 7542, '콤': 7543, '콥': 7544, '콧': 7545, '콩': 7546, '콰': 7547, '쾌': 7548, '쿄': 7549, '쿠': 7550, '쿠키': 7551, '쿡': 7552, '쿤': 7553, '쿨': 7554, '쿵': 7555, '쿼': 7556, '쿼터': 7557, '퀄': 7558, '퀘': 7559, '퀴': 7560, '퀵': 7561, '퀸': 7562, '큐': 7563, '큘': 7564, '크': 7565, '크레': 7566, '큰': 7567, '클': 7568, '클라우드': 7569, '클럽': 7570, '클리': 7571, '큼': 7572, '키': 7573, '키로': 7574, '킥': 7575, '킨': 7576, '킬': 7577, '킴': 7578, '킷': 7579, '킹': 7580, '타': 7581, '타를': 7582, '타수': 7583, '타운': 7584, '타워': 7585, '타임': 7586, '타자': 7587, '타점': 7588, '탁': 7589, '탄': 7590, '탈': 7591, '탈삼진': 7592, '탐': 7593, '탑': 7594, '탓': 7595, '탔': 7596, '탕': 7597, '태': 7598, '태영': 7599, '태평양': 7600, '택': 7601, '탠': 7602, '탤': 7603, '탬': 7604, '탭': 7605, '탰': 7606, '탱': 7607, '탱크': 7608, '터': 7609, '터미널': 7610, '턱': 7611, '턴': 7612, '털': 7613, '텀': 7614, '텁': 7615, '텃': 7616, '텅': 7617, '테': 7618, '테크': 7619, '텍': 7620, '텐': 7621, '텔': 7622, '텔레콤': 7623, '템': 7624, '텝': 7625, '텨': 7626, '톈': 7627, '토': 7628, '토록': 7629, '토크': 7630, '톡': 7631, '톤': 7632, '톨': 7633, '톰': 7634, '톱': 7635, '통': 7636, '통신': 7637, '통합': 7638, '통화': 7639, '퇴': 7640, '투': 7641, '투데이': 7642, '투수': 7643, '투어': 7644, '투자': 7645, '투자증권': 7646, '투표': 7647, '툭': 7648, '툰': 7649, '툴': 7650, '툼': 7651, '퉁': 7652, '퉈': 7653, '튀': 7654, '튕': 7655, '튜': 7656, '튠': 7657, '튬': 7658, '트': 7659, '트랙': 7660, '트로': 7661, '트리': 7662, '특': 7663, '특별': 7664, '특위': 7665, '특징주': 7666, '특히': 7667, '튼': 7668, '튿': 7669, '틀': 7670, '틈': 7671, '틋': 7672, '티': 7673, '티브': 7674, '틱': 7675, '틴': 7676, '틸': 7677, '팀': 7678, '팀장': 7679, '팁': 7680, '팅': 7681, '파': 7682, '파운드': 7683, '파이어': 7684, '파크': 7685, '팍': 7686, '팎': 7687, '판': 7688, '판매': 7689, '팔': 7690, '팜': 7691, '팝': 7692, '팟': 7693, '팠': 7694, '팡': 7695, '팥': 7696, '패': 7697, '패밀리': 7698, '패션': 7699, '팩': 7700, '팬': 7701, '팬들': 7702, '팰': 7703, '팸': 7704, '팽': 7705, '퍼': 7706, '펀': 7707, '펀드': 7708, '펄': 7709, '펌': 7710, '펑': 7711, '페': 7712, '페스티벌': 7713, '페이스': 7714, '펙': 7715, '펜': 7716, '펠': 7717, '펫': 7718, '펴': 7719, '편': 7720, '펼': 7721, '폄': 7722, '폈': 7723, '평': 7724, '평가': 7725, '평균': 7726, '폐': 7727, '포': 7728, '포럼': 7729, '포스트': 7730, '포인트': 7731, '포토': 7732, '포트': 7733, '폭': 7734, '폭력': 7735, '폰': 7736, '폴': 7737, '폴리': 7738, '폼': 7739, '퐁': 7740, '표': 7741, '표를': 7742, '푸': 7743, '푸드': 7744, '푹': 7745, '푼': 7746, '풀': 7747, '품': 7748, '풋': 7749, '풍': 7750, '퓨': 7751, '퓰': 7752, '프': 7753, '프라': 7754, '프로': 7755, '프로골프': 7756, '프로그램': 7757, '프로야구': 7758, '프리': 7759, '픈': 7760, '플': 7761, '플라이': 7762, '플랜트': 7763, '플러스': 7764, '플레이': 7765, '픔': 7766, '피': 7767, '피스': 7768, '피아': 7769, '피안타': 7770, '피해': 7771, '픽': 7772, '핀': 7773, '필': 7774, '필드': 7775, '필름': 7776, '핌': 7777, '핍': 7778, '핏': 7779, '핑': 7780, '핑크': 7781, '하': 7782, '하거나': 7783, '하게': 7784, '하겠다': 7785, '하겠다고': 7786, '하겠다는': 7787, '하고': 7788, '하기': 7789, '하기도': 7790, '하기로': 7791, '하나': 7792, '하느냐': 7793, '하는': 7794, '하는데': 7795, '하늘': 7796, '하니': 7797, '하다': 7798, '하다고': 7799, '하다는': 7800, '하더라도': 7801, '하던': 7802, '하도록': 7803, '하라': 7804, '하라고': 7805, '하려': 7806, '하려고': 7807, '하려는': 7808, '하려면': 7809, '하며': 7810, '하면': 7811, '하면서': 7812, '하면서도': 7813, '하세요': 7814, '하여': 7815, '하우스': 7816, '하이닉스': 7817, '하자': 7818, '하지': 7819, '하지만': 7820, '학': 7821, '학과': 7822, '학교': 7823, '학년': 7824, '학생': 7825, '학습': 7826, '학원': 7827, '한': 7828, '한국': 7829, '한국시간': 7830, '한다': 7831, '한다고': 7832, '한다는': 7833, '한다면': 7834, '한테': 7835, '할': 7836, '함': 7837, '함께': 7838, '함에': 7839, '함으로써': 7840, '함을': 7841, '합': 7842, '합니다': 7843, '핫': 7844, '핫이슈': 7845, '항': 7846, '항공': 7847, '해': 7848, '해달라': 7849, '해서': 7850, '해수욕장': 7851, '해야': 7852, '해온': 7853, '해왔다': 7854, '해졌다': 7855, '해주고': 7856, '해주는': 7857, '해진': 7858, '핵': 7859, '핸': 7860, '핸드': 7861, '햄': 7862, '햇': 7863, '했': 7864, '했고': 7865, '했기': 7866, '했는데': 7867, '했는지': 7868, '했다': 7869, '했다고': 7870, '했다는': 7871, '했던': 7872, '했습니다': 7873, '했어요': 7874, '했었다': 7875, '했으나': 7876, '했으며': 7877, '했으면': 7878, '했을': 7879, '했지만': 7880, '행': 7881, '행복': 7882, '행사': 7883, '행위': 7884, '행정': 7885, '향': 7886, '허': 7887, '허가': 7888, '헉': 7889, '헌': 7890, '헐': 7891, '험': 7892, '헛': 7893, '헝': 7894, '헤': 7895, '헨': 7896, '헬': 7897, '헷': 7898, '혀': 7899, '혁': 7900, '혁명': 7901, '혁신': 7902, '현': 7903, '현대': 7904, '현장': 7905, '현재': 7906, '현지시각': 7907, '현지시간': 7908, '혈': 7909, '혐': 7910, '협': 7911, '협동조합': 7912, '협력': 7913, '협상': 7914, '협의체': 7915, '협의회': 7916, '협정': 7917, '협회': 7918, '혔': 7919, '혔다': 7920, '형': 7921, '혜': 7922, '혜진': 7923, '혜택': 7924, '호': 7925, '호는': 7926, '호선': 7927, '호텔': 7928, '호텔에서': 7929, '혹': 7930, '혼': 7931, '홀': 7932, '홀딩스': 7933, '홀에서': 7934, '홈': 7935, '홈런': 7936, '홈쇼핑': 7937, '홉': 7938, '홍': 7939, '홍보': 7940, '화': 7941, '화를': 7942, '화학': 7943, '확': 7944, '확인': 7945, '환': 7946, '환경': 7947, '활': 7948, '활동': 7949, '활동을': 7950, '황': 7951, '황금어장': 7952, '회': 7953, '회담': 7954, '회를': 7955, '회말': 7956, '회사': 7957, '회의를': 7958, '회의에서': 7959, '회장': 7960, '회초': 7961, '획': 7962, '횟': 7963, '횡': 7964, '효': 7965, '효과': 7966, '효율': 7967, '후': 7968, '후보': 7969, '훈': 7970, '훈련': 7971, '훌': 7972, '훔': 7973, '훗': 7974, '훙': 7975, '훤': 7976, '훨': 7977, '훼': 7978, '휘': 7979, '휠': 7980, '휩': 7981, '휴': 7982, '흉': 7983, '흐': 7984, '흑': 7985, '흔': 7986, '흘': 7987, '흙': 7988, '흠': 7989, '흡': 7990, '흥': 7991, '흩': 7992, '희': 7993, '희망': 7994, '흰': 7995, '히': 7996, '힌': 7997, '힐': 7998, '힐링캠프': 7999, '힘': 8000, '힙': 8001}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}