{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "korean_frame_token_0_1.0_gamma_10.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dolmani38/Summary2/blob/main/multi-discriminator%20GAN%200830.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkQCxNatSIOk"
      },
      "source": [
        "# Multi-Discriminator GAN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZjIW9VwyjDf"
      },
      "source": [
        "ABSTRACT\n",
        "\n",
        "----\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K87VNBbeRLFF"
      },
      "source": [
        "#4. Implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQZeBAf8NxAR"
      },
      "source": [
        "## 4.1 기본 설정..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAdXzWGuKSBT",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8ba7dde-36ff-47ad-fbeb-4ddcbf52f3f7"
      },
      "source": [
        "if True:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "newO0mBXKVnE",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20d434ac-97cb-4851-a7d5-1b3841053061"
      },
      "source": [
        "#!pip install keybert\n",
        "!pip3 install transformers\n",
        "!pip3 install sentence-transformers\n",
        "\n",
        "#!pip install sentence-transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.2)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.9.0+cu102)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.0.12)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.10.0+cu102)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.9.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.96)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.62.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.10.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.6.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.45)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (5.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmIxp0FnKXif",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "228f6cb9-3e23-4e00-a897-29ebfa1522d9"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# set seeds for reproducability\n",
        "from numpy.random import seed\n",
        "#seed(1)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string, os \n",
        "\n",
        "import urllib.request\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3J0n_lhKcgm",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "114660e5-2b41-45df-efab-d288010b791c"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ue_4ZfdRKfdX",
        "trusted": true
      },
      "source": [
        "# Print iterations progress\n",
        "class ProgressBar:\n",
        "\n",
        "    def __init__(self,total=20, prefix = '', suffix = '', decimals = 1, length = 20, fill = '|', printEnd = \"\\r\"):\n",
        "        self.total = total\n",
        "        self.prefix = prefix\n",
        "        self.suffix = suffix\n",
        "        self.decimals = decimals\n",
        "        self.length = length\n",
        "        self.fill = fill\n",
        "        self.printEnd = printEnd\n",
        "        self.ite = 0\n",
        "        self.back_filledLength = 0\n",
        "\n",
        "    def printProgress(self,iteration, text):\n",
        "        self.ite += iteration\n",
        "        percent = (\"{0:.\" + str(self.decimals) + \"f}\").format(100 * (self.ite / float(self.total)))\n",
        "        filledLength = int(self.length * self.ite // self.total)\n",
        "        bar = self.fill * filledLength + '.' * (self.length - filledLength)\n",
        "        if filledLength > self.back_filledLength or percent == 100:\n",
        "            print(f'\\r{self.prefix} |{bar}| {percent}% {self.suffix}  {text}', end=\"\", flush=True)\n",
        "            # Print New Line on Complete\n",
        "            if self.ite == self.total: \n",
        "                print()\n",
        "        self.back_filledLength = filledLength    "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNHI0G6JKc5h",
        "trusted": true
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Zsv-LVkKmfL"
      },
      "source": [
        "##4.2 Grammar Discriminator Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VQdGLciKc_y",
        "trusted": true
      },
      "source": [
        "from transformers import BertTokenizer, BertTokenizerFast,AutoTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset, random_split\n",
        "\n",
        "import time\n",
        "import random\n",
        "import datetime\n",
        "\n",
        "# 간단한 전처리\n",
        "def clean_text(txt):\n",
        "    txt = txt.replace('\\n',' ')\n",
        "    txt = txt.replace('\\r',' ')    \n",
        "    txt = txt.replace('=','')\n",
        "    txt = txt.replace('\\\"','')   \n",
        "    txt = txt.replace('\\'','')\n",
        "    #txt = txt.replace(',','')\n",
        "    txt = txt.replace('..','')\n",
        "    txt = txt.replace('...','')\n",
        "    txt = txt.replace(' .','.')\n",
        "    txt = txt.replace('.','. ')\n",
        "    txt = txt.replace('  ',' ')\n",
        "    txt = txt.replace('  ',' ')    \n",
        "    txt = txt.replace('  ',' ')   \n",
        "    txt = txt.replace('  ',' ')           \n",
        "    txt = txt.replace('  ',' ')\n",
        "    txt = txt.replace('  ',' ')    \n",
        "    txt = txt.replace('  ',' ')   \n",
        "    txt = txt.replace('  ',' ')             \n",
        "    return txt.strip()\n",
        "\n",
        "def shuffling(txt):\n",
        "    txt_list = txt.split(' ')\n",
        "    random.shuffle(txt_list)\n",
        "    return ' '.join(txt_list)\n",
        "\n",
        "def collect_training_dataset_for_grammar_discriminator(sentences_dataset):\n",
        "\n",
        "    sentences = []\n",
        "    labels = []\n",
        "\n",
        "    for txtss in sentences_dataset:\n",
        "        txtss = clean_text(txtss)\n",
        "        txts = txtss.strip().split('.')\n",
        "        for txt in txts:  \n",
        "            txt = txt.strip()\n",
        "            if len(txt) > 10:\n",
        "                #ko_grammar_dataset.append([txt,1])\n",
        "                txt = txt.replace('.','')\n",
        "                tf = random.choice([True,False])\n",
        "                # 정상 또는 비정상 둘중에 하나만 데이터셋에 추가\n",
        "                if (tf):\n",
        "                    sentences.append(txt) # '.'의 위치를 보고 True, False를 판단 하기 땜에...\n",
        "                    labels.append(1)\n",
        "                else:\n",
        "                    sentences.append(shuffling(txt))\n",
        "                    labels.append(0)\n",
        "\n",
        "    return sentences,labels\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "class Grammar_Discriminator:\n",
        "\n",
        "\n",
        "    def __init__(self, pretraoned_kobert_model_name='kykim/bert-kor-base', input_dir=None):\n",
        "\n",
        "        if input_dir is None:\n",
        "            self.tokenizer = BertTokenizerFast.from_pretrained(pretraoned_kobert_model_name)\n",
        "            self.discriminator = BertForSequenceClassification.from_pretrained(\n",
        "                                    pretraoned_kobert_model_name, # Use the 12-layer BERT model, with an uncased vocab.\n",
        "                                    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                                                    # You can increase this for multi-class tasks.   \n",
        "                                    output_attentions = False, # Whether the model returns attentions weights.\n",
        "                                    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "                                )            \n",
        "        else:\n",
        "            self.__load_model(input_dir)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def set_dataset(self, sentences,labels):\n",
        "        # Print the original sentence.\n",
        "        print(' Original: ', sentences[0])\n",
        "\n",
        "        # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "        input_ids = []\n",
        "        attention_masks = []\n",
        "\n",
        "        # For every sentence...\n",
        "        for sent in sentences:\n",
        "            # `encode_plus` will:\n",
        "            #   (1) Tokenize the sentence.\n",
        "            #   (2) Prepend the `[CLS]` token to the start.\n",
        "            #   (3) Append the `[SEP]` token to the end.\n",
        "            #   (4) Map tokens to their IDs.\n",
        "            #   (5) Pad or truncate the sentence to `max_length`\n",
        "            #   (6) Create attention masks for [PAD] tokens.\n",
        "            encoded_dict = self.tokenizer.encode_plus(\n",
        "                                sent,                      # Sentence to encode.\n",
        "                                add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                                max_length = 64,           # Pad & truncate all sentences.\n",
        "                                pad_to_max_length = True,\n",
        "                                return_attention_mask = True,   # Construct attn. masks.\n",
        "                                return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                                truncation = True,\n",
        "                        )\n",
        "            \n",
        "            # Add the encoded sentence to the list.    \n",
        "            input_ids.append(encoded_dict['input_ids'])\n",
        "            \n",
        "            # And its attention mask (simply differentiates padding from non-padding).\n",
        "            attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "        # Convert the lists into tensors.\n",
        "        input_ids = torch.cat(input_ids, dim=0)\n",
        "        attention_masks = torch.cat(attention_masks, dim=0)\n",
        "        labels = torch.tensor(labels)\n",
        "\n",
        "        # Print sentence 0, now as a list of IDs.\n",
        "        print('Original: ', sentences[0])\n",
        "        print('Token IDs:', input_ids[0])\n",
        "\n",
        "        # Training & Validation Split\n",
        "        # Divide up our training set to use 90% for training and 10% for validation.\n",
        "\n",
        "        # Combine the training inputs into a TensorDataset.\n",
        "        dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "        # Create a 90-10 train-validation split.\n",
        "\n",
        "        # Calculate the number of samples to include in each set.\n",
        "        train_size = int(0.9 * len(dataset))\n",
        "        val_size = len(dataset) - train_size\n",
        "\n",
        "        # Divide the dataset by randomly selecting samples.\n",
        "        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "        print('{:>5,} training samples'.format(train_size))\n",
        "        print('{:>5,} validation samples'.format(val_size))\n",
        "\n",
        "        # The DataLoader needs to know our batch size for training, so we specify it \n",
        "        # here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "        # size of 16 or 32.\n",
        "        self.batch_size = 32\n",
        "\n",
        "        # Create the DataLoaders for our training and validation sets.\n",
        "        # We'll take training samples in random order. \n",
        "        self.train_dataloader = DataLoader(\n",
        "                    train_dataset,  # The training samples.\n",
        "                    sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "                    batch_size = self.batch_size # Trains with this batch size.\n",
        "                )\n",
        "\n",
        "        # For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "        self.validation_dataloader = DataLoader(\n",
        "                    val_dataset, # The validation samples.\n",
        "                    sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "                    batch_size = self.batch_size # Evaluate with this batch size.\n",
        "                )        \n",
        "\n",
        "\n",
        "\n",
        "    def train(self,epochs=4):\n",
        "        # Tell pytorch to run this model on the GPU.\n",
        "        self.discriminator.cuda()\n",
        "\n",
        "        # Get all of the model's parameters as a list of tuples.\n",
        "        params = list(self.discriminator.named_parameters())\n",
        "\n",
        "        print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "        print('==== Embedding Layer ====\\n')\n",
        "\n",
        "        for p in params[0:5]:\n",
        "            print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "        print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "        for p in params[5:21]:\n",
        "            print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "        print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "        for p in params[-4:]:\n",
        "            print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))  \n",
        "\n",
        "        # Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "        # I believe the 'W' stands for 'Weight Decay fix\"\n",
        "        self.optimizer = AdamW(self.discriminator.parameters(),\n",
        "                        lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                        eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                        )\n",
        "\n",
        "        # Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "        # We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "        # training data.\n",
        "        #epochs = 2\n",
        "\n",
        "        # Total number of training steps is [number of batches] x [number of epochs]. \n",
        "        # (Note that this is not the same as the number of training samples).\n",
        "        total_steps = len(self.train_dataloader) * epochs\n",
        "\n",
        "        # Create the learning rate scheduler.\n",
        "        scheduler = get_linear_schedule_with_warmup(self.optimizer, \n",
        "                                                    num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                                    num_training_steps = total_steps)\n",
        "            \n",
        "        # This training code is based on the `run_glue.py` script here:\n",
        "        # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "        # Set the seed value all over the place to make this reproducible.\n",
        "        seed_val = 42\n",
        "\n",
        "        random.seed(seed_val)\n",
        "        np.random.seed(seed_val)\n",
        "        torch.manual_seed(seed_val)\n",
        "        torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "        # We'll store a number of quantities such as training and validation loss, \n",
        "        # validation accuracy, and timings.\n",
        "        training_stats = []\n",
        "\n",
        "        # Measure the total training time for the whole run.\n",
        "        total_t0 = time.time()\n",
        "\n",
        "        # For each epoch...\n",
        "        for epoch_i in range(0, epochs):\n",
        "            \n",
        "            # ========================================\n",
        "            #               Training\n",
        "            # ========================================\n",
        "            \n",
        "            # Perform one full pass over the training set.\n",
        "\n",
        "            print(\"\")\n",
        "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "            print('Training...')\n",
        "\n",
        "            # Measure how long the training epoch takes.\n",
        "            t0 = time.time()\n",
        "\n",
        "            # Reset the total loss for this epoch.\n",
        "            total_train_loss = 0\n",
        "\n",
        "            # Put the model into training mode. Don't be mislead--the call to \n",
        "            # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "            # `dropout` and `batchnorm` layers behave differently during training\n",
        "            # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "            self.discriminator.train()\n",
        "\n",
        "            # For each batch of training data...\n",
        "            for step, batch in enumerate(self.train_dataloader):\n",
        "\n",
        "                # Progress update every 40 batches.\n",
        "                if step % 40 == 0 and not step == 0:\n",
        "                    # Calculate elapsed time in minutes.\n",
        "                    elapsed = format_time(time.time() - t0)\n",
        "                    \n",
        "                    # Report progress.\n",
        "                    print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(self.train_dataloader), elapsed))\n",
        "\n",
        "                # Unpack this training batch from our dataloader. \n",
        "                #\n",
        "                # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "                # `to` method.\n",
        "                #\n",
        "                # `batch` contains three pytorch tensors:\n",
        "                #   [0]: input ids \n",
        "                #   [1]: attention masks\n",
        "                #   [2]: labels \n",
        "                b_input_ids = batch[0].to(device)\n",
        "                b_input_mask = batch[1].to(device)\n",
        "                b_labels = batch[2].to(device)\n",
        "\n",
        "                # Always clear any previously calculated gradients before performing a\n",
        "                # backward pass. PyTorch doesn't do this automatically because \n",
        "                # accumulating the gradients is \"convenient while training RNNs\". \n",
        "                # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "                self.discriminator.zero_grad()        \n",
        "\n",
        "                # Perform a forward pass (evaluate the model on this training batch).\n",
        "                # The documentation for this `model` function is here: \n",
        "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "                # It returns different numbers of parameters depending on what arguments\n",
        "                # arge given and what flags are set. For our useage here, it returns\n",
        "                # the loss (because we provided labels) and the \"logits\"--the model\n",
        "                # outputs prior to activation.\n",
        "                outputs = self.discriminator(b_input_ids, \n",
        "                                    token_type_ids=None, \n",
        "                                    attention_mask=b_input_mask, \n",
        "                                    labels=b_labels)\n",
        "                loss, logits = outputs.loss, outputs.logits\n",
        "                # Accumulate the training loss over all of the batches so that we can\n",
        "                # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "                # single value; the `.item()` function just returns the Python value \n",
        "                # from the tensor.\n",
        "                total_train_loss += loss.item()\n",
        "\n",
        "                # Perform a backward pass to calculate the gradients.\n",
        "                loss.backward()\n",
        "\n",
        "                # Clip the norm of the gradients to 1.0.\n",
        "                # This is to help prevent the \"exploding gradients\" problem.\n",
        "                torch.nn.utils.clip_grad_norm_(self.discriminator.parameters(), 1.0)\n",
        "\n",
        "                # Update parameters and take a step using the computed gradient.\n",
        "                # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "                # modified based on their gradients, the learning rate, etc.\n",
        "                self.optimizer.step()\n",
        "\n",
        "                # Update the learning rate.\n",
        "                scheduler.step()\n",
        "\n",
        "            # Calculate the average loss over all of the batches.\n",
        "            avg_train_loss = total_train_loss / len(self.train_dataloader)            \n",
        "            \n",
        "            # Measure how long this epoch took.\n",
        "            training_time = format_time(time.time() - t0)\n",
        "\n",
        "            print(\"\")\n",
        "            print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "            print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "                \n",
        "            # ========================================\n",
        "            #               Validation\n",
        "            # ========================================\n",
        "            # After the completion of each training epoch, measure our performance on\n",
        "            # our validation set.\n",
        "\n",
        "            print(\"\")\n",
        "            print(\"Running Validation...\")\n",
        "\n",
        "            t0 = time.time()\n",
        "\n",
        "            # Put the model in evaluation mode--the dropout layers behave differently\n",
        "            # during evaluation.\n",
        "            self.discriminator.eval()\n",
        "\n",
        "            # Tracking variables \n",
        "            total_eval_accuracy = 0\n",
        "            total_eval_loss = 0\n",
        "            nb_eval_steps = 0\n",
        "\n",
        "            # Evaluate data for one epoch\n",
        "            for batch in self.validation_dataloader:\n",
        "                \n",
        "                # Unpack this training batch from our dataloader. \n",
        "                #\n",
        "                # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "                # the `to` method.\n",
        "                #\n",
        "                # `batch` contains three pytorch tensors:\n",
        "                #   [0]: input ids \n",
        "                #   [1]: attention masks\n",
        "                #   [2]: labels \n",
        "                b_input_ids = batch[0].to(device)\n",
        "                b_input_mask = batch[1].to(device)\n",
        "                b_labels = batch[2].to(device)\n",
        "                \n",
        "                # Tell pytorch not to bother with constructing the compute graph during\n",
        "                # the forward pass, since this is only needed for backprop (training).\n",
        "                with torch.no_grad():        \n",
        "\n",
        "                    # Forward pass, calculate logit predictions.\n",
        "                    # token_type_ids is the same as the \"segment ids\", which \n",
        "                    # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "                    # The documentation for this `model` function is here: \n",
        "                    # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "                    # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "                    # values prior to applying an activation function like the softmax.\n",
        "                    outputs = self.discriminator(b_input_ids, \n",
        "                                        token_type_ids=None, \n",
        "                                        attention_mask=b_input_mask,\n",
        "                                        labels=b_labels)\n",
        "                loss, logits = outputs.loss, outputs.logits\n",
        "                # Accumulate the validation loss.\n",
        "                total_eval_loss += loss.item()\n",
        "\n",
        "                # Move logits and labels to CPU\n",
        "                logits = logits.detach().cpu().numpy()\n",
        "                label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "                # Calculate the accuracy for this batch of test sentences, and\n",
        "                # accumulate it over all batches.\n",
        "                total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "                \n",
        "\n",
        "            # Report the final accuracy for this validation run.\n",
        "            avg_val_accuracy = total_eval_accuracy / len(self.validation_dataloader)\n",
        "            print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "            # Calculate the average loss over all of the batches.\n",
        "            avg_val_loss = total_eval_loss / len(self.validation_dataloader)\n",
        "            \n",
        "            # Measure how long the validation run took.\n",
        "            validation_time = format_time(time.time() - t0)\n",
        "            \n",
        "            print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "            print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "            # Record all statistics from this epoch.\n",
        "            training_stats.append(\n",
        "                {\n",
        "                    'epoch': epoch_i + 1,\n",
        "                    'Training Loss': avg_train_loss,\n",
        "                    'Valid. Loss': avg_val_loss,\n",
        "                    'Valid. Accur.': avg_val_accuracy,\n",
        "                    'Training Time': training_time,\n",
        "                    'Validation Time': validation_time\n",
        "                }\n",
        "            )\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"Training complete!\")\n",
        "\n",
        "        print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "            \n",
        "\n",
        "        return training_stats\n",
        "\n",
        "    def save_model(self, output_dir = './model_save/'):\n",
        "        # Create output directory if needed\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "\n",
        "        print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "        # They can then be reloaded using `from_pretrained()`\n",
        "        model_to_save = self.discriminator.module if hasattr(self.discriminator, 'module') else self.discriminator  # Take care of distributed/parallel training\n",
        "        model_to_save.save_pretrained(output_dir)\n",
        "        self.tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "        # Good practice: save your training arguments together with the trained model\n",
        "        # torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n",
        "\n",
        "    def __load_model(self, input_dir = './drive/MyDrive/Colab Notebooks/summary/en_grammar_check_model'):\n",
        "        print('Loading BERT tokenizer...')\n",
        "        self.tokenizer = BertTokenizerFast.from_pretrained(input_dir)\n",
        "        self.discriminator = BertForSequenceClassification.from_pretrained(input_dir)\n",
        "\n",
        "    def transfer_learning(self, sentences, train_for = True):\n",
        "        \n",
        "        input_ids = []\n",
        "        attention_masks = []\n",
        "\n",
        "        # For every sentence...\n",
        "        for sent in sentences:\n",
        "            # `encode_plus` will:\n",
        "            #   (1) Tokenize the sentence.\n",
        "            #   (2) Prepend the `[CLS]` token to the start.\n",
        "            #   (3) Append the `[SEP]` token to the end.\n",
        "            #   (4) Map tokens to their IDs.\n",
        "            #   (5) Pad or truncate the sentence to `max_length`\n",
        "            #   (6) Create attention masks for [PAD] tokens.\n",
        "            encoded_dict = self.tokenizer.encode_plus(\n",
        "                                sent,                      # Sentence to encode.\n",
        "                                add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                                max_length = 64,           # Pad & truncate all sentences.\n",
        "                                pad_to_max_length = True,\n",
        "                                return_attention_mask = True,   # Construct attn. masks.\n",
        "                                return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                                truncation = True,\n",
        "                        )\n",
        "            # Add the encoded sentence to the list.    \n",
        "            input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "            # And its attention mask (simply differentiates padding from non-padding).\n",
        "            attention_masks.append(encoded_dict['attention_mask'])\n",
        "        \n",
        "        if train_for:\n",
        "            b_labels = torch.ones(len(sentences),dtype=torch.long).to(device)\n",
        "        else:\n",
        "            b_labels = torch.zeros(len(sentences),dtype=torch.long).to(device)\n",
        "        #print(b_labels)\n",
        "        # Convert the lists into tensors.\n",
        "        input_ids = torch.cat(input_ids, dim=0).to(device)\n",
        "        attention_masks = torch.cat(attention_masks, dim=0).to(device)    \n",
        "        #if str(discriminator1.device) == 'cpu':\n",
        "        #    pass\n",
        "        #else:\n",
        "        #    input_ids = input_ids.to(device)\n",
        "        #    attention_masks = attention_masks.to(device)        \n",
        "\n",
        "        outputs = self.discriminator(input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=attention_masks, \n",
        "                                labels=b_labels)\n",
        "\n",
        "        #print(outputs)\n",
        "        #return torch.sigmoid(outputs[0][:,1])\n",
        "        #return outputs[0][:,1]\n",
        "        return outputs['loss'], outputs['logits']\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjqloLeQpyxz"
      },
      "source": [
        "# 문법 학습을 위한 데이터셋 구성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "p-dd_3RRp2Qz",
        "outputId": "97e4fb10-9b5b-4d09-9aa9-33047e69a716"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/summary/korean_news_corpus.csv')\n",
        "df"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>contents</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>문 대통령 변창흠 국토장관 사의표명 사실상 수용</td>\n",
              "      <td>정만호 국민소통수석이 12일 오후 청와대 춘추관 대브리핑룸에서 변창흠 국토부 장관 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>계급장 수여하는 문 대통령</td>\n",
              "      <td>(아산뉴스1) 이광호 기자 문재인 대통령이 12일 오후 충남 아산시 경찰대학에서 열...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>계급장 수여하는 문 대통령</td>\n",
              "      <td>(아산뉴스1) 이광호 기자 문재인 대통령이 12일 오후 충남 아산시 경찰대학에서 열...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>수상자 메달 걸어주는 문 대통령</td>\n",
              "      <td>(아산뉴스1) 이광호 기자 문재인 대통령이 12일 오후 충남 아산시 경찰대학에서 열...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>정몽구 서울아산병원에 50억 쾌척</td>\n",
              "      <td>인재 양성·소외 계층 지원 등 계획 “부친 질병·가난 악순환 끊기 원해 국내 최고 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140559</th>\n",
              "      <td>[건축과도시] 북한산을 캔버스 삼아. 미술관 또 하나의 작품이 되다</td>\n",
              "      <td>&lt;은평구 진관동 사비나 미술관&gt; 서울시 은평구 진관동에 자리잡은 사비나미술관. 삼각...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140560</th>\n",
              "      <td>조선후기 문인 김조순 별장 그린 옥호정도 첫 공개</td>\n",
              "      <td>국립중앙박물관 서화실 개편해 32점 새롭게 전시 옥호정도[국립중앙박물관 제공연합뉴스...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140561</th>\n",
              "      <td>안성 청룡사 대웅전에서 목재 곡자 발견</td>\n",
              "      <td>문화재청(청장 정재숙)의 국고보조와 기술지도로 안성시에서 시행하고 있는 안성 청룡사...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140562</th>\n",
              "      <td>156년전 ㄱ자 곡자 찾았다 안성 청룡사 기둥 밑에서</td>\n",
              "      <td>안성 청룡사 대웅전에서 발견된 곡자 【서울뉴시스】 이수지 기자 안성 청룡사 대웅전에...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140563</th>\n",
              "      <td>[김중기의 필름통] 새 영화 도굴 나인스 게이트 앙상블</td>\n",
              "      <td>영화 도굴 스틸컷 ◆도굴 감독: 박정배 출연: 이제훈 조우진 신혜선 도굴을 소재로 ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>140564 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        title                                           contents\n",
              "0                  문 대통령 변창흠 국토장관 사의표명 사실상 수용  정만호 국민소통수석이 12일 오후 청와대 춘추관 대브리핑룸에서 변창흠 국토부 장관 ...\n",
              "1                              계급장 수여하는 문 대통령  (아산뉴스1) 이광호 기자 문재인 대통령이 12일 오후 충남 아산시 경찰대학에서 열...\n",
              "2                              계급장 수여하는 문 대통령  (아산뉴스1) 이광호 기자 문재인 대통령이 12일 오후 충남 아산시 경찰대학에서 열...\n",
              "3                           수상자 메달 걸어주는 문 대통령  (아산뉴스1) 이광호 기자 문재인 대통령이 12일 오후 충남 아산시 경찰대학에서 열...\n",
              "4                          정몽구 서울아산병원에 50억 쾌척  인재 양성·소외 계층 지원 등 계획 “부친 질병·가난 악순환 끊기 원해 국내 최고 ...\n",
              "...                                       ...                                                ...\n",
              "140559  [건축과도시] 북한산을 캔버스 삼아. 미술관 또 하나의 작품이 되다  <은평구 진관동 사비나 미술관> 서울시 은평구 진관동에 자리잡은 사비나미술관. 삼각...\n",
              "140560            조선후기 문인 김조순 별장 그린 옥호정도 첫 공개  국립중앙박물관 서화실 개편해 32점 새롭게 전시 옥호정도[국립중앙박물관 제공연합뉴스...\n",
              "140561                  안성 청룡사 대웅전에서 목재 곡자 발견  문화재청(청장 정재숙)의 국고보조와 기술지도로 안성시에서 시행하고 있는 안성 청룡사...\n",
              "140562          156년전 ㄱ자 곡자 찾았다 안성 청룡사 기둥 밑에서  안성 청룡사 대웅전에서 발견된 곡자 【서울뉴시스】 이수지 기자 안성 청룡사 대웅전에...\n",
              "140563         [김중기의 필름통] 새 영화 도굴 나인스 게이트 앙상블  영화 도굴 스틸컷 ◆도굴 감독: 박정배 출연: 이제훈 조우진 신혜선 도굴을 소재로 ...\n",
              "\n",
              "[140564 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9ouEN_yp4uG",
        "outputId": "8bb1f717-96d1-4898-f6d4-53132bb6f0f6"
      },
      "source": [
        "df = df.dropna(axis=0)\n",
        "df['contents'].count()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "140536"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aU37ZYJICJgn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "outputId": "6b1f2890-6b12-4437-8244-66af27c2d262"
      },
      "source": [
        "df['contents'][0]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'정만호 국민소통수석이 12일 오후 청와대 춘추관 대브리핑룸에서 변창흠 국토부 장관 사의 표명 관련 브리핑을 하고 있다. 연합뉴스 문재인 대통령이 변창흠 국토교통부 장관의 사의 표명을 사실상 수용했다. 정만호 청와대 국민소통수석은 12일 오후 청와대 춘추관 대브리핑룸에서 브리핑을 통해 변창흠 국토교통부 장관이 이날 한국토지주택공사(LH) 직원 등의 3기 신도시 땅 투기 의혹과 관련해 문재인 대통령에게 사의를 표명했다고 전했다. 문 대통령은 변 장관의 사의 표명에 책임지는 모습을 보일 수밖에 없다고 밝혔다. 이어 다만 2·4 대책의 차질없는 추진이 매우 중요하다며 변 장관 주도로 추진한 공공주도형 공급대책과 관련된 입법의 기초작업까지는 마무리해야 한다고 말했다. 이는 문 대통령이 사실상 변 장관의 사의를 수용한 것으로 해석된다. 이에 앞서 변창흠 국토교통부 장관은 LH 땅 투기 의혹 사건과 관련한 책임론에 대해 “자리에 연연하지 않는다”면서 “(청와대의) 결정에 따르겠다”고 말했다. 변 장관은 이날 국회 국토교통위원회 전체회의에 참석해 “LH 사태로 국민들이 걱정하는 부분을 해소할 수 있게 최대한 대안을 만들고 LH가 근본적으로 다시 태어날 수 있도록 책임지고 추진하겠다”고 언급하고 “그 역할이 충분하다고 평가되지 못했을 때 언제든지 자리에 연연하지 않고 결정에 따르겠다”고 말했다. 특히 경찰에 수사의뢰된 20명 중 11명은 변 장관이 LH 사장 재임 시절 땅 투기에 나선 것으로 드러나면서 LH 사장 출신인 변 장관에 대한 책임론이 대두되던 상황이었다. 전창훈 기자 jch@busan. com ▶ 네이버에서 부산일보 구독하기 클릭! ▶ 부산닷컴 회원가입. 회원 전환하면 부산일보 지면보기 무료이벤트 ▶ 부산일보 홈 바로가기'"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "-e484q4qqA74",
        "outputId": "fcd8b705-6be9-4b6e-f816-0cb57a3a43a3"
      },
      "source": [
        "import re\n",
        "import sys\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# 간단한 전처리\n",
        "def clean_text(txt):\n",
        "    txt = txt.replace('\\n',' ')\n",
        "    txt = txt.replace('\\r',' ')    \n",
        "    txt = txt.replace('=','')\n",
        "    txt = txt.replace('\\\"','')   \n",
        "    txt = txt.replace('\\'','')\n",
        "    txt = txt.replace('”','')\n",
        "    txt = txt.replace('“','')\n",
        "    txt = txt.replace('’','')\n",
        "    #txt = txt.replace(',','')\n",
        "    txt = txt.replace('..','')\n",
        "    txt = txt.replace('...','')\n",
        "    #txt = txt.replace('.','. ')\n",
        "    txt = txt.replace('.','. ')\n",
        "    txt = txt.replace('  ',' ')\n",
        "    txt = txt.replace('  ',' ')    \n",
        "    txt = txt.replace('  ',' ')   \n",
        "    txt = txt.replace('  ',' ')           \n",
        "    txt = txt.replace('  ',' ')\n",
        "    txt = txt.replace('  ',' ')    \n",
        "    txt = txt.replace('  ',' ')   \n",
        "    txt = txt.replace('  ',' ')             \n",
        "    return txt.strip()\n",
        "    \n",
        "# 검사...\n",
        "pattens = [\"[34569][0-9]{3}[\\;.\\;-\\; ][0-9]{4}[\\;.\\;-\\; ][0-9]{4}[\\;.\\;-\\; ][0-9]{4}\",\n",
        "           \"[0-9]{2,3}[\\:\\s\\;.\\;,\\;-;)][0-9]{3,4}[\\:\\s\\;.\\;,\\;-][0-9]{4}\",\n",
        "           \"[0-9]{1}[0-9]{1}[\\W]?[0-1]{1}[0-9]{1}[\\W]?[0-3]{1}[\\W]?[0-9]{1}[\\W]?[1-4]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}\",\n",
        "           \"[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{6}|[0-9]{3}[\\:\\s\\;.\\;,\\;-]([0-9]{5,6}[\\:\\s\\;.\\;,\\;-][0-9]{3}|[0-9]{6}[\\:\\s\\;.\\;,\\;-][0-9]{5}|[0-9]{2,3}[\\:\\s\\;.\\;,\\;-][0-9]{6}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{7}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{4,6}[\\:\\s\\;.\\;,\\;-][0-9]|[0-9]{5}[\\:\\s\\;.\\;,\\;-][0-9]{3}[\\:\\s\\;.\\;,\\;-][0-9]{2}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{5}[\\:\\s\\;.\\;,\\;-][0-9]{3}|[0-9]{4}[\\:\\s\\;.\\;,\\;-][0-9]{4}[\\:\\s\\;.\\;,\\;-][0-9]{3}|[0-9]{6}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{3}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{7})|[0-9]{4}[\\:\\s\\;.\\;,\\;-]([0-9]{3}[\\:\\s\\;.\\;,\\;-][0-9]{6}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{6}[\\:\\s\\;.\\;,\\;-][0-9])|[0-9]{5}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{6}|[0-9]{6}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{5,6}\"\n",
        "           ]\n",
        "\n",
        "filters = []\n",
        "for p in pattens:\n",
        "    filters.append(re.compile(p))\n",
        "\n",
        "sentences = []\n",
        "df = df.dropna(axis=0)\n",
        "cnt = df['contents'].count()\n",
        "#print('Total row count:',cnt)\n",
        "i=0\n",
        "for raw_text in df['contents']:\n",
        "    i=i+1\n",
        "    try:\n",
        "        if i%100 == 0:\n",
        "            percent = (\"{0:.2f}\").format(100 * (i / float(cnt)))\n",
        "            print(f'\\r {percent}% {i}/{str(cnt)}', end=\"\", flush=True)\n",
        "\n",
        "        docs = nltk.sent_tokenize(clean_text(raw_text))\n",
        "        for txt in docs:\n",
        "            if txt.find('▶') > -1 or txt.find('@') > -1 or txt.find('ⓒ') > -1: \n",
        "                pass\n",
        "            else:\n",
        "                txt = txt.strip()\n",
        "                if any(chr.isdigit() for chr in txt) :\n",
        "                    pass\n",
        "                else:\n",
        "                    sentences.append(txt)\n",
        "    except KeyboardInterrupt as ki:\n",
        "        raise ki        \n",
        "    except:\n",
        "        pass #print(\"Unexpected error:\", sys.exc_info()[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            " 3.56% 5000/140536"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-b09b8dcf4c1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m                     \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mki\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mki\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mpass\u001b[0m \u001b[0;31m#print(\"Unexpected error:\", sys.exc_info()[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-b09b8dcf4c1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\r {percent}% {i}/{str(cnt)}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtxt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'▶'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'@'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ⓒ'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[1;32m     94\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m# Standard word tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1235\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m         \"\"\"\n\u001b[0;32m-> 1237\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \"\"\"\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \"\"\"\n\u001b[1;32m   1315\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXzrjItILZrJ"
      },
      "source": [
        "import json  \n",
        "import zipfile  \n",
        "\n",
        "d1 = None  \n",
        "data = None  \n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/Colab Notebooks/summary/data/사설잡지_2.train_original.json.zip\", \"r\") as z:\n",
        "    with z.open('train_original.json') as f:  \n",
        "        data = f.read()  \n",
        "        d1 = json.loads(data.decode(\"utf-8\"))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFEcPMCdOWQK"
      },
      "source": [
        "for i in range(len(d1)):\n",
        "    for txt in d1[i]['article_original']:\n",
        "        sentences.append(txt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DssGeQnFO4Jv"
      },
      "source": [
        "d2 = None  \n",
        "data = None  \n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/Colab Notebooks/summary/data/신문기사_2.train_original.json.zip\", \"r\") as z:\n",
        "    with z.open('train_original.json') as f:  \n",
        "        data = f.read()  \n",
        "        d2 = json.loads(data.decode(\"utf-8\"))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIMJc8gkRV6z"
      },
      "source": [
        "' '.join(d2[1]['article_original'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPEzF3fWP7MN"
      },
      "source": [
        "for i in range(len(d2)):\n",
        "    for txt in d1[2]['article_original']:\n",
        "        sentences.append(txt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pM9ZrtFcqEn0"
      },
      "source": [
        "import re\n",
        "import sys\n",
        "import io\n",
        "\n",
        "#텍스트 정제(전처리)\n",
        "def cleanText(readData):\n",
        "    #텍스트에 포함되어 있는 특수 문자 제거\n",
        "    text = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》◆◇●🎧○▲\\t―△━▷]', '', readData)\n",
        "    return text.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piBg9aJkqHH-"
      },
      "source": [
        "c_sentences = []\n",
        "for sentence in sentences:\n",
        "    s = cleanText(sentence)\n",
        "    c = len(s.split())\n",
        "    if c >= 3 and c < 10 and s.find('재배포') < 0 and s.find('기자') < 0  and s.find('유투브') < 0 and s.find('www') < 0 and s.find('com') < 0 and s.find('접속하기') < 0 and s.find('http') < 0 and s.find('뉴스') < 0 and s.find('일보') < 0 :\n",
        "        if s.endswith(('다','요')):\n",
        "            c_sentences.append(s.strip())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ljb_u8pcqJxe"
      },
      "source": [
        "len(c_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TscPqOMtqKdB"
      },
      "source": [
        "c_sentences[:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51dbd376qMyB"
      },
      "source": [
        "ko_sentences_dataset = c_sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk93tR2Nuk8t"
      },
      "source": [
        "# 문법 discriminator의 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7Zf2oRMMXmH",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "400b809e-7b27-4b3f-c3a7-0dc53e8c8952"
      },
      "source": [
        "use_pretrained_model = True\n",
        "\n",
        "if use_pretrained_model:\n",
        "    #g_discriminator = Grammar_Discriminator(input_dir = '/content/drive/MyDrive/Colab Notebooks/summary/model_save')\n",
        "    g_discriminator = Grammar_Discriminator(input_dir = '/content/drive/MyDrive/Colab Notebooks/summary/ko_grammar_model3')\n",
        "else:\n",
        "    sentences,labels = collect_training_dataset_for_grammar_discriminator(ko_sentences_dataset)\n",
        "    print(len(sentences))\n",
        "    g_discriminator = Grammar_Discriminator()\n",
        "    g_discriminator.set_dataset(sentences,labels)\n",
        "    g_discriminator.train(epochs=1)\n",
        "    g_discriminator.save_model(output_dir='/content/drive/MyDrive/Colab Notebooks/summary/ko_grammar_model3')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKRx8zA0uY-B"
      },
      "source": [
        "if False: ## 추가적인 fine-tuning\n",
        "    g_discriminator.train(epochs=10)\n",
        "    g_discriminator.save_model(output_dir='/content/drive/MyDrive/Colab Notebooks/summary/ko_grammar_model3')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-kyzdkT-G2m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5955a840-e586-4589-f141-307599a26c13"
      },
      "source": [
        "txt = ['최근 날씨가 포근해지면서 산을 찾는 사람들도 늘고 있는데요','서비스를 음원 플랫폼 스포티파이가 국내 론칭한다']\n",
        "g_discriminator.discriminator.to(device)\n",
        "g_discriminator.transfer_learning(txt)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(4.3092, device='cuda:0', grad_fn=<NllLossBackward>),\n",
              " tensor([[-4.0743,  4.2727],\n",
              "         [ 4.2365, -4.3814]], device='cuda:0', grad_fn=<AddmmBackward>))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d96kaCAHKuUc"
      },
      "source": [
        "##4.3 Static similarity discriminator class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZDpXe7XKxeg",
        "trusted": true
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import BertTokenizer\n",
        "from scipy.signal import find_peaks\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.misc import electrocardiogram\n",
        "import scipy\n",
        "\n",
        "\n",
        "class Similarity_Discriminator:\n",
        "    '''\n",
        "    _instance = None\n",
        "    _embedder = None\n",
        "    def __new__(cls,pre_trained_model_name='stsb-roberta-large'):\n",
        "        if cls._instance is None:\n",
        "            print('Creating Similarity_Discriminator object')\n",
        "            cls._instance = super(Similarity_Discriminator, cls).__new__(cls)\n",
        "            # Put any initialization here.\n",
        "            cls._embedder = SentenceTransformer(pre_trained_model_name)\n",
        "        return cls._instance\n",
        "\n",
        "    '''\n",
        "\n",
        "    def __init__(self,pre_trained_model_name='xlm-r-large-en-ko-nli-ststb'):\n",
        "        print('Creating Similarity_Discriminator object')\n",
        "        # Put any initialization here.\n",
        "        self._embedder = SentenceTransformer(pre_trained_model_name)  \n",
        "        #self.cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "    def encode(self,texts):\n",
        "        return self._embedder.encode(texts,show_progress_bar=False)\n",
        "\n",
        "    def similarity(self, query_text, org_text_emb):\n",
        "        queries = nltk.sent_tokenize(query_text)\n",
        "        query_embeddings = self._embedder.encode(queries,show_progress_bar=False)\n",
        "        #query_embeddings = self._embedder.encode(queries,show_progress_bar=False)\n",
        "        #print(queries)\n",
        "        #print(org_text_emb)\n",
        "        \n",
        "        if len(query_embeddings) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        cos_scores = scipy.spatial.distance.cdist(query_embeddings, org_text_emb, \"cosine\")\n",
        "        similarity_score = 1.0 - np.mean(np.min(cos_scores,axis=0))\n",
        "        '''\n",
        "        for query, query_embedding in zip(queries, query_embeddings):\n",
        "            distances = scipy.spatial.distance.cdist([query_embedding], [org_text_emb], \"cosine\")[0]\n",
        "            results = zip(range(len(distances)), distances)\n",
        "            for idx, distance in results:\n",
        "                scores.append(1-distance)\n",
        "        '''\n",
        "        return similarity_score  \n",
        " "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sQZ36GuMumP"
      },
      "source": [
        "###4.3.1 한국어 문장 유사도 pre-trained model 적용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9Miao14Muww",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2459a436-a584-4cd3-d628-e311965157cb"
      },
      "source": [
        "s_discriminator = Similarity_Discriminator()\n",
        "#s_discriminator = Similarity_Discriminator()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating Similarity_Discriminator object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xnk9GsQ0K1t1"
      },
      "source": [
        "# 4.4 Document source class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhvXMrSO-CiD"
      },
      "source": [
        "## 두 문장을 합치는 rule 변환기... --> 이거... ML로 나중에 바꿔야..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_W1Wqq26MjQ"
      },
      "source": [
        "combine_matching_table = {}\n",
        "\n",
        "combine_matching_table['어요.'] = '고'\n",
        "combine_matching_table['지요.'] = '고'\n",
        "combine_matching_table['답니다.'] = '고'\n",
        "combine_matching_table['보거라.'] = '봐,'\n",
        "combine_matching_table['간단다.'] = '가니,'\n",
        "combine_matching_table['돼.'] = '되,'\n",
        "combine_matching_table['해.'] = '하며,'\n",
        "combine_matching_table['다.'] = '고'\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giRiIsfR6DV6"
      },
      "source": [
        "def combine_sentence(txt):\n",
        "    for c in combine_matching_table.keys():\n",
        "        if txt.endswith(c):\n",
        "            txt = txt.replace(c,combine_matching_table[c])\n",
        "    return txt"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwhMHwwefy-N"
      },
      "source": [
        "\n",
        "conjunction_table = ['그러던','그래서','그러나','그런데','그리고','그랬더니','그러니까','하지만','그래서']"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnBm6RCvNIWG"
      },
      "source": [
        "## 4.4.2 source class 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsJKbtc2K4xN",
        "trusted": true
      },
      "source": [
        "\n",
        "\n",
        "class Source:\n",
        "\n",
        "    def __init__(self,org_text):\n",
        "        self.org_text = org_text\n",
        "\n",
        "    def __crean_text(self, txt):\n",
        "        txt = txt.replace('\\n',' ')\n",
        "        txt = txt.replace('\\r',' ')    \n",
        "        txt = txt.replace('=','')\n",
        "        txt = txt.replace('\\\"','')   \n",
        "        txt = txt.replace('\\'','')\n",
        "        txt = txt.replace(',','')\n",
        "        txt = txt.replace('..','')\n",
        "        txt = txt.replace('...','')\n",
        "        txt = txt.replace(' .','.')\n",
        "        txt = txt.replace('.','. ')\n",
        "        txt = txt.replace('  ',' ')\n",
        "        txt = txt.replace('  ',' ')    \n",
        "        txt = txt.replace('  ',' ')   \n",
        "        txt = txt.replace('  ',' ')           \n",
        "        txt = txt.replace('  ',' ')\n",
        "        txt = txt.replace('  ',' ')    \n",
        "        txt = txt.replace('  ',' ')   \n",
        "        txt = txt.replace('  ',' ')           \n",
        "        return txt.strip()\n",
        "\n",
        "    def set_key_rate(self,s_discriminator):\n",
        "        #self.full_text = self.__crean_text(self.full_text.strip())\n",
        "        self.org_text = self.__crean_text(self.org_text.strip())\n",
        "        print('-'*50)\n",
        "        print(self.org_text)\n",
        "        print('-'*50)\n",
        "        self.org_sentences = np.array(nltk.sent_tokenize(self.org_text))\n",
        "        for i,sents in enumerate(self.org_sentences):\n",
        "            for cj in conjunction_table: \n",
        "                if sents.startswith(cj):\n",
        "                    self.org_sentences[i] = sents[len(cj):].strip()\n",
        "\n",
        "        #self.full_sentences = np.array(nltk.sent_tokenize(self.full_text))\n",
        "        \n",
        "        #self.org_term_set = (' ' + self.org_text + ' ').split(' ')\n",
        "        self.org_term_set = (' '.join(self.org_sentences)).strip().split(' ')\n",
        "        self.org_source_length = len(self.org_term_set)\n",
        "        self.term_table = {}\n",
        "        self.seps = []\n",
        "        #morp_table = {}\n",
        "\n",
        "        for index, word in zip(range(len(self.org_term_set)),self.org_term_set):\n",
        "            self.term_table[index] = word\n",
        "            if word.endswith(('.','?')):\n",
        "                self.seps.append(index)\n",
        "                if self.org_source_length - 1 == index:\n",
        "                    pass\n",
        "                else:\n",
        "                    self.term_table[index] = combine_sentence(word)\n",
        "\n",
        "        #print(self.term_table.values())\n",
        "        #print('------------------------------------------------------------------')\n",
        "\n",
        "        self.s_discriminator = s_discriminator\n",
        "        # 원문의 embedding...\n",
        "        self.org_text_emb = self.s_discriminator.encode(self.org_sentences)\n",
        "        #self.full_text_emb = self.s_discriminator.encode(self.full_sentences)\n",
        "        #top_n = int(len(self.term_table) * comp_rate)\n",
        "        #print('top_n',top_n)\n",
        "        #self.story_peaks = [i+1 for i in range(top_n)]\n",
        "\n",
        "    def get_org_sample(self, num):\n",
        "        return self.org_sentences[np.random.choice(len(self.org_sentences), num)]\n",
        "\n",
        "    def get_source_embedded_code(self):\n",
        "        return self.org_text_emb\n",
        "\n",
        "    def get_random_text(self,rate=0.7):\n",
        "        cnt = int(len(self.term_table) * rate)\n",
        "        a = list(self.term_table.keys())\n",
        "        b = np.random.choice(a, cnt)\n",
        "        c = [fruit for fruit in a if fruit not in b]\n",
        "        txt = []\n",
        "        for i in c:\n",
        "            txt.append(self.term_table[i])\n",
        "        return ' '.join(txt).strip(), hash(tuple(b))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UAeFBYMMxKY",
        "outputId": "e2cec046-6b4c-4216-cb46-7f9dc6a9db5e"
      },
      "source": [
        "txt = \"\"\"\n",
        "황금마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼어요. \n",
        "그러니까 반드시 밤 열두시가 되기 전에 돌아와야 해요.\n",
        "\"\"\"\n",
        "s = Source(txt)\n",
        "s.set_key_rate(s_discriminator)\n",
        "s.get_random_text()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "황금마차는 호박으로 흰말은 생쥐로 마부는 도마뱀으로 변하게 돼어요. 그러니까 반드시 밤 열두시가 되기 전에 돌아와야 해요.\n",
            "--------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('호박으로 흰말은 생쥐로 밤 열두시가 되기 전에 돌아와야', 6553471788267561388)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XY59mdNK8ub"
      },
      "source": [
        "# 4.5 Generator class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5CLF3WcK6lp",
        "trusted": true
      },
      "source": [
        "from functools import reduce\n",
        "\n",
        "\n",
        "# custom weights initialization called on netG and netD\n",
        "'''\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Linear') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.06)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.05)\n",
        "        m.bias.data.fill_(0)\n",
        "'''\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "        Simple Generator w/ MLP\n",
        "    \"\"\"\n",
        "    '''\n",
        "    def __init__(self, input_size=1024):\n",
        "        super(Generator, self).__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(input_size, input_size*2),\n",
        "            nn.BatchNorm1d(input_size*2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(input_size*2, input_size*3),\n",
        "            nn.BatchNorm1d(input_size*3),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(input_size*3, input_size*3),\n",
        "            nn.BatchNorm1d(input_size*3),\n",
        "            nn.LeakyReLU(0.2),            \n",
        "            nn.Linear(input_size*3, input_size*2),\n",
        "            nn.BatchNorm1d(input_size*2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(input_size*2, input_size),\n",
        "            #nn.BatchNorm1d(term_length*4),\n",
        "            nn.Tanh() # -1 ~ 1\n",
        "        )\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, input_size=1024):\n",
        "        super(Generator, self).__init__()\n",
        "        l1 = nn.Linear(input_size, input_size*4)\n",
        "        l1.weight.data.normal_(0.0, 0.03)\n",
        "        bn = nn.BatchNorm1d(input_size*4)\n",
        "        bn.weight.data.normal_(0.0, 0.05)\n",
        "        bn.bias.data.fill_(0)        \n",
        "        l2 = nn.Linear(input_size*4, input_size)\n",
        "        l2.weight.data.normal_(0.01, 0.05)\n",
        "        self.layer = nn.Sequential(\n",
        "            l1,\n",
        "            bn,\n",
        "            nn.LeakyReLU(0.2),\n",
        "            l2,\n",
        "            #nn.BatchNorm1d(term_length*4),\n",
        "            nn.Tanh() # -1 ~ 1\n",
        "        )\n",
        "\n",
        "    def forward(self, x, bias):\n",
        "        #biased_noise = torch.randn(N,_NOISE_DIM)\n",
        "        # stroy peak에 해당하는 term에게 평균값에 해당하는 bias를 추가 한다.\n",
        "                 \n",
        "        y_ = self.layer(x)\n",
        "        y = torch.add(y_,bias)\n",
        "        #y = nn.Sigmoid()(y)\n",
        "\n",
        "        return y, y_\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co1MnRG8a4Vq"
      },
      "source": [
        "# multi-discriminator에 대한 Adaptive discriminant factor 를 구하기 위한 학습\n",
        "\n",
        "ref : https://realpython.com/python-ai-neural-network/\n",
        "\n",
        "colab 수식입력 : \n",
        "\n",
        "https://wikidocs.net/1679\n",
        "\n",
        "https://en.wikipedia.org/wiki/Help:Displaying_a_formula#Formatting_using_TeX\n",
        "\n",
        "Original GAN의 목적함수\n",
        "$$ \\min_{G}\\max_{D} V(D,G) = E_{x\\sim p_{data}(x)}[logD(x)] + E_{z\\sim p_{z}(z)}[log(1-D(G(z)))] $$\n",
        "\n",
        "Multi-Discriminator GAN은 discriminator가 각 목적에 의하여 여러개 (N개) 있다.\n",
        "MDGAN의 목적함수\n",
        "\n",
        "$$ \\min_{G}\\max_{D_{i\\sim N}} V(D_{i\\sim N},G) = \\sum_{i=1}^N \\{E_{x\\sim p_{data}(x)}[logD_i(x)] + E_{z\\sim p_{z}(z)}[log(1-D_i(G(z)))]\\} $$\n",
        "\n",
        "여기서\n",
        "\n",
        "$$ L(D_i,G) =  E_{x\\sim p_{data}(x)}[logD_i(x)] + E_{z\\sim p_{z}(z)}[log(1-D_i(G(z)))] $$\n",
        "\n",
        "이라하고 단순화 하면\n",
        "\n",
        "$$ \\min_{G}\\max_{D_{i\\sim N}} V(D_{i\\sim N},G) = \\sum_{i=1}^N L(D_i,G) $$\n",
        "\n",
        "와 같이 된다.\n",
        "\n",
        "문제점은 GAN의 특성상, 특정 Discriminator가 학습에 있어 지배적으로 loss 함수에 영향을 미치게 되어 각각의 Discriminator가 골고루 학습에 참여하지 못하고 의도하지 않은 결과를 만들게 된다. 이러한 문제점을 극복하기 위해 다음의 두가지 제안을 한다.\n",
        "\n",
        "1) 목적함수에 각 Loss 에 대한 표준편차 (standard-deviation) 를 반영하여 각 Discriminator에 대한 Loss가 상호 유사한 수준을 유지하면 학습이 진행되도록 한다.\n",
        "\n",
        "$$ \\min_{G}\\max_{D_{i\\sim N}} V(D_{i\\sim N},G) = \\sum_{i=1}^N L(D_i,G) + STD_{i \\sim N}(L(D_i,G))$$\n",
        "\n",
        "2) 1)의 제안에 추가하여, 각 discriminator에 의한 loss를 제어하기 위해, adaptive discriminant factor (ADF) 를 적용하고, 학습의 진행 과정에서 이를 최적화 한다. \n",
        "\n",
        "$$ \\min_{G}\\max_{D_{i\\sim N}} V(D_{i\\sim N},G) = \\sum_{i=1}^N f_iL(D_i,G) + STD_{i \\sim N}(L(D_i,G))$$\n",
        "\n",
        "여서서 \n",
        "\n",
        "fi = adaptive discriminant factor for discriminator i \n",
        "\n",
        "중요한 것은, 학습과정에서 Li을 작게 (학습의 방향)하기 위해서는 fi는 역으로 커져야 한다. 그래야, 전체 Loss function에서 비중이 증대되어 더 적극적인 학습이 이루어 지게 된다. 따라서, fi의 최적화 방향은 기존의 gradient decent와 반대 방향이 되어야 한다.\n",
        "\n",
        "$$ f_i^{t+1} = f_i^t + \\alpha \\frac{\\partial V}{\\partial f_i}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLVVmQdxLBHZ"
      },
      "source": [
        "##4.6 Summarizer class (GAN training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0RQOPpQgUTE"
      },
      "source": [
        "# 학습기..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nd8GTS7HKz1H",
        "trusted": true
      },
      "source": [
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "from scipy.special import expit\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "class SAM_Summarizer:\n",
        "\n",
        "    def __init__(self,g_discriminator,s_discriminator):\n",
        "        self.g_discriminator = g_discriminator\n",
        "        #self.c_discriminator = c_discriminator\n",
        "        self.s_discriminator = s_discriminator\n",
        "        self.m = nn.Sigmoid()\n",
        "\n",
        "    def ready(self,source):\n",
        "        self.source = source  \n",
        "        #self.source.analysis_frame_terms(self.s_discriminator)\n",
        "        self.generator = Generator(input_size=self.source.org_source_length)\n",
        "        #self.generator.apply(weights_init)\n",
        "        return self\n",
        "\n",
        "    def summarize(self,epochs=10,batch_size=1,learning_rate=2e-4, display = False):\n",
        "        history = self.__train(epochs,batch_size,learning_rate,display)\n",
        "        if display:\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            plt.plot(history['gen_g_loss'],label='grammar loss')\n",
        "            plt.plot(history['gen_l_loss'],label='compression loss')\n",
        "            plt.plot(history['gen_s_loss'],label='similarity loss')\n",
        "\n",
        "            plt.plot(history['total loss'],label='total loss')\n",
        "            plt.plot(history['losses std'],label='standard deviation of losses')\n",
        "            \n",
        "            #if 'dis_loss' in history:\n",
        "            #    plt.plot(history['dis_loss'],label='discriminator grammar loss')\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "        return history\n",
        "\n",
        "    # text의 생성 for torch\n",
        "    def __text_gen2(self, p_txt, gen_length):\n",
        "        gtext = []\n",
        "        sorted_noise, i = torch.sort(p_txt, descending=True)\n",
        "        order, i = torch.sort(i[:gen_length], descending=False)\n",
        "        #print(len(order))\n",
        "        #print(gen_length)\n",
        "        assert len(order) == gen_length\n",
        "        order = order.cpu().detach().numpy()\n",
        "        for k in order:\n",
        "            gtext.append((self.source.term_table[k],k))\n",
        "        return gtext\n",
        "\n",
        "    def __text_gen3(self, p_txt):\n",
        "        gtext = []\n",
        "\n",
        "        for order,p in enumerate(p_txt):\n",
        "            if p > 0.0:\n",
        "                gtext.append(self.source.term_table[order])\n",
        "        return gtext\n",
        "\n",
        "    def __text_gen4(self, p_txt):\n",
        "        gtext = \"\"\n",
        "        indexs = []\n",
        "        for order,p in enumerate(p_txt):\n",
        "            if p > 0.0:\n",
        "                gtext += self.source.term_table[order] + ' '\n",
        "                indexs.append(order)\n",
        "        return gtext.strip(),indexs\n",
        "\n",
        "\n",
        "    def __discrete_gradient(self,weights,use_gpu=False, verbose=0):\n",
        "        fake_gen_out = torch.zeros(weights.shape).to(device)\n",
        "        #fake_com_out = torch.zeros(weights.shape).to(device)\n",
        "        fake_sim_out = torch.zeros(weights.shape).to(device)\n",
        "        fake_len_out = torch.zeros(weights.shape).to(device) \n",
        "\n",
        "        #real_text = self.source.get_org_sample(weights.shape[0])\n",
        "        fake_outs = []\n",
        "        #real_outs = []\n",
        "        apply_order = []\n",
        "        for i, noise in enumerate(weights):\n",
        "            #gtext = self.__text_gen2(noise,gen_length)\n",
        "            gtext,tk = self.__text_gen4(noise)\n",
        "            fake_outs.append(gtext)\n",
        "            apply_order.append((i,tk))\n",
        "  \n",
        "        D_z_loss, fake_gmr_out=self.g_discriminator.transfer_learning(fake_outs,train_for = False)\n",
        "\n",
        "        o_sim_out = []\n",
        "        o_len_out = []\n",
        "        for fake_text in fake_outs:\n",
        "            o_sim_out.append(self.s_discriminator.similarity(fake_text,self.source.org_text_emb))\n",
        "\n",
        "            l = ((1 - len(fake_text.split(' '))/self.source.org_source_length)-0.5) * 2\n",
        "\n",
        "            o_len_out.append(l)\n",
        "            #print(1 - len(fake_text.split(' '))/self.source.org_source_length)\n",
        "            #o_len_out.append(-len(fake_text.split(' '))/self.source.org_source_length)\n",
        "        \n",
        "        \n",
        "        for j, (i,tk) in enumerate(apply_order):\n",
        "\n",
        "            try:\n",
        "                '''\n",
        "                a = torch.tanh( fake_gmr_out[j,1])\n",
        "                if a > 0 :\n",
        "                    fake_gen_out[:] = -0.5\n",
        "                    fake_gen_out[i,tk] = a\n",
        "                else:\n",
        "                    fake_gen_out[:] = 0.5\n",
        "                    fake_gen_out[i,tk] = a\n",
        "                '''\n",
        "                fake_gen_out[i,tk] = torch.tanh( fake_gmr_out[j,1])\n",
        "                fake_sim_out[i,tk] = o_sim_out[j]\n",
        "                fake_len_out[:] = o_len_out[j]\n",
        "                fake_len_out[i,tk] = 0 #torch.tensor(fake_text_len/self.source.org_source_length).to(device)\n",
        "                #fake_len_out[:] = o_len_out[j] #torch.tensor(fake_text_len/self.source.org_source_length).to(device)\n",
        "                #print(o_len_out[j])\n",
        "            except Exception as ex:\n",
        "                print(j,i,tk)\n",
        "                print(fake_gmr_out)\n",
        "                raise ex\n",
        "\n",
        "        return fake_gen_out, fake_sim_out, fake_len_out #fake_com_out, fake_sim_out #, D_z_loss, D_x_loss\n",
        "\n",
        "\n",
        "    def __train(self, epochs=10,batch_size=10,learning_rate=2e-4,display = False):\n",
        "        # In the Deepmind paper they use RMSProp however then Adam optimizer\n",
        "        # improves training time\n",
        "        #generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "        # This method returns a helper function to compute cross entropy loss\n",
        "        #cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "        # Set the seed value all over the place to make this reproducible.\n",
        "        seed_val = int(random.random()*100)\n",
        "\n",
        "        random.seed(seed_val)\n",
        "        np.random.seed(seed_val)\n",
        "        torch.manual_seed(seed_val)\n",
        "        torch.cuda.manual_seed_all(seed_val)\n",
        "        \n",
        "        criterion = nn.BCELoss()\n",
        "        #D_opt = torch.optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "        G_opt = AdamW(self.generator.parameters(),\n",
        "                        lr = 2e-3, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                        eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                        )\n",
        "        # Create the learning rate scheduler.\n",
        "        scheduler = get_linear_schedule_with_warmup(G_opt, \n",
        "                                                    num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                                    num_training_steps = epochs)\n",
        "        \n",
        "        #gen_length = len(self.source.story_peaks) + int(len(self.source.story_peaks)*self.frame_expansion_ratio)\n",
        "        pb = ProgressBar(epochs,prefix='Train...')\n",
        "        gen_gmr_loss_history = []\n",
        "        gen_len_loss_history = []\n",
        "        gen_sim_loss_history = []\n",
        "        dis_loss_history = []    \n",
        "        total_loss_history = []\n",
        "        losses_std_history = []\n",
        "\n",
        "        #model 들은 cuda로 보낸다.\n",
        "        self.g_discriminator.discriminator.to(device)\n",
        "        self.g_discriminator.discriminator.eval() # 학습하지 않는다...\n",
        "        #self.c_discriminator.discriminator.to(device)\n",
        "        #self.c_discriminator.discriminator.eval() # 학습하지 않는다...\n",
        "\n",
        "        self.generator.to(device)       \n",
        "        self.generator.train()\n",
        "\n",
        "        #self.bias_w = init_bias\n",
        "        initial_bias = 0\n",
        "        G_s_loss = torch.tensor(0)\n",
        "        #G_c_loss = torch.tensor(0)\n",
        "        G_g_loss = torch.tensor(0)\n",
        "\n",
        "\n",
        "        dfs = torch.tensor([ 1.0, 1.0, 1.0], device=device, dtype=torch.float, requires_grad=True)\n",
        "\n",
        "        for i in range(epochs):\n",
        "   \n",
        "            if True:\n",
        "                noise = torch.randn(batch_size,self.source.org_source_length).to(device) \n",
        "                bias = torch.zeros_like(noise).to(device)\n",
        "                #if i < epochs/4:\n",
        "                #bias = torch.randn(batch_size,self.source.org_source_length).to(device) / 4 \n",
        "                #else:\n",
        "                \n",
        "                #bias = torch.randn(batch_size,self.source.org_source_length).to(device) \n",
        "                sw, sw0 = self.generator(noise,bias)\n",
        "                with torch.no_grad():                \n",
        "                    fake_gmr_out, fake_sim_out, fake_len_out = self.__discrete_gradient(sw)\n",
        "\n",
        "                #print(fake_len_out)\n",
        "\n",
        "                sw1 = sw * fake_sim_out\n",
        "                G_s_loss = -torch.mean(sw1) \n",
        "                sw2 = sw * fake_gmr_out\n",
        "                G_g_loss = -torch.mean(sw2) \n",
        "                sw3 = sw * fake_len_out\n",
        "                G_l_loss = -torch.mean(sw3) * 10\n",
        "\n",
        "                dsc_loss = torch.stack([G_g_loss,G_s_loss,G_l_loss])\n",
        "\n",
        "                G_loss = torch.dot(dfs,dsc_loss) + torch.std(dsc_loss)\n",
        "                #G_loss = G_g_loss + G_s_loss\n",
        "\n",
        "                self.generator.zero_grad()\n",
        "                G_loss.backward()\n",
        "                #print('backward:')\n",
        "                G_opt.step()\n",
        "                scheduler.step()\n",
        "                '''\n",
        "                learning_rate = 0.1\n",
        "                with torch.no_grad():\n",
        "                    dfs += learning_rate * dfs.grad\n",
        "                    dfs.grad = None                    \n",
        "                    dfs[dfs < 0] = 0.1                \n",
        "                '''\n",
        "\n",
        "            gen_gmr_loss_history.append(G_g_loss.cpu().detach().numpy())\n",
        "            #gen_com_loss_history.append(G_c_loss.cpu().detach().numpy())\n",
        "            gen_sim_loss_history.append(G_s_loss.cpu().detach().numpy())\n",
        "            #dis_loss_history.append(D_loss.cpu().detach().numpy())\n",
        "            gen_len_loss_history.append(G_l_loss.cpu().detach().numpy())\n",
        "\n",
        "            #pb.printProgress(+1,f'{i+1}/{epochs} epochs, beta:{dfs} Generator / grammar loss:{G_g_loss}  similarity loss:{G_s_loss}') #,   Discriminator grammar_loss:{D_loss}        ')\n",
        "            #pb.printProgress(+1,'{}/{} epochs, beta:{}, grammar loss:{:.4f}  similarity loss:{:.4f} length loss:{:.4f}'.format(i+1,epochs,dfs,G_g_loss,G_s_loss,G_l_loss)) #,   Discriminator grammar_loss:{D_loss}        ')\n",
        "            pb.printProgress(+1,'{}/{} epochs,grammar loss:{:.4f}  similarity loss:{:.4f} length loss:{:.4f}'.format(i+1,epochs,G_g_loss,G_s_loss,G_l_loss)) #,   Discriminator grammar_loss:{D_loss}        ')\n",
        "            \n",
        "            total_loss_history.append(torch.sum(dsc_loss).item())\n",
        "            losses_std_history.append(torch.std(dsc_loss).item())\n",
        "\n",
        "            \n",
        "        self.generator.eval()\n",
        "        #self.g_discriminator.discriminator.eval()\n",
        "        if display:\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            xs = np.arange(self.source.org_source_length)\n",
        "            plt.bar(xs+0.0,sw0[0].cpu().detach().numpy(),label='before activation weights',width=0.2)\n",
        "            plt.bar(xs+0.2,sw[0].cpu().detach().numpy(),label='after activation weights',width=0.2)\n",
        "            plt.bar(xs+0.4,bias[0].cpu().detach().numpy(),label='bias weights',width=0.2)         \n",
        "            plt.legend()        \n",
        "            plt.show()\n",
        "\n",
        "        return  {'gen_g_loss':gen_gmr_loss_history,'gen_s_loss':gen_sim_loss_history,'gen_l_loss':gen_len_loss_history,'total loss':total_loss_history,'losses std':losses_std_history} #,'dis_loss':dis_loss_history }\n",
        "\n",
        "    def get_summary(self, count):\n",
        "        #texts = []\n",
        "        self.generator.cpu()\n",
        "        self.generator.eval()\n",
        "        #gen_length = len(self.source.story_peaks) + int(len(self.source.story_peaks)*self.frame_expansion_ratio)\n",
        "        noise = torch.randn(count,self.source.org_source_length)\n",
        "        bias = torch.zeros_like(noise)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            sw,sw0 = self.generator(noise,bias)\n",
        "            #sw,sw0 = self.generator(noise)\n",
        "\n",
        "        max_score = 0\n",
        "        max_sim = 0\n",
        "        comp_rate = 0\n",
        "        best_text = \"\"\n",
        "\n",
        "        for p_txt in sw:\n",
        "            gtext = self.__text_gen3(p_txt)\n",
        "            text = ' '.join(gtext)\n",
        "            #print('>>',text)\n",
        "            sim_score = self.s_discriminator.similarity(text,self.source.org_text_emb)\n",
        "            if sim_score > max_sim:\n",
        "                best_text = text.strip()\n",
        "                loss, out=self.g_discriminator.transfer_learning([text],train_for = False)\n",
        "                max_score = out[0,1].item()\n",
        "                comp_rate = 1 - len(best_text.split(' '))/self.source.org_source_length\n",
        "                max_sim = sim_score\n",
        "            #texts.append([text.strip(),out,sim_score])\n",
        "        return best_text, max_score, max_sim, comp_rate\n",
        "\n",
        "    def get_samples(self,count):\n",
        "        self.generator.cpu()\n",
        "        self.generator.eval()\n",
        "        noise = torch.randn(count,self.source.org_source_length)\n",
        "        bias = torch.zeros_like(noise)\n",
        "        with torch.no_grad():\n",
        "            sw,sw0 = self.generator(noise,bias)\n",
        "        #samples = []\n",
        "        best_text = \"\"\n",
        "        max_score = 0\n",
        "        for p_txt in sw:\n",
        "            gtext = self.__text_gen3(p_txt)\n",
        "            text = ' '.join(gtext).strip()\n",
        "            #print(text)\n",
        "            loss, out=self.g_discriminator.transfer_learning([text],train_for = False)\n",
        "            sim_score = self.s_discriminator.similarity(text,self.source.org_text_emb)    \n",
        "            comp_rate = 1 - len(text.split(' '))/self.source.org_source_length\n",
        "            #samples.append((text,out[0,1].item(),sim_score,comp_rate))\n",
        "            score = out[0,1].item() + sim_score + comp_rate\n",
        "            if max_score < score:\n",
        "                max_score = score\n",
        "                best_text = text\n",
        "        #return [best_text for i in range(count)], max_score\n",
        "        return best_text, max_score\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCdfO9iuLH6D"
      },
      "source": [
        "#5. Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_eAwIPLb4aj"
      },
      "source": [
        "## 비교 대상 요약 알고리즘 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhcoXuPMGy09"
      },
      "source": [
        "def sam_wgan4(text, epochs=50, batch_size=32,display=False, retry = True, retry_count = 0):\n",
        "    if retry_count > 7:\n",
        "        raise Exception(\"Can't summarize the text\")\n",
        "\n",
        "    source = Source(text)\n",
        "    source.set_key_rate(s_discriminator)\n",
        "    summarizer = SAM_Summarizer(g_discriminator,s_discriminator)\n",
        "    summarizer.ready(source)\n",
        "    hist = summarizer.summarize(epochs,batch_size=2,learning_rate=5e-3,display=display)\n",
        "    samples, max_score = summarizer.get_samples(batch_size)\n",
        "    #print(samples)\n",
        "    if retry and max_score < 4.1:\n",
        "        print('max score:',max_score)\n",
        "        return sam_wgan4(text, epochs+10, batch_size,display=display,retry_count=retry_count+1)\n",
        "    return samples, max_score"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FstAHWGQ8KR"
      },
      "source": [
        "txt = \"\"\"\n",
        "옛날 어느 집에 귀여운 여자 아기가 태어났어요.\n",
        "아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요.\n",
        "\"\"\"\n",
        "sam_wgan4(txt,epochs=100,display= True,retry = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yh-ixQLxlCOB",
        "outputId": "dd320666-3f87-46bb-c73e-7d5d09b336ea"
      },
      "source": [
        "txt = \"\"\"\n",
        "서산시의회(의장 임재관) 가충순·이수의 의원이 (사)한국지역신문협회에서 수여하는 우수의정대상을 받았다. 가충순 의원과 이수의 의원은 16일 팔봉면 폰타나 리조트에서 열린 한국지역신문협회 하계 워크샵에서 지역사회 발전을 위해 활발한 의정활동을 펼친 공로를 인정받아 우수의정대상을 수상했다.\n",
        "\"\"\"\n",
        "sam_wgan4(txt,epochs=50,display= False,retry = True)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "서산시의회(의장 임재관) 가충순·이수의 의원이 (사)한국지역신문협회에서 수여하는 우수의정대상을 받았다. 가충순 의원과 이수의 의원은 16일 팔봉면 폰타나 리조트에서 열린 한국지역신문협회 하계 워크샵에서 지역사회 발전을 위해 활발한 의정활동을 펼친 공로를 인정받아 우수의정대상을 수상했다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs,grammar loss:-0.0787  similarity loss:-0.0671 length loss:-0.0797\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('서산시의회(의장 임재관) 가충순·이수의 의원이 (사)한국지역신문협회에서 우수의정대상을 받았고 가충순 이수의 의원은 팔봉면 한국지역신문협회 발전을 활발한 공로를 인정받아 수상했다.',\n",
              " 5.641237378764923)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "0S301yeelEvG"
      },
      "source": [
        "full_text = \"\"\"\n",
        "옛날 어느 집에 귀여운 여자 아기가 태어났어요.\n",
        "아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요.\n",
        "그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요.\n",
        "소녀의 아버지는 홀로 남은 소녀가 걱정되었어요.\n",
        "그래서 얼마 지나서 새어머니를 맞이했어요.\n",
        "새어머니는 소녀보다 나이가 위인 두명의 딸을 데리고 왔어요.\n",
        "그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요.\n",
        "새어머니는 소녀가 자기 딸들보다 예쁘고 착한게 못마땅했어요.\n",
        "그런데 이번에는 아버지마저 돌아가셨어요.\n",
        "소녀는 쓸고, 닦고, 하녀처럼 하루 종일 집안일을 도맡아 했어요.\n",
        "집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시 쉬곤 했어요.\n",
        "그러던 어느날, 왕궁에서 무도회가 열렸어요.\n",
        "신데렐라의 집에도 무도회 초대장이 왔어요.\n",
        "새어머니는 언니들을 데리고 무도회장으로 떠났어요.\n",
        "신데렐라도 무도회에 가고 싶었어요.\n",
        "혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요.\n",
        "그때 어디선가 마법사 할머니가 나타났어요.\n",
        "신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요.\n",
        "할머니는 소녀를 무도회에 보내줄테니 호박 한개와 생쥐 두마리, 도마뱀을 가지고 오라 했어요.\n",
        "마법사 할머니가 이것들을 보면서 주문을 외웠어요.\n",
        "그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금마차로 변했어요.\n",
        "이번에는 생쥐와 도마뱀을 건드렸어요.\n",
        "그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했어요.\n",
        "신데렐라의 낡은 옷은 구슬 장식이 반짝이는 예쁜 드레스로 바뀌었어요.\n",
        "할머니는 신데렐라에게 반짝반짝 빛나는 유리구두를 신겨 주었어요.\n",
        "그리고 밤 열두시가 되면 모든게 처음대로 돌아간다고 알려주었어요.\n",
        "황금마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼어요. \n",
        "그러니까 반드시 밤 열두시가 되기 전에 돌아와야 해요.\n",
        "신데렐라는 황금마차를 타고 왕궁 무도회장으로 가서 멋진 왕자님을 만났어요.\n",
        "왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요.\n",
        "왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고 신데렐라하고만 춤을 추었어요.\n",
        "신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요.\n",
        "어느덧 시간이 흘러 열두시가 되었어요. \n",
        "벽시계의 열두시를 알리는 종소리에 신데렐라는 화들짝 놀랐어요.\n",
        "신데렐라가 허둥지둥 왕궁을 빠져나가는데, 유리구두 한짝이 벗겨졌어요.\n",
        "하지만 구두를 주울 시간이 없었어요.\n",
        "신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리구두 한짝을 주웠어요.\n",
        "왕자님은 유리구두를 가지고 임금님께 가서 말했어요.\n",
        "이 유리구두의 주인과 결혼하겠어요.\n",
        "그래서 신하들은 유리구두의 주인을 찾아 온 나라를 돌아다녔어요.\n",
        "드디어 신데렐라의 집에까지 신하들이 도착했어요.\n",
        "언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리구두는 너무 작았어요.\n",
        "그때 신데렐라가 조용히 다가와 자기도 한번 신어보게 해달라고 부탁했어요.\n",
        "신데렐라는 신하에게서 받은 유리구두를 신었어요.\n",
        "유리구두는 신데렐라의 발에 꼭 맞았어요.\n",
        "신하들은 신데렐라를 왕궁으로 데리고 갔어요.\n",
        "그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7mVuIMzsEdN"
      },
      "source": [
        "txt = \"\"\"\n",
        "옛날 어느 집에 귀여운 여자 아기가 태어났어요.\n",
        "아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요.\n",
        "\"\"\"\n",
        "sam_wgan4(txt.strip(),display= True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKZ6xV0W7cEw"
      },
      "source": [
        "# EncoderDecoderModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1LCjLMi7bAh"
      },
      "source": [
        "pre_trained_kobert_model_name='kykim/bert-kor-base'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_Ue51tr7rwm"
      },
      "source": [
        "from transformers import EncoderDecoderModel, BertTokenizerFast\n",
        "import torch\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained(pre_trained_kobert_model_name)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERrGJOdd_zSM",
        "outputId": "41d8ee8f-0e16-4e7b-a4c8-19864fc35709"
      },
      "source": [
        "op = tokenizer('옛날 어느 집에 귀여운 여자 아기가 태어났어요.[SEP]아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.', return_tensors=\"pt\",padding=\"max_length\", truncation=True, max_length=64)\n",
        "print(op)\n",
        "print(\"Tokens (str)      : {}\".format([tokenizer.convert_ids_to_tokens(s) for s in op['input_ids'].tolist()[0]]))\n",
        "print(\"Tokens (int)      : {}\".format(op['input_ids'].tolist()[0]))\n",
        "print(\"Tokens (attn_mask): {}\\n\".format(op['attention_mask'].tolist()[0]))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[    2, 17463, 14385, 14662, 15886, 14891, 17818, 33791, 13972,  2016,\n",
            "             3, 35244,  4215,  8669,  8035,  8669, 19206,  8044, 17364, 14125,\n",
            "          8472, 26268, 18857,  8048, 17292,  2016,     3,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
            "Tokens (str)      : ['[CLS]', '옛날', '어느', '집에', '귀여운', '여자', '아기가', '태어났', '##어요', '.', '[SEP]', '아기는', '무', '##럭', '##무', '##럭', '자라', '##서', '예쁘고', '마음', '##씨', '고운', '소녀', '##가', '되었어요', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
            "Tokens (int)      : [2, 17463, 14385, 14662, 15886, 14891, 17818, 33791, 13972, 2016, 3, 35244, 4215, 8669, 8035, 8669, 19206, 8044, 17364, 14125, 8472, 26268, 18857, 8048, 17292, 2016, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Tokens (attn_mask): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ohxxzo-Mcvoz"
      },
      "source": [
        "# EncoderDecoder 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cL0pg5TF-gVC",
        "outputId": "6b6f6fcb-03e6-4779-91ca-bebc6da36a6f"
      },
      "source": [
        "from transformers import EncoderDecoderModel, BertTokenizerFast\n",
        "\n",
        "try:\n",
        "    del model\n",
        "    print('delete model')\n",
        "except Exception as ex:\n",
        "    pass\n",
        "\n",
        "use_pretrained_model = False\n",
        "\n",
        "if use_pretrained_model:\n",
        "    model = EncoderDecoderModel.from_pretrained(\"/content/drive/MyDrive/GAN_ENDE/model\")\n",
        "    print('Load from pre-trained model...')\n",
        "else:\n",
        "    model = EncoderDecoderModel.from_encoder_decoder_pretrained(pre_trained_kobert_model_name, pre_trained_kobert_model_name) # initialize Bert2Bert from pre-trained checkpoints\n",
        "    print('Initialize with ',pre_trained_kobert_model_name)\n",
        "\n",
        "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
        "model.config.eos_token_id = tokenizer.sep_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "model.config.vocab_size = model.config.encoder.vocab_size\n",
        "\n",
        "model.config.max_length = 142\n",
        "model.config.min_length = 56\n",
        "model.config.no_repeat_ngram_size = 3\n",
        "model.config.early_stopping = True\n",
        "model.config.length_penalty = 2.0\n",
        "model.config.num_beams = 4\n",
        "\n",
        "N_EPOCHS = 100"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at kykim/bert-kor-base were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at kykim/bert-kor-base were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertLMHeadModel were not initialized from the model checkpoint at kykim/bert-kor-base and are newly initialized: ['bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Initialize with  kykim/bert-kor-base\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PU0yl7rY_Fut"
      },
      "source": [
        "from transformers import BertTokenizer, AutoTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
        "\n",
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "        \n",
        "criterion = nn.CrossEntropyLoss(ignore_index = tokenizer.pad_token_id)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xUyXy2FE6X4"
      },
      "source": [
        "def train2(model, iterator, optimizer, criterion, scheduler,clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    optimizer.zero_grad()\n",
        "    for i, batch in enumerate(iterator):\n",
        "\n",
        "        input_ids = torch.tensor(batch[\"input_ids\"]).to(device)\n",
        "        attention_mask = torch.tensor(batch[\"attention_mask\"]).to(device)\n",
        "        decoder_input_ids= torch.tensor(batch[\"decoder_input_ids\"]).to(device)\n",
        "        decoder_attention_mask= torch.tensor(batch[\"decoder_attention_mask\"]).to(device)\n",
        "        labels= torch.tensor(batch[\"labels\"]).to(device)\n",
        "\n",
        "            \n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask,\n",
        "                        decoder_input_ids=decoder_input_ids, \n",
        "                        decoder_attention_mask=decoder_attention_mask,\n",
        "                        labels=labels)\n",
        "\n",
        "        loss = outputs.loss #* b_rewards\n",
        "        loss.backward()\n",
        "        ##print('loss',loss)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        ##print('epoch_loss',epoch_loss)\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhNcMCKA_Tel"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZAmw1Lf_VtA"
      },
      "source": [
        "def generate_summary(text):\n",
        "    # cut off at BERT max length 512\n",
        "    sens = nltk.sent_tokenize(text)\n",
        "    assert(len(sens) == 2)\n",
        "    inputs = tokenizer(sens[0].strip()+'[SEP]'+sens[1].strip(), padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "    input_ids = inputs.input_ids.to(device)\n",
        "    attention_mask = inputs.attention_mask.to(device)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    outputs = model.generate(input_ids, attention_mask=attention_mask).cpu().detach().numpy()[0]\n",
        "    o=[]\n",
        "    for token in outputs:\n",
        "        if token == tokenizer.pad_token_id:\n",
        "            break\n",
        "        o.append(token)\n",
        "    output_str = tokenizer.batch_decode([o], skip_special_tokens=True)[0]\n",
        "\n",
        "    return output_str"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tE9xBH9qWO9y"
      },
      "source": [
        "documents = []\n",
        "#documents.append(full_text)\n",
        "for i in range(len(d2)):\n",
        "    documents.append(' '.join(d2[i]['article_original']))\n",
        "\n",
        "for i in range(len(d1)):\n",
        "    documents.append(' '.join(d1[i]['article_original']))"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "OcXLb1WXtnx_",
        "outputId": "3d1c9f9d-1353-4654-b063-ab5ce8b4232e"
      },
      "source": [
        "documents[1]"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'서산시의회(의장 임재관) 가충순·이수의 의원이 (사)한국지역신문협회에서 수여하는 우수의정대상을 받았다. 가충순 의원과 이수의 의원은 16일 팔봉면 폰타나 리조트에서 열린 한국지역신문협회 하계 워크샵에서 지역사회 발전을 위해 활발한 의정활동을 펼친 공로를 인정받아 우수의정대상을 수상했다. 지난해 6월 제7회 전국동시지방선거를 통해 등원한 두 의원은 산업건설위원회에서 열정적인 의정활동을 펼치고 있다. 가충순 의원은 5분발언, 행정사무감사, 시정질문을 통해 자동차 연비테스트 연구시설 유치, 천수만 염해피해 재발 방지, 서산시 대표 농산물 육성 등 지역의 크고 작은 문제를 개선하기 위해 노력하고 있다. 이수의 의원은 지난 행정사무감사에서 대산공단 기업 임원을 참고인으로 출석시켜 지역인재채용 및 관내업체·자재 활용을 확대할 것을 제안하며 기존 행정사무감사의 틀을 깨는 등 다양한 의정활동을 펼쳐나가고 있다. 가충순 의원은 \"시의원이라면 마땅히 해야할 일을 한 것 뿐인데 상까지 주시니 몸 둘 바를 모르겠다\"며 \"항상 초심을 잊지 않고 지역 발전을 위해 최선을 다하겠다\"고 소감을 밝혔다. 이수의 의원은 \"믿고 뽑아주신 주민들을 위해 당연히 해야할 일을 했을 뿐인데 상까지 받게 돼 영광\"이라며 \"시민들이 자부심을 느낄 수 있는 지역사회를 만들어 나가기 위해 앞으로도 최선을 다 하겠다\"고 말했다.'"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5R-TIbRWRJ9"
      },
      "source": [
        "\n",
        "\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "\n",
        "use_pretrained_model = False\n",
        "\n",
        "prepare_dataset_config = {}\n",
        "dataset_iterator = []\n",
        "\n",
        "if use_pretrained_model:\n",
        "    # Opening JSON file\n",
        "    with open('/content/drive/MyDrive/GAN_ENDE/model/prepare_dataset_config.json','r') as f:\n",
        "        # returns JSON object as \n",
        "        # a dictionary\n",
        "        prepare_dataset_config = json.load(f)\n",
        "\n",
        "    with open(\"/content/drive/MyDrive/GAN_ENDE/data.bin\", \"rb\") as fp:   # Unpickling\n",
        "        dataset_iterator = pickle.load(fp)\n",
        "\n",
        "else:\n",
        "    prepare_dataset_config['curren_doc_num'] = 0\n",
        "    prepare_dataset_config['total_doc_num'] = len(documents)\n",
        "    prepare_dataset_config['preparation_time'] = 0\n",
        "\n",
        "    with open('/content/drive/MyDrive/GAN_ENDE/model/prepare_dataset_config.json', 'w') as fp:\n",
        "        json.dump(prepare_dataset_config, fp)\n",
        "\n",
        "    with open(\"/content/drive/MyDrive/GAN_ENDE/data.bin\", \"wb\") as fp:   #Pickling\n",
        "        pickle.dump(dataset_iterator, fp)\n"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMIeT4SKYRd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4428fe30-388f-4d48-d4b1-bf8450b20e48"
      },
      "source": [
        "prepare_dataset_config"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'curren_doc_num': 0, 'preparation_time': 0, 'total_doc_num': 313962}"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upTk6zI2gQiB",
        "outputId": "c0851231-dddf-44d7-da1f-967e00b1c8b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(dataset_iterator)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abW0juJN7zmE"
      },
      "source": [
        "## dataset 만들기...???"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RRUZz027yPs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "75e650a3-117a-444e-d5ed-c52a9e9c3d44"
      },
      "source": [
        "\n",
        "\n",
        "for cnt in range(prepare_dataset_config['curren_doc_num'],len(documents)):\n",
        "    doc_start_time = time.time()\n",
        "\n",
        "    doc = documents[cnt]\n",
        "    org_sentences = np.array(nltk.sent_tokenize(doc.strip()))\n",
        "    first_summary = org_sentences[0] + ' ' + org_sentences[1]\n",
        "    for i in range(0,len(org_sentences),2):\n",
        "        txt = org_sentences[i]\n",
        "        if i < len(org_sentences)-1:\n",
        "            txt += org_sentences[i+1]\n",
        "\n",
        "        try:\n",
        "            summary, max_score = sam_wgan4(txt,epochs=50,batch_size=batch_size)\n",
        "            print('max_score:{} compress_rate {:4f} text [{}]'.format(max_score,(len(summary)/len(txt)),summary))\n",
        "            dataset_iterator.append((txt,summary))\n",
        "        except Exception as ex:\n",
        "            print(ex)\n",
        "\n",
        "    with open(\"/content/drive/MyDrive/GAN_ENDE/data.bin\", \"wb\") as fp:   #Pickling\n",
        "        pickle.dump(dataset_iterator, fp)\n",
        "    doc_end_time = time.time()\n",
        "    sum_mins, sum_secs = epoch_time(doc_start_time, doc_end_time)\n",
        "    print('')\n",
        "    print(f'Document:{cnt+1} Summarizing time: {sum_mins}m {sum_secs}s')\n",
        "    print('')\n",
        "    prepare_dataset_config['curren_doc_num'] = cnt+1\n",
        "    prepare_dataset_config['preparation_time'] += doc_end_time - doc_start_time\n",
        "    with open('/content/drive/MyDrive/GAN_ENDE/model/experiment_config.json', 'w') as fp:\n",
        "        json.dump(experiment_config, fp)\n",
        "    print(prepare_dataset_config)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "지난해 고령화와 유례가 드문 겨울 한파 등 영향으로 우리나라 사망자 수가 통계 작성 이후 가장 많았다. 폐렴과 치매의 일종인 알츠하이머병은 지난해 사망원인 순위 3위와 9위로 전년보다 각각 한 단계 두 단계 상승하는 등 노인성 질병에 의한 사망률이 급증하는 추세다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs,grammar loss:0.0081  similarity loss:-0.0064 length loss:-0.0039\n",
            "max_score:5.520869007727443 compress_rate 0.668919 text [유례가 겨울 영향으로 우리나라 사망자 수가 통계 작성 이후 가장 많았고 폐렴과 치매의 순위 3위와 9위로 전년보다 각각 한 단계 두 단계 상승하는 등 노인성 질병에 의한 추세다.]\n",
            "--------------------------------------------------\n",
            "‘연령표준화 사망률’(표준인구 10만 명당 사망자 수)은 울산·충북·부산 순으로 높게 나타났다. ■작년 사망자 29만 8820명 역대 최다 24일 통계청이 발표한 2018년 사망원인통계를 보면 지난해 총 사망자 수는 전년 대비 4. 7%(1만 3286명) 증가한 29만 8820명으로 관련 통계를 작성한 1983년 이후 가장 많았으며 5년 연속 증가세를 보였다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs,grammar loss:-0.0378  similarity loss:-0.0313 length loss:-0.0537\n",
            "max_score:5.316455834011087 compress_rate 0.671569 text [‘연령표준화 10만 명당 울산·충북·부산 순으로 ■작년 사망자 29만 8820명 24일 통계청이 발표한 2018년 사망원인통계를 총 사망자 수는 전년 대비 7%(1만 3286명) 증가한 29만 8820명으로 1983년 이후 많았으며 연속 보였다.]\n",
            "--------------------------------------------------\n",
            "통계청은 인구 구조의 고령화와 지난해 1~2월 유례가 드문 한파 등을 그 원인으로 꼽았다. 지난해 조사망률(인구 10만 명당 사망자 수) 역시 582. 5명으로 전년보다 4. 5%(25. 1명) 증가해 5년 연속 늘었다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs,grammar loss:-0.0198  similarity loss:-0.0125 length loss:-0.0153\n",
            "max_score:5.623673202493029 compress_rate 0.487395 text [통계청은 인구 드문 한파 등을 원인으로 꼽았고 지난해 조사망률(인구 역시 582. 5명으로 연속 늘었다.]\n",
            "--------------------------------------------------\n",
            "특히 80세 이상의 사망자가 전체 사망자의 절반에 가까운 46. 3%로 10년 전보다 14. 3%포인트(P)나 증가했다. ■폐렴·알츠하이머병 사망률 순위 ‘껑충’ 사망원인별로 보면 지난해 암(악성신생물)에 의한 사망률(이하 인구 10만 명당 사망자 수)은 154. 3명으로 전년보다 0. 2% 증가했다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs,grammar loss:-0.0924  similarity loss:-0.0483 length loss:-0.0000\n",
            "max_score:4.986040283453548 compress_rate 0.624242 text [특히 이상의 사망자가 전체 사망자의 절반에 3%로 10년 전보다 14. 3%포인트(P)나 증가했고 ■폐렴·알츠하이머병 보면 지난해 암(악성신생물)에 의한 명당 사망자 전년보다 증가했다.]\n",
            "--------------------------------------------------\n",
            "1983년 관련 통계를 집계한 이래 줄곧 암이 사망원인 1위로 집계됐다. 특히 폐렴(4위→3위)과 치매의 일종인 알츠하이머병(11위→9위)에 의한 사망률 순위 상승이 두드러졌다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs,grammar loss:-0.0865  similarity loss:-0.0612 length loss:-0.0353\n",
            "max_score:5.230640326512724 compress_rate 0.673469 text [1983년 집계한 이래 1위로 집계됐고 폐렴(4위→3위)과 치매의 알츠하이머병(11위→9위)에 의한 상승이 두드러졌다.]\n",
            "--------------------------------------------------\n",
            "폐렴 사망률은 2004년 10위에서 꾸준히 순위가 상승하고 있고 알츠하이머병 사망률 역시 통계 작성 이래 10대 사인에 처음 포함됐다. 지난해 알츠하이머병에 의한 사망률은 12. 0명으로 전년(9. 8명) 대비 22. 5% 증가했다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs,grammar loss:0.0061  similarity loss:-0.0037 length loss:-0.0031\n",
            "max_score:4.675388536291152 compress_rate 0.472441 text [사망률은 2004년 꾸준히 순위가 있고 사망률 역시 작성 10대 사망률은 전년(9. 8명) 대비 22. 5%]\n",
            "--------------------------------------------------\n",
            "알츠하이머병 사망률은 10년 전(3. 8명)과 비교하면 무려 214. 2% 증가했다. 폐렴 사망률은 45. 4명으로 전년(37. 8명) 대비 20. 0% 증가했다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs,grammar loss:-0.0841  similarity loss:-0.0375 length loss:-0.0487\n",
            "max_score:4.933221893693687 compress_rate 0.517647 text [사망률은 10년 8명)과 무려 2% 폐렴 사망률은 45. 8명) 대비 증가했다.]\n",
            "--------------------------------------------------\n",
            "알코올 관련 사망률은 9. 6명으로 전년보다 2. 0% 늘었다. ■자살률 5년 만에 증가…베르테르 효과 영향 지난해 자살에 의한 사망자는 1만 3670명으로 전년보다 9. 7%(1207명) 증가했다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs,grammar loss:-0.0456  similarity loss:-0.0312 length loss:-0.0123\n",
            "max_score:5.3895330943699165 compress_rate 0.596330 text [알코올 사망률은 9. 6명으로 늘었고 ■자살률 효과 의한 사망자는 1만 3670명으로 전년보다 9. 7%(1207명)]\n",
            "--------------------------------------------------\n",
            "자살률은 26. 6명으로 전년보다 2. 3명(9. 5%) 증가했다. 자살률은 2013년 28. 5명 2014년 27. 3명 2015년 26. 5명 2016년 25. 6명 2017년 24. 3명 등 4년 연속 줄어들다가 5년 만에 증가세로 돌아섰다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs,grammar loss:0.0111  similarity loss:-0.0073 length loss:0.0371\n",
            "max_score:5.375598049816805 compress_rate 0.563910 text [자살률은 6명으로 3명(9. 5%) 2013년 5명 2014년 3명 26. 2016년 3명 등 4년 연속 5년 만에 증가세로 돌아섰다.]\n",
            "--------------------------------------------------\n",
            "자살은 10∼30대까지 사망원인 순위 1위를 차지했고 40∼50대에서도 2위를 기록했다. 김진 통계청 인구동향과장은 자살에는 베르테르 효과 즉 유명인 자살이 영향을 준다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs,grammar loss:-0.0050  similarity loss:-0.0103 length loss:-0.0264\n",
            "max_score:5.64656361108401 compress_rate 0.515464 text [자살은 10∼30대까지 사망원인 순위 2위를 기록했고 통계청 자살에는 유명인 자살이 준다.]\n",
            "--------------------------------------------------\n",
            "2011년 이후 유명인 자살이 줄면서 자살이 줄었는데 지난해에는 유명인 자살이 있어 영향을 줬다고 설명했다. 지역 간 연령구조 차이를 표준화한 사망률(표준인구 10만 명당 사망자 수)을 보면 울산(355. 3명) 충북(352. 6명) 부산(350. 8명)이 높았고 서울(283. 3명)과 경기(306. 8명)가 낮았다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs,grammar loss:-0.0663  similarity loss:-0.0550 length loss:-0.0241\n",
            "max_score:5.633600057288012 compress_rate 0.596591 text [유명인 자살이 유명인 영향을 줬다고 설명했고 지역 연령구조 사망률(표준인구 울산(355. 3명) 충북(352. 6명) 부산(350. 8명)이 높았고 서울(283. 3명)과 8명)가 낮았다.]\n",
            "--------------------------------------------------\n",
            "사인별로 연령표준화 사망률이 높은 지역을 보면 암은 경남(101. 5명) 심장 질환은 경남(44. 6명) 뇌혈관 질환은 울산(30. 6명) 폐렴은 경북(30. 3명) 운수사고는 전남(14. 4명) 고의적 자해(자살)는 충남(29. 8명)이었다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs,grammar loss:-0.0392  similarity loss:-0.0305 length loss:-0.0317\n",
            "max score: 3.2003767995128363\n",
            "--------------------------------------------------\n",
            "사인별로 연령표준화 사망률이 높은 지역을 보면 암은 경남(101. 5명) 심장 질환은 경남(44. 6명) 뇌혈관 질환은 울산(30. 6명) 폐렴은 경북(30. 3명) 운수사고는 전남(14. 4명) 고의적 자해(자살)는 충남(29. 8명)이었다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   60/60 epochs,grammar loss:-0.0890  similarity loss:-0.0440 length loss:-0.0679\n",
            "max score: 4.049941919927533\n",
            "--------------------------------------------------\n",
            "사인별로 연령표준화 사망률이 높은 지역을 보면 암은 경남(101. 5명) 심장 질환은 경남(44. 6명) 뇌혈관 질환은 울산(30. 6명) 폐렴은 경북(30. 3명) 운수사고는 전남(14. 4명) 고의적 자해(자살)는 충남(29. 8명)이었다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   70/70 epochs,grammar loss:-0.1827  similarity loss:-0.1270 length loss:-0.0290\n",
            "max_score:4.531161909317729 compress_rate 0.548148 text [연령표준화 높은 암은 심장 뇌혈관 질환은 울산(30. 6명) 경북(30. 전남(14. 고의적 자해(자살)는 충남(29. 8명)이었다.]\n",
            "Summarizing time: 0m 9s\n",
            "{'curren_doc_num': 8, 'total_doc_num': 313962, 'learing_time': 7061.694827795029}\n",
            "--------------------------------------------------\n",
            "서산시의회(의장 임재관) 가충순·이수의 의원이 (사)한국지역신문협회에서 수여하는 우수의정대상을 받았다. 가충순 의원과 이수의 의원은 16일 팔봉면 폰타나 리조트에서 열린 한국지역신문협회 하계 워크샵에서 지역사회 발전을 위해 활발한 의정활동을 펼친 공로를 인정받아 우수의정대상을 수상했다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs,grammar loss:-0.0555  similarity loss:-0.0459 length loss:-0.0444\n",
            "max_score:5.453739220768437 compress_rate 0.584906 text [서산시의회(의장 임재관) (사)한국지역신문협회에서 우수의정대상을 받았고 가충순 의원과 이수의 의원은 16일 리조트에서 열린 발전을 활발한 의정활동을 인정받아 수상했다.]\n",
            "--------------------------------------------------\n",
            "지난해 6월 제7회 전국동시지방선거를 통해 등원한 두 의원은 산업건설위원회에서 열정적인 의정활동을 펼치고 있다. 가충순 의원은 5분발언 행정사무감사 시정질문을 통해 자동차 연비테스트 연구시설 유치 천수만 염해피해 재발 방지 서산시 대표 농산물 육성 등 지역의 크고 작은 문제를 개선하기 위해 노력하고 있다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs,grammar loss:-0.0084  similarity loss:-0.0133 length loss:-0.0004\n",
            "max_score:5.378417037495974 compress_rate 0.494253 text [6월 제7회 전국동시지방선거를 등원한 의정활동을 행정사무감사 통해 연비테스트 염해피해 방지 육성 등 지역의 크고 작은 문제를 개선하기 위해 노력하고 있다.]\n",
            "--------------------------------------------------\n",
            "이수의 의원은 지난 행정사무감사에서 대산공단 기업 임원을 참고인으로 출석시켜 지역인재채용 및 관내업체·자재 활용을 확대할 것을 제안하며 기존 행정사무감사의 틀을 깨는 등 다양한 의정활동을 펼쳐나가고 있다. 가충순 의원은 시의원이라면 마땅히 해야할 일을 한 것 뿐인데 상까지 주시니 몸 둘 바를 모르겠다며 항상 초심을 잊지 않고 지역 발전을 위해 최선을 다하겠다고 소감을 밝혔다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs,grammar loss:-0.0010  similarity loss:-0.0065 length loss:0.0011\n",
            "max_score:5.511067383306073 compress_rate 0.556075 text [이수의 의원은 지난 대산공단 기업 임원을 출석시켜 지역인재채용 및 활용을 제안하며 의정활동을 펼쳐나가고 가충순 의원은 시의원이라면 마땅히 한 뿐인데 상까지 몸 모르겠다며 초심을 잊지 않고 다하겠다고 소감을 밝혔다.]\n",
            "--------------------------------------------------\n",
            "이수의 의원은 믿고 뽑아주신 주민들을 위해 당연히 해야할 일을 했을 뿐인데 상까지 받게 돼 영광이라며 시민들이 자부심을 느낄 수 있는 지역사회를 만들어 나가기 위해 앞으로도 최선을 다 하겠다고 말했다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   50/50 epochs,grammar loss:0.0078  similarity loss:-0.0068 length loss:0.0347\n",
            "max score: 1.8652376782157878\n",
            "--------------------------------------------------\n",
            "이수의 의원은 믿고 뽑아주신 주민들을 위해 당연히 해야할 일을 했을 뿐인데 상까지 받게 돼 영광이라며 시민들이 자부심을 느낄 수 있는 지역사회를 만들어 나가기 위해 앞으로도 최선을 다 하겠다고 말했다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   60/60 epochs,grammar loss:0.0109  similarity loss:-0.0102 length loss:-0.0107\n",
            "max_score:5.374678544396877 compress_rate 0.577586 text [이수의 의원은 믿고 뽑아주신 주민들을 위해 일을 했을 뿐인데 상까지 영광이라며 수 있는 나가기 위해 다 하겠다고 말했다.]\n",
            "Summarizing time: 0m 9s\n",
            "{'curren_doc_num': 8, 'total_doc_num': 313962, 'learing_time': 7061.694827795029}\n",
            "--------------------------------------------------\n",
            "지난 2004년 시작해 조선대 학생들의 대표적인 행사로 자리매김한 ‘조선대 국토대장정’이 지난 5일 해단식을 끝으로 일정을 마감했다. 조선대는 지난 5일 오후 서석홀 4층 대호전기홀에서 ‘2019학년도 조선대학교 제16기 국토대장정’ 해단식을 열었다.\n",
            "--------------------------------------------------\n",
            "Train... ||||||||||||||.......| 66.0%   33/50 epochs,grammar loss:-0.0313  similarity loss:-0.0248 length loss:-0.0230"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-04829e31230f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msam_wgan4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'max_score:{} compress_rate {:4f} text [{}]'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mdataset_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-7150e82f9278>\u001b[0m in \u001b[0;36msam_wgan4\u001b[0;34m(text, epochs, batch_size, display, retry, retry_count)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msummarizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSAM_Summarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_discriminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms_discriminator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0msummarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#print(samples)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-57-35364b48056d>\u001b[0m in \u001b[0;36msummarize\u001b[0;34m(self, epochs, batch_size, learning_rate, display)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-57-35364b48056d>\u001b[0m in \u001b[0;36m__train\u001b[0;34m(self, epochs, batch_size, learning_rate, display)\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0msw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msw0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m                     \u001b[0mfake_gmr_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_sim_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_len_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__discrete_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m#print(fake_len_out)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-57-35364b48056d>\u001b[0m in \u001b[0;36m__discrete_gradient\u001b[0;34m(self, weights, use_gpu, verbose)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mo_len_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfake_text\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfake_outs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mo_sim_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_discriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg_text_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg_source_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-fe99af6fcd77>\u001b[0m in \u001b[0;36msimilarity\u001b[0;34m(self, query_text, org_text_emb)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morg_text_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mqueries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mquery_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embedder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;31m#query_embeddings = self._embedder.encode(queries,show_progress_bar=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m#print(queries)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0msentences_sorted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlength_sorted_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mstart_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Batches\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0msentences_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences_sorted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wYZtPeMY_aFW",
        "outputId": "86eae936-655e-4dce-a37a-6f4d093a6f63"
      },
      "source": [
        "\n",
        "encoder_max_length = 64\n",
        "decoder_max_length = 64\n",
        "batch_size = 32 #4 # 64, 128\n",
        "\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "model.to(device)\n",
        "\n",
        "for cnt in range(experiment_config['curren_doc_num'],len(documents)):\n",
        "    doc_start_time = time.time()\n",
        "    doc = documents[cnt]\n",
        "    dataset_iterator = []\n",
        "    org_sentences = np.array(nltk.sent_tokenize(doc.strip()))\n",
        "    first_summary = org_sentences[0] + ' ' + org_sentences[1]\n",
        "    for i in range(0,len(org_sentences),2):\n",
        "        txt = org_sentences[i]\n",
        "        txt2 = txt\n",
        "        if i < len(org_sentences)-1:\n",
        "            txt +=  '[SEP]' + org_sentences[i+1]\n",
        "            txt2 += org_sentences[i+1]\n",
        "\n",
        "        batch = {}\n",
        "        batch['input_ids'] = []\n",
        "        batch['attention_mask'] = []\n",
        "        batch['decoder_input_ids'] = []\n",
        "        batch['decoder_attention_mask'] = []\n",
        "        batch['labels'] = []\n",
        "        try:\n",
        "            samples, max_score = sam_wgan4(txt2,epochs=100,batch_size=batch_size)\n",
        "            print('  max_score:',max_score)\n",
        "            #for (text,g,s,l) in samples:\n",
        "            for text in samples:\n",
        "                inputs = g_discriminator.tokenizer(txt, padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n",
        "                outputs = g_discriminator.tokenizer(text, padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n",
        "                batch[\"input_ids\"].append(inputs.input_ids)\n",
        "                batch[\"attention_mask\"].append(inputs.attention_mask)\n",
        "                batch[\"decoder_input_ids\"].append(outputs.input_ids)\n",
        "                batch[\"decoder_attention_mask\"].append(outputs.attention_mask)\n",
        "                batch[\"labels\"].append(outputs.input_ids.copy())\n",
        "            dataset_iterator.append(batch)\n",
        "        except Exception as ex:\n",
        "            print(ex)\n",
        "\n",
        "    # Total number of training steps is [number of batches] x [number of epochs]. \n",
        "    # (Note that this is not the same as the number of training samples).\n",
        "    total_steps = len(dataset_iterator) * N_EPOCHS\n",
        "\n",
        "    # Create the learning rate scheduler.\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                                num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                                num_training_steps = total_steps)\n",
        "    \n",
        "    for epoch in range(N_EPOCHS):\n",
        "        \n",
        "        start_time = time.time()\n",
        "        train_loss = train2(model, dataset_iterator, optimizer, criterion, scheduler,CLIP)\n",
        "        end_time = time.time()\n",
        "        \n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "        \n",
        "        #if valid_loss < best_valid_loss:\n",
        "        #    best_valid_loss = valid_loss\n",
        "        #    torch.save(model.state_dict(), 'tut6-model.pt')\n",
        "        \n",
        "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "        #print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "        print('\\t---------------------------------------------')\n",
        "        print('\\tOrg text:',first_summary)\n",
        "        sum_txt = generate_summary(first_summary)\n",
        "        print('\\tSummarized text:',sum_txt)\n",
        "        print('\\tCompression rate:{:4f}'.format(len(sum_txt)/len(first_summary)))\n",
        "        print('\\t---------------------------------------------')\n",
        "        print('\\tTarget text:',generate_summary('옛날 어느 집에 귀여운 여자 아기가 태어났어요.[SEP]아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.'))\n",
        "\n",
        "    doc_end_time = time.time()\n",
        "    model.save_pretrained(\"/content/drive/MyDrive/GAN_ENDE/model\")\n",
        "    experiment_config['curren_doc_num'] = cnt+1\n",
        "    experiment_config['learing_time'] += doc_end_time - doc_start_time\n",
        "    with open('/content/drive/MyDrive/GAN_ENDE/model/experiment_config.json', 'w') as fp:\n",
        "        json.dump(experiment_config, fp)\n",
        "    print(experiment_config)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   100/100 epochs,grammar loss:0.0063  similarity loss:-0.0089 length loss:-0.0273\n",
            "max score: 2.2572441034977864\n",
            "--------------------------------------------------\n",
            "집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   110/110 epochs,grammar loss:0.0172  similarity loss:-0.0060 length loss:-0.0097\n",
            "max score: 0\n",
            "--------------------------------------------------\n",
            "집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   120/120 epochs,grammar loss:-0.2215  similarity loss:-0.1408 length loss:-0.1638\n",
            "max score: 2.492455797068108\n",
            "--------------------------------------------------\n",
            "집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   130/130 epochs,grammar loss:-0.0522  similarity loss:-0.0386 length loss:-0.0349\n",
            "max score: 1.9251431504900849\n",
            "--------------------------------------------------\n",
            "집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   140/140 epochs,grammar loss:0.0092  similarity loss:-0.0052 length loss:0.0154\n",
            "max score: 0\n",
            "--------------------------------------------------\n",
            "집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   150/150 epochs,grammar loss:-0.1010  similarity loss:-0.0704 length loss:-0.0048\n",
            "max score: 2.8938180997312237\n",
            "--------------------------------------------------\n",
            "집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   160/160 epochs,grammar loss:-0.0568  similarity loss:-0.0383 length loss:-0.0593\n",
            "max score: 3.1156981877124093\n",
            "--------------------------------------------------\n",
            "집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   170/170 epochs,grammar loss:-0.2676  similarity loss:-0.1785 length loss:-0.1837\n",
            "max score: 2.9709810608249034\n",
            "Can't summarize the text\n",
            "--------------------------------------------------\n",
            "우정사업본부 노사에 따르면 19일 아침 충남 당진우체국 소속 집배원 강 모씨가 자택에서 숨진 채 발견됐다. 강씨는 49세로 특별한 병력이 없었고 올해 3월 건강검진에서도 특이 소견이 없었다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   100/100 epochs,grammar loss:-0.0228  similarity loss:-0.0136 length loss:-0.0000\n",
            "  max_score: 5.244418280899595\n",
            "--------------------------------------------------\n",
            "전국우정노조는 강씨의 사인을 과로사로 추정하고 있다. 우정노조는 우정사업본부와 정부는 그동안 중노동 과로로 죽어가는 집배원을 살리기 위해서는 인력을 증원해야 한다는 우정노조의 정당한 요구를 묵살해왔다며 이번 강씨 사망은 예견된 인재이자 타살이라고 주장했다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   100/100 epochs,grammar loss:-0.2501  similarity loss:-0.1876 length loss:-0.1840\n",
            "  max_score: 5.530302785064759\n",
            "--------------------------------------------------\n",
            "이어 정부는 노사 합의사항인 집배원 인력 증원과 완전한 주 5일제가 당장 이행될 수 있도록 직접 나서야 한다고 덧붙였다. 우정노조는 집배원의 완전한 주 5일제 및 인력 증원을 위해 24일 전 조합원 쟁의행위 찬반투표 25일 쟁의행위 찬반투표 관련 기자회견 30일 전 조합원 총파업 출정식을 거쳐 다음 달 9일 전면 총파업을 할 계획이다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   100/100 epochs,grammar loss:-0.0066  similarity loss:-0.0181 length loss:-0.0029\n",
            "  max_score: 5.240928108671082\n",
            "--------------------------------------------------\n",
            "우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 불구하고 또다시 사망사고가 발생해 송구스럽다면서 우정노조와 공동으로 사망사고조사위원회를 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔다. 앞서 한국노총은 전날 열린 경제사회노동위원회(경사노위) 의제개발조정위원회에서 집배원 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회 설치를 제안했다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   100/100 epochs,grammar loss:-0.0039  similarity loss:-0.0112 length loss:-0.0247\n",
            "  max_score: 5.461374388150194\n",
            "--------------------------------------------------\n",
            "한국노총은 집배원의 노동시간이 연간 2745시간(2017년 기준)이고 장시간·중노동에 따른 만성적 질환과 사고위험 직무 스트레스 등에 노출돼 있다면서 특별위원회를 통해 집배원의 노동시간 단축 및 적정 인력 배치 등 근본적인 대책을 마련해야 한다고 강조했다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   100/100 epochs,grammar loss:-0.2236  similarity loss:-0.2113 length loss:-0.1948\n",
            "  max_score: 5.113406642566965\n",
            "Epoch: 01 | Time: 0m 5s\n",
            "\tTrain Loss: 8.573 | Train PPL: 5286.798\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 특히 17년간 학생들의 정서를 돌보면서 문제들을 해결하기 위한 마음에서 박사과정을 별도로 공부한 김동옥 원장이 진행함으로써 그동안 터득한 노하우를 위한 마음으로 진행되고 있고 수업에 참여한 자신만을 너무 좋았고 것이 우리 아이들을 건강하게 성장시키는 비결이라며 강좌 없는 혁신도시에 극동대 더 말했다.\n",
            "\tCompression rate:6.000000\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 특히 17년간 학생들의 정서를 돌보면서 문제들을 해결하기 위한 마음에서 박사과정을 별도로 공부한 김동옥 원장이 진행함으로써 그동안 터득한 노하우를 위한 마음으로 진행되고 있고 수업에 참여한 자신만을 너무 좋았고 것이 우리 아이들을 건강하게 성장시키는 비결이라며 강좌 없는 혁신도시에 극동대 더 말했다.\n",
            "Epoch: 02 | Time: 0m 5s\n",
            "\tTrain Loss: 5.261 | Train PPL: 192.625\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 특히 17년간 학생들의 정서를 돌보면서 문제들을 해결하기 위한 마음에서 박사과정을 별도로 공부한 김동옥 원장이 진행함으로써 그동안 터득한 노하우를 위한 마음으로 진행되고 있고 수업에 참여한 자신만을 너무 좋았고 것이 우리 아이들을 건강하게 성장시키는 비결이라며 강좌 없는 혁신도시에 극동대 더 말했다.\n",
            "\tCompression rate:6.000000\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 특히 17년간 학생들의 정서를 돌보면서 문제들을 해결하기 위한 마음에서 박사과정을 별도로 공부한 김동옥 원장이 진행함으로써 그동안 터득한 노하우를 위한 마음으로 진행되고 있고 수업에 참여한 자신만을 너무 좋았고 것이 우리 아이들을 건강하게 성장시키는 비결이라며 강좌 없는 혁신도시에 극동대 더 말했다.\n",
            "Epoch: 03 | Time: 0m 5s\n",
            "\tTrain Loss: 2.958 | Train PPL:  19.264\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 그는 두 아이 중 shc에서 케어를 받은 적이 다른 아이는 케어를 받았지만 그때 바흐는 않았다고 반박했고 원고 측 증인으로 다른 피해 아동이 단 두명에서 그치지 않는다고 주장했다.\n",
            "\tCompression rate:3.571429\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 그는 두 아이 중 shc에서 케어를 받은 적이 다른 아이는 케어를 받았지만 그때 바흐는 않았다고 반박했고 원고 측 증인으로 다른 피해 아동이 단 두명에서 그치지 않는다고 주장했다.\n",
            "Epoch: 04 | Time: 0m 5s\n",
            "\tTrain Loss: 1.393 | Train PPL:   4.028\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 강씨의 과로사로 추정하고 있고 우정사업본부와 정부는 그동안 죽어가는 집배원을 위해서는 인력을 증원해야 정당한 강씨 사망은 인재이자 타살이라고 주장했다.\n",
            "\tCompression rate:3.000000\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 강씨의 과로사로 추정하고 있고 우정사업본부와 정부는 그동안 죽어가는 집배원을 위해서는 인력을 증원해야 정당한 강씨 사망은 인재이자 타살이라고 주장했다.\n",
            "Epoch: 05 | Time: 0m 5s\n",
            "\tTrain Loss: 0.476 | Train PPL:   1.609\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 강씨의 과로사로 추정하고 있고 우정사업본부와 정부는 그동안 죽어가는 집배원을 위해서는 인력을 증원해야 정당한 강씨가 발견됐고 강씨는 49세로 특별한 건강검진과도 관련해 집배원 강씨 사망은 인재이자 타살이살이라고 주장했다\n",
            "\tCompression rate:4.357143\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 강씨의 과로사로 추정하고 있고 우정사업본부와 정부는 그동안 죽어가는 집배원을 위해서는 인력을 증원해야 정당한 강씨는 49세로 특별한 건강검진에서도 특이 소견이견견을 통해 집배원의 완전한 건강검진과도 관련돼 있고 우정노조는 집배원 완전한 5일제 인력이라며\n",
            "Epoch: 06 | Time: 0m 5s\n",
            "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 관련 30일 쟁의행위 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:4.928571\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 07 | Time: 0m 5s\n",
            "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:4.500000\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 08 | Time: 0m 5s\n",
            "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 강씨의 과로사로 추정하고 있고 우정사업본부와 정부는 그동안 죽어가는 집배원을 위해서는 인력을 증원해야 정당한 강씨 사망은 인재이자 타살이라고 주장했다.\n",
            "\tCompression rate:3.000000\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원들은 집배원을 위해서는 인력을 증원해야 정당한 강씨 사망은 인재이자 타살이라고 주장했다.\n",
            "Epoch: 09 | Time: 0m 5s\n",
            "\tTrain Loss: 0.030 | Train PPL:   1.030\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반반투표 25일 쟁의행위 찬반투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 강씨 사망은 인재이자 타살이라고 주장했다.\n",
            "Epoch: 10 | Time: 0m 5s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "\tCompression rate:5.142857\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 11 | Time: 0m 5s\n",
            "\tTrain Loss: 0.010 | Train PPL:   1.010\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:4.500000\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 12 | Time: 0m 5s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:4.500000\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 13 | Time: 0m 5s\n",
            "\tTrain Loss: 0.014 | Train PPL:   1.014\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반반투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 14 | Time: 0m 5s\n",
            "\tTrain Loss: 0.010 | Train PPL:   1.010\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 15 | Time: 0m 5s\n",
            "\tTrain Loss: 0.050 | Train PPL:   1.051\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반반투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 16 | Time: 0m 5s\n",
            "\tTrain Loss: 0.007 | Train PPL:   1.007\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반반투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 17 | Time: 0m 5s\n",
            "\tTrain Loss: 0.020 | Train PPL:   1.021\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "\tCompression rate:5.142857\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 18 | Time: 0m 5s\n",
            "\tTrain Loss: 0.004 | Train PPL:   1.004\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:6.857143\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 19 | Time: 0m 5s\n",
            "\tTrain Loss: 0.008 | Train PPL:   1.008\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "Epoch: 20 | Time: 0m 5s\n",
            "\tTrain Loss: 0.003 | Train PPL:   1.003\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반선거 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "Epoch: 21 | Time: 0m 5s\n",
            "\tTrain Loss: 0.004 | Train PPL:   1.005\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬 반투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 22 | Time: 0m 5s\n",
            "\tTrain Loss: 0.025 | Train PPL:   1.025\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬 반투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 23 | Time: 0m 5s\n",
            "\tTrain Loss: 0.001 | Train PPL:   1.001\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 24 | Time: 0m 5s\n",
            "\tTrain Loss: 0.001 | Train PPL:   1.001\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 25 | Time: 0m 5s\n",
            "\tTrain Loss: 0.001 | Train PPL:   1.001\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 26 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 27 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 28 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 29 | Time: 0m 5s\n",
            "\tTrain Loss: 0.001 | Train PPL:   1.001\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 30 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 31 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 32 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 33 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 34 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 35 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 36 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 37 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 38 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 39 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 40 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 41 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 42 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 43 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 44 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 45 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 46 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 47 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 48 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 49 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 50 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 51 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 52 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 53 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 54 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 55 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 56 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 57 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 58 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 59 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 60 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 61 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 62 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 63 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 64 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 65 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 66 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 67 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 68 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 69 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 70 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 71 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 72 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 73 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 74 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 75 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 76 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 77 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 78 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 79 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 80 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 81 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 82 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 83 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 84 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 85 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 86 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 87 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 88 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 89 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 90 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 91 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 92 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 93 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 94 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 95 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 96 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 97 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 98 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 99 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "Epoch: 100 | Time: 0m 5s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 집배원이 또 사망했다. 올해만 과로사 등 9번째다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:5.035714\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 우정사업본부는 안전보건 관리 추진 및 노동시간 단축노력에도 또다시 발생해 송구스럽다면서 우정노조와 구성해 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 집배원 노동조건 개선 특별위원회\n",
            "{'curren_doc_num': 7, 'total_doc_num': 313962, 'learing_time': 5762.937702894211}\n",
            "--------------------------------------------------\n",
            "채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   100/100 epochs,grammar loss:0.0076  similarity loss:-0.0059 length loss:-0.0072\n",
            "  max_score: 5.57751832826654\n",
            "--------------------------------------------------\n",
            "해당 사업에는 수원시와 성남시 등 5개 지방자치단체가 제안서를 제출했고 이 가운데 수원시 성남시 부산시가 1차 평가를 통과했다. 이번 2차 평가에서는 3개 지자체를 대상으로 PPT 발표와 현장실사를 진행 이달 말께 최종 지자체가 선정ㆍ발표될 예정이다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   100/100 epochs,grammar loss:-0.0051  similarity loss:-0.0091 length loss:-0.0245\n",
            "  max_score: 5.72639727098924\n",
            "--------------------------------------------------\n",
            "트램은 전기 배터리로 움직여 오염물질 배출이 적은 친환경 교통수단이다. 전철보다 공사비가 매우 저렴하고 공사기간이 짧다는 장점을 갖고 있다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   100/100 epochs,grammar loss:-0.1968  similarity loss:-0.1847 length loss:-0.2342\n",
            "max score: 2.6644500479648996\n",
            "--------------------------------------------------\n",
            "트램은 전기 배터리로 움직여 오염물질 배출이 적은 친환경 교통수단이다. 전철보다 공사비가 매우 저렴하고 공사기간이 짧다는 장점을 갖고 있다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   110/110 epochs,grammar loss:0.0108  similarity loss:-0.0102 length loss:0.0188\n",
            "max score: 3.966590458507329\n",
            "--------------------------------------------------\n",
            "트램은 전기 배터리로 움직여 오염물질 배출이 적은 친환경 교통수단이다. 전철보다 공사비가 매우 저렴하고 공사기간이 짧다는 장점을 갖고 있다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   120/120 epochs,grammar loss:-0.0036  similarity loss:-0.0185 length loss:0.0208\n",
            "max score: 3.006541551445169\n",
            "--------------------------------------------------\n",
            "트램은 전기 배터리로 움직여 오염물질 배출이 적은 친환경 교통수단이다. 전철보다 공사비가 매우 저렴하고 공사기간이 짧다는 장점을 갖고 있다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   130/130 epochs,grammar loss:-0.3169  similarity loss:-0.2696 length loss:-0.3132\n",
            "  max_score: 5.604220354773087\n",
            "--------------------------------------------------\n",
            "이러한 트램 유치를 놓고 수원시와 성남시가 치열한 경쟁을 벌이고 있다. 먼저 수원시는 ‘준비된 트램 도시’를 강조하며 염태영 시장을 중심으로 발 빠르게 움직이고 있다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   100/100 epochs,grammar loss:-0.0384  similarity loss:-0.0274 length loss:-0.0263\n",
            "  max_score: 5.628180901682651\n",
            "--------------------------------------------------\n",
            "수원시는 염 시장이 민선 5기 시장에 취임한 지난 2010년 7월 ‘친환경 교통수단 사업계획’을 수립한 뒤 도시철도팀을 만들고 트램 전담 직원을 배치해 9년간 트램 도입에 관한 연구만 진행하도록 했다. 수원역~장안구청의 6㎞ 노선에 트램 도입을 계획 중인 수원시는 전체 트램노선 중 장안문~kt위즈파크의 1. 5㎞ 구간을 실증노선으로 제안한 바 있다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   100/100 epochs,grammar loss:-0.0311  similarity loss:-0.0242 length loss:-0.0448\n",
            "  max_score: 5.3258240442186695\n",
            "--------------------------------------------------\n",
            "수원시는 트램 노선에 세계문화유산인 수원화성과 14개 전통시장 광교산 등이 있어 트램 이용자 확보도 원만한 점을 강점으로 꼽았다. 또 수원시는 트램 도입과 관련한 여러 토론회를 개최하고 국토교통부ㆍ경찰청의 제도개선 태스크포스에도 참가했다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   100/100 epochs,grammar loss:0.0063  similarity loss:-0.0047 length loss:-0.0041\n",
            "  max_score: 5.471105662597092\n",
            "--------------------------------------------------\n",
            "이밖에 트램 운행에 근거가 되는 도시철도법ㆍ철도안전법ㆍ도로교통법 개정안 등 ‘트램 3법’ 마련을 위해 법안 초안 구상 국회 발의 및 통과 등을 노력한 점을 내세우고 있다. 수원시 관계자는 “수원시처럼 트램 도입을 위해 오랜 기간 준비한 도시는 없다”며 “철저한 준비를 통해 반드시 트램을 유치해 사람 중심 교통체계를 완성할 것”이라고 말했다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   100/100 epochs,grammar loss:-0.0707  similarity loss:-0.0564 length loss:-0.0740\n",
            "  max_score: 5.5565080748581845\n",
            "--------------------------------------------------\n",
            "이에 질세라 성남시도 트램 유치를 위해 PPT 발표내용과 현장방문 평가위원 동선 등을 빈틈없이 준비하는 모습이다. 성남시는 판교테크노밸리 기업과 공동기술개발을 통한 기업경쟁력 강화 시민ㆍ환경단체의 전폭적인 지지 등을 강점으로 부각하고 있다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   100/100 epochs,grammar loss:-0.2557  similarity loss:-0.2022 length loss:-0.3248\n",
            "  max_score: 5.4535995003406645\n",
            "--------------------------------------------------\n",
            "또 지난 2009년 판교신도시를 조성하면서 운중로 중앙화단에 트램을 위한 공간을 8ｍ 확보한 점도 이번 평가에서 유리하게 작용할 것이라는 분석이다. 성남시 관계자는 “판교지역은 대중교통 인프라 확충이 절실히 필요하다”며 “성남시의 역량을 하나로 모아 트램 실증 노선 최종 후보에 선정되도록 최선을 다할 것”이라고 밝혔다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   100/100 epochs,grammar loss:0.0132  similarity loss:-0.0104 length loss:-0.0040\n",
            "  max_score: 5.593263385584046\n",
            "Epoch: 01 | Time: 0m 9s\n",
            "\tTrain Loss: 6.114 | Train PPL: 452.131\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 음성 극동대학교 평생교육원 ( 김동옥 원장 ) 은 지난 충북 맹동면에 있는 극동대 아이를 개설했고 사진은 김동옥 강좌 개강식에 참석한 모습. 수원시 수원시 이달 이달 수원시 수원시 수원시 추진 트램 트램 트램 수원시 수원시 염정식을 통해 단축 트램 트램노노 개설을 거쳐 다음 전면 총파\n",
            "\tCompression rate:0.771845\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 따르면 19일 충남 당진우체국 소속 집배원 강 모씨가 발견됐고 강씨는 49세로 특별한 건강검진에서도 특이 소견이 이달 1일 다른 5일제 인력 증원을 위해 24일 전 총파업 출정식을 통해 인력을 증원해야 한다고 강조했다.\n",
            "Epoch: 02 | Time: 0m 9s\n",
            "\tTrain Loss: 3.129 | Train PPL:  22.858\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 따르면 19일 충남 당진우체국 소속 집배원 강 모씨가 발견됐고 강씨는 49세로 특별한 건강검진에서도 특이 소견이 선출됐고 이번 3개 지자체를 대상으로 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 있다.\n",
            "\tCompression rate:0.645631\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 따르면 19일 충남 당진우체국 소속 집배원 강 모씨가 발견됐고 강씨는 49세로 특별한 건강검진에서도 특이 소견이 제기됐고 이번 3개 지자체를 대상으로 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 있다.\n",
            "Epoch: 03 | Time: 0m 9s\n",
            "\tTrain Loss: 1.728 | Train PPL:   5.630\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 이어 정부는 노사 합의사항인 집배원 완전한 주 5일제가 당장 이행될 수 나서야 한다고 덧붙였고 우정노조는 집배원의 완전한 5일제 인력 증원을 위해 24일 전 조합원 찬반투표 25일 쟁의행위 찬반 투표 관련 30일 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:0.684466\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 해당 사업에는 등 5개 지방자치단체가 제안서를 제출했고 이번 3개 지자체를 대상으로 진행하면서 흰 가운과 청진기를 맨 모습으로 진행하도록 한다고 밝혔고 이번 3개 지방자치단체를 위해 인력 증원을 위해 인력을 증원해야 한다고 덧붙였고 수원에 한 점 의혹 없도록 사고경위를 면밀하게 조사할 계획이라고 밝혔고 한국노총은 전날 열린 장시간 노동과 과로사 문제와 관련해 주장했다.\n",
            "Epoch: 04 | Time: 0m 9s\n",
            "\tTrain Loss: 0.853 | Train PPL:   2.348\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 수원시는 배터리로 움직여 오염물질 배출이 적은 교통수단이고 성남시 평가를 통과했고 성남시 경쟁력 도시로 거듭나기 위한 수원시의 제1라운드 경쟁이 25일 충남 당진우체국 소속 집배원 강 모씨가 발견됐고 성남시 평가를 반영했고 성남시 평가를 통해 토론했고 성남시의 2차 평가 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:0.834951\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 이에 성남시도 트램 유치를 위해 ppt 발표내용과 평가위원 등을 준비하는 모습이고 기업경쟁력 전폭적인 지지 등을 부각했고 성남시 인프라 확충이 절실히 성남시도 wpi나 부모가 제안서를 제출했고 성남시 평가를 통과했고 성남시 경쟁력 도시로 거듭나기 위한 수원시에 한 점 의혹 없도록 사고경위를 면밀하게 조사할 예정이다.\n",
            "Epoch: 05 | Time: 0m 9s\n",
            "\tTrain Loss: 0.444 | Train PPL:   1.559\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 이에 성남시도 트램 유치를 위해 ppt 발표내용과 평가위원 등을 준비하는 모습이고 기업경쟁력 전폭적인 지지 등을 부각했고 또 도입과 여러 토론회를 개최하고 제도개선 태스크포스포스포스에 전 총파업 출정식을 거쳐 다음 전면 총파\n",
            "\tCompression rate:0.606796\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 이에 성남시도 트램 유치를 위해 ppt 발표내용과 평가위원 등을 준비하는 모습이고 기업경쟁력 전폭적인 지지 등을 부각했고 또 도입이 절실히 준비된 트램 전체 트램 전체 교통체계를 완성했고 이번 3개 지자체를 대상으로 진행 계획이라고 밝혔다.\n",
            "Epoch: 06 | Time: 0m 9s\n",
            "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시 등에 주관하는 저상트램 실증노선 중 장안문 ~ kt위즈파크에 선정되도록 다할 것 이라고 밝혔다.\n",
            "\tCompression rate:0.577670\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 이밖에 개정안 3법 마련을 위해 법안 구상 및 통과 등을 내세우고 있고 수원시 관계자는 도입을 위해 기간 도시는 철저한 준비를 통해 트램을 유치해 사람 중심 교통체계를 완성했고 이번 3개 지자체를 대상으로 조사할 계획이라고 밝혔다.\n",
            "Epoch: 07 | Time: 0m 9s\n",
            "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계를 완성할 수 나서야 한다고 덧붙였고 수원역 ~ 장안문 ~ kt위즈파크의 1. 실증 후보 후보에 선정되도록 다할 것 이라고 밝혔다. 철저한 준비를 통해 트램을 트램 유치에 선정하고 있다.\n",
            "\tCompression rate:0.669903\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안문 ~ kt위즈파크가 먼저 트램 실증 후보에 선정되도록 다할 것 이라고 밝혔다.\n",
            "Epoch: 08 | Time: 0m 9s\n",
            "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성했고 이번 3개 지자체와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 23일 수원시와 성남시 등 5개 지방자치단체가 참가했다. 사람 중심 교통체계체계 완성하고 이번 3개 평가 총파업 출정식에서도 특이 소견이\n",
            "\tCompression rate:0.757282\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 이에 성남시도 트램 유치를 위해 ppt 발표내용과 평가위원 등을 준비하는 모습이고 기업경쟁력 전폭적인 지지 등을 부각했고 또 도입과 여러 토론회를 개최하고 제도개선 태스크포스에 참석한 모습. 실증노선으로 바흐는 5월 31일까지 특별한 건강검진에서도 특이 소견이\n",
            "Epoch: 09 | Time: 0m 9s\n",
            "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 수원시는 노선에 세계문화유산인 수원화성과 14개 전통시장 광교산 등이 있어 이용자 확보도 원만한 점을 꼽았고 또 도입과 여러 토론회를 개최하고 제도개선 태스크포스에도 참가 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다.\n",
            "\tCompression rate:0.766990\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 노선에 세계문화유산인 수원화성과 14개 전통시장 광교산 등이 있어 이용자 확보도 원만한 점을 꼽았고 또 도입과 여러 토론회를 개최하고 제도개선 태스크포스에도 참가했다. 준비된 트램 전체 트램노선 2차 평가가 25일 진행된다.\n",
            "Epoch: 10 | Time: 0m 9s\n",
            "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "\tCompression rate:0.577670\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "Epoch: 11 | Time: 0m 9s\n",
            "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다 수원역 ~ 장안구청의 전 총파업 출정식을 준비하는 모습이고 기업경쟁력 전폭적인 지지 등을 부각하고 있다.\n",
            "\tCompression rate:0.859223\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 또 지난 판교신도시를 조성하면서 운중로 중앙화단에 트램을 점도 작용할 것이라는 분석이고 성남시 인프라 확충이 절실히 성남시의 하나로 모아 트램 실증 후보에 선정되도록 다할 것 이라고 밝혔다. 이후\n",
            "Epoch: 12 | Time: 0m 9s\n",
            "\tTrain Loss: 0.041 | Train PPL:   1.042\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "\tCompression rate:0.577670\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "Epoch: 13 | Time: 0m 9s\n",
            "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시과 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 만족했고 또 도입과 여러 토론회를 개최하고 제도개선 태스크포스에도 참가했다.\n",
            "\tCompression rate:0.810680\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "Epoch: 14 | Time: 0m 9s\n",
            "\tTrain Loss: 0.050 | Train PPL:   1.051\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시가 성남시의 제2라운드 경쟁이 23일 수원시과 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 만족했고 또 도입과 여러 토론회를 개최하고 이번 3개 지자체를 대상으로 진행 예정이다.\n",
            "\tCompression rate:0.864078\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 이밖에 개정안 3법 마련을 위해 법안 구상 및 통과 등을 내세우고 있고 수원시 관계자는 도입을 위해 기간 도시는 철저한 준비를 통해 트램을 유치해 사람 중심 교통체계를 완성하도록 다할 것 이라고 말했다.\n",
            "Epoch: 15 | Time: 0m 9s\n",
            "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "\tCompression rate:0.577670\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 이밖에 개정안 3법 마련을 위해 법안 구상 및 통과 등을 내세우고 있고 수원시 관계자는 도입을 위해 기간 도시는 철저한 준비를 통해 트램을 유치해 사람 중심 교통체계를 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시는 성남시의 하나로 모아 트램 실증노선 2차 평가가 25일 진행된다.\n",
            "Epoch: 16 | Time: 0m 9s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 수원시는 노선에 세계문화유산인 수원화성과 14개 전통시장 광교산 등이 있어 이용자 확보도 원만한 점을 꼽았고 또 도입과 여러 토론회를 개최하고 제도개선 태스크포스에도 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다.\n",
            "\tCompression rate:0.601942\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 또 지난 판교신도시를 조성하면서 운중로 중앙화단에 트램을 점도 작용할 것이라는 분석이고 성남시 인프라 확충이 절실히 성남시의 하나로 모아 트램 실증 후보에 선정되도록 다할 것 이라고 밝혔고 수원시 관계자는 도입을 위해 기간 도시는 철저한 준비를 통해 트램을 유치해 사람 중심 교통체계 완성 이라고 밝혔다.\n",
            "Epoch: 17 | Time: 0m 9s\n",
            "\tTrain Loss: 0.041 | Train PPL:   1.042\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 수원시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 만족했고 또 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 전폭적인 지지 등을 부각하고 있다.\n",
            "\tCompression rate:0.888350\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 이밖에 개정안 3법 마련을 위해 법안 구상 및 통과 등을 내세우고 있고 수원시 관계자는 도입을 위해 기간 도시는 철저한 준비를 통해 트램을 유치해 사람 중심 교통체계를 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다.\n",
            "Epoch: 18 | Time: 0m 9s\n",
            "\tTrain Loss: 0.009 | Train PPL:   1.009\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "\tCompression rate:0.577670\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 이밖에 개정안 3법 마련을 위해 법안 구상 및 통과 등을 내세우고 있고 수원시 관계자는 도입을 위해 기간 도시는 철저한 준비를 통해 트램을 유치해 사람 중심 교통체계를 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다.\n",
            "Epoch: 19 | Time: 0m 9s\n",
            "\tTrain Loss: 0.010 | Train PPL:   1.010\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "\tCompression rate:0.577670\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "Epoch: 20 | Time: 0m 9s\n",
            "\tTrain Loss: 0.017 | Train PPL:   1.017\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "\tCompression rate:0.577670\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 또 지난 판교신도시를 조성하면서 운중로 중앙화단에 트램을 점도 작용할 것이라는 분석이고 성남시 인프라 확충이 절실히 성남시의 하나로 모아 트램 실증 후보에 선정되도록 다할 것 이라고 밝혔고 이번 3개 지자체를 대상으로 진행 예정이다.\n",
            "Epoch: 21 | Time: 0m 9s\n",
            "\tTrain Loss: 0.003 | Train PPL:   1.003\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "\tCompression rate:0.577670\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 또 지난 판교신도시를 조성하면서 운중로 중앙화단에 트램을 점도 작용할 것이라는 분석이고 성남시 인프라 확충이 절실히 성남시의 하나로 모아 트램 실증 후보에 선정되도록 다할 것 이라고 밝혔고 이번 3개 지자체를 대상으로 진행 예정이다.\n",
            "Epoch: 22 | Time: 0m 9s\n",
            "\tTrain Loss: 0.006 | Train PPL:   1.006\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "\tCompression rate:0.577670\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 또 지난 판교신도시를 조성하면서 운중로 중앙화단에 트램을 점도 작용할 것이라는 분석이고 성남시 인프라 확충이 절실히 성남시의 하나로 모아 트램 실증 후보에 선정되도록 다할 것 이라고 밝혔고 이번 3개 지자체를 대상으로 진행 예정이다.\n",
            "Epoch: 23 | Time: 0m 9s\n",
            "\tTrain Loss: 0.002 | Train PPL:   1.002\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "\tCompression rate:0.577670\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 또 지난 판교신도시를 조성하면서 운중로 중앙화단에 트램을 점도 작용할 것이라는 분석이고 성남시 인프라 확충이 절실히 성남시의 하나로 모아 트램 실증 후보에 선정되도록 다할 것 이라고 밝혔고 이번 3개 지자체를 대상으로 진행 예정이다.\n",
            "Epoch: 24 | Time: 0m 9s\n",
            "\tTrain Loss: 0.014 | Train PPL:   1.014\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "\tCompression rate:0.577670\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "Epoch: 25 | Time: 0m 9s\n",
            "\tTrain Loss: 0.007 | Train PPL:   1.007\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "\tCompression rate:0.577670\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 트램 유치를 놓고 수원시와 성남시가 먼저 준비된 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 움직여 오염물질 배출이 적은 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원역 ~ 장안구청의 트램 전체 트램노선으로 바 있다.\n",
            "Epoch: 26 | Time: 0m 9s\n",
            "\tTrain Loss: 0.020 | Train PPL:   1.021\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "\tCompression rate:0.577670\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 또 지난 판교신도시를 조성하면서 운중로 중앙화단에 트램을 점도 작용할 것이라는 분석이고 성남시 인프라 확충이 절실히 성남시의 하나로 모아 트램 실증 후보에 선정되도록 다할 것 이라고 밝혔고 또 도입과 여러 토론회를 개최하고 제도개선 태스크포스에도 참가했다.\n",
            "Epoch: 27 | Time: 0m 9s\n",
            "\tTrain Loss: 0.023 | Train PPL:   1.024\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "\tCompression rate:0.577670\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "Epoch: 28 | Time: 0m 9s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "\tCompression rate:0.577670\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "Epoch: 29 | Time: 0m 9s\n",
            "\tTrain Loss: 0.002 | Train PPL:   1.002\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "\tCompression rate:0.577670\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "Epoch: 30 | Time: 0m 9s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시과 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 만족했고 이번 3개 지자체를 개최하고 제도개선 태스크포스에도 참가했다.\n",
            "\tCompression rate:0.796117\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "Epoch: 31 | Time: 0m 9s\n",
            "\tTrain Loss: 0.013 | Train PPL:   1.013\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시과 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 나란히 나란히 선 모습과 여러 토론회를 개최하고 제도개선 태스크포스에도 참가했다.\n",
            "\tCompression rate:0.825243\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 이밖에 개정안 3법 마련을 위해 법안 구상 및 통과 등을 내세우고 있고 수원시 관계자는 도입을 위해 기간 도시는 철저한 준비를 통해 트램을 유치해 사람 중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 진행된다.\n",
            "Epoch: 32 | Time: 0m 9s\n",
            "\tTrain Loss: 0.039 | Train PPL:   1.040\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시과 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 나란히 나란히 나란히 선 모습과 여러 토론회를 개최하고 제도개선 태스크포스에도 참가했다.\n",
            "\tCompression rate:0.844660\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 이밖에 개정안 3법 마련을 위해 법안 구상 및 통과 등을 내세우고 있고 수원시 관계자는 도입을 위해 기간 도시는 철저한 준비를 통해 트램을 유치해 사람 중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 진행된다.\n",
            "Epoch: 33 | Time: 0m 9s\n",
            "\tTrain Loss: 0.004 | Train PPL:   1.004\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시과 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 기업경쟁력 전폭적인 지지 등을 내세우고 있고 수원시 관계자는 도입을 위해 ppt 발표내용과 평가위원을 위해 기간했다.\n",
            "\tCompression rate:0.922330\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 이밖에 개정안 3법 마련을 위해 법안 구상 및 통과 등을 내세우고 있고 수원시 관계자는 도입을 위해 기간 도시는 철저한 준비를 통해 트램을 유치해 사람 중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 진행된다.\n",
            "Epoch: 34 | Time: 0m 9s\n",
            "\tTrain Loss: 0.001 | Train PPL:   1.001\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시과 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 기업경쟁력 전폭적인 지지 등을 내세우고 있고 수원시 관계자는 도입을 위해 24일 진행된다.\n",
            "\tCompression rate:0.849515\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 이밖에 개정안 3법 마련을 위해 법안 구상 및 통과 등을 내세우고 있고 수원시 관계자는 도입을 위해 기간 도시는 철저한 준비를 통해 트램을 유치해 사람 중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 진행된다.\n",
            "Epoch: 35 | Time: 0m 9s\n",
            "\tTrain Loss: 0.003 | Train PPL:   1.003\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시과 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 기업경쟁력 전폭적인 지지 등을 내세우고 있고 수원시 관계자는 도입을 위해 24일 진행된다.\n",
            "\tCompression rate:0.849515\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 이밖에 개정안 3법 마련을 위해 법안 구상 및 통과 등을 내세우고 있고 수원시 관계자는 도입을 위해 기간 도시는 철저한 준비를 통해 트램을 유치해 사람 중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 진행된다.\n",
            "Epoch: 36 | Time: 0m 9s\n",
            "\tTrain Loss: 0.001 | Train PPL:   1.001\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시과 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 기업경쟁력 전폭적인 지지 등을 부각하고 있다.\n",
            "\tCompression rate:0.728155\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 이밖에 개정안 3법 마련을 위해 법안 구상 및 통과 등을 내세우고 있고 수원시 관계자는 도입을 위해 기간 도시는 철저한 준비를 통해 트램을 유치해 사람 중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 진행된다.\n",
            "Epoch: 37 | Time: 0m 9s\n",
            "\tTrain Loss: 0.001 | Train PPL:   1.001\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시과 성남시 등에 주관하는 저상트램 실증노선 중 장안문 ~ kt위즈파크에 세계문화유산인 수원역 ~ 장안구청의 트램노선 2차 평가가 25일 진행된다.\n",
            "\tCompression rate:0.800971\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 또 지난 판교신도시를 조성하면서 운중로 중앙화단에 트램을 점도 작용할 것이라는 분석이고 성남시 인프라 확충이 절실히 성남시의 하나로 모아 트램 실증 후보에 선정되도록 다할 것 이라고 밝혔고 또 도입과 여러 토론회를 개최하고 제도개선 태스크포스에도 참가했다.\n",
            "Epoch: 38 | Time: 0m 9s\n",
            "\tTrain Loss: 0.001 | Train PPL:   1.001\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시과 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 기업경쟁력 전폭적인 지지 등을 내세우고 있고 수원시 관계자는 도입을 위해 24일 진행된다.\n",
            "\tCompression rate:0.849515\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 또 지난 판교신도시를 조성하면서 운중로 중앙화단에 트램을 점도 작용할 것이라는 분석이고 성남시 인프라 확충이 절실히 성남시의 하나로 모아 트램 실증 후보에 선정되도록 다할 것 이라고 밝혔고 또 도입과 여러 토론회를 개최하고 제도개선 태스크포스에도 참가했다.\n",
            "Epoch: 39 | Time: 0m 9s\n",
            "\tTrain Loss: 0.033 | Train PPL:   1.034\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시과 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 기업경쟁력 전폭적인 지지 등을 부각하고 있다.\n",
            "\tCompression rate:0.728155\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 또 지난 판교신도시를 조성하면서 운중로 중앙화단에 트램을 점도 작용할 것이라는 분석이고 성남시 인프라 확충이 절실히 성남시의 하나로 모아 트램 실증 후보에 선정되도록 다할 것 이라고 밝혔고 이번 3개 지자체를 대상으로 진행 예정이다.\n",
            "Epoch: 40 | Time: 0m 9s\n",
            "\tTrain Loss: 0.001 | Train PPL:   1.001\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 성남 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 만족했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1.\n",
            "\tCompression rate:0.825243\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 이밖에 개정안 3법 마련을 위해 법안 구상 및 통과 등을 내세우고 있고 수원시 관계자는 도입을 위해 기간 도시는 철저한 준비를 통해 트램을 유치해 사람 중심 교통체계 완성 판교을 유치하면서 운중로 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "Epoch: 41 | Time: 0m 9s\n",
            "\tTrain Loss: 0.002 | Train PPL:   1.002\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 2차 경쟁이 23일 수원시와 2차 평가가 25일 진행된다 2차 2차 평가가 2차 2차 평가 25일 2차 평가가 23일 2차 평가가 26일 진행된다.\n",
            "\tCompression rate:0.752427\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "Epoch: 42 | Time: 0m 9s\n",
            "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "\tCompression rate:0.577670\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "Epoch: 43 | Time: 0m 9s\n",
            "\tTrain Loss: 0.006 | Train PPL:   1.006\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "\tCompression rate:0.577670\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "Epoch: 44 | Time: 0m 9s\n",
            "\tTrain Loss: 0.002 | Train PPL:   1.002\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가 25일 쟁의행위 찬반투표 관련 30일 전폭적인 지지 등을 부각하고 있다.\n",
            "\tCompression rate:0.631068\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 쟁의행위 찬반투표 관련 30일 전폭적인 지지 등을 부각하고 있다.\n",
            "Epoch: 45 | Time: 0m 9s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시 위한 트램 실증노선 2차 평가 25일 쟁의행위 찬반투표 관련 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1기 취임과 여러 토론회를 개최하고 제도개선 태스크포스에도 참가했다.\n",
            "\tCompression rate:0.922330\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 이밖에 개정안 3법 마련을 위해 법안 구상 및 통과 등을 내세우고 있고 수원시 관계자는 도입을 위해 기간 도시는 철저한 준비를 통해 트램을 유치해 사람 중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 있다.\n",
            "Epoch: 46 | Time: 0m 9s\n",
            "\tTrain Loss: 0.006 | Train PPL:   1.006\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "\tCompression rate:0.577670\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "Epoch: 47 | Time: 0m 9s\n",
            "\tTrain Loss: 0.033 | Train PPL:   1.033\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "\tCompression rate:0.577670\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "Epoch: 48 | Time: 0m 9s\n",
            "\tTrain Loss: 0.002 | Train PPL:   1.002\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시 등 5개 토론회를 개최하고 저상트램 실증노선 2차 평가가 25일 진행된다 지난 충북 맹동면에 세계문화유산인 수원화성과 14개 전통시장 광교산 등이 있어 있어 이용자 확보도 원만한 점을 꼽았고 또 도입과 여러 토론회가 개최하고 제도개선 태스크포스에도 참가했다.\n",
            "\tCompression rate:1.111650\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "Epoch: 49 | Time: 0m 9s\n",
            "\tTrain Loss: 0.003 | Train PPL:   1.003\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교경쟁력 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시과 성남시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다.\n",
            "\tCompression rate:0.655340\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 또 지난 판교신도시를 조성하면서 운중로 중앙화단에 트램을 점도 작용할 것이라는 분석이고 성남시 인프라 확충이 절실히 성남시의 하나로 모아 트램 실증 후보에 선정되도록 다할 것 이라고 밝혔고 또 도입과 여러 토론회를 개최하고 제도개선 태스크포스에도 참가했다.\n",
            "Epoch: 50 | Time: 0m 9s\n",
            "\tTrain Loss: 0.012 | Train PPL:   1.012\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시과 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 실증 노증노선으로 바 있다.\n",
            "\tCompression rate:0.679612\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 또 지난 판교신도시를 조성하면서 운중로 중앙화단에 트램을 점도 작용할 것이라는 분석이고 성남시 인프라 확충이 절실히 성남시의 하나로 모아 트램 실증 후보에 선정되도록 다할 것 이밖에 성남시와 성남시의 제2라운드 경쟁이 23일 수원시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다.\n",
            "Epoch: 51 | Time: 0m 9s\n",
            "\tTrain Loss: 0.004 | Train PPL:   1.004\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시과 성남시의 1. 실증노선 2차 평가가 25일 진행된다.\n",
            "\tCompression rate:0.558252\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 또 지난 판교신도시를 조성하면서 운중로 중앙화단에 트램을 점도 작용할 것이라는 분석이고 성남시 인프라 확충이 절실히 성남시의 하나로 모아 트램 실증 후보에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 실증선으로 바 있다.\n",
            "Epoch: 52 | Time: 0m 9s\n",
            "\tTrain Loss: 0.017 | Train PPL:   1.017\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시의의 제2라운드 경쟁이 23일 수원시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다 지난 판교 경쟁이 23일 진행된다.\n",
            "\tCompression rate:0.674757\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다.\n",
            "Epoch: 53 | Time: 0m 9s\n",
            "\tTrain Loss: 0.011 | Train PPL:   1.012\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시 등이 있어 이용자 확보도 원만한 점을 꼽았고 또 도입과 여러 토론회를 개최하고 제도개선 태스크포스에도 참가했다.\n",
            "\tCompression rate:0.621359\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 또 지난 판교신도시를 조성하면서 운중로 중앙화단에 트램을 점도 작용할 것이라는 분석이고 성남시 인프라 확충이 절실히 성남시가 성남시가 먼저 성남시의 하나로 모아 트램 실증 후보에 선정되도록 다할 것 이라고 밝혔고 성남시가 제안서를 제출했고 이번 3개 지자체가 제기를 위해 기간 도시는 철저한 준비를 통해 트램 중심으로 움직이고 있다.\n",
            "Epoch: 54 | Time: 0m 9s\n",
            "\tTrain Loss: 0.004 | Train PPL:   1.004\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 수원시와 성남시의 제2라운드의 성남시 중앙화성과 14개 전통시장 광교산 등이 있어 이용자 확보도 원만한 점을 꼽았고 또 도입과 여러 토론회를 개최하고 제도개선 태스크포스에도 참가했다.\n",
            "\tCompression rate:0.796117\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 이밖에 개정안 3법 마련을 위해 법안 구상 및 통과 등을 내세우고 있고 수원시 관계자는 도입을 위해 기간 도시는 철저한 준비를 통해 트램을 유치해 사람 중심 교통체계 완성 판교 경쟁력을 완성할 것 이라고 말했다.\n",
            "Epoch: 55 | Time: 0m 9s\n",
            "\tTrain Loss: 0.010 | Train PPL:   1.010\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시화성과 14개 전통시장 광교산 등이 있어 이용자 확보자 확보도 원만한 점을 꼽았고 또 도입과 여러 토론회를 개최하고 제도개선 태스크포스에도 참가했다.\n",
            "\tCompression rate:0.694175\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "Epoch: 56 | Time: 0m 9s\n",
            "\tTrain Loss: 0.015 | Train PPL:   1.015\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시화성과 14개 전통시장 광교산 등이 있어 이용자 확보자 확보도 원만한 점을 꼽았고 또 도입과 여러 토론회를 개최하고 제도개선 태스크포스에도 참가했다.\n",
            "\tCompression rate:0.694175\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "Epoch: 57 | Time: 0m 9s\n",
            "\tTrain Loss: 0.003 | Train PPL:   1.003\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 23일 수원시와 성남 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 나란히 도입과 여러 토론회를 개최하고 제도개선 태스크포스에도 참가했다.\n",
            "\tCompression rate:0.771845\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "Epoch: 58 | Time: 0m 9s\n",
            "\tTrain Loss: 0.009 | Train PPL:   1.009\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 성남 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 나란히 도입과 여러 토론회를 개최하고 저상즈파크의 1. 사람 중심 교통체계를 완성할 것 이라고 밝혔다.\n",
            "\tCompression rate:0.878641\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "Epoch: 59 | Time: 0m 9s\n",
            "\tTrain Loss: 0.001 | Train PPL:   1.001\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시 와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 나란히 도입과 여러 토론회를 개최하고 저상즈파크의 1. 사람 중심 교통체계를 완성할 것 이라고 말했다.\n",
            "\tCompression rate:0.888350\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "Epoch: 60 | Time: 0m 9s\n",
            "\tTrain Loss: 0.005 | Train PPL:   1.005\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 성남 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 나란히 도입과 여러 토론회를 개최하고 제도개선 태스크포스에도 참가했다.\n",
            "\tCompression rate:0.791262\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 이밖에 개정안 3법 마련을 위해 법안 구상 및 통과 등을 내세우고 있고 수원시 관계자는 도입을 위해 기간 도시는 철저한 준비를 통해 트램을 유치해 사람 중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 1. 실증노선 2차 평가가 25일 진행된다.\n",
            "Epoch: 61 | Time: 0m 9s\n",
            "\tTrain Loss: 0.001 | Train PPL:   1.001\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 성남 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 판교 인프라 확충이 제안서를 제출했고 이번 3개 지자체는 철저한 준비를 통해 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 철저한 트램 실증 후보에 선정되도록 다했다.\n",
            "\tCompression rate:1.072816\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 이밖에 개정안 3법 마련을 위해 법안 구상 및 통과 등을 내세우고 있고 수원시 관계자는 도입을 위해 기간 도시는 철저한 준비를 통해 트램을 유치해 사람 중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 1. 실증노선으로 바 있다.\n",
            "Epoch: 62 | Time: 0m 9s\n",
            "\tTrain Loss: 0.001 | Train PPL:   1.001\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 서울시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 나란히 나란히 강점위즈파크의 1.\n",
            "\tCompression rate:0.718447\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 또 지난 판교신도시를 조성하면서 운중로 중앙화단에 트램을 점도 작용할 것이라는 분석이고 성남시 인프라 확충이 절실히 성남시의 하나로 모아 트램 실증 후보에 주관하는 저상트램 실증노선 2차 평가가 25일 진행되고 있고 수원시 관계자는 도입을 위해 기간 도시는 철저한 준비를 통해 트램을 유치해 사람 중심 교통체계를 완성할 것 이라고 밝혔다.\n",
            "Epoch: 63 | Time: 0m 9s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 서울시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행하도록 했고 또 도입과 여러 토론회를 개최하고 제도개선 태스크포스에도 참가했다.\n",
            "\tCompression rate:0.825243\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 또 지난 판교신도시를 조성하면서 운중로 중앙화단에 트램을 점도 작용할 것이라는 분석이고 성남시 인프라 확충이 절실히 성남시의 하나로 모아 트램 실증 경쟁이 23일 수원시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다.\n",
            "Epoch: 64 | Time: 0m 9s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 서울시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행하도록 했고 또 도입과 여러 토론회를 개최하고 제도개선 태스크포스에도 참가했다.\n",
            "\tCompression rate:0.825243\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 또 지난 판교신도시를 조성하면서 운중로 중앙화단에 트램을 점도 작용할 것이라는 분석이고 성남시 인프라 확충이 절실히 성남시의 하나로 모아 트램 실증 경쟁이 23일 수원시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다.\n",
            "Epoch: 65 | Time: 0m 9s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 서울시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행하도록 했고 또 도입과 여러 토론회를 개최하고 제도개선 태스크포스에도 참가했다.\n",
            "\tCompression rate:0.825243\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 또 지난 판교신도시를 조성하면서 운중로 중앙화단에 트램을 점도 작용할 것이라는 분석이고 성남시 인프라 확충이 절실히 성남시의 하나로 모아 트램 실증 경쟁이 23일 수원시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다.\n",
            "Epoch: 66 | Time: 0m 9s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 서울시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행하도록 했고 또 도입과 여러 토론회를 개최하고 제도개선 태스크포스에도 참가했다.\n",
            "\tCompression rate:0.825243\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 또 지난 판교신도시를 조성하면서 운중로 중앙화단에 트램을 점도 작용할 것이라는 분석이고 성남시 인프라 확충이 절실히 성남시의 하나로 모아 트램 실증 경쟁이 23일 수원시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다.\n",
            "Epoch: 67 | Time: 0m 9s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 서울시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 25일 진행된다.\n",
            "\tCompression rate:0.645631\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 또 지난 판교신도시를 조성하면서 운중로 중앙화단에 트램을 점도 작용할 것이라는 분석이고 성남시 인프라 확충이 절실히 성남시의 하나로 모아 트램 실증 경쟁이 23일 수원시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다.\n",
            "Epoch: 68 | Time: 0m 9s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 서울시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 나란히 나란히 강점전 전폭적인 지지 등을 부각하고 있다.\n",
            "\tCompression rate:0.781553\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 또 지난 판교신도시를 조성하면서 운중로 중앙화단에 트램을 점도 작용할 것이라는 분석이고 성남시 인프라 확충이 절실히 성남시의 하나로 모아 트램 실증 경쟁이 23일 수원시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다.\n",
            "Epoch: 69 | Time: 0m 9s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 서울시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 나란히 나란히 강점전 전폭적인 지지 등을 부각하고 있다.\n",
            "\tCompression rate:0.781553\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 또 지난 판교신도시를 조성하면서 운중로 중앙화단에 트램을 점도 작용할 것이라는 분석이고 성남시 인프라 확충이 절실히 성남시의 하나로 모아 트램 실증 후보에 선정되도록 다할 것 이라고 밝혔다.로 움직여 오염물질 배출이 적은 저상트램 실증노선 2차 평가가 25일 진행된다.\n",
            "Epoch: 70 | Time: 0m 9s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 서울시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 나란히 우리 국토를 조성하면서 운중로 중앙화단에 참가했다.\n",
            "\tCompression rate:0.786408\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 또 지난 판교신도시를 조성하면서 운중로 중앙화단에 트램을 점도 작용할 것이라는 분석이고 성남시 인프라 확충이 절실히 성남시의 하나로 모아 트램 실증 후보에 선정되도록 다할 것 이라고 밝혔다.로 움직여 오염물질 배출이 적은 저상트램 실증노선 2차 평가가 25일 진행된다.\n",
            "Epoch: 71 | Time: 0m 9s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 서울시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 나란히 우리 국토를 조성하면서 운중로 중앙화단에 참가했다.\n",
            "\tCompression rate:0.786408\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 또 지난 판교신도시를 조성하면서 운중로 중앙화단에 트램을 점도 작용할 것이라는 분석이고 성남시 인프라 확충이 절실히 성남시의 하나로 모아 트램 실증 후보에 선정되도록 다할 것 이라고 밝혔다.로 움직여 오염물질 배출이 적은 저상트램 실증노선 2차 평가가 25일 진행된다.\n",
            "Epoch: 72 | Time: 0m 9s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 서울시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 나란히 우리 국토를 조성하면서 운중로 중앙화단에 참가했다.\n",
            "\tCompression rate:0.786408\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 또 지난 판교신도시를 조성하면서 운중로 중앙화단에 트램을 점도 작용할 것이라는 분석이고 성남시 인프라 확충이 절실히 성남시의 하나로 모아 트램 실증 후보에 선정되도록 다할 것 이라고 밝혔다.로 움직여 오염물질 배출이 적은 저상트램 실증노선 2차 평가가 25일 진행된다.\n",
            "Epoch: 73 | Time: 0m 9s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 서울시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 나란히 우리 국토를 조성하면서 운중로 중앙화단에 참가했다.\n",
            "\tCompression rate:0.786408\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 또 지난 판교신도시를 조성하면서 운중로 중앙화단에 트램을 점도 작용할 것이라는 분석이고 성남시 인프라 확충이 절실히 성남시의 하나로 모아 트램 실증 후보에 선정되도록 다할 것 이라고 밝혔다.로 움직여 오염물질 배출이 적은 저상트램 실증노선 2차 평가가 25일 진행된다.\n",
            "Epoch: 74 | Time: 0m 9s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 서울시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 나란히 우리 국토를 조성하면서 운중로 중앙화단에 참가했다.\n",
            "\tCompression rate:0.786408\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 또 지난 판교신도시를 조성하면서 운중로 중앙화단에 트램을 점도 작용할 것이라는 분석이고 성남시 인프라 확충이 절실히 성남시의 하나로 모아 트램 실증 후보에 선정되도록 다할 것 이라고 밝혔다.로 움직여 오염물질 배출이 적은 저상트램 실증노선 2차 평가가 25일 진행된다.\n",
            "Epoch: 75 | Time: 0m 9s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 서울시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 나란히 우리 국토를 조성하면서 운중로 중앙화단에 참가했다.\n",
            "\tCompression rate:0.786408\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 또 지난 판교신도시를 조성하면서 운중로 중앙화단에 트램을 점도 작용할 것이라는 분석이고 성남시 인프라 확충이 절실히 성남시의 하나로 모아 트램 실증 후보에 선정되도록 다할 것 이라고 밝혔다.로 움직여 오염물질 배출이 적은 저상트램 실증노선 2차 평가가 25일 진행된다.\n",
            "Epoch: 76 | Time: 0m 9s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 서울시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 나란히 우리 국토를 조성하면서 운중로 중앙화단에 참가했다.\n",
            "\tCompression rate:0.786408\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 또 지난 판교신도시를 조성하면서 운중로 중앙화단에 트램을 점도 작용할 것이라는 분석이고 성남시 인프라 확충이 절실히 성남시의 하나로 모아 트램 실증 후보에 선정되도록 다할 것 이라고 밝혔다.로 움직여 오염물질 배출이 적은 저상트램 실증노선 2차 평가가 25일 진행된다.\n",
            "Epoch: 77 | Time: 0m 9s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 서울시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 나란히 우리 국토를 조성하면서 운중로 중앙화단에 참가했다.\n",
            "\tCompression rate:0.786408\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 또 지난 판교신도시를 조성하면서 운중로 중앙화단에 트램을 점도 작용할 것이라는 분석이고 성남시 인프라 확충이 절실히 성남시의 하나로 모아 트램 실증 후보에 선정되도록 다할 것 이라고 밝혔다.로 움직여 오염물질 배출이 적은 저상트램 실증노선 2차 평가가 25일 진행된다.\n",
            "Epoch: 78 | Time: 0m 9s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 서울시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 나란히 우리 국토를 조성하면서 운중로 중앙화단에 참가했다.\n",
            "\tCompression rate:0.786408\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 또 지난 판교신도시를 조성하면서 운중로 중앙화단에 트램을 점도 작용할 것이라는 분석이고 성남시 인프라 확충이 절실히 성남시의 하나로 모아 트램 실증 후보에 선정되도록 다할 것 이라고 밝혔다.로 움직여 오염물질 배출이 적은 저상트램 실증노선 2차 평가가 25일 진행된다.\n",
            "Epoch: 79 | Time: 0m 9s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 서울시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 나란히 우리 국토를 조성하면서 운중로 중앙화단에 참가했다.\n",
            "\tCompression rate:0.786408\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 또 지난 판교신도시를 조성하면서 운중로 중앙화단에 트램을 점도 작용할 것이라는 분석이고 성남시 인프라 확충이 절실히 성남시의 하나로 모아 트램 실증 후보에 주관하는 저상트램 실증노선으로 바흐는 배터리로 움직여 오염물질 배출이 적은 교통수단이고 또 도입을 위해 기간 도시는 철저한 준비를 통해 트램 중심으로 움직이고 있다.\n",
            "Epoch: 80 | Time: 0m 9s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 서울시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 나란히 우리 국토를 조성하면서 운중로 중앙화단에 참가했다.\n",
            "\tCompression rate:0.786408\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 또 지난 판교신도시를 조성하면서 운중로 중앙화단에 트램을 점도 작용할 것이라는 분석이고 성남시 인프라 확충이 절실히 성남시의 하나로 모아 트램 실증 후보에 주관하는 저상트램 실증노선으로 바흐는 배터리로 움직여 오염물질 배출이 적은 교통수단이고 또 도입을 위해 기간 도시는 철저한 준비를 통해 트램 중심으로 움직이고 있다.\n",
            "Epoch: 81 | Time: 0m 9s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 서울시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 나란히 우리 국토를 조성하면서 운중로 중앙화단에 참가했다.\n",
            "\tCompression rate:0.786408\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "Epoch: 82 | Time: 0m 9s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 서울시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 나란히 우리 국토를 조성하면서 운중로 중앙화단에 참가했다.\n",
            "\tCompression rate:0.786408\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "Epoch: 83 | Time: 0m 9s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 서울시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 나란히 우리 국토를 조성하면서 운중로 중앙화단에 참가했다.\n",
            "\tCompression rate:0.786408\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "Epoch: 84 | Time: 0m 9s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 서울시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 나란히 우리 국토를 조성하면서 운중로 중앙화단에 참가했다.\n",
            "\tCompression rate:0.786408\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "Epoch: 85 | Time: 0m 9s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 서울시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 나란히 우리 국토를 조성하면서 운중로 중앙화단에 참가했다.\n",
            "\tCompression rate:0.786408\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "Epoch: 86 | Time: 0m 9s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 서울시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 나란히 우리 국토를 조성하면서 운중로 중앙화단에 참가했다.\n",
            "\tCompression rate:0.786408\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "Epoch: 87 | Time: 0m 9s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 서울시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 나란히 우리 국토를 조성하면서 운중로 중앙화단에 참가했다.\n",
            "\tCompression rate:0.786408\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "Epoch: 88 | Time: 0m 9s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 서울시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 나란히 우리 국토를 조성하면서 운중로 중앙화단에 참가했다.\n",
            "\tCompression rate:0.786408\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "Epoch: 89 | Time: 0m 9s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 서울시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 나란히 우리 국토를 조성하면서 운중로 중앙화단에 참가했다.\n",
            "\tCompression rate:0.786408\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "Epoch: 90 | Time: 0m 9s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 서울시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 나란히 우리 국토를 조성하면서 운중로 중앙화단에 참가했다.\n",
            "\tCompression rate:0.786408\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "Epoch: 91 | Time: 0m 9s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 서울시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 나란히 우리 국토를 조성하면서 운중로 중앙화단에 참가했다.\n",
            "\tCompression rate:0.786408\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "Epoch: 92 | Time: 0m 9s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 서울시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 나란히 우리 국토를 조성하면서 운중로 중앙화단에 참가했다.\n",
            "\tCompression rate:0.786408\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "Epoch: 93 | Time: 0m 9s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 서울시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 나란히 우리 국토를 조성하면서 운중로 중앙화단에 참가했다.\n",
            "\tCompression rate:0.786408\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "Epoch: 94 | Time: 0m 9s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 서울시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 나란히 우리 국토를 조성하면서 운중로 중앙화단에 참가했다.\n",
            "\tCompression rate:0.786408\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "Epoch: 95 | Time: 0m 9s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 서울시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 나란히 우리 국토를 조성하면서 운중로 중앙화단에 참가했다.\n",
            "\tCompression rate:0.786408\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "Epoch: 96 | Time: 0m 9s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 서울시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 나란히 우리 국토를 조성하면서 운중로 중앙화단에 참가했다.\n",
            "\tCompression rate:0.786408\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "Epoch: 97 | Time: 0m 9s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 서울시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 나란히 우리 국토를 조성하면서 운중로 중앙화단에 참가했다.\n",
            "\tCompression rate:0.786408\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "Epoch: 98 | Time: 0m 9s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 서울시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 나란히 우리 국토를 조성하면서 운중로 중앙화단에 참가했다.\n",
            "\tCompression rate:0.786408\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "Epoch: 99 | Time: 0m 9s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 서울시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 나란히 우리 국토를 조성하면서 운중로 중앙화단에 참가했다.\n",
            "\tCompression rate:0.786408\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "Epoch: 100 | Time: 0m 9s\n",
            "\tTrain Loss: 0.000 | Train PPL:   1.000\n",
            "\t---------------------------------------------\n",
            "\tOrg text: 채태병 기자 1차 평가 나란히 통과… 강점 홍보하며 오늘 2차 평가 총력전 수원 “사람중심 교통체계 완성”… 성남 “판교 기업 경쟁력 강화” ‘국내 1호 트램’ 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 막을 올렸다. 23일 수원시와 성남시 등에 따르면 한국철도기술연구원이 주관하는 ‘무가선 저상트램 실증노선 공모’ 2차 평가가 24일과 25일 진행된다.\n",
            "\tSummarized text: 나란히 강점 홍보하며 오늘 2차 평가 총력전 사람중심 교통체계 완성 판교 경쟁력 도시로 거듭나기 위한 수원시와 성남시의 제2라운드 경쟁이 23일 수원시와 서울시와 성남시 등에 주관하는 저상트램 실증노선 2차 평가가 25일 진행된다. 나란히 우리 국토를 조성하면서 운중로 중앙화단에 참가했다.\n",
            "\tCompression rate:0.786408\n",
            "\t---------------------------------------------\n",
            "\tTarget text: 수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n",
            "{'curren_doc_num': 8, 'total_doc_num': 313962, 'learing_time': 7061.694827795029}\n",
            "--------------------------------------------------\n",
            "대유위니아그룹의 위니아딤채가 발효기술과 음식 별 맞춤보관 기능이 돋보이는 2020년형 김치냉장고 딤채의 광고 영상을 공개했다고 26일 밝혔다. 광고는 우리집 딤채를 소개합니다라는 컨셉으로 하나의 딤채로 네 대의 딤채 효과를 누릴 수 있는 사용성을 강조한다.\n",
            "--------------------------------------------------\n",
            "Train... |||||||||||||||||||||| 100.0%   100/100 epochs,grammar loss:-0.0358  similarity loss:-0.0253 length loss:-0.0242\n",
            "  max_score: 4.15597487458851\n",
            "--------------------------------------------------\n",
            "딤채 한 대가 지원하는 다양한 식재료 저장 기능을 소개하는 것으로 한겨울 땅속 김장김치와 묵은지 이유식 재료·샐러드 빙온숙성 고기 등 성질이 다른 여러 식품을 최적의 조건으로 보관할 수 있음을 보여준다. 2020년형 딤채는 오리지널 땅속 냉각 기능과 새롭게 추가된 이유식 재료·샐러드 보관모드 업계 최초 빙온 고기숙성 모드 등으로 식품에 최적화된 숙성 및 보관 기능을 지원한다.\n",
            "--------------------------------------------------\n",
            "Train... ||||||...............| 25.0%   25/100 epochs,grammar loss:0.0193  similarity loss:-0.0169 length loss:-0.0234"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-0591e48d913f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msam_wgan4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'  max_score:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m#for (text,g,s,l) in samples:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-7150e82f9278>\u001b[0m in \u001b[0;36msam_wgan4\u001b[0;34m(text, epochs, batch_size, display, retry, retry_count)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msummarizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSAM_Summarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_discriminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms_discriminator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0msummarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#print(samples)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-f3b71981a2f1>\u001b[0m in \u001b[0;36msummarize\u001b[0;34m(self, epochs, batch_size, learning_rate, display)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-f3b71981a2f1>\u001b[0m in \u001b[0;36m__train\u001b[0;34m(self, epochs, batch_size, learning_rate, display)\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0msw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msw0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m                     \u001b[0mfake_gmr_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_sim_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_len_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__discrete_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m#print(fake_len_out)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-f3b71981a2f1>\u001b[0m in \u001b[0;36m__discrete_gradient\u001b[0;34m(self, weights, use_gpu, verbose)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mo_len_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfake_text\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfake_outs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mo_sim_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_discriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg_text_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg_source_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-fe99af6fcd77>\u001b[0m in \u001b[0;36msimilarity\u001b[0;34m(self, query_text, org_text_emb)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morg_text_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mqueries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mquery_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embedder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;31m#query_embeddings = self._embedder.encode(queries,show_progress_bar=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m#print(queries)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    127\u001b[0m            \u001b[0mBy\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtensors\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0mstacked\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mconvert_to_numpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0mmatrix\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \"\"\"\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshow_progress_bar\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mshow_progress_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetEffectiveLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetEffectiveLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEBUG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1658\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m         \"\"\"\n\u001b[0;32m-> 1660\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1662\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   1639\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1640\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1641\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1642\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   1639\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1640\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1641\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1642\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   1639\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1640\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1641\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1642\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   1639\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1640\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1641\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1642\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   1638\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training mode is expected to be boolean\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1639\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1640\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1641\u001b[0m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1642\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mchildren\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1525\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mchild\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m         \"\"\"\n\u001b[0;32m-> 1527\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_children\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTSvu8EnFD3_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de95991f-1a2c-49fd-decc-463c31573473"
      },
      "source": [
        "print(generate_summary('서산시의회(의장 임재관) 가충순·이수의 의원이 (사)한국지역신문협회에서 수여하는 우수의정대상을 받았다. 가충순 의원과 이수의 의원은 16일 팔봉면 폰타나 리조트에서 열린 한국지역신문협회 하계 워크샵에서 지역사회 발전을 위해 활발한 의정활동을 펼친 공로를 인정받아 우수의정대상을 수상했다. '))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "수원시는 5기 취임한 지난 2010년 교통수단 수립한 뒤 트램 전담 직원을 트램 도입에 관한 연구만 진행하도록 했고 수원역 ~ 장안구청의 트램 전체 트램노선 중 장안문 ~ kt위즈파크의 1. 실증노선으로 바 있다.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}