{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "6 - Attention is All You Need.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e843fd1663a54a21bd80fe5059bb5d58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b60044fc002b41fd8d7ba17a3f3904e7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1348576ab1b74187ab52dddd97ee6ee2",
              "IPY_MODEL_dd5d6392d61c46529d067c4f92318416"
            ]
          }
        },
        "b60044fc002b41fd8d7ba17a3f3904e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1348576ab1b74187ab52dddd97ee6ee2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_041502c76fba4b3e9ed0923995955767",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 371391,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 371391,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c57a951586dc4bcc9154079dcf0b01ef"
          }
        },
        "dd5d6392d61c46529d067c4f92318416": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6dc6507a3e05495e97cb6db0a955d275",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 371k/371k [00:00&lt;00:00, 1.79MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7e842d30889745f0bf7b6d07a44714a3"
          }
        },
        "041502c76fba4b3e9ed0923995955767": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c57a951586dc4bcc9154079dcf0b01ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6dc6507a3e05495e97cb6db0a955d275": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7e842d30889745f0bf7b6d07a44714a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c4981dcb970241e787cda0b571af3251": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_50acb7070d204a7e8f0a0569c3c62348",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b32a8150a0c343dd9801332513bee5d5",
              "IPY_MODEL_d9e110c4095a444cb7d60a1ccd74f7c0"
            ]
          }
        },
        "50acb7070d204a7e8f0a0569c3c62348": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b32a8150a0c343dd9801332513bee5d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_419e5615a8a74c47805ca71940dccb45",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 77779,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 77779,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c1e24b593c844b43bc206e950dbfa751"
          }
        },
        "d9e110c4095a444cb7d60a1ccd74f7c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8d6d2d722cb549eca29f77907d7aa7a8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 77.8k/77.8k [00:00&lt;00:00, 1.69MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f0405a09c88f4b4499d621ee69d834a6"
          }
        },
        "419e5615a8a74c47805ca71940dccb45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c1e24b593c844b43bc206e950dbfa751": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8d6d2d722cb549eca29f77907d7aa7a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f0405a09c88f4b4499d621ee69d834a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dolmani38/Summary2/blob/main/korean_sentence_construction_v0.1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTKaX6slIIAy"
      },
      "source": [
        "# encoder/decoder를 이용하여 keyword를 입력하면, 유추할 수 있는 완성된 문장을 자동 생성한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hp2D7nhwnY38"
      },
      "source": [
        "# 여기서부터 테스트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1S2PM1n2ePq"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchtext\n",
        "#from torchtext.datasets import Multi30k\n",
        "#from torchtext.data import Field, BucketIterator\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBOQ4O8S2oOv"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8G99UP3gzA-",
        "outputId": "3c8888c9-9439-4716-bfca-1b59e54574d3"
      },
      "source": [
        "import urllib.request\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# 간단한 전처리\n",
        "def clean_text(txt):\n",
        "    txt = txt.replace('\\n',' ')\n",
        "    txt = txt.replace('\\r',' ')    \n",
        "    txt = txt.replace('=','')\n",
        "    txt = txt.replace('\\\"','')   \n",
        "    txt = txt.replace('\\'','')\n",
        "    #txt = txt.replace(',','')\n",
        "    txt = txt.replace('..','')\n",
        "    txt = txt.replace('...','')\n",
        "    #txt = txt.replace('.','. ')\n",
        "    txt = txt.replace('.','. ')\n",
        "    txt = txt.replace('  ',' ')\n",
        "    txt = txt.replace('  ',' ')    \n",
        "    txt = txt.replace('  ',' ')   \n",
        "    txt = txt.replace('  ',' ')           \n",
        "    txt = txt.replace('  ',' ')\n",
        "    txt = txt.replace('  ',' ')    \n",
        "    txt = txt.replace('  ',' ')   \n",
        "    txt = txt.replace('  ',' ')             \n",
        "    return txt.strip()\n",
        "\n",
        "def collect_training_dataset_for_s2s_generator(source_urls=[]):\n",
        "    ko_sentences_dataset = []\n",
        "    for url in source_urls:\n",
        "        raw_text = urllib.request.urlopen(url).read().decode('utf-8')\n",
        "        ko_sentences_dataset += nltk.sent_tokenize(clean_text(raw_text))\n",
        "\n",
        "    sentences = []\n",
        "    for txt in ko_sentences_dataset:\n",
        "        txt = txt.strip()\n",
        "        if len(txt) > 40 and txt.endswith('다.'):\n",
        "            #ko_grammar_dataset.append([txt,1])\n",
        "            txt = txt.replace('.','')\n",
        "            sentences.append(txt)\n",
        "\n",
        "    return sentences"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "696xz3yTqbs9",
        "outputId": "ee5d2113-edd7-4b09-969f-b6312bbf2e96"
      },
      "source": [
        "if True:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7jRZO8ANIkp"
      },
      "source": [
        "# dataset 구성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "pFoSKILDiVDE",
        "outputId": "93643dbf-eb3c-4ad4-873f-00625c75bc9d"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/summary/korean_news_corpus.csv')\n",
        "df"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>contents</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>문 대통령 변창흠 국토장관 사의표명 사실상 수용</td>\n",
              "      <td>정만호 국민소통수석이 12일 오후 청와대 춘추관 대브리핑룸에서 변창흠 국토부 장관 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>계급장 수여하는 문 대통령</td>\n",
              "      <td>(아산뉴스1) 이광호 기자 문재인 대통령이 12일 오후 충남 아산시 경찰대학에서 열...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>계급장 수여하는 문 대통령</td>\n",
              "      <td>(아산뉴스1) 이광호 기자 문재인 대통령이 12일 오후 충남 아산시 경찰대학에서 열...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>수상자 메달 걸어주는 문 대통령</td>\n",
              "      <td>(아산뉴스1) 이광호 기자 문재인 대통령이 12일 오후 충남 아산시 경찰대학에서 열...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>정몽구 서울아산병원에 50억 쾌척</td>\n",
              "      <td>인재 양성·소외 계층 지원 등 계획 “부친 질병·가난 악순환 끊기 원해 국내 최고 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140559</th>\n",
              "      <td>[건축과도시] 북한산을 캔버스 삼아. 미술관 또 하나의 작품이 되다</td>\n",
              "      <td>&lt;은평구 진관동 사비나 미술관&gt; 서울시 은평구 진관동에 자리잡은 사비나미술관. 삼각...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140560</th>\n",
              "      <td>조선후기 문인 김조순 별장 그린 옥호정도 첫 공개</td>\n",
              "      <td>국립중앙박물관 서화실 개편해 32점 새롭게 전시 옥호정도[국립중앙박물관 제공연합뉴스...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140561</th>\n",
              "      <td>안성 청룡사 대웅전에서 목재 곡자 발견</td>\n",
              "      <td>문화재청(청장 정재숙)의 국고보조와 기술지도로 안성시에서 시행하고 있는 안성 청룡사...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140562</th>\n",
              "      <td>156년전 ㄱ자 곡자 찾았다 안성 청룡사 기둥 밑에서</td>\n",
              "      <td>안성 청룡사 대웅전에서 발견된 곡자 【서울뉴시스】 이수지 기자 안성 청룡사 대웅전에...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140563</th>\n",
              "      <td>[김중기의 필름통] 새 영화 도굴 나인스 게이트 앙상블</td>\n",
              "      <td>영화 도굴 스틸컷 ◆도굴 감독: 박정배 출연: 이제훈 조우진 신혜선 도굴을 소재로 ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>140564 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        title                                           contents\n",
              "0                  문 대통령 변창흠 국토장관 사의표명 사실상 수용  정만호 국민소통수석이 12일 오후 청와대 춘추관 대브리핑룸에서 변창흠 국토부 장관 ...\n",
              "1                              계급장 수여하는 문 대통령  (아산뉴스1) 이광호 기자 문재인 대통령이 12일 오후 충남 아산시 경찰대학에서 열...\n",
              "2                              계급장 수여하는 문 대통령  (아산뉴스1) 이광호 기자 문재인 대통령이 12일 오후 충남 아산시 경찰대학에서 열...\n",
              "3                           수상자 메달 걸어주는 문 대통령  (아산뉴스1) 이광호 기자 문재인 대통령이 12일 오후 충남 아산시 경찰대학에서 열...\n",
              "4                          정몽구 서울아산병원에 50억 쾌척  인재 양성·소외 계층 지원 등 계획 “부친 질병·가난 악순환 끊기 원해 국내 최고 ...\n",
              "...                                       ...                                                ...\n",
              "140559  [건축과도시] 북한산을 캔버스 삼아. 미술관 또 하나의 작품이 되다  <은평구 진관동 사비나 미술관> 서울시 은평구 진관동에 자리잡은 사비나미술관. 삼각...\n",
              "140560            조선후기 문인 김조순 별장 그린 옥호정도 첫 공개  국립중앙박물관 서화실 개편해 32점 새롭게 전시 옥호정도[국립중앙박물관 제공연합뉴스...\n",
              "140561                  안성 청룡사 대웅전에서 목재 곡자 발견  문화재청(청장 정재숙)의 국고보조와 기술지도로 안성시에서 시행하고 있는 안성 청룡사...\n",
              "140562          156년전 ㄱ자 곡자 찾았다 안성 청룡사 기둥 밑에서  안성 청룡사 대웅전에서 발견된 곡자 【서울뉴시스】 이수지 기자 안성 청룡사 대웅전에...\n",
              "140563         [김중기의 필름통] 새 영화 도굴 나인스 게이트 앙상블  영화 도굴 스틸컷 ◆도굴 감독: 박정배 출연: 이제훈 조우진 신혜선 도굴을 소재로 ...\n",
              "\n",
              "[140564 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xInRrlum7PiB",
        "outputId": "fd24e6b3-cba0-4f2b-90ef-eed93059bcbc"
      },
      "source": [
        "df = df.dropna(axis=0)\n",
        "df['contents'].count()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "140536"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cbf_yNd3quK8",
        "outputId": "2dd7a782-bded-481f-c537-e4d8f390dfe4"
      },
      "source": [
        "import re\n",
        "import sys\n",
        "# 검사...\n",
        "pattens = [\"[34569][0-9]{3}[\\;.\\;-\\; ][0-9]{4}[\\;.\\;-\\; ][0-9]{4}[\\;.\\;-\\; ][0-9]{4}\",\n",
        "           \"[0-9]{2,3}[\\:\\s\\;.\\;,\\;-;)][0-9]{3,4}[\\:\\s\\;.\\;,\\;-][0-9]{4}\",\n",
        "           \"[0-9]{1}[0-9]{1}[\\W]?[0-1]{1}[0-9]{1}[\\W]?[0-3]{1}[\\W]?[0-9]{1}[\\W]?[1-4]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}\",\n",
        "           \"[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{6}|[0-9]{3}[\\:\\s\\;.\\;,\\;-]([0-9]{5,6}[\\:\\s\\;.\\;,\\;-][0-9]{3}|[0-9]{6}[\\:\\s\\;.\\;,\\;-][0-9]{5}|[0-9]{2,3}[\\:\\s\\;.\\;,\\;-][0-9]{6}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{7}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{4,6}[\\:\\s\\;.\\;,\\;-][0-9]|[0-9]{5}[\\:\\s\\;.\\;,\\;-][0-9]{3}[\\:\\s\\;.\\;,\\;-][0-9]{2}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{5}[\\:\\s\\;.\\;,\\;-][0-9]{3}|[0-9]{4}[\\:\\s\\;.\\;,\\;-][0-9]{4}[\\:\\s\\;.\\;,\\;-][0-9]{3}|[0-9]{6}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{3}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{7})|[0-9]{4}[\\:\\s\\;.\\;,\\;-]([0-9]{3}[\\:\\s\\;.\\;,\\;-][0-9]{6}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{6}[\\:\\s\\;.\\;,\\;-][0-9])|[0-9]{5}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{6}|[0-9]{6}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{5,6}\"\n",
        "           ]\n",
        "\n",
        "filters = []\n",
        "for p in pattens:\n",
        "    filters.append(re.compile(p))\n",
        "\n",
        "sentences = []\n",
        "df = df.dropna(axis=0)\n",
        "cnt = df['contents'].count()\n",
        "#print('Total row count:',cnt)\n",
        "i=0\n",
        "for raw_text in df['contents']:\n",
        "    i=i+1\n",
        "    try:\n",
        "        if i%100 == 0:\n",
        "            percent = (\"{0:.2f}\").format(100 * (i / float(cnt)))\n",
        "            print(f'\\r {percent}% {i}/{str(cnt)}', end=\"\", flush=True)\n",
        "\n",
        "        docs = nltk.sent_tokenize(clean_text(raw_text))\n",
        "        for txt in docs:\n",
        "            if txt.find('▶') > -1 or txt.find('@') > -1 or txt.find('ⓒ') > -1: \n",
        "                pass\n",
        "            else:\n",
        "                txt = txt.strip()\n",
        "                if any(chr.isdigit() for chr in txt) :\n",
        "                    pass\n",
        "                else:\n",
        "                    sentences.append(txt)\n",
        "    except KeyboardInterrupt as ki:\n",
        "        raise ki        \n",
        "    except:\n",
        "        pass #print(\"Unexpected error:\", sys.exc_info()[0])      \n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 99.97% 140500/140536"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYBs7FYFrEze",
        "outputId": "9cd15069-b024-4b4e-f735-472e5d4fb2f3"
      },
      "source": [
        "len(sentences)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2967202"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT_bAT4ix3yn"
      },
      "source": [
        "import re\n",
        "import sys\n",
        "import io\n",
        "\n",
        "#텍스트 정제(전처리)\n",
        "def cleanText(readData):\n",
        "    #텍스트에 포함되어 있는 특수 문자 제거\n",
        "    text = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》◆◇●🎧○▲\\t―△━▷]', '', readData)\n",
        "    return text"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lBwyjW8x55f"
      },
      "source": [
        "c_sentences = []\n",
        "for sentence in sentences:\n",
        "    s = cleanText(sentence)\n",
        "    c = len(s.split())\n",
        "    if c >= 3 and c < 10 and s.find('재배포') < 0 and s.find('기자') < 0  and s.find('유투브') < 0 and s.find('www') < 0 and s.find('com') < 0 and s.find('접속하기') < 0 and s.find('http') < 0 and s.find('뉴스') < 0 and s.find('일보') < 0 :\n",
        "        if s.endswith(('다','요')):\n",
        "            c_sentences.append(s.strip())"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wW7kLnpyiYF",
        "outputId": "2fd46cbf-57d1-48b5-af29-94f4da33f906"
      },
      "source": [
        "len(c_sentences)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "867766"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAGY99EK5Zqn",
        "outputId": "d81cc41e-0b74-49e2-dbaf-c1ce26b7f260"
      },
      "source": [
        "c_sentences[6000:]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['이명박 정부에서 보금자리주택이란 이름으로 추진한 공공임대주택이다',\n",
              " '보금자리주택을 비판하며 출범한 박근혜 정부에서도 논란은 계속됐다',\n",
              " '주거안정의 틀인 장기공공임대 약속을 지키기 위해서다',\n",
              " '공공성이 강한 임대주택 용지를 수익 실현 도구로 이용하려는 셈이다',\n",
              " '이는 결국 세금 부담으로 돌아올 가능성이 크다',\n",
              " '단 공기업의 수익 추구와 직원의 사익 추구는 별개다',\n",
              " '그럼에도 직원들은 수차례 선을 넘었다',\n",
              " '공공임대아파트 임차권을 양도 승인해 주는 권한을 악용한 것이다',\n",
              " '문재인 정부 들어서도 비리는 이어졌다',\n",
              " '브로커들에게서 뇌물을 받은 혐의다',\n",
              " 'LH 투기 의혹이 공론화되자 이들도 주목을 받게 됐다',\n",
              " '김헌동 경실련 본부장은 아예 LH 해체론을 주장했다',\n",
              " '부동산 정책을 복지 차원으로 접근해 공공주택을 확보해야 한다는 취지다',\n",
              " '말투가 또박또박하고 힘이 있었다',\n",
              " '처음에 들어갈 때는 전경련 조사역이었다',\n",
              " '이름만 걸쳐 놓은 비상근 위원장이 아니었다',\n",
              " '상임위원장처럼 매우 적극적으로 일했다',\n",
              " '영국 산업위원회와 함께 매년 한국과 영국을 오가며 합동회의를 했다',\n",
              " '이를 기화로 정 회장은 본격적으로 국제적인 명사로 부상하기 시작했다',\n",
              " '정 회장은 국제협력위원회를 최대한 활용하는 글로벌 감각을 가진 사람이었다',\n",
              " '“나는 기억력이 나쁘지 않지만 메모를 열심히 하는 습관이 있다',\n",
              " '국가에 대한 사명감 때문이었다',\n",
              " '낮에는 수행하고 저녁에는 다음날 일정을 잡아야 했다',\n",
              " '연설문도 써야 했다',\n",
              " '지금 생각하면 그 힘든 일을 어떻게 견뎠나 싶다',\n",
              " '“꼭 그런 것은 아니다',\n",
              " '따사로운 면도 있다',\n",
              " '자네 하와이에서 쉬었다 와’ 이렇게 배려해 주곤 했다',\n",
              " '그래서 연설문 작성 때 수시로 이 표현을 넣었다',\n",
              " '“타고 난 측면이 있다',\n",
              " '다만 그는 신문 사설을 열심히 읽었다',\n",
              " '“이봐 해봤어 정신이다',\n",
              " '중간 관리자들은 원래 힘든 일을 두려워하기 마련이다',\n",
              " '“현대중공업 초창기에 배를 만들어 그리스와 중동의 선주에게 건네줬다',\n",
              " '실무자들이 반대하자 이봐 해봤어라고 면박을 주며 공기 단축을 지시했다',\n",
              " '도저히 그 기한에 배를 만들 수 없다고 본 것이다',\n",
              " '그 때마다 정 회장은 이봐 해 봤어’라며 면박을 줬다',\n",
              " '하지만 정 회장은 비용 절감에 대한 복안이 있었다',\n",
              " '제일 핵심은 공사기간 단축이었다',\n",
              " '공기를 단축하면 인건비와 장비 임대료를 줄일 수 있다',\n",
              " '또 수금을 앞당길 수 있다',\n",
              " '서울 외환 시장이 난리가 났었다',\n",
              " '” 정 회장이 역발상의 대가인 것처럼 들린다',\n",
              " '정 회장은 이런 파격적인 발상을 과감하게 밀어부치는 실천력이 있었다',\n",
              " '길이 없으면 만들어갔다',\n",
              " '내가 통역을 했다',\n",
              " '드러커는 정 회장을 만나자마자 제가 잘못했습니다’라고 사과를 했다',\n",
              " '어리둥절하고 있는데 드러커가 말을 이어갔다',\n",
              " '모든 경영자들이 나를 경영의 구루스승라고 한다',\n",
              " '그런데도 나는 한국에 오기 전에 당신에 대해 잘 몰랐다',\n",
              " '그래서 사과한다’고 했다',\n",
              " '하지만 정 회장은 아무런 지지 기반이 없었다',\n",
              " '그런 점에서 드러커가 높이 평가했다',\n",
              " '“자신의 사업보다도 국가를 먼저 생각하는 애국지사적인 기업인이라는 점이다',\n",
              " '정 회장은 주식 공개를 늦춘 이유에 대해 이렇게 말했다',\n",
              " '주식 투자는 돈 있는 사람들이 하는 것이다',\n",
              " '현대중공업도 예외가 아니었다',\n",
              " '그러나 미리 그런 상황에 대비한 마이크 시스템도 없었다',\n",
              " '농성 현장은 순식간에 아수라장이 되고 말았다',\n",
              " '” 현대중공업과 현대자동차는 노사분규가 잦기로 유명하다',\n",
              " '내가 나오라고 할 사람이 없어서 관뒀다’고 했다',\n",
              " '노동자에 대한 비난은 없었다',\n",
              " '남의 부축을 받는 것을 매우 싫어했다',\n",
              " '“건강 관리를 위해 별도로 운동을 한 것 같지는 않다',\n",
              " '다만 평생 동안 새벽에 집에서 사무실까지 걸어서 출근했다',\n",
              " '그는 독일 바덴바덴의 올림픽 유치 홍보관 개관식에도 나타나지 않았다',\n",
              " '오직 정 회장만이 유치가 가능하다고 믿었다',\n",
              " '“대한민국 국민들이 모두 놀랐는데 특히 전두환 대통령이 놀랐다',\n",
              " '정 회장은 정부에 대해 할 말은 하는 사람이었다',\n",
              " '우리 정부가 사회주의 정부가 됐다’고 비판했다',\n",
              " '그러자 정 회장은 나는 흥분하지 않았다’며 목소리를 높였다',\n",
              " '노태우 대통령이 당선자 시절이었다',\n",
              " '그래서 전 대통령이 노태우 당선자에게 찾아 갔었다',\n",
              " '그러나 노 당선자는 참모진과 상의해보겠다’고 하면서 결국 거절했다고 한다',\n",
              " '무역협회대한상의중소기업중앙회는 법정 단체이므로 기업들이 의무적으로 가입하고 회비를 내야 한다',\n",
              " '이에 반해 전경련과 경총은 자율 단체이다',\n",
              " '“우선 민간 경제계의 목소리를 정부에 반영할 언로가 막혔다',\n",
              " '우리 경제인들이 한국 안보에 앞장 서야 한다',\n",
              " '정부는 나서기가 어렵다',\n",
              " '주한 미군 사령관인 베시 대장 싱글러브 참모장소장도 참석했다',\n",
              " '지금은 고인이 된 안기부 김영광 판단기획국장이 전경련을 찾아왔다',\n",
              " '안기부가 이 정보를 늦게 파악해 박정희 대통령이 노발대발했다고 한다',\n",
              " '그래서 정주영 회장을 찾아와 그 일을 맡아달라고 부탁했다',\n",
              " '정 회장이 이를 수락하고 우리에게 그 일을 하라고 지시했다',\n",
              " '그리고 북한의 계획을 무산시키는데 성공했다',\n",
              " '국가경제에 대한 장기적인 발전 비전을 가져야 한다',\n",
              " '“첫째 국가 발전을 위한 장기적 안목을 가져야 한다',\n",
              " '둘째 사회 기여 정신을 가져야 한다',\n",
              " '“예전에는 기업과 정부간에 대화 창구가 열려 있었다',\n",
              " '기업인들이 언제나 정부 각료와 실무자들과 대화가 가능했다',\n",
              " '하지만 지금은 이런 대화 통로가 대부분 막혀 있는 느낌이다',\n",
              " '민간기업의 의견을 제대로 반영하는 통로가 없다',\n",
              " '그래서 요즘 기업인들은 정말 기업하기 어려워졌다고 한다',\n",
              " '삼성 거인’의 단점 스마트폰을 꺼내 시계를 들여다 봤다',\n",
              " '마무리 지어야 할 시간이다',\n",
              " '“김영삼 대통령 시절이다',\n",
              " '김영삼 정부 동안 현대그룹은 시련의 연속이었다',\n",
              " '그 때가 정 회장이 가장 어려웠던 시기가 아닌가 싶다',\n",
              " '“아무튼 나에게 여러차례 그렇게 말했다',\n",
              " '“성격이 급하고 남들에게 지는 것을 참지 못한다',\n",
              " '특히 분을 삭이지 못하는 성격이었다',\n",
              " '현행 방역조치가 장기간 유지되면서 자영업자들은 극심한 고통을 호소하고 있다',\n",
              " '윤씨는 형평성에 대해 불만을 토로하기도 했다',\n",
              " '쌓일대로 쌓인 적자를 견디지 못해 폐업한다는 사례도 잇따랐다',\n",
              " '영업일이 늘수록 적자만 쌓인다라고 한탄했다',\n",
              " '배신·도피·실종·갈등에 관한 이야기로 시간의 경계를 넘나들며 회전목마처럼 순환한다',\n",
              " '다양한 인물들이 화자로 번갈아 나온다',\n",
              " '자기 처지와 사연을 들려주면서 이야기의 밀도가 높아진다',\n",
              " '듣는 사람이 짧은 말을 더 잘 기억한다는 이유에서다',\n",
              " '중요한 말하기 순간을 여섯 가지로 나누고 맞춤형 전략을 설명한다',\n",
              " '짧게 말하는 건 장담하는데 남는 장사다',\n",
              " '각 지역의 문화·역사·공간보다 축제를 채우는 사람들에게 더 주목한다',\n",
              " '이들의 진심을 세세하게 읽어내며 축제의 본질에 다가간다',\n",
              " '너무 빨리 발표한 것 아니냐 이런 지적도 있는데요',\n",
              " '그런 부분들은 조사단계에서는 한계가 좀 있습니다',\n",
              " '그런 걸 조사 과정에서는 알 수가 없는 거거든요',\n",
              " '이런 의견 주셨습니다',\n",
              " '어쨌든 조사는 강제권한이 없기 때문에 한계가 있을 수밖에 없고요',\n",
              " '박영선 후보의 특검 제안에 대한 여야 입장까지 듣고 오시겠습니다',\n",
              " '이런 차원에서 제안을 하신 거라고 봐요',\n",
              " '왜냐하면 야당 입장에서는 자꾸 그런 눈초리가 있을 수 있잖아요',\n",
              " '그런 논란을 없애기 위해서 그러는 건데요',\n",
              " '이런 것들은 국민적 합의와 요청이 있으면 하는 거예요',\n",
              " '김근식 그러니까 박영선 지금 서울시장 후보잖아요',\n",
              " '길어도 한 달 이상은 걸린단 말이에요',\n",
              " '그러면 서울시장 보궐선거가 끝나는 겁니다',\n",
              " '그런데 국민들이 아까 어떻게 느끼냐는 겁니다',\n",
              " '검경 협력 수사는 당연한 거고요',\n",
              " '굉장히 믿을 수가 없다',\n",
              " '그런데 그런 자세는 저는 옳지 못하다고 생각하고 있습니다',\n",
              " '그러니까 검찰 관계자가 이른바 우리가 무당이냐고 그랬다는 거잖아요',\n",
              " '경찰은 못할 것이다',\n",
              " '이번에 처음 해 보는 겁니다',\n",
              " '이건 무슨 소리야 이렇게 되는 거거든요',\n",
              " 'LH 투기 의혹이 정치권으로도 번지고 있는 이런 상황입니다',\n",
              " '오늘도 여당은 국회가 먼저 솔선수범을 보이자 국민의힘을 압박하고 나섰고요',\n",
              " '국민의힘은 여당이 물타기를 하고 있다',\n",
              " '민주당 먼저 조사하면 우리도 스스로 전수조사하겠다 이런 입장을 밝혔습니다',\n",
              " '지체하지 말고 바로 행동에 옮겨야 합니다',\n",
              " '더불어민주당은 준비가 돼 있습니다',\n",
              " '앵커 이 문제는 김근식 실장님께 먼저 여쭤보겠습니다',\n",
              " '그놈이 그놈이라는 생각이 많지 않겠어요',\n",
              " '그래서 그 의혹이 충분히 있기 때문에 보좌진도 해야 되고요',\n",
              " '여기도 여야를 가릴 필요가 있습니다',\n",
              " '그런데 국민들은 진짜 할까 이런 의문을 아직도 가지십니다',\n",
              " '조상호 계속 마지막 뉘앙스가 있거든요',\n",
              " '그러니까 의원들은 간단해요',\n",
              " '할 수 있는 일은 정말 되게 쉽습니다',\n",
              " '개인정보 제공 동의서만 써서 주시면 돼요',\n",
              " '이건 안 하겠다는 얘기거든요',\n",
              " '이게 꼭 투기지역도 아니고요',\n",
              " '현재 문제가 되고 있는 개발지역하고도 꽤 거리가 있거든요',\n",
              " '김근식 아직은 가시화되고 있지는 않고요',\n",
              " '왜냐하면 배우자가 땅을 어디에 갖고 있다',\n",
              " '그런데 그 이후의 대응이 되게 부적절했었거든요',\n",
              " '그리고 특히 본인은 여러 차례 청렴성을 강조해서 얘기했다',\n",
              " '그냥 계속해서 청렴성만 강조한다고 그게 청렴해지는 게 아니잖아요',\n",
              " '그게 본인이 해야 할 마지막 일이다',\n",
              " '앵커 그것까지 하고 거취를 결정해야 된다는 말씀이신가요',\n",
              " '조상호 그게 그렇게 오래 걸릴 거라고 생각하지 않습니다',\n",
              " '그것도 일부 의원의 입장입니다',\n",
              " '그러다 보니까 땅장사라고 하는 기본적인 수익구조가 생길 수밖에 없어요',\n",
              " '토지공사와 주택공사가 결합하면서 사실 주택공사는 적자 투성이었습니다',\n",
              " '저희도 월요일부터 지금까지 계속 LH 얘기를 하고 있으니까요',\n",
              " '여기에 대한 여론조사를 한 부분이 있는데요',\n",
              " '그래픽 나올 때까지 제가 말로 설명을 드리겠습니다',\n",
              " '조상호 당연한 결과라고 생각합니다',\n",
              " '그리고 서울시를 둘러싸고 있는 수도권도 마찬가지고요',\n",
              " '이 부분도 얼마나 영향을 미칠까 주목해 봐야 될 부분인데요',\n",
              " '박영선 후보도 그런데 거리를 두지는 않는 모습인데요',\n",
              " '이 얘기 함께 듣고 오시겠습니다',\n",
              " '제가 실제로 확인하기도 했고요',\n",
              " '앵커 부인하려 해도 부인할 수 없는 상수라고 보셨는데요',\n",
              " '그래서 오히려 본인과는 가장 자연스럽게 연락할 수 있을 것이다',\n",
              " '특히 과거에 적폐수사 관련해서 진행하면서 서로 인연들이 있었거든요',\n",
              " '그러니까 편하게 통화할 거다 이 말을 한 것이라고 봅니다',\n",
              " '인터뷰는 한 시간 넘게 이어졌다',\n",
              " '“북한은 대미외교를 복원하기 위해 매우 애쓰고 있다',\n",
              " '당시 스웨덴 스톡홀름 실무회담도 이 부서에서 관리했다',\n",
              " '“최선희는 북미회담과 대미관계를 누구보다도 잘 아는 전문가다',\n",
              " '” 대북제재로 해외공관들도 많이 어려웠을 것 같다',\n",
              " '코로나가 풀리면 더 감축될 것이다',\n",
              " '대외영역이 그만큼 좁아진 것이다',\n",
              " '“한 마디로 제로다',\n",
              " '한반도 비핵화의 개념 자체를 명확히 해야한다',\n",
              " '이 정권의 최대 과제는 제재 해제와 핵 군축이다',\n",
              " '최대우방국이 한국과 수교한다는 걸 그때 처음으로 알게 됐다',\n",
              " '삐라 종이 질이 좋아서 태우려 해도 불도 붙지 않았다',\n",
              " '“생각보다 많이 의식한다',\n",
              " '하지만 통일을 포기하는 건 남북한 모두 발전을 포기하는 것이다',\n",
              " '시간을 가지고 차이를 줄여가길 바란다',\n",
              " '당연히 B기업이 더 비싼 기업입니다',\n",
              " '이커머스 기업은 어떨까요',\n",
              " '이제 본론으로 돌아와 쿠팡의 몸값을 들여다볼까요',\n",
              " '주가는 기대감으로 형성되니 과거가 아닌 미래의 실적 추정치를 활용합니다',\n",
              " '두 회사의 사업모델이 다르기 때문입니다',\n",
              " '쿠팡은 매출의 대부분을 직매입을 토대로 한 로켓배송으로 올리고 있습니다',\n",
              " '또 물리적인 물류 인프라 즉 창고와 배송인력도 확보해 왔습니다',\n",
              " '네이버의 거래액을 구성하는 또 다른 사업은 네이버쇼핑입니다',\n",
              " '하지만 가격비교 플랫폼은 오픈마켓보다도 밸류에이션 멀티플이 낮습니다',\n",
              " '네이버와 쿠팡의 차이가 사라지고 있다',\n",
              " '쿠팡의 강점인 풀필먼트판매자의 배송 포장 재고관리 대행 서비스도 시작했어요',\n",
              " '심지어는 이륜차 물류 인프라도 묵묵히 확장해 왔습니다',\n",
              " '이밖에도 최근들어 이마트와의 지분 교환 얘기도 나오고 있네요',\n",
              " '네이버 소비자들이 느끼는 쿠팡의 가장 큰 장점은 빠른 배송입니다',\n",
              " '소비자 입장에서 느낄 차이가 점점 줄어들 것입니다',\n",
              " '차기 시즌은 아직 정해지지 않았다',\n",
              " '인용보도 시 출처를 밝혀주시기 바랍니다',\n",
              " '최경영  불행 중 다행이다',\n",
              " '그런데 다만 정부의 발표가 매우 부적절한 부분은 있습니다',\n",
              " '그런데 그런 발표 방식은 좀 문제가 있다고 생각하고 있습니다',\n",
              " '최경영  차명은 안 되잖아요',\n",
              " '개인정보보호법 때문에 개개인의 모두 정보 동의를 받아야 합니다',\n",
              " '이건 행정적인 조사입니다',\n",
              " '그리고 차명거래 이 부분은 사실상 강제수사 영역으로 넘어가요',\n",
              " '최경영  쏘쏘 님이라는 분이 이런 문자를 주셨는데요',\n",
              " '“법무사무실에 근무하는 사람입니다',\n",
              " '주민번호로 근저당권자가 LH나 국토부 직원인지 확인 가능하고요',\n",
              " '은행 대출인 경우 채무자가 실제 소유주일 확률이 높고요',\n",
              " '그것도 조사해야 합니다',\n",
              " '그런데 이런 것도 같이 조사를 정부가 했는지 모르겠습니다',\n",
              " '현재까지 밝혀진 내용은 거기까지입니다',\n",
              " '최경영  신도시 쪽에 토지뿐만 아니고 어떻게 생각하세요',\n",
              " '공공 공직자가 재건축 재개발 지역에 들어가기는 쉽지 않습니다',\n",
              " '워낙 거기는 금액 단위가 큽니다',\n",
              " '훨씬 더 큰 영역이고요',\n",
              " '이런 계산이 나오거든요',\n",
              " '최경영  공직자는 LH 같은 경우에는 공사 직원이잖아요',\n",
              " '최경영  헌법상 불가능해요',\n",
              " '그런데 투기 환수는 행정적으로 가능할 것 같아요',\n",
              " '최경영  또 다른 영역이다',\n",
              " '다 입법이 됐으면 좋겠습니다',\n",
              " '오늘 말씀 감사하고요',\n",
              " '민변 민생경제위원장 김태근 변호사와 이야기 나눠봤습니다',\n",
              " '전흥문은 근처의 풀밭으로 숨어들었고 강용휘는 금천교 쪽으로 도망쳤다',\n",
              " '그날 밤 정조의 암살 시도는 이렇게 실패로 끝나고 말았다',\n",
              " '하지만 이번에는 문을 통해 들어갈 수는 없었다',\n",
              " '궁궐로 통하는 모든 문의 경호가 강화됐기 때문이었다',\n",
              " '전흥문이 체포되자 정조는 심야에 친국을 명령했다',\n",
              " '장소는 창덕궁의 숙장문肅章門이었다',\n",
              " '하지만 노론이 오군영을 장악한 탓에 왕권을 위협했고 역모사건도 잇달았다',\n",
              " '즉위 후 정조는 군제개혁이 시급하다고 판단했다',\n",
              " '신도시 정책은 일정 구역을 지정하고 전면 정비하는 방식으로 이뤄진다',\n",
              " '때로는 이들의 반대로 사업이 차질을 빚기도 한다',\n",
              " '그 기준은 정부가 신도시를 발표하는 당일인 공람공고일이다',\n",
              " '그 이전부터 땅을 소유하고 있어야 보상 대상이 된다',\n",
              " '보상 방법에는 현금 보상 외에도 대토代土제도가 있다',\n",
              " '정부는 작년엔 협의양도인택지 대상자를 아파트 특별공급 대상으로도 편입시켜줬다',\n",
              " '이 외에도 각종 이주대책 생활대책 등이 이뤄진다',\n",
              " '변창흠 국토부 장관도 좋은 의견이라며 공감을 표시했다',\n",
              " '정부는 이 같은 방안을 다각도로 검토 중인 것으로 전해졌다',\n",
              " '일간스포츠 안민구 라면 업계가 때 이른 비빔면 경쟁에 돌입했다',\n",
              " '경쟁사보다 한발 앞선 신제품 출시로 시장점유율을 확대하겠다는 전략으로 풀이된다',\n",
              " '제품 이름은 세 가지 주재료의 앞글자를 따서 지었다',\n",
              " 'TV 광고는 이날부터 전파를 탄다',\n",
              " '좋은 이미지로 동네가 소문 나야지이게 뭔 꼴인지 모르겠다',\n",
              " '주로 쓰레기매립장 관리가 되지 않는 듯한 비닐하우스 등이 즐비했다',\n",
              " 'LH 때문 아니냐라고 말했다',\n",
              " '온라인 커뮤니티에서도 정부 발표를 믿을 수 없다는 분위기다',\n",
              " '이같은 결과는 어느 정도 예견됐다',\n",
              " '당연히 실명으로 이뤄진 거래만 대상이었다',\n",
              " '“정부조사는 믿을 수 없다”는 부정 여론만 자극한 꼴이 됐다',\n",
              " '나아가 토지중심’의 거래도 가능할 것으로 보인다',\n",
              " '그럼에도 입점 업체의 플랫폼에 대한 의존도는 상당히 컸다',\n",
              " '특히 해당 플랫폼 유형별로 답변 비율이 크게 갈렸다',\n",
              " '창업시 판로확보 수단으로서 플랫폼에 대한 의존도가 높아지는 것이다',\n",
              " '직접 재배할 수 없다면 다른 채소에 분산투자하는 것도 방법이다',\n",
              " '“어렸을 때 스페인에 놀러 간 적이 있어요',\n",
              " '그게 스페인 대파칼솟인 걸 알고 깜짝 놀랐었어요',\n",
              " '이런 그에게 대파를 대신할 수 있는 대안 요리법을 물었다',\n",
              " '“참나물은 고기 특히 닭·오리 같은 가금류와 잘 어울려요',\n",
              " '참나물을 양념장에 버무리고 파닭처럼 닭과 함께 곁들여 먹으면 된다',\n",
              " '채 셰프는 냉이와 달래를 제안했다',\n",
              " '길쭉하고 아삭한 식감은 파와 비슷하면서도 더욱 향긋하다',\n",
              " '봄 기운 가득한 골뱅이를 만날 기회다',\n",
              " '최근에는 속초시 홍보대사까지 맡은 그의 이야기를 들어봤다',\n",
              " '너무 만족하고 속상해하는 팬분들이 많으셔서 걱정이에요',\n",
              " '감사하고 사랑한다고 팬들에게 꼭 말하고 싶어요',\n",
              " '사실 신승태는 어릴 때부터 될성부른 떡잎이었다',\n",
              " '특별한 이유 없이 그냥 춤과 노래가 좋았다고 했다',\n",
              " '어린 시절 사진을 보면 온통 춤추고 있는 모습뿐일 정도다',\n",
              " '그는 “무엇보다 갯마당 선생님들이 너무 멋있고 동경의 대상이었다',\n",
              " '신승태는 “트로트는 어릴 적부터도 하고 싶었던 음악이다',\n",
              " '신승태는 “사실 그리 모범적인 학생은 아니었던 것 같다',\n",
              " '이 인터뷰를 통해 너무너무 감사하다고 말씀 드리고 싶다”고 전했다',\n",
              " '강원도의 힘을 보여주겠다”고 했다',\n",
              " '한국토지주택공사LH 직원들의 땅 투기 의혹이다',\n",
              " '한두 번 해본 솜씨가 아니다',\n",
              " '그곳에는 생소한 왕버들 나무를 촘촘히 심어놨다',\n",
              " '토지개발에 따른 보상을 노린 것 말고 다른 설명은 불가능하다',\n",
              " '어쩌면 이번에 걸린 사람들은 상대적으로 순진했다고 해야 할지 모른다',\n",
              " '이번 LH 투기 의혹은 더 참을 수 없는 결정타였다',\n",
              " '그러니 정의로운 결과는 도저히 기대할 수 없게 됐다',\n",
              " '또다시 고양이에게 생선을 맡기는 어리석음을 범할 수는 없다',\n",
              " '비리를 저지른 다음에 부당한 이익을 환수하는 것만으로 충분하지 않다',\n",
              " '처음부터 비리를 저지를 틈을 주지 말아야 한다',\n",
              " '이번 기회에 LH의 역할과 기능을 진지하게 재검토해봐야 하는 이유다',\n",
              " 'LH의 핵심 사업 모델을 요약하면 땅장사’라고 할 수 있다',\n",
              " '심하게 말하면 이 법은 LH 특혜법’이라고 할 수 있다',\n",
              " '사유재산 침해나 보상가격을 둘러싼 논란이 끊이지 않았던 이유다',\n",
              " '이제는 최소화할 때가 됐다',\n",
              " '어쩌면 이번 LH 투기 의혹은 빙산의 일각에 불과할지 모른다',\n",
              " '변창흠 국토부 장관에겐 다시 한번 크게 실망했다',\n",
              " '집값 안정을 위해 대규모 주택공급이 필요하다는 것은 이해한다',\n",
              " '하지만 이 방법밖에 없는 건지 의문이다',\n",
              " '더 늦기 전에 문 대통령의 결단이 필요해 보인다',\n",
              " '이 장관을 보려고 산수유꽃 필 무렵이면 방방곡곡에서 상춘객이 몰려온다',\n",
              " '예년보다 한참 이르다는 화신花信을 듣고서였다',\n",
              " '산수유 마을 산수유나무는 잎보다 먼저 꽃을 피운다',\n",
              " '겉꽃잎이 먼저 열리고 속꽃잎이 불꽃놀이하듯 확 피어난다',\n",
              " '구례군 산동면의 마을들은 지리산 마루금을 병풍처럼 두른 산골 마을이다',\n",
              " '산동山洞이라는 이름이 산골 마을이라는 뜻이다',\n",
              " '이 개천이 서시천이다',\n",
              " '그러니까 산동의 마을들은 섬진강 지천 서시천 상류의 산골 마을이다',\n",
              " '이 마을들을 구례 산수유 마을’이라 부른다',\n",
              " '여느 마을의 소나무처럼 산수유나무가 흔한 마을들이다',\n",
              " '예부터 산수유 열매는 신비의 영약으로 통했다',\n",
              " '산동의 마을에서는 산수유 열매 팔아다 자식을 학교에 보냈다',\n",
              " '대학 나무’라는 별명이 그렇게 나왔다',\n",
              " '요즘에는 산수유 열매 시세가 예전 같지 않다',\n",
              " '값싼 중국산이 대거 수입되어서다',\n",
              " '대신 마을의 명성은 높아졌다',\n",
              " '이번엔 꽃 덕분이다',\n",
              " '산수유꽃은 가장 먼저 꽃망울을 터뜨리는 봄꽃이다',\n",
              " '산수유 마을이 노란 기운으로 아득하면 비로소 봄이 시작되었다는 뜻이다',\n",
              " '난감한 계절 올해는 산수유꽃이 일찍 피었다',\n",
              " '겨울이 안 추웠다',\n",
              " '구례군청에 확인하니 겨울 평균기온이 예년보다 높았다',\n",
              " '반곡마을은 이미 상춘객으로 붐볐다',\n",
              " '십수 년 전에는 상위마을이 제일 먼저 알려졌었다',\n",
              " '노랗게 반짝이는 산촌과 고즈넉한 돌담길 풍경이 그윽했다',\n",
              " '상위마을 다음에는 현천마을이 유명세를 치렀다',\n",
              " '인증 사진 신흥 명소다',\n",
              " '서너 해 전부터는 상위마을 아랫동네인 반곡마을이 떴다',\n",
              " '올해도 산수유꽃 축제는 열리지 않는다',\n",
              " '행사장 출입을 막은 건 아니지만 시끌벅적한 이벤트는 다 취소했다',\n",
              " '외려 호젓하게 꽃놀이를 즐길 수 있겠다 싶기도 하다',\n",
              " '지역 상인에겐 타격이 크다',\n",
              " '전남 구례 산동 산수유 마을 올해도 봄이 오셨다',\n",
              " '새 계절이 시작되었다',\n",
              " '봄이 돌아와 줘 고맙고 예전처럼 맞아주지 못해 미안하다',\n",
              " '전국적으로 집단감염이 이어지고 있는 영향이 큰데요',\n",
              " '아침까지는 내륙 일부 지방에서 안개가 끼어 교통안전에도 주의해야겠다',\n",
              " '강원영동과 경상동해안은 대체로 흐리겠다',\n",
              " '전남남해안과 일부 내륙에도 바람이 강하게 부는 곳이 있겠다',\n",
              " '전 LH전북본부장의 주거지에서는 유서 등이 발견됐다',\n",
              " '그는 전북에서 본부장으로 근무할 때 바람직하지 않은 일을 했다',\n",
              " '국민에게 죄송하다는 내용의 유서를 남긴 것으로 전해졌다',\n",
              " '투기 전모를 다 드러내야 한다고 말했다',\n",
              " '명운을 걸고 수사해야 한다고 주문했다',\n",
              " '미세먼지 농도는 수도권·세종·충남은 나쁨 그 밖의 권역은 좋음보통으로 예상된다',\n",
              " '그의 주거지에선 유서 등이 발견됐다',\n",
              " '다만 일부 상황에 대한 예외를 적용한다',\n",
              " '마켓컬리 마켓컬리를 운영하는 컬리가 IPO기업공개를 추진한다',\n",
              " '뉴욕 증시 상장 가능성도 열어뒀다',\n",
              " '이 과정에서 접촉자가 수백 명에 달했다',\n",
              " '김경수 경남지사가 브리핑을 하고 있다',\n",
              " '이후 변 장관은 문대통령에게 사의를 표명한 것으로 알려졌다',\n",
              " '강원 영동과 경상 동해안은 대체로 흐리겠다',\n",
              " '미세먼지와 안개가 겹쳐 시야도 짧아질 수 있다고 당부했다',\n",
              " '수도권·충남·제주권은 나쁨 수준의 대기질이 예상된다',\n",
              " '그밖의 권역은 보통’ 수준의 대기질을 보이겠다',\n",
              " '다만 일부 예외를 허용했다',\n",
              " '직계가족 모임 기준은 강화됐다',\n",
              " '가족 간 감염이 계속 발생하고 있어서다',\n",
              " '친환경 차량전기·하이브리드·수소차 등은 제외된다',\n",
              " '일부 시설과 모임에 대한 방역 수칙을 강화했다',\n",
              " '목욕탕에서 세신사와 대화하는 것도 금지된다',\n",
              " '대신 그동안 폐쇄했던 사우나·찜질시설은 운영할 수 있다',\n",
              " '인원 제한은 없었다',\n",
              " '돌잔치 전문점은 방역 수칙이 완화된다',\n",
              " '앞으로는 전문점에서 하는 돌잔치는 결혼·장례식과 같은 행사로 분류된다',\n",
              " '일반 음식점에서 하는 돌잔치는 인정되지 않는다',\n",
              " '시간에 상관없이 영업할 수 있다',\n",
              " '보급형 스마트폰들이 다시 대거 등장하고 있습니다',\n",
              " '국내에선 단연 삼성전자 제품들이 눈에 띕니다',\n",
              " '삼성전자는 다작으로 대응하고 있습니다',\n",
              " '전체적인 제품 크기는 기존 A시리즈들과 큰 차이는 없습니다',\n",
              " '삼성페이와 방수방진은 지원하지 않습니다',\n",
              " '간단한 웹서핑 중에도 반응 속도가 느립니다',\n",
              " 'GPS로 구동되는 내비게이션은 실행됩니다',\n",
              " '야간 환경에서도 제법 괜찮은 사진을 구현합니다',\n",
              " '갤럭시 브랜드를 공유한다는 게 가장 큰 장점인 스마트폰입니다',\n",
              " '이는 서울 송파구 내 가장 높은 시세를 자랑한다',\n",
              " '법원에 들어가는 순간까지 반성의 모습은 전혀 보이지 않았다고 하는데요',\n",
              " '그리고 나흘 뒤 이곳 인천지법에서는 A씨에 대한 구속영장실질심사가 열렸습니다',\n",
              " 'A씨  살인 왜 하셨나요억울해서요',\n",
              " '피해자에게 하시고 싶은 말 있으세요',\n",
              " '그런데 교육청이 통합학급에 대해선 별다른 지침을 내놓지 않은 것이다',\n",
              " '교육당국의 부실한 운영 지침이 장애학생 교육의 사각지대를 만든다는 지적이다',\n",
              " '서울 여의도 순복음교회에서 대면 예배가 진행되고 있다',\n",
              " '특히 교인 간 모임을 통해 감염이 더욱 확산하고 있다',\n",
              " '하지만 예배 후 가진 모임이 문제가 됐다',\n",
              " '유통시장 점유율을 확대하는 데 공격적으로 나서겠다는 얘기다',\n",
              " '그러면서 “공격적이고 계획적으로 투자를 지속해야 한다”고 목소리를 높였다',\n",
              " '촘촘한 전국 당일배송망을 구축하려면 더 많은 기지가 필요하다는 이유에서다',\n",
              " '쿠팡 임직원도 상당한 차익을 거둘 전망이다',\n",
              " '전국민의 분노와 상대적 박탈감을 꾹꾹 눌러담은 표현입니다',\n",
              " '도 커지고 있습니다',\n",
              " '우선 정부기관인 금융위원회는 자본시장법과 공무원 행동강령의 적용을 받습니다',\n",
              " '공공기관에서 해제된 한국거래소는 어떨까요',\n",
              " '사기업은 좀 다를까 싶지만 그렇지도 않습니다',\n",
              " '그러나 조사는 진행되지 못했다',\n",
              " '이번에도 여야의 엇갈린 분위기 속 불발 조짐이 감지되고 있다',\n",
              " '김 대표 대행은\\xa0민주당은 전수조사가 준비돼 있다',\n",
              " '국민의힘도 참여해달라고 거듭 요청했다',\n",
              " '시민단체들은 이번 기회에 반드시 국회의원 전수조사가 이뤄져야 한다는 입장이다',\n",
              " '선호 공기업인 LH의 이미지가 심각하게 훼손됐기 때문이다',\n",
              " 'LH에 대한 혁신안도 이와 함께 논의할 전망이다',\n",
              " '경찰 수사도 가속도가 붙을 전망이다',\n",
              " '관련 현안 한국투자증권 김규정 자산승계연구소장과 함께 얘기 나눠보겠습니다',\n",
              " '부동산 전문가이시니까 여러 가지 물어볼 게 많습니다',\n",
              " '생각보다는 맹탕조사가 아니냐 이런 비판도 있어요',\n",
              " '앵커 그런 곳이 전국에 산재해 있더라고요',\n",
              " '거론되는 곳들이 몇 군데가 사실 있습니다',\n",
              " '이런 곳에서도 투기 정황이 드러나지 않을까 우려하는 목소리도 있거든요',\n",
              " '앵커 확대 조사할 필요가 있다',\n",
              " '김규정 그런 곳들을 의심해 볼 수 있습니다',\n",
              " '그게 가능할지는 사실 좀 현재로써는 의구심도 듭니다',\n",
              " '김규정 사실상 굉장히 어렵다고 보여집니다',\n",
              " '실제로 건설 시장이라고 해야 될까요',\n",
              " '앵커 앞으로 여러 가지 조사방법을 강구하고 있습니다',\n",
              " '강제수사를 통해서 거래내역을 들여다볼 수 있는 방법이 있긴 하나봐요',\n",
              " '어떻게 조사가 이뤄질지 좀 더 지켜봐야 될 것 같고요',\n",
              " '부동산시장 잠깐 물어볼게요',\n",
              " '앵커 걱정하고 있겠군요',\n",
              " '일단 정부정책을 믿어야 될 것 같습니다',\n",
              " '앵커 최근에 전세값도 안정을 찾는 분위기고요',\n",
              " '정부의 정책을 지금 믿고 기다리는 분들이 많습니다',\n",
              " '그리고 예정대로 공급대책이 추진이 돼야 되겠습니다',\n",
              " '지금까지 한국투자증권 김규정 자산승계연구소장과 얘기 나눠봤습니다',\n",
              " '라고 패러디가 되고 있고요',\n",
              " '분노 담긴 각종 신조어도 등장했습니다',\n",
              " '버스정류장에 붙은 광고인데요',\n",
              " '두 사람의 SNS 메신저 대화 형식입니다',\n",
              " '이제야 이 광고가 이해된다고 했습니다',\n",
              " 'LH에 대한 신뢰는 산산조각 났습니다',\n",
              " '불공정하다고 생각하는 사람이 절반을 넘었고요',\n",
              " '좋은 의도로 출범했다고 할지라도 LH의 권한은 너무 비대해졌습니다',\n",
              " '너무 많은 권한이 한곳에 집중되면 부패하기 마련입니다',\n",
              " '국회의원 투기 전수조사도 방식에 대해 의견이 엇갈렸다',\n",
              " '홍정민 민주당 원내대변인은 특검에 대해 합의되지 않았다고 했다',\n",
              " '여야는 변창흠 국토교통부 장관 거취에 대해서도 논의를 했다',\n",
              " '무리하지 않는데 나의 처절한 경험이 도움이 되었으면 좋겠다',\n",
              " '아이고 아이고 어머니의 통곡 소리가 들려오는 듯하다',\n",
              " '떨어질 때마다 물을 탔던 것도 이실직고 한다',\n",
              " '끅끅 통곡하는 어머니 옆에서 오열하는 아내가 보인다',\n",
              " '못난 내 과거의 모습이다',\n",
              " '그리고 온갖 소식들이 들려오기 시작했다',\n",
              " '조금씩 퍼지기 시작한 정보가 새어 나오다 못해 터져 나왔다',\n",
              " '처음엔 관심을 두지 않았다',\n",
              " '급등락 하는 주식을 보는 마음은 그런 것이다',\n",
              " '이런 마음이 쌓이다 보니 어느 순간 주식이 불어나기 시작했다',\n",
              " '어차피 기준은 없었다',\n",
              " '소식을 듣고 샀으니 소식이 들어올 때마다 주식은 늘어났다',\n",
              " '어느샌가 여기서 수익을 보기로 굳건히 결심한 상태가 돼 버렸다',\n",
              " '그리고 어느 날 절망적인 소식이 들려왔다',\n",
              " '합병 기대가 사라진 듯합니다',\n",
              " '이미 추세가 깨졌습니다',\n",
              " '안타깝지만 이번엔 실패입니다',\n",
              " '실패를 재각인 시키는 리딩 업자의 문자가 전달됐고 손절을 당부했다',\n",
              " '하지만 나는 그 당부를 따를 수 없었다',\n",
              " '결국 나는 손절하라는 소식에도 물을 탔다',\n",
              " '이미 많이 떨어졌음에도 혼자서 행복 회로를 돌리며 고집을 부렸다',\n",
              " '그제야 정신이 번쩍 들었다',\n",
              " '그들을 이해할 때가 아니었다',\n",
              " '아무리 생각해도 그건 아니다',\n",
              " '내가 찾은 이유는 이렇다',\n",
              " '첫째는 공부한다고 달라질 것 없다는 경험 때문이다',\n",
              " '가치가 확정적이지 않은 주식은 키보드처럼 객관적인 비교가 쉽지 않다',\n",
              " '그러다 차라리 노력이라도 하지 말자는 자기 방어적인 태도가 생겨난다',\n",
              " '두 번째는 처음부터 애착이 없었기 때문이다',\n",
              " '대다수가 단기적으로 큰 수익을 바란다',\n",
              " '이런 상황에서 해당 종목에 시간과 공을 들이기는 쉽지 않다',\n",
              " '급히 사야 하는 마당에 공부할 시간이 있을 리도 없다',\n",
              " '여차하면 던지겠다는 각오가 깔려 있는데 애착이 생길 리 만무하다',\n",
              " '여기까지가 이해할 수 없는 행동에 대한 자가 진단이다',\n",
              " '길게 보고 내 것을 산다고 생각하면 되는 거다',\n",
              " '내가 말하고도 허탈하다',\n",
              " '방법으로 얘기가 옮겨가면 이게 또 묘연해지기 때문이다',\n",
              " '아무리 생각해도 묘책은 떠오르지 않는다',\n",
              " '그런데 단 한 가지 분명히 되새겨지는 것은 있다',\n",
              " '다시는 이렇게 하지 말자는 반복된 후회와 다짐이 그것이다',\n",
              " '겪다 보면 자연스레 멀어지고 가까워지는 것들이 생긴다',\n",
              " '사람은 경험해 보지 않고 배우는 것이 쉽지 않다',\n",
              " '그리고 경험해보기 전엔 뭐가 뭔지도 알지 못한다',\n",
              " '그래서 뭐든 해봐야 한다',\n",
              " '조심할 것은 그 경험으로 인한 영향력의 크기다',\n",
              " '스릴을 경험하는 것이 처음부터 스카이 다이빙일 필요는 없다',\n",
              " '낙하산을 깜빡했네 했던 경험은 정말이지 누구에게도 권하고 싶지 않다',\n",
              " '다양한 경험을 무리하지 않는 선에게 쌓아가길 소망한다',\n",
              " '좀 더뎌도 괜찮다',\n",
              " '국회로 자료가 나가면 언론으로 공소장이 흘러들어가는 관행을 끊겠다는 이유였다',\n",
              " '추 전 장관과 언론의 골은 점점 깊어져만 갔다',\n",
              " '사방에서 다 관찰된다',\n",
              " '하던 대로 할 거면 개혁 없이 그냥 살면 된다',\n",
              " '그리고 비공개라고 하지만 제한공개공소장 전문이 아닌 공소요지만 공개였다',\n",
              " '그런데 바로 이 사건이 그 적용 사례가 됐다',\n",
              " '야권에선 가자마자 정권 비호한다고 했다',\n",
              " '법무부 훈령은 내부 규정이다',\n",
              " '그런 걱정하지 말아달라고 했다',\n",
              " '그 덕분에 엄청 많이 공부했다',\n",
              " '언론이 제게 질문을 안 한다',\n",
              " '프레임을 씌워놓고 끝이었다',\n",
              " '공소장 비공개만해도 언론이 피해를 입는다고 생각했던 것 같다',\n",
              " '윤석열 총장 징계 청구 사유 하나하나도 엄청난 것들이었다',\n",
              " '사방에서 다 관찰된다',\n",
              " '하지만 법조계에서도 문재인 정부의 검찰개혁을 못 믿겠다고 했다',\n",
              " '제 발언권은 거의 봉쇄당했다',\n",
              " '특히 울산사건은 제가 당 대표여서 너무 잘 알고 있다',\n",
              " '그런 선거에는 공약이 영향을 미치지 않는다',\n",
              " '차별화할 필요도 없었다',\n",
              " '울산사건 수사가 너무 과했다',\n",
              " '그러니 공소장이 공개되면 차라리 저한테도 좀 나았다',\n",
              " '하지만 공소장을 공개했다는 선례를 제가 남기면 개혁은 어렵다',\n",
              " '징계청구 이전에 감찰이라는 시간을 보냈다',\n",
              " '그 감찰을 하는 분들도 되게 힘들어 했다',\n",
              " '검사들이 검찰총장을 상대로 한다는 것이 쉬운 일이 아니다',\n",
              " '그런 조직을 감찰한다',\n",
              " '쉽게 할 수 없다',\n",
              " '중간에 다 도망갔다',\n",
              " '그런데도 끝까지 해준 사람은 대단한 거다',\n",
              " '법무부 감찰담당관 박은정 검사가 그랬다',\n",
              " '단단한 결기 원칙을 지키고자 하는 마음 그건 대단한 용기였다',\n",
              " '사실 장관은 일정한 거리를 유지하다 마지막에 결과를 보고받을 뿐이다',\n",
              " '개입할 수도 쉽게 격려해줄 수도 없다',\n",
              " '박 검사가 정말 괴로웠을 거다',\n",
              " '그 모든 게 저한테는 괴로운 일이었다',\n",
              " '정말 힘든 여정이었고 솔직히 하루도 편한 날은 없었다',\n",
              " '박은정 검사도 그렇고 비슷하다',\n",
              " '그 사람이 틀려서 혼자 남은 게 아니다',\n",
              " '그때는 국민만 믿고 가는 거다',\n",
              " '사람들은 그 검사들을 추미애 라인이라고 부른다',\n",
              " '그 불이익도 감수한 거다',\n",
              " '이런 문제는 온몸으로 부딪치면서 알려야지 사람들이 관심 갖는다',\n",
              " '개복을 다 했다',\n",
              " '이 엄청난 환부를 열어서 보여줬다고 생각한다',\n",
              " '정말 고장 났다고 알려지는 데까지 오기가 어렵다',\n",
              " '이런 상황들이 되니까 해야 된다고 느낀 거다',\n",
              " '어찌 보면 개혁할 수 있는 환경이 만들어졌다',\n",
              " '많은 민생사건이 적체될 수밖에 없다',\n",
              " '장관에서 물러났는데도 강한 검찰개혁 의지를 갖고 있는 것 같다',\n",
              " '그 질문은 너무 앞서 나갔다',\n",
              " '정치인은 늘 희망을 제시해야 한다',\n",
              " '한국토지주택공사LH 직원들의 신도시 땅 투기 파문이 연일 확산되고 있습니다',\n",
              " '그러면서 민주당은 준비돼 있다',\n",
              " '국민의힘도 참여해주기 바란다고 요구했습니다',\n",
              " '특검은 박영선 서울시장 후보가 예고 없이 제안했습니다',\n",
              " '특검에 대해서도 시간 끌기 의도가 있다고 본다고 일축했습니다',\n",
              " '국민의힘 오세훈 서울시장 후보도 특검을 하려면 시간이 많이 걸린다',\n",
              " '그 동안에 중요한 증거는 다 인멸될 것이라고 했습니다',\n",
              " '시민들의 허탈과 분노를 생각하면 그 마음을 이해하고도 남는다고 썼다',\n",
              " '그러니 국민들이 허탈해 하는 것”이라고 비판했다',\n",
              " '민주당은 준비돼있고 국민의힘도 함께할 것으로 기대한다”고 말했다',\n",
              " '국민의힘 주호영 원내대표는 시간끌기라고 했다',\n",
              " '민주당은 준비돼있고 국민의힘도 함께할 것으로 기대한다”고 말했다',\n",
              " '민주당은 준비돼있고 국민의힘도 함께할 것으로 기대한다고 말했다',\n",
              " '하지만 주호영 원내대표는 특검부터 부정적이었다',\n",
              " '블라인드 계정만으로 이용자를 특정할 수 없는 셈이다',\n",
              " '국민의힘도 참여해달라고 촉구했다',\n",
              " '그러면서 야당의 LH 투기의혹 권력형 게이트 주장을 비판했다',\n",
              " 'LH 투기는 내부정보를 이용한 투기라고 반박했다',\n",
              " '근본적 제도 개혁도 추진하겠다고 거듭 강조했다',\n",
              " '영화 범죄와의 전쟁’ 한 장면에서 나오는 대사다',\n",
              " '정세균 국무총리의 목요 브리핑에선 온통 LH 관련 질문만 쏟아졌다',\n",
              " '어차피 예상된 결과였다',\n",
              " '악질적 투기는 훨씬 더 치밀하고 꼼꼼하게 진행됐을 것이다',\n",
              " '결국 경찰 수사에서 밝혀내야 할 부분이다',\n",
              " '반드시 빠뜨리지 말아야 할 것이 퇴직자들’이다',\n",
              " '민주당 김태년 원내대표가 국민의힘 주호영 원내대표를 찾아가는 형식으로 진행되는데요',\n",
              " '이렇게 또 뵈니까 새롭습니다',\n",
              " '지금 코로나 때문에 우리 국민들께서 많이 힘들어하십니다',\n",
              " '어제 저는 국회의원들의 부동산 전수조사를 국민의힘과 국회의장님께 제안을 드렸습니다',\n",
              " '국회의장님께 건의드렸고 또 공감도 하셨는데요',\n",
              " '그래서 국회의원들의 부동산과 관련한 전수조사를 제안을 드렸고요',\n",
              " '지체할 이유가 없을 것 같습니다',\n",
              " '민주당은 준비가 되어 있고 국민의힘도 함께할 것이라고 기대합니다',\n",
              " '저희들은 두 가지 원칙을 가지고 있습니다',\n",
              " '가급적 빨리 한다',\n",
              " '이 두 가지 원칙을 가지고 있습니다',\n",
              " '국민의힘도 참여해달라”고 요청했다',\n",
              " '부산시장 보궐선거를 겨냥한 정략적 문제제기라는 것이다',\n",
              " '민주당은 이미 준비가 끝났으며 국민의힘도 동참해달라 했다',\n",
              " '국민의힘도 참여해달라고 했다',\n",
              " '지체하지 말고 바로 행동에 옮겨야한다고 했다',\n",
              " '이어 민주당은 추호도 공직자의 투기를 덮거나 감쌀 의도가 없다',\n",
              " '김 원내대표는 이날 국회에서 열린 중앙선대위원회 회의에서 이같이 밝혔다',\n",
              " '또한 불법 투기 공직자는 퇴출하고 투기이익도 환수하겠다고 했다',\n",
              " '그러면서 민주당은 공직자의 투기를 덮거나 감쌀 의도가 없다',\n",
              " '국민의힘도 참여해달라고 호응을 촉구했다',\n",
              " '선거전략이겠으나 국민의 분노와 허탈감을 정쟁수단으로 삼아선 안 된다고 말했다',\n",
              " '국민의힘도 참여해주길 바란다고 밝혔다',\n",
              " '이에 김종인 국민의힘 비상대책위원장은 한 번 해보자고 응수했다',\n",
              " '그러면서 민주당은 추호도 공직자의 투기를 덮거나 감쌀 의도가 없다',\n",
              " '국민의힘도 참여해달라고 요청했다',\n",
              " '그는 내가 어리석었다고 배신감의 눈물을 흘리며 허탈해했다',\n",
              " '아르헨티나에서 영화관 테마카페 엘카피탄을 운영하는 사장 노르베르토 로이소의 이야기다',\n",
              " '사장의 종업원 챙기기는 이때부터 시작됐다',\n",
              " '하지만 로이소 사장의 카페는 이때 문을 열지 못했다',\n",
              " '영화관을 겸하고 있는 테마카페라는 이유로 영화관 규정이 적용된 때문이다',\n",
              " '하지만 이날 그는 일생일대의 배신감을 느꼈다',\n",
              " '로이소 사장은 자신의 사연을 SNS사회관계망서비스에 올리고 배신감을 토로했다',\n",
              " '사연이 세상에 알려지면서 그에겐 언론의 인터뷰 요청이 쇄도했다',\n",
              " '인터넷엔 로이소 사장을 격려하는 위로의 댓글이 쇄도하고 있다',\n",
              " '신도시 주민들은 허탈감을 감추지 못했다',\n",
              " '너는”이라고 묻자 남성은 “난 LH 다녀”라고 답한다',\n",
              " '이후 두 사람은 스킨십을 나눈다',\n",
              " '그런데 이 가게가 철거 위기에 처했습니다',\n",
              " '중소벤처기업부는 을지OB베어를 백년가게로 선정하기도 했습니다',\n",
              " '하지만 서울시는 이에 대해 마땅한 대책은 없다고 밝혔습니다',\n",
              " '서울 청계천·을지로 일대를 정비하는 세운 재정비 사업의 일환입니다',\n",
              " '하지만 시민단체들은 이런 재정비 계획을 여전히 비판합니다',\n",
              " '이런 우려 속 세운재정비 사업은 계속 진행되고 있습니다',\n",
              " '저작권은 JTBC에 있습니다',\n",
              " '최창렬 용인대 교양학부 교수 자리 함께했습니다',\n",
              " '그렇지 않고서야 이렇게 엄청나게 여러가지가 벌써 나오고 있잖아요',\n",
              " '차라리 처음부터 수사로 들어가는 게 낫지 않겠어요',\n",
              " '농지는 뭐 다 아시다시피 농민만 소유할 수 있는 거예요',\n",
              " '부동산 투기 사슬 이런 게 있다고 생각을 합니다',\n",
              " '정부도 이제 그런 민심을 의식해서 부동산 범죄와의 전쟁을 선포한다',\n",
              " '옛날에 노태우 대통령 시절에 범죄와의 전쟁 선포했잖아요',\n",
              " '지금은 대표 직무대행까지 맡고 있습니다',\n",
              " '국회 차원의 부동산 투기 전수조사를 실시하자 이렇게 요구를 했습니다',\n",
              " '다 유아무야되다 말았고 다 용두사미로 그치고 말았거든요',\n",
              " '전수조사를 한다 하더라도 별로 효율성이 없을 가능성이 대단히 높아요',\n",
              " '저는 이것도 하나의 농단이다',\n",
              " '국정농단만 농단이 아니에요 이런 것도 농단이에요',\n",
              " '단지 이게 공공영역의 문제만도 아니거든요',\n",
              " '문재인 정부는 이거 안 했잖아요',\n",
              " '최창렬용인대 교양학부 교수 저는 결정적 영향을 줄 것 같아요',\n",
              " '최창렬 용인대 교양학부 교수였습니다',\n",
              " '말씀 잘 들었습니다',\n",
              " '강인선·배성규의 모닝라이브 \\xa0 안녕하세요',\n",
              " '어제 정세균 총리의 발표를 들으면서 정말 허탈했습니다',\n",
              " '아래 링크를 클릭해주세요',\n",
              " '거래 기록이 자동 저장되고 위·변조도 불가능합니다',\n",
              " '이런 열품의 배경은 뭘까요',\n",
              " '이대로 가다가는 의료체계가 붕괴할 수도 있다는 우려가 제기되고 있습니다',\n",
              " '코로나 신규 확진자 수가 좀처럼 줄지 않고 있습니다',\n",
              " '변이 바이러스 감염자도 늘었습니다',\n",
              " '이에 따라 홍콩에 대한 중국의 지배력이 극대화될 전망입니다',\n",
              " '전인대에서 어떤 결정을 내린 것인지 읽어보시지요',\n",
              " '기사보기 금요일\\xa0 아침 \\xa0 강인선의 모닝 라이브 는 여기까지입니다',\n",
              " '대부분 직원들이 몸을 낮추고 조심히 행동하려는 분위기다',\n",
              " '이런 걸 전해드리고 싶었다”라고 말했다',\n",
              " '나도 마찬가지”라고 말해 논란을 빚었다',\n",
              " '투기 정보에 접근할 수 있는 사람은 굉장히 제한적”이라고 말했다',\n",
              " '풍자물들은 대부분 허탈감이나 분노를 표현한 내용이라는데요',\n",
              " '제약회사의 백신 납품이 차일피일 늦어지는 이유는 분명했다',\n",
              " '아니나 다를까 일본에서 반론이 쏟아졌다',\n",
              " '그야말로 배보다 배꼽이 더 크다는 말처럼 말이다',\n",
              " '일본 관료의 일본어 실력도 도마 위에 올랐다',\n",
              " '법안 심사는 고사하고 시작부터 삐걱거리기 시작했다',\n",
              " 'LH 직원은 투자하지 말란 법 있나요',\n",
              " '전문가는 청년층의 분노가 결국 불공정에서 비롯된 것이라고 지적했다',\n",
              " '부러우면 우리 회사로 이직하든지라고 비꼬았다',\n",
              " '극혐극히 혐오스러움이라고 했다',\n",
              " 'LH 직원들의 망언은 이번이 처음이 아니다',\n",
              " '이런 것에 대한 반발이 굉장히 심각한 상태다',\n",
              " '너는이라고 묻자 남자는 LH 다녀라고 답한다',\n",
              " '이 또한 최근 인터넷에서 빠르게 퍼지는 게시물 중 하나다',\n",
              " '눈물 난다라고 적었다',\n",
              " '한자어를 그대로 해석해 복을 가져오는 부인’인가 생각할 수 있겠다',\n",
              " '이 복덕방을 자주 들락거리는 가정주부를 복부인이라고 부르며 조롱한 것이다',\n",
              " '라고 불리었고 가쾌는 구한말까지도 존재했던 것으로 알려져 있다',\n",
              " '복덕방도 덩달아 바빠졌다',\n",
              " '어떤 강력한 대책도 투기를 완전히 근절하지 못하고 있다',\n",
              " '공직자와 정치권을 다 털면 투기한 사람들이 수두룩할 겁니다',\n",
              " '최근 한 취재원은 단호하게 말했다',\n",
              " '수화기 너머로 들리는 그의 음성은 확신에 찬 목소리였다',\n",
              " '부동산 투기 의혹은 정치권으로 번지고 있다',\n",
              " '경기 하남시 한 시의원도 땅 투기 의혹을 받는다',\n",
              " '그러나 실제 전수조사가 이뤄질지는 미지수다',\n",
              " '만약 국회의원 전수조사가 실시된다더라도 실효성에는 의문부호가 붙는다',\n",
              " '전형적인 물타기 전략이라는 국민의힘의 주장이 와닿는 이유다',\n",
              " '꼭꼭 숨긴 투기 정황을 찾아내는 것은 어려운 일이기 때문이다',\n",
              " '정치권의 투기 의혹으로 국민의 실망감이 더해지고 있다',\n",
              " '공직사회에 대한 불신이 깊어질 수밖에 없는 현실이다',\n",
              " '공직자의 안일하고 부도덕한 태도에 민심은 격노하고 있다',\n",
              " '국회의원 전원에 대한 전수조사를 한다면 제대로 해야 할 것이다',\n",
              " '국민이 곧이곧대로 믿을 수 있을 정도로 말이다',\n",
              " '” “이번에는 정말 아무 거리낌 없이 야당을 찍을 예정이다',\n",
              " '전국에 생중계된 국민과의 대화’에서다',\n",
              " '문 대통령의 평소 화법과는 다른 확언’이었다',\n",
              " '출발은 책임 추궁이다',\n",
              " '이태원클라쓰 여신강림 경이로운 소문 스위트홈 등이 주인공이다',\n",
              " '이처럼 웹툰 산업은 뚜렷한 성장세를 보이는 분야 중 하나다',\n",
              " '유료 결제 역시 덩달아 증가했다',\n",
              " '웹툰 관련주들에 대한 투자자들의 관심도 높아졌다',\n",
              " '키다리스튜디오 디앤씨미디어 대원미디어 등의 주가가 급등한 것도 같은 시기다',\n",
              " '다만 최근 주춤거리는 흐름이다',\n",
              " '여기 반가운 얼굴이 있다',\n",
              " '” 지난해 봄 방소민 홍은주가 작가로 탄생하는 순간이었다',\n",
              " '드라마 작가는 지망생’이 신인’이 되기까지 문이 매우 좁다',\n",
              " '당선작 없음’이라는 발표를 보면 허탈했다',\n",
              " '이들은 합격 후 두세 달 동안 수상작을 수정했다',\n",
              " '단막극 공모전은 귀한 기회다',\n",
              " '현재 tvN 외 단막극은 KBS 드라마스페셜 JTBC 드라마페스타가 있다',\n",
              " '방 작가는 “당선된 시트콤 분야는 오펜에만 있다',\n",
              " '힘겹게 데뷔전을 치렀지만 이들의 앞날은 아직 물음표다',\n",
              " 'LH 직원과 지인이 공동으로 땅을 사기도 했다',\n",
              " 'LH 직원이 가장 좋은 직업이라는 신 직업등급표까지 등장하기도 했습니다',\n",
              " '하지만 구체적인 방안은 나오지 않았습니다',\n",
              " '변창흠 국토교통부 장관도 LH의 역할을 고민하고 있다고 밝히기도 했습니다',\n",
              " '그는 “대한민국이 공정해지려면 결정하고 집행하는 사람들이 먼저 공정해야 한다',\n",
              " '불신의 불길이 점점 퍼지고 있습니다',\n",
              " '” “LH뿐 아니라 가덕도 땅 주인들도 조사해야 합니다',\n",
              " '” “투기가 LH만의 문제라는 보장은 없다',\n",
              " '가덕도 땅도 조사해야 한다',\n",
              " '가덕도도 외지인 땅이 그렇게 많다던데 의심해봐야 한다',\n",
              " '눈물 난다”고 적었다',\n",
              " '“LH가 신의 직장이었다”며 비꼬는 반응들도 있다',\n",
              " 'LH 직원이 가장 좋은 직업이라는 신 직업등급표까지 등장하기도 했습니다',\n",
              " '하지만 구체적인 방안은 나오지 않았습니다',\n",
              " '변창흠 국토교통부 장관도 LH의 역할을 고민하고 있다고 밝히기도 했습니다',\n",
              " '자물쇠로 친 울타리 안쪽은 LH광명시흥사업본부입니다',\n",
              " '평생 이곳에 터 잡고 살았던 주민들은 화가 난다고 말합니다',\n",
              " '공공기관의 말이라면 이젠 더 이상 믿어지지가 않습니다',\n",
              " '양은정  경기 광명시 허탈함도 느꼈고요',\n",
              " '개발계획이 엎어질까 걱정이 되는데 어디 하소연할 곳도 없습니다',\n",
              " '김세정  시흥광명신도시대책위 사무국장 주민의 한 사람으로서 굉장한 분노를 느낍니다',\n",
              " '한편으론 신도시 추진 일정이 흔들리까봐 걱정도 하고 있습니다',\n",
              " '국토부 직원 중에선 한 명도 나오지 않았다',\n",
              " '예상보다 턱없이 적은 수치로 보이기 때문이다',\n",
              " '당초 국민들은 투기 실상이 속시원히 드러날 것으로 생각했었다',\n",
              " '그런데 뚜껑을 열어보니 태산명동 서일필泰山鳴動 鼠一匹이었다',\n",
              " '이에 따라 실효성도 없는 뒷북 조사라는 비판이 들끓고 있다',\n",
              " '사태의 실체가 극히 일부만 드러났을 뿐이라는 반응이 지배적이다',\n",
              " '국민들은 분노를 넘어 허탈감마저 느낀다',\n",
              " '향후 합동조사단의 추가조사 결과를 기다려봐야 하겠지만 이마저도 믿음이 가지않는다',\n",
              " '처음부터 한계가 뚜렷한 조사였다',\n",
              " '약속이 실현되려면 국가의 수사역량이 총동원된 전방위적 강제 수사가 필요하다',\n",
              " '공허해 보이는 조사로는 성난 민심을 수습할 수 없다',\n",
              " '집권 말기 레임덕만 가중될 뿐이다',\n",
              " '민심이 천심임을 정부와 여당은 명심해야 한다',\n",
              " '워시는 드러켄밀러의 자문 담당이다',\n",
              " '애크먼이 쿠팡에 언제 얼마를 투자했는지는 알려지지 않았다',\n",
              " '그의 주거지에선 유서 등이 발견됐다',\n",
              " '다만 일부 상황에 대한 예외를 적용한다',\n",
              " '마켓컬리 마켓컬리를 운영하는 컬리가 IPO기업공개를 추진한다',\n",
              " '뉴욕 증시 상장 가능성도 열어뒀다',\n",
              " '워시는 드러켄밀러의 자문 담당이다',\n",
              " '애크먼이 쿠팡에 언제 얼마를 투자했는지는 알려지지 않았다',\n",
              " 'NYSE에서 CPNG라는 종목 코드로 거래된다',\n",
              " '하지만 성장 가능성에는 물음표를 던지는 시각들은 늘고 있네요',\n",
              " '영국 파이낸셜타임스의 지적인데요',\n",
              " '이렇게 힘든 일이라서 그런 걸까요',\n",
              " '최신원 SK네트웍스 회장에 대한 얘기입니다',\n",
              " '이 때문에 SKC와 SK네트웍스 주식매매는 정지되기도 했습니다',\n",
              " '회사에 큰 부담이 아닐 수 없는데요',\n",
              " '고객사인 현대차와는 달리 경쟁사인 SK이노베이션과는 치킨게임 중입니다',\n",
              " '조카의 경영권 도전에 대해 박찬구 금호석유화학 회장이 반격카드를 내놨습니다',\n",
              " '조카의 안보다는 밑돌긴 하지만 주주배당에도 신경쓴 흔적이 보입니다',\n",
              " '이사진 교체에 있어서는 요즘 화두인 ESG가 고려됐습니다',\n",
              " '물가상승률보다 임금이 오르지 않는다 요즘 젊은 세대들의 불만입니다',\n",
              " '이번주 CEO 풍향계는 여기까지입니다',\n",
              " '디지털데일리가 퇴근 즈음해서 읽을 수 있는 DD퇴근길 코너를 마련했습니다',\n",
              " '혹시 오늘 디지털데일리 기사를 놓치지는 않으셨나요',\n",
              " '이커머스 기업 쿠팡이 뉴욕 증시에 화려하게 입성했습니다',\n",
              " '비트코인이 에너지 딜레마에 직면했습니다',\n",
              " '채굴은 거래보다 심각하다는 지적이 나옵니다',\n",
              " '이는 아르헨티나의 연간 전력 사용량보다 많은 것입니다',\n",
              " '가장 적극적인 곳은 채굴 업체들입니다',\n",
              " '“가장 저렴하게 재생에너지를 쓸 수 있기 때문”이라는 설명입니다',\n",
              " '기존 기업들도 예외는 아닙니다',\n",
              " '애플의 모뎀칩은 대만 TSMC가 생산할 가능성이 높다고 합니다',\n",
              " '센터 설립으로 반도체 설계 독립은 더욱 빨라질 전망입니다',\n",
              " '이러한 자급제 단말 수요 상당수는 알뜰폰으로 유입되고 있는데요',\n",
              " '다음주 주요 일정 먼저 살펴볼까요',\n",
              " 'FOMC 회의를 기점으로 금리를 둘러싼 시장의 불안심리가 진정될지가 관건입니다',\n",
              " '총량에는 변화없이 채권 매입 규모를 일시적으로 늘리겠다는 겁니다',\n",
              " '미 연준 입장에서는 고민이 되는 대목입니다',\n",
              " '당장의 관심은 연준의 향후 경기전망입니다',\n",
              " '앵커 이베이코리아 매각을 위한 예비입찰이 다음주에 있습니다',\n",
              " '시기적으로 쿠팡 상장 등과 맞물려 관심을 받고 있습니다',\n",
              " '또 SK바이오사이언스도 내주 코스피에 상장합니다',\n",
              " '상장 이후 어떤 주가 흐름을 보일지 관심입니다',\n",
              " '이에 따라 한국 기업들의 미국 증시 상장 도미노가 시작됐다',\n",
              " '이후 차익 실현 물량이 나오면서 상승폭을 일부 반납했다',\n",
              " '유통시장 점유율을 확대하는 데 공격적으로 나서겠다는 얘기다',\n",
              " '그러면서 “공격적이고 계획적으로 투자를 지속해야 한다”고 목소리를 높였다',\n",
              " '촘촘한 전국 당일배송망을 구축하려면 더 많은 기지가 필요하다는 이유에서다',\n",
              " '쿠팡 임직원도 상당한 차익을 거둘 전망이다',\n",
              " '앵커 첫번째 소식 가계대출과 관련된 소식 준비하셨네요',\n",
              " '어제 한국은행이 국회에 제출하는 통화신용정책보고서가 공개됐는데요',\n",
              " '때문에 정부가 이달안에 가계대출 관리 방안을 내놓겠다고 했는데요',\n",
              " '이때 이베이코리아 인수전에 참여할 기업들의 윤곽이 드러날 것으로 보입니다',\n",
              " '앵커 매각가가 가장 관심일텐데 얼마정도가 될까요',\n",
              " '하지만 가격에 대해서는 논란이 많습니다',\n",
              " '앵커 이베이코리아 매각을 위한 예비입찰이 다음주에 있습니다',\n",
              " '시기적으로 쿠팡 상장 등과 맞물려 관심을 받고 있습니다',\n",
              " '어떤 결과가 나올지 관심입니다',\n",
              " '앵커 이어서 다음주 주요 증시 일정도 살펴볼까요',\n",
              " 'FOMC 회의를 기점으로 금리를 둘러싼 시장의 불안심리가 진정될지가 관건입니다',\n",
              " '총량에는 변화없이 채권 매입 규모를 일시적으로 늘리겠다는 겁니다',\n",
              " '미 연준 입장에서는 고민이 되는 대목입니다',\n",
              " '당장의 관심은 연준의 향후 경기전망입니다',\n",
              " '그는 한국인들의 창의성이 한강의 기적을 만들었다',\n",
              " '매물로 나온 요기요 인수 의향이 있는지에 대한 답이었다',\n",
              " '김 의장은 한국 전자상거래 시장 규모가 절대로 작지 않다',\n",
              " '나스닥이 아닌 NYSE를 택한 이유에 대해서도 설명했다',\n",
              " '주요 외신들도 쿠팡의 뉴욕 증시 입성에 지대한 관심을 보였다',\n",
              " '손정의 회장이 이끄는 일본 소프트뱅크그룹도 상당한 투자이익을 올리게 됐다',\n",
              " '우선 IPO 기업에 투자하는 대표적인 ETF는 르네상스 IPO ETF다',\n",
              " '해운사의 대응 방안은 친환경 연료로 작동하는 선박의 도입이다',\n",
              " '관련 투자도 늘리고 있다',\n",
              " '삼성중공업도 개발 과정에 파트너사를 늘리고 있다',\n",
              " '차세대 연료 선박에는 고도화된 기술력이 필요하다',\n",
              " '암모니아와 메탄올 추진 유조선도 개발하고 있다',\n",
              " '중국 지앙난조선은 암모니아 추진 초대형 액화석유가스LPG선 개념승인을 받았다',\n",
              " '일본은 친환경 연료 추진선 상용화 작업에 들어갔다',\n",
              " '수주 전망도 밝다',\n",
              " '지구온난화의 주원인은 단연 온실가스 배출 증가다',\n",
              " '이 중에서도 가장 많은 비중을 차지하는 것은 이산화탄소다',\n",
              " '수소는 재생에너지를 이용한 수전해로 생산이 가능하다는 장점이 있다',\n",
              " '이러한 인프라는 연료의 가용성을 확보하는 주요한 자원이 된다',\n",
              " '이 회사의 주력 상품은 창상피복제습윤드레싱다',\n",
              " '국내 최초로 자가점착성 보더 제품을 국산화한 바 있다',\n",
              " '바이오전자서명인증 서비스의 사업영역 확장 후 IPO를 재추진한다는 계획이다',\n",
              " '대표주관사인 신한금융투자와 논의한 결과로 향후 상장을 재추진한다는 방침이다',\n",
              " '최근 국제CC인증 및 국가정보원 암호화모듈 인증KCMVP을 취득한 바 있다',\n",
              " '보험사 저축은행 중앙회에 해당 서비스를 공급하며 상용화에 성공했다',\n",
              " '한국투자증권이 상장을 주관하고 있다',\n",
              " 'SK텔레콤과 우버 연합군도 플랫폼 대결을 위한 실탄 마련에 나섰다',\n",
              " '양사의 서비스는 당분간 별도로 운영하지만 하나의 플랫폼으로 통합될 전망이다',\n",
              " '장기적으론 차량에 그치지 않고 드론을 통한 플라잉카로도 확장할 예정이다',\n",
              " '쿠팡 성공에 자극받은 경쟁사 마켓컬리도 상장을 추진한다',\n",
              " '쿠팡의 이번 미국 IPO는 대성공이다',\n",
              " '기존에 상장설’을 부인해온 마켓컬리가 전략을 바꾼 것으로 보인다',\n",
              " '이를 위해 쿠팡은 다양한 모바일 어플리케이션을 지속 선보이고 있다',\n",
              " '쿠팡은 전국 주요 지점에 통합 물류센터를 마련할 방침이다',\n",
              " '쿠팡이 구상하고 있는 풀필먼트통합물류대행 사업도 더 확대될 것으로 예상된다',\n",
              " '아마존은 이 풀필먼트 사업으로 큰 성장을 이뤘다',\n",
              " '로켓와우 멤버십을 강화하는 마케팅 비용을 늘릴 가능성도 없지 않다',\n",
              " '로켓와우는 쿠팡의 유료 멤버십 서비스다',\n",
              " '콘텐츠 시장에서도 투자 확대가 예상된다',\n",
              " '주 분야는 스마트 물류 쪽이 될 것으로 보인다',\n",
              " '아마존 바라기답게 AWS 같은 클라우드 서비스로 진출할 가능성도 있다',\n",
              " '늘 유동성 압박에 시달려 왔기 때문이다',\n",
              " '그래서 외부 인수보다는 내부 신사업 론칭을 우선순위로 진행해왔다',\n",
              " '아마존은 홀푸드마켓을 인수하면서 오프라인 경쟁력을 강화한 바 있다',\n",
              " '시큐센이 코스닥 이전상장 예비심사 청구를 철회했다',\n",
              " '바이오전자서명 사업을 확장해 재추진할 계획이다',\n",
              " '이번에 대표 주관사인 신한금융투자와 논의를 거쳐 예비심사 청구를 철회했다',\n",
              " '바이오 전자서명·인증 서비스 사업영역 확장 후 재추진한다는 계획이다',\n",
              " '최근 대형 보험사와 저축은행 중앙회에 공급되면서 상용화됐다',\n",
              " '쿠팡은 이날 화려한 미 증시 데뷔전을 치렀다',\n",
              " '김 대표는 투자은행 골드만삭스 출신이다',\n",
              " '이는 미국과 영국 중국 일본보다 앞선 수치다',\n",
              " '뉴욕증시 상장을 포기하지 않겠다는 얘기다',\n",
              " '바이오전자서명인증 서비스의 사업영역 확장 후 이전상장을 재추진한다는 계획이다',\n",
              " '특히 국내 스타트업의 해외진출을 지원하는 글로벌펀드의 예산 확대를 건의했다',\n",
              " '모태펀드를 운용하는 한국벤처투자에 대한 불만도 나왔다',\n",
              " '이번 쿠팡의 기업공개IPO는 미국 외 기업으로 알리바바 이후 최대규모다',\n",
              " '알리바바 이후 미국에 상장한 최대 규모 외국 기업이 됐다',\n",
              " '소프트뱅크그룹과 우버테크놀로지스·도요타자동차 등이 투자가로 참여하고 있다',\n",
              " '바이오전자서명인증 서비스의 사업영역 확장 후 이전상장을 재추진한다는 계획이다',\n",
              " 'HK이노엔은 내달 한국거래소에 상장예비심사 청구서를 제출할 계획입니다',\n",
              " '티몬도 하반기 국내 상장을 추진 중이다',\n",
              " '신세계그룹의 SSG닷컴도 상장 가능성이 있는 업체로 거론된다',\n",
              " '에셋플러스 자산운용 회장과 성공적인 투자법 여쭤보겠습니다',\n",
              " '아니면 쿠팡의 굉장한 매력은 뭔가요',\n",
              " '저는 늘 좋은 기업은 당신 곁에 있다',\n",
              " '그게 좋은 회사다',\n",
              " '그런데 쿠팡이 우리 일상을 지배해왔잖아요',\n",
              " '그거 없으면 불편하거든요',\n",
              " '바로 그게 주식이에요',\n",
              " '그런데 주식을 하게 되면 많은 사람이 정보와 비법을 찾잖아요',\n",
              " '지금 사실 다 알아요',\n",
              " '더 이상 사실이 중요한 사회가 아니거든요',\n",
              " '스마트폰 등장하면서 사실은 추가적인 자료가 필요 없다',\n",
              " '그 사실을 의심하고 회사가 상상하는 자기 몫이다',\n",
              " '가치 측면에서 마찬가지예요',\n",
              " '재무제표라는 사실 관계를 증명하는 표가 있거든요',\n",
              " '재무제표에 안 나와 있거든요',\n",
              " '그러니까 그러면 어떤 사실은 누구나 얻을 수 있다고 하셨는데요',\n",
              " '그러면 없는 물건이 있는 다른 유통 회사를 뛰어넘잖아요',\n",
              " '비즈니스 모델을 분석하는 게 지금의 투자의 원칙이라고 저는 믿어요',\n",
              " '그것만 미뤄봐서도 성장도 가치에 굉장히 중요한 요소예요',\n",
              " '그런데 성장 가치에서 가치를 빼버리고 성장 투자다',\n",
              " '자산가치만이 가치 투자라는 것에 대해서는 잘못됐다',\n",
              " '지금 추세대로 보시면 위험한 상황인가요',\n",
              " '더 성장할 수 있나요',\n",
              " '그래서 사실은 어쨌든 가장 가격을 그러니까 시가총액이 가격이잖아요',\n",
              " '그 충격의 와중에 이런 기업들 당연히 충격을 받을 거예요',\n",
              " '빚도 꼭 빚이 나쁜 거 아니거든요',\n",
              " '부동산이든 주식이든 싫어할 때 빚을 지는 건 좋은 빚이다',\n",
              " '지금 빚은 피하는 게 낫겠다',\n",
              " '좋은 돈인 것 같아요',\n",
              " '여러분 돈이죠 자기 현재 직장 다니면 그 돈이 있어요',\n",
              " '오히려 그 돈을 즐겁게 떨어지면 싼 값에 사니까 즐겁잖아요',\n",
              " '나 연금 바꾸겠다',\n",
              " '계속 쌓이게 돼 있잖아요',\n",
              " '그러니까 저는 좋은 기업은 오래 함께 해야한다는 게 생각이거든요',\n",
              " '좋은 기업을 사서 수면제를 먹고 있어라라는 게 수면제론 이거든요',\n",
              " '나쁜 기업을 보고 수면제를 먹어버리면 사약 같은 거예요',\n",
              " '우리 월급 받을 때 연금이 쌓이잖아요',\n",
              " '여러분들은 그 자산이 있다',\n",
              " '그리고 더 좋은 집을 더 싸게 살 기회다',\n",
              " '주식과 펀드를 하지 않고서는 자산 관리가 안 됩니다',\n",
              " '피하고 부동산으로 가요',\n",
              " '그나마 작년에 정말 좋은 투자를 했다',\n",
              " '그러니까 그나마 아주 좋은 결과를 얻었잖아요',\n",
              " '그런데 능력이 사실 매니저라는 게 굉장히 어렵거든요',\n",
              " '매니저조차 인간이기 때문에 오르면 공포스러워 해요',\n",
              " '아니 오르면 더 흥분해요',\n",
              " '방치하는 경우도 적지 않게 봤습니다',\n",
              " '두 번째 좋은 맛집은 주방장 출신의 주인이 있어요',\n",
              " '주방장이 떠나도 맛이 잘 안 바뀌어요',\n",
              " '자이언트스텝 기업공개IPO를 두고 기관들의 관심이 쏟아졌다',\n",
              " '이는 역대 최고치다',\n",
              " '한국투자증권이 상장을 주관하고 있다',\n",
              " '티몬도 하반기 국내 상장을 추진 중이다',\n",
              " '신세계그룹의 SSG닷컴도 상장 가능성이 있는 업체로 거론된다',\n",
              " '전국적으로 집단감염이 이어지고 있는 영향이 큰데요',\n",
              " '그의 주거지에선 유서 등이 발견됐다',\n",
              " '다만 일부 상황에 대한 예외를 적용한다',\n",
              " '마켓컬리 마켓컬리를 운영하는 컬리가 IPO기업공개를 추진한다',\n",
              " '뉴욕 증시 상장 가능성도 열어뒀다',\n",
              " '다만 일부 예외를 허용했다',\n",
              " '직계가족 모임 기준은 강화됐다',\n",
              " '가족 간 감염이 계속 발생하고 있어서다',\n",
              " '일부 시설과 모임에 대한 방역 수칙을 강화했다',\n",
              " '목욕탕에서 세신사와 대화하는 것도 금지된다',\n",
              " '대신 그동안 폐쇄했던 사우나·찜질시설은 운영할 수 있다',\n",
              " '인원 제한은 없었다',\n",
              " '돌잔치 전문점은 방역 수칙이 완화된다',\n",
              " '앞으로는 전문점에서 하는 돌잔치는 결혼·장례식과 같은 행사로 분류된다',\n",
              " '일반 음식점에서 하는 돌잔치는 인정되지 않는다',\n",
              " '시간에 상관없이 영업할 수 있다',\n",
              " '그간 운영이 금지된 수도권 사우나와 찜질방은 다시 문을 엽니다',\n",
              " '다만 위험도를 최소화하기 위해 핵심 방역수칙을 준수해야 한다',\n",
              " '다만 개선 요청이 많았던 일부 방역 수칙은 다소 완화했습니다',\n",
              " '또 최근 주말 이동량이 계속 증가하는 추세도 우려된다고 설명했습니다',\n",
              " '개선 요청이 많았던 일부 방역 기준은 완화됐습니다',\n",
              " '다만 김씨도 현재의 영업제한은 과하다고 지적했다',\n",
              " '전국적으로 집단감염이 이어지고 있는 영향이 큰데요',\n",
              " '지금까지 보도국에서 전해드렸습니다',\n",
              " '한영규 해설위원과 함께 자세히 짚어보겠습니다',\n",
              " '그래서 신규 확진자가 줄길 바랐는데 좀처럼 줄지 않고 있어요',\n",
              " '그래서 소상공인이나 자영업자들은 아무래도 불만이 있고요',\n",
              " '그래서 영업시간이나 인원제한 이런 걸 완화해 주길 바라고 있습니다',\n",
              " '거기다가 좀 더 예외가 추가됐는데요',\n",
              " '좀 더 방역조치를 강화한다고 들었거든요',\n",
              " '한영규 오스트리아에서 백신은 동일한 생산단위가 있습니다',\n",
              " '미국 유래 변이바이러스가 발견됐다고요',\n",
              " '이런 게 다 병행돼야 될 것 같습니다',\n",
              " '한영규 해설위원과 함께했습니다',\n",
              " '소공연은 정부와 지자체의 추가적인 지원도 요구했다',\n",
              " '이에 방역당국은 현재의 사회적 거리두기를 재연장하기로 했다',\n",
              " '이 외의 시설 등은 기존의 방역수칙이 적용된다',\n",
              " '놀라운 건 이지연은 지난해 갓 데뷔한 신예라는 점이다',\n",
              " '그 때 봤던 엘리자벳’이 제 인생 첫 뮤지컬 관극이었어요',\n",
              " '조금 늦은 감이 있지만 데뷔 소감을 들어볼까요',\n",
              " '공연이 시작되고 무대에 오르는데 너무 짜릿했어요',\n",
              " '그래 이 맛에 내가 뮤지컬을 하지’라는 생각이 절로 들더라고요',\n",
              " '그라피티’는 저에게 정말 행운 같은 작품이에요',\n",
              " '현재 공연되고 있는 뮤지컬 검은 사제들’에는 어떻게 참여하게 됐나요',\n",
              " '사실 제작사를 보고 오디션을 지원하게 됐어요',\n",
              " '공연 제작사 알앤디웍스의 작품들을 유독 좋아했거든요',\n",
              " '영신 역으로 오디션을 봤다가 마귀 역할을 맡게 됐다고요',\n",
              " '마귀를 표현하기 위해 가장 고심했던 부분은요',\n",
              " '사실 춤추는 것을 정말 두려워하고 부끄러워했어요',\n",
              " '왜냐하면 대학교 때도 제가 춤추면 친구들이 다 웃었거든요',\n",
              " '관객들에게 마귀의 매력을 어필해보자면요',\n",
              " '원작인 영화와 뮤지컬이 각각 가지고 있는 매력은 무엇일까요',\n",
              " '우선 영화는 디테일이 장점이라고 생각해요',\n",
              " '반면에 무대의 가장 큰 장점은 생동감 아닐까요',\n",
              " '검은 사제들’이 향후 재연을 한다면 또 참여하실 의향은 있으신가요',\n",
              " '그리고 저는 그 때도 마귀를 하고 싶어요',\n",
              " '사실 최부제가 하고 싶기도 한데 현실적으론 쉽지 않으니까요',\n",
              " '앙상블로서 작품에 출연하면서 힘든 점도 있나요',\n",
              " '그 부분이 조금 힘들었는데 열심히 목 관리를 하고 있습니다',\n",
              " '앞으로 꼭 도전하고 싶은 작품이나 캐릭터가 있다면요',\n",
              " '하고 싶은 작품이 정말 많은데요',\n",
              " '뮤지컬 호프’의 과거호프 셜록홈즈’의 왓슨도 해보고 싶어요',\n",
              " '하고 싶은 캐릭터가 정말 많아요',\n",
              " '그리고 아직은 조금 조심스럽지만 아이다’의 아이다를 정말 해보고 싶어요',\n",
              " '말한 대로 다 됐으면 좋겠네요',\n",
              " '뮤지컬 배우로서의 신념도 궁금합니다',\n",
              " '배우로서의 최종 목표는요',\n",
              " '지나치게 다수 인원이 밀집하는 것을 방지하기 위해서다',\n",
              " '코로나 백신 접종 이후 다소 경각심이 무뎌진 것도 사실이다',\n",
              " '모임 주관자를 기준으로 직계가족을 산정한다',\n",
              " '본인과 배우자를 중심으로 부모·조부모와 자녀와 그 배우자·손주 등이다',\n",
              " '수도권의 국공립 카지노 역시 제한적으로 운영이 허용됩니다',\n",
              " '투기 전모를 다 드러내야 한다고 말했다',\n",
              " '유흥시설은 타 업종과의 형평성을 고려해 운영시간 제한을 해제합니다',\n",
              " '추가된 방역수칙 준수를 전제로 사우나·찜질시설은 운영할 수 있다',\n",
              " '국내에 이와 유사한 사례가 신고된 적은 없다',\n",
              " '아스트라제네카 백신 접종을 지속하는 나라도 여럿 있다',\n",
              " '아스트라제네카 백신 임상시험 때도 이상반응으로 혈전색전증이 나오진 않았다',\n",
              " '이렇게 생긴 혈전이 혈관을 돌아다니다 어딘가를 막는 게 혈전색전증이다',\n",
              " '또한 접종자 수에 비해 혈전색전증이 나타난 사례는 극히 적다',\n",
              " '임상적으로 색전증이 크게 드문 질환은 아니라는 얘기다',\n",
              " '검사는 까다롭지 않고 결과도 당일 나온다',\n",
              " '다만 사의를 표명한 건 아니라고 했다',\n",
              " '조사 결과를 보고 물러나야 할 것 같다고 말했다',\n",
              " '결단이 필요하다고 했다',\n",
              " '전국적으로 집단감염이 이어지고 있는 영향이 큰데요',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQDZ-X6iiT1U",
        "outputId": "1b7ded62-3c65-45c1-ad21-4728f9f8b50b"
      },
      "source": [
        "!pip install keybert\n",
        "!pip install sentence-transformers==0.3.0\n",
        "!pip install transformers==3.0.2"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keybert\n",
            "  Downloading https://files.pythonhosted.org/packages/5d/f4/65853bfc2ded495af3341a3f8938f17799d15a65be0150cb57774e1fb59f/keybert-0.2.0.tar.gz\n",
            "Collecting sentence-transformers>=0.3.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/aa/f672ce489063c4ee7a566ebac1b723c53ac0cea19d9e36599cc241d8ed56/sentence-transformers-1.0.4.tar.gz (74kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.7/dist-packages (from keybert) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from keybert) (1.19.5)\n",
            "Collecting transformers<5.0.0,>=3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 41.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (1.8.1+cu101)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 50.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2->keybert) (1.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 47.6MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 44.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (3.10.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers>=0.3.8->keybert) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (3.4.1)\n",
            "Building wheels for collected packages: keybert, sentence-transformers, sacremoses\n",
            "  Building wheel for keybert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keybert: filename=keybert-0.2.0-cp37-none-any.whl size=10599 sha256=168bf189bcc413d120aaf64acaff2ee8911ee147f8e83ca7fcc525b3012445c8\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/d7/16/04bab6677a4dfa9fd8ab2b350bac915d60f5378b83d6f5a372\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-1.0.4-cp37-none-any.whl size=114307 sha256=c9970ff057dd72acc2a17095a1fde99059ff08c331f5b9169730d83e1fd48982\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/ea/89/d0d2e013d951b6d23270aa9ca4018b82632ab7cd933c331316\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=42a2a28853102dc1113bc0e25e4374e4d944a9e114678409e64178bb242a2683\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n",
            "Successfully built keybert sentence-transformers sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers, sentencepiece, sentence-transformers, keybert\n",
            "Successfully installed keybert-0.2.0 sacremoses-0.0.44 sentence-transformers-1.0.4 sentencepiece-0.1.95 tokenizers-0.10.2 transformers-4.5.1\n",
            "Collecting sentence-transformers==0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/23/833e0620753a36cb2f18e2e4a4f72fd8c49c123c3f07744b69f8a592e083/sentence-transformers-0.3.0.tar.gz (61kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: transformers>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.3.0) (4.5.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.3.0) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.3.0) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.3.0) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.3.0) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.3.0) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.3.0) (3.2.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (3.10.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (0.0.44)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (0.10.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.1->sentence-transformers==0.3.0) (3.7.4.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers==0.3.0) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers==0.3.0) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers>=3.0.2->sentence-transformers==0.3.0) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (2020.12.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers>=3.0.2->sentence-transformers==0.3.0) (3.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.0.2->sentence-transformers==0.3.0) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.3.0-cp37-none-any.whl size=86754 sha256=4f2a2259ae45ef6ee8f66ad8d96a37e2bfc7d97d7df73cdf8bf5a4c4050fb3cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/23/85/85d6a9a6c68f0625a1ecdaad903bb0a78df058c10cf74f9de4\n",
            "Successfully built sentence-transformers\n",
            "\u001b[31mERROR: keybert 0.2.0 has requirement sentence-transformers>=0.3.8, but you'll have sentence-transformers 0.3.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: sentence-transformers\n",
            "  Found existing installation: sentence-transformers 1.0.4\n",
            "    Uninstalling sentence-transformers-1.0.4:\n",
            "      Successfully uninstalled sentence-transformers-1.0.4\n",
            "Successfully installed sentence-transformers-0.3.0\n",
            "Collecting transformers==3.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 16.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.1.95)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.0.44)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/59/68c7e3833f535615fb97d33ffcb7b30bbf62bc7477a9c59cd19ad8535d72/tokenizers-0.8.1rc1-cp37-cp37m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 42.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (4.41.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.2) (2.4.7)\n",
            "\u001b[31mERROR: keybert 0.2.0 has requirement sentence-transformers>=0.3.8, but you'll have sentence-transformers 0.3.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Found existing installation: tokenizers 0.10.2\n",
            "    Uninstalling tokenizers-0.10.2:\n",
            "      Successfully uninstalled tokenizers-0.10.2\n",
            "  Found existing installation: transformers 4.5.1\n",
            "    Uninstalling transformers-4.5.1:\n",
            "      Successfully uninstalled transformers-4.5.1\n",
            "Successfully installed tokenizers-0.8.1rc1 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ug1iNzVNQlU"
      },
      "source": [
        "## keyword 추출을 위한 keyBERT 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLXE7zoSieJg",
        "outputId": "f8a1d0b8-8709-41e8-8262-19dd92af7356"
      },
      "source": [
        "from keybert import KeyBERT\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "sentence_model = SentenceTransformer(\"xlm-r-large-en-ko-nli-ststb\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.80G/1.80G [00:59<00:00, 30.2MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7q8egfDPoTU"
      },
      "source": [
        "key_model = KeyBERT(model=sentence_model)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBN_3lM8wrqN"
      },
      "source": [
        "def extract_key(sentence):\n",
        "    #print(s)\n",
        "    top_n= int(len(sentence.split())/3)\n",
        "    keys = key_model.extract_keywords(sentence,top_n=top_n)\n",
        "    x= {}\n",
        "    for k,p in keys:\n",
        "        x[k] = sentence.lower().find(k)\n",
        "        #print(x[k],k)\n",
        "    keys = list(dict(sorted(x.items(), key=lambda item: item[1])).keys())\n",
        "    return keys\n",
        "\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZp15IHCzYTr",
        "outputId": "34a48fbb-948c-47f5-87cd-587cb1a44b8a"
      },
      "source": [
        "i = 200\n",
        "[' '.join(extract_key(c_sentences[i])),c_sentences[i]]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['미래에셋자산운용이 상장지수펀드etf 매각한',\n",
              " '미래에셋자산운용이 최근 호주 상장지수펀드ETF 운용사인 베타쉐어즈BetaShares를 매각한 것으로 알려졌다']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaqtMJycNZqL"
      },
      "source": [
        "## 한국어 tokenizer 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9eKRmmQjr6z"
      },
      "source": [
        "# Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team and Jangwon Park\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\" Tokenization classes for KoBert model.\"\"\"\n",
        "\n",
        "\n",
        "import logging\n",
        "import os\n",
        "import unicodedata\n",
        "from shutil import copyfile\n",
        "\n",
        "from transformers import PreTrainedTokenizer\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "VOCAB_FILES_NAMES = {\"vocab_file\": \"tokenizer_78b3253a26.model\",\n",
        "                     \"vocab_txt\": \"vocab.txt\"}\n",
        "\n",
        "PRETRAINED_VOCAB_FILES_MAP = {\n",
        "    \"vocab_file\": {\n",
        "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/tokenizer_78b3253a26.model\",\n",
        "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/tokenizer_78b3253a26.model\",\n",
        "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/tokenizer_78b3253a26.model\"\n",
        "    },\n",
        "    \"vocab_txt\": {\n",
        "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/vocab.txt\",\n",
        "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/vocab.txt\",\n",
        "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/vocab.txt\"\n",
        "    }\n",
        "}\n",
        "\n",
        "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n",
        "    \"monologg/kobert\": 512,\n",
        "    \"monologg/kobert-lm\": 512,\n",
        "    \"monologg/distilkobert\": 512\n",
        "}\n",
        "\n",
        "PRETRAINED_INIT_CONFIGURATION = {\n",
        "    \"monologg/kobert\": {\"do_lower_case\": False},\n",
        "    \"monologg/kobert-lm\": {\"do_lower_case\": False},\n",
        "    \"monologg/distilkobert\": {\"do_lower_case\": False}\n",
        "}\n",
        "\n",
        "SPIECE_UNDERLINE = u'▁'\n",
        "\n",
        "\n",
        "class KoBertTokenizer(PreTrainedTokenizer):\n",
        "    \"\"\"\n",
        "        SentencePiece based tokenizer. Peculiarities:\n",
        "            - requires `SentencePiece <https://github.com/google/sentencepiece>`_\n",
        "    \"\"\"\n",
        "    vocab_files_names = VOCAB_FILES_NAMES\n",
        "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n",
        "    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n",
        "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            vocab_file,\n",
        "            vocab_txt,\n",
        "            do_lower_case=False,\n",
        "            remove_space=True,\n",
        "            keep_accents=False,\n",
        "            bos_token=\"[SOS]\",\n",
        "            eos_token=\"[EOS]\",\n",
        "            unk_token=\"[UNK]\",\n",
        "            sep_token=\"[SEP]\",\n",
        "            pad_token=\"[PAD]\",\n",
        "            cls_token=\"[CLS]\",\n",
        "            mask_token=\"[MASK]\",\n",
        "            **kwargs):\n",
        "        super().__init__(\n",
        "            bos_token=bos_token,\n",
        "            eos_token=eos_token,\n",
        "            unk_token=unk_token,\n",
        "            sep_token=sep_token,\n",
        "            pad_token=pad_token,\n",
        "            cls_token=cls_token,\n",
        "            mask_token=mask_token,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "        # Build vocab\n",
        "        self.token2idx = dict()\n",
        "        self.idx2token = []\n",
        "        with open(vocab_txt, 'r', encoding='utf-8') as f:\n",
        "            for idx, token in enumerate(f):\n",
        "                token = token.strip()\n",
        "                self.token2idx[token] = idx\n",
        "                self.idx2token.append(token)\n",
        "\n",
        "        try:\n",
        "            import sentencepiece as spm\n",
        "        except ImportError:\n",
        "            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n",
        "                           \"pip install sentencepiece\")\n",
        "\n",
        "        self.do_lower_case = do_lower_case\n",
        "        self.remove_space = remove_space\n",
        "        self.keep_accents = keep_accents\n",
        "        self.vocab_file = vocab_file\n",
        "        self.vocab_txt = vocab_txt\n",
        "\n",
        "        self.sp_model = spm.SentencePieceProcessor()\n",
        "        self.sp_model.Load(vocab_file)\n",
        "\n",
        "        self.token2idx[bos_token] = len(self.token2idx)\n",
        "        self.idx2token.append(bos_token)\n",
        "        self.token2idx[eos_token] = len(self.token2idx)\n",
        "        self.idx2token.append(eos_token)   \n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return len(self.idx2token)\n",
        "\n",
        "    def get_vocab(self):\n",
        "        return dict(self.token2idx, **self.added_tokens_encoder)\n",
        "\n",
        "    def __getstate__(self):\n",
        "        state = self.__dict__.copy()\n",
        "        state[\"sp_model\"] = None\n",
        "        return state\n",
        "\n",
        "    def __setstate__(self, d):\n",
        "        self.__dict__ = d\n",
        "        try:\n",
        "            import sentencepiece as spm\n",
        "        except ImportError:\n",
        "            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n",
        "                           \"pip install sentencepiece\")\n",
        "        self.sp_model = spm.SentencePieceProcessor()\n",
        "        self.sp_model.Load(self.vocab_file)\n",
        "\n",
        "    def preprocess_text(self, inputs):\n",
        "        if self.remove_space:\n",
        "            outputs = \" \".join(inputs.strip().split())\n",
        "        else:\n",
        "            outputs = inputs\n",
        "        outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')\n",
        "\n",
        "        if not self.keep_accents:\n",
        "            outputs = unicodedata.normalize('NFKD', outputs)\n",
        "            outputs = \"\".join([c for c in outputs if not unicodedata.combining(c)])\n",
        "        if self.do_lower_case:\n",
        "            outputs = outputs.lower()\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def _tokenize(self, text, return_unicode=True, sample=False):\n",
        "        \"\"\" Tokenize a string. \"\"\"\n",
        "        text = self.preprocess_text(text)\n",
        "\n",
        "        if not sample:\n",
        "            pieces = self.sp_model.EncodeAsPieces(text)\n",
        "        else:\n",
        "            pieces = self.sp_model.SampleEncodeAsPieces(text, 64, 0.1)\n",
        "        new_pieces = []\n",
        "        for piece in pieces:\n",
        "            if len(piece) > 1 and piece[-1] == str(\",\") and piece[-2].isdigit():\n",
        "                cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))\n",
        "                if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n",
        "                    if len(cur_pieces[0]) == 1:\n",
        "                        cur_pieces = cur_pieces[1:]\n",
        "                    else:\n",
        "                        cur_pieces[0] = cur_pieces[0][1:]\n",
        "                cur_pieces.append(piece[-1])\n",
        "                new_pieces.extend(cur_pieces)\n",
        "            else:\n",
        "                new_pieces.append(piece)\n",
        "\n",
        "        return new_pieces\n",
        "\n",
        "    def _convert_token_to_id(self, token):\n",
        "        \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\n",
        "        return self.token2idx.get(token, self.token2idx[self.unk_token])\n",
        "\n",
        "    def _convert_id_to_token(self, index, return_unicode=True):\n",
        "        \"\"\"Converts an index (integer) in a token (string/unicode) using the vocab.\"\"\"\n",
        "        return self.idx2token[index]\n",
        "\n",
        "    def convert_tokens_to_string(self, tokens):\n",
        "        \"\"\"Converts a sequence of tokens (strings for sub-words) in a single string.\"\"\"\n",
        "        out_string = \"\".join(tokens).replace(SPIECE_UNDERLINE, \" \").strip()\n",
        "        return out_string\n",
        "\n",
        "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
        "        \"\"\"\n",
        "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
        "        by concatenating and adding special tokens.\n",
        "        A KoBERT sequence has the following format:\n",
        "            single sequence: [CLS] X [SEP]\n",
        "            pair of sequences: [CLS] A [SEP] B [SEP]\n",
        "        \"\"\"\n",
        "        if token_ids_1 is None:\n",
        "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "        sep = [self.sep_token_id]\n",
        "        return cls + token_ids_0 + sep + token_ids_1 + sep\n",
        "\n",
        "    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n",
        "        \"\"\"\n",
        "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
        "        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n",
        "        Args:\n",
        "            token_ids_0: list of ids (must not contain special tokens)\n",
        "            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n",
        "                for sequence pairs\n",
        "            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n",
        "                special tokens for the model\n",
        "        Returns:\n",
        "            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\n",
        "        \"\"\"\n",
        "\n",
        "        if already_has_special_tokens:\n",
        "            if token_ids_1 is not None:\n",
        "                raise ValueError(\n",
        "                    \"You should not supply a second sequence if the provided sequence of \"\n",
        "                    \"ids is already formated with special tokens for the model.\"\n",
        "                )\n",
        "            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n",
        "\n",
        "        if token_ids_1 is not None:\n",
        "            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n",
        "        return [1] + ([0] * len(token_ids_0)) + [1]\n",
        "\n",
        "    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n",
        "        \"\"\"\n",
        "        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n",
        "        A KoBERT sequence pair mask has the following format:\n",
        "        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
        "        | first sequence    | second sequence\n",
        "        if token_ids_1 is None, only returns the first portion of the mask (0's).\n",
        "        \"\"\"\n",
        "        sep = [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "        if token_ids_1 is None:\n",
        "            return len(cls + token_ids_0 + sep) * [0]\n",
        "        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n",
        "\n",
        "    def save_vocabulary(self, save_directory):\n",
        "        \"\"\" Save the sentencepiece vocabulary (copy original file) and special tokens file\n",
        "            to a directory.\n",
        "        \"\"\"\n",
        "        if not os.path.isdir(save_directory):\n",
        "            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\n",
        "            return\n",
        "\n",
        "        # 1. Save sentencepiece model\n",
        "        out_vocab_model = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_file\"])\n",
        "\n",
        "        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_model):\n",
        "            copyfile(self.vocab_file, out_vocab_model)\n",
        "\n",
        "        # 2. Save vocab.txt\n",
        "        index = 0\n",
        "        out_vocab_txt = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_txt\"])\n",
        "        with open(out_vocab_txt, \"w\", encoding=\"utf-8\") as writer:\n",
        "            for token, token_index in sorted(self.token2idx.items(), key=lambda kv: kv[1]):\n",
        "                if index != token_index:\n",
        "                    logger.warning(\n",
        "                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n",
        "                        \" Please check that the vocabulary is not corrupted!\".format(out_vocab_txt)\n",
        "                    )\n",
        "                    index = token_index\n",
        "                writer.write(token + \"\\n\")\n",
        "                index += 1\n",
        "\n",
        "        return out_vocab_model, out_vocab_txt\n",
        "\n",
        "    def tokenize_wl(self,sentence,max_length=128):\n",
        "        tokens = ['[SOS]']+self.tokenize(sentence)+['[EOS]']\n",
        "        '''\n",
        "        if len(tokens) < max_length:\n",
        "            tokens += ['[PAD]' for i in range(max_length-len(tokens))]\n",
        "        else:\n",
        "            tokens = tokens[0:max_length]\n",
        "            tokens[max_length-1] = \"[EOS]\"\n",
        "        '''    \n",
        "        #print(len(tokens))\n",
        "        #print(tokens)\n",
        "        return tokens\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0,
          "referenced_widgets": [
            "e843fd1663a54a21bd80fe5059bb5d58",
            "b60044fc002b41fd8d7ba17a3f3904e7",
            "1348576ab1b74187ab52dddd97ee6ee2",
            "dd5d6392d61c46529d067c4f92318416",
            "041502c76fba4b3e9ed0923995955767",
            "c57a951586dc4bcc9154079dcf0b01ef",
            "6dc6507a3e05495e97cb6db0a955d275",
            "7e842d30889745f0bf7b6d07a44714a3",
            "c4981dcb970241e787cda0b571af3251",
            "50acb7070d204a7e8f0a0569c3c62348",
            "b32a8150a0c343dd9801332513bee5d5",
            "d9e110c4095a444cb7d60a1ccd74f7c0",
            "419e5615a8a74c47805ca71940dccb45",
            "c1e24b593c844b43bc206e950dbfa751",
            "8d6d2d722cb549eca29f77907d7aa7a8",
            "f0405a09c88f4b4499d621ee69d834a6"
          ]
        },
        "id": "DkzG9Nraj0Wz",
        "outputId": "594f2057-e3e1-4361-a253-0718b2a5c889"
      },
      "source": [
        "pretraoned_kobert_model_name='monologg/kobert'\n",
        "tokenizer = KoBertTokenizer.from_pretrained(pretraoned_kobert_model_name)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e843fd1663a54a21bd80fe5059bb5d58",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=371391.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c4981dcb970241e787cda0b571af3251",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=77779.0, style=ProgressStyle(descriptio…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPwSs7rsTAiu",
        "outputId": "d390a035-acb0-402d-f0e9-fe90b8fcc223"
      },
      "source": [
        "print(tokenizer.get_vocab())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'[UNK]': 0, '[PAD]': 1, '[CLS]': 2, '[SEP]': 3, '[MASK]': 4, '!': 5, \"!'\": 6, '!”': 7, '\"': 8, '#': 9, '$': 10, '%': 11, '%)': 12, '&': 13, '&#34;': 14, \"'\": 15, \"'(\": 16, \"',\": 17, '(': 18, '(0': 19, '(1': 20, '(10': 21, '(12': 22, '(15': 23, '(17': 24, '(18': 25, '(19': 26, '(2': 27, '(20': 28, '(23': 29, '(24': 30, '(25': 31, '(3': 32, '(4': 33, '(5': 34, '(6': 35, '(7': 36, '(8': 37, '(9': 38, '(?)': 39, ')': 40, \")'\": 41, '),': 42, ')’': 43, '*': 44, '+': 45, ',': 46, '-': 47, '----------------': 48, '-1': 49, '-2': 50, '-20': 51, '-3': 52, '-4': 53, '.': 54, '...': 55, '...\"': 56, \"...'\": 57, '...”': 58, '/': 59, '0': 60, '0%': 61, '0%)': 62, '0.0': 63, '0.00': 64, '0.1': 65, '0.1%': 66, '0.2%': 67, '0.3': 68, '0.3%': 69, '0.4%': 70, '0.5': 71, '0.5%': 72, '0.6': 73, '0.6%': 74, '0.7': 75, '0.7%': 76, '0.8': 77, '0.8%': 78, '00': 79, '000.0': 80, '00000': 81, '01': 82, '02': 83, '02-': 84, '03': 85, '04': 86, '05': 87, '06': 88, '07': 89, '08': 90, '09': 91, '0:00:00': 92, '1': 93, '1%': 94, '1%)': 95, '1)': 96, '1,000': 97, '1.3%': 98, '1.4%': 99, '1.5%': 100, '1.6': 101, '1.6%': 102, '1.7%': 103, '1.8': 104, '10': 105, '100': 106, '1000': 107, '11': 108, '12': 109, '13': 110, '14': 111, '15': 112, '16': 113, '17': 114, '18': 115, '19': 116, '1⁄2': 117, '1⁄4': 118, '2': 119, '2%': 120, '2)': 121, '2.0': 122, '2.3%': 123, '2.5': 124, '2.5%': 125, '2.8%': 126, '20': 127, '200': 128, '2000': 129, '2011': 130, '2012': 131, '2013': 132, '21': 133, '22': 134, '23': 135, '24': 136, '25': 137, '26': 138, '27': 139, '28': 140, '29': 141, '3': 142, '3%': 143, '30': 144, '300': 145, '3000': 146, '31': 147, '32': 148, '33': 149, '34': 150, '35': 151, '36': 152, '37': 153, '38': 154, '39': 155, '3⁄4': 156, '4': 157, '4%': 158, '4%)': 159, '40': 160, '400': 161, '4000': 162, '41': 163, '42': 164, '43': 165, '44': 166, '45': 167, '46': 168, '47': 169, '48': 170, '49': 171, '5': 172, '5%': 173, '5%)': 174, '5,000': 175, '50': 176, '500': 177, '5000': 178, '51': 179, '52': 180, '53': 181, '54': 182, '55': 183, '56': 184, '57': 185, '58': 186, '59': 187, '6': 188, '6%': 189, '60': 190, '600': 191, '6000': 192, '61': 193, '62': 194, '63': 195, '64': 196, '65': 197, '66': 198, '67': 199, '68': 200, '69': 201, '7': 202, '7%': 203, '7%)': 204, '7.0': 205, '7.5%': 206, '70': 207, '700': 208, '7000': 209, '71': 210, '72': 211, '73': 212, '74': 213, '75': 214, '76': 215, '77': 216, '78': 217, '79': 218, '8': 219, '8%': 220, '8%)': 221, '80': 222, '800': 223, '8000': 224, '81': 225, '82': 226, '83': 227, '84': 228, '85': 229, '86': 230, '87': 231, '88': 232, '89': 233, '9': 234, '9%': 235, '9%)': 236, '90': 237, '900': 238, '9000': 239, '91': 240, '92': 241, '93': 242, '94': 243, '95': 244, '96': 245, '97': 246, '98': 247, '99': 248, ':': 249, '://': 250, ':00': 251, ';': 252, '<': 253, '=': 254, '=\"\"': 255, '=\"\">': 256, '>': 257, '?': 258, '?\"': 259, '??': 260, '???': 261, '????': 262, '?”': 263, 'A': 264, 'AM': 265, 'AP': 266, 'AR': 267, 'AS': 268, 'AT': 269, 'B': 270, 'BC': 271, 'BO': 272, 'BS': 273, 'C': 274, 'CC': 275, 'CD': 276, 'CI': 277, 'D': 278, 'DB': 279, 'DC': 280, 'DI': 281, 'E': 282, 'ER': 283, 'EU': 284, 'F': 285, 'FC': 286, 'FI': 287, 'FIFA': 288, 'FTA': 289, 'G': 290, 'GB': 291, 'GDP': 292, 'GM': 293, 'H': 294, 'HD': 295, 'I': 296, 'IA': 297, 'IB': 298, 'IC': 299, 'II': 300, 'IM': 301, 'IN': 302, 'IP': 303, 'IS': 304, 'IT': 305, 'J': 306, 'K': 307, 'KB': 308, 'KBS': 309, 'KT': 310, 'L': 311, 'LED': 312, 'LG': 313, 'LPGA': 314, 'LS': 315, 'M': 316, 'MBC': 317, 'MBN': 318, 'MC': 319, 'MOU': 320, 'MS': 321, 'N': 322, 'NA': 323, 'NE': 324, 'NLL': 325, 'NS': 326, 'NTSB': 327, 'New': 328, 'O': 329, 'OC': 330, 'OS': 331, 'OSEN': 332, 'P': 333, 'PC': 334, 'PD': 335, 'PGA': 336, 'PI': 337, 'PM': 338, 'POP': 339, 'PS': 340, 'Q': 341, 'R': 342, 'S': 343, 'SBS': 344, 'SI': 345, 'SK': 346, 'SNS': 347, 'SP': 348, 'SS': 349, 'ST': 350, 'T': 351, 'TI': 352, 'TS': 353, 'TV': 354, 'The': 355, 'U': 356, 'V': 357, 'W': 358, 'X': 359, 'Y': 360, 'Z': 361, '[': 362, ']': 363, '^': 364, '_': 365, '`': 366, 'a': 367, 'ab': 368, 'ac': 369, 'ad': 370, 'al': 371, 'all': 372, 'am': 373, 'an': 374, 'ar': 375, 'as': 376, 'at': 377, 'ation': 378, 'ay': 379, 'b': 380, 'bp': 381, 'c': 382, 'ch': 383, 'cm': 384, 'co': 385, 'com': 386, 'ct': 387, 'd': 388, 'e': 389, 'ed': 390, 'el': 391, 'en': 392, 'ent': 393, 'er': 394, 'es': 395, 'est': 396, 'et': 397, 'f': 398, 'g': 399, 'go': 400, 'h': 401, 'ha': 402, 'ho': 403, 'http': 404, 'i': 405, 'ic': 406, 'id': 407, 'il': 408, 'in': 409, 'ing': 410, 'ir': 411, 'is': 412, 'it': 413, 'j': 414, 'k': 415, 'kW': 416, 'kg': 417, 'km': 418, 'kr': 419, 'l': 420, 'le': 421, 'lo': 422, 'm': 423, 'mm': 424, 'n': 425, 'net': 426, 'o': 427, 'ol': 428, 'on': 429, 'or': 430, 'ow': 431, 'p': 432, 'q': 433, 'quot': 434, 'r': 435, 'ra': 436, 're': 437, 'ri': 438, 'ro': 439, 's': 440, 'st': 441, 't': 442, 'ter': 443, 'th': 444, 'tion': 445, 'u': 446, 'ul': 447, 'um': 448, 'un': 449, 'ur': 450, 'us': 451, 'ut': 452, 'v': 453, 'ver': 454, 'w': 455, 'www': 456, 'x': 457, 'y': 458, 'z': 459, '{': 460, '|': 461, '}': 462, '~': 463, '~1': 464, '~20': 465, '~3': 466, '~5': 467, '~6': 468, '~8': 469, '¡': 470, '¤': 471, '§': 472, '\\xad': 473, '®': 474, '°': 475, '±': 476, '¶': 477, '·': 478, '¿': 479, 'Æ': 480, '×': 481, 'Ø': 482, 'ß': 483, 'æ': 484, '÷': 485, 'ø': 486, '́': 487, '̧': 488, 'μ': 489, 'ᄀ': 490, 'ᄋ': 491, 'ᄏ': 492, 'ᄒ': 493, 'ᅵ': 494, 'ᆞ': 495, '―': 496, '‘': 497, '’': 498, '’(': 499, '’,': 500, '“': 501, '”': 502, '”,': 503, '′': 504, '※': 505, '⁄': 506, '↑': 507, '→': 508, '↓': 509, '↓]': 510, '∼': 511, '─': 512, '───': 513, '│': 514, '├': 515, '┼': 516, '▁': 517, '▁\"': 518, '▁&': 519, \"▁'\": 520, \"▁'2013\": 521, '▁(': 522, '▁*': 523, '▁-': 524, '▁/': 525, '▁0': 526, '▁0.2': 527, '▁00:0': 528, '▁1': 529, '▁1%': 530, '▁1,2': 531, '▁1.5': 532, '▁10': 533, '▁10%': 534, '▁100': 535, '▁100%': 536, '▁1000': 537, '▁11': 538, '▁12': 539, '▁120': 540, '▁13': 541, '▁14': 542, '▁15': 543, '▁150': 544, '▁16': 545, '▁17': 546, '▁18': 547, '▁19': 548, '▁1990': 549, '▁1997': 550, '▁1998': 551, '▁1~2': 552, '▁2': 553, '▁20': 554, '▁20%': 555, '▁200': 556, '▁2000': 557, '▁2001': 558, '▁2002': 559, '▁2003': 560, '▁2004': 561, '▁2005': 562, '▁2006': 563, '▁2007': 564, '▁2008': 565, '▁2009': 566, '▁2010': 567, '▁2011': 568, '▁2012': 569, '▁2013': 570, '▁2013-0': 571, '▁2013.0': 572, '▁2013.07.1': 573, '▁2013.07.2': 574, '▁2014': 575, '▁2015': 576, '▁2016': 577, '▁2017': 578, '▁21': 579, '▁22': 580, '▁23': 581, '▁24': 582, '▁25': 583, '▁26': 584, '▁27': 585, '▁28': 586, '▁29': 587, '▁2~3': 588, '▁3': 589, '▁3.0': 590, '▁3.3': 591, '▁30': 592, '▁30%': 593, '▁300': 594, '▁3000': 595, '▁31': 596, '▁32': 597, '▁33': 598, '▁34': 599, '▁35': 600, '▁36': 601, '▁37': 602, '▁38': 603, '▁39': 604, '▁4': 605, '▁40': 606, '▁40%': 607, '▁400': 608, '▁45': 609, '▁48': 610, '▁5': 611, '▁50': 612, '▁50%': 613, '▁500': 614, '▁5000': 615, '▁55': 616, '▁6': 617, '▁60': 618, '▁60%': 619, '▁65': 620, '▁7': 621, '▁70': 622, '▁70%': 623, '▁8': 624, '▁80': 625, '▁80%': 626, '▁9': 627, '▁90': 628, '▁:': 629, '▁<': 630, '▁=': 631, '▁>': 632, '▁?': 633, '▁??': 634, '▁A': 635, '▁APP': 636, '▁B': 637, '▁C': 638, '▁CCTV': 639, '▁CEO': 640, '▁CES': 641, '▁CGV': 642, '▁CJ': 643, '▁D': 644, '▁E': 645, '▁ELS': 646, '▁ETF': 647, '▁F': 648, '▁FA': 649, '▁G': 650, '▁GS': 651, '▁H': 652, '▁HOT': 653, '▁Home': 654, '▁I': 655, '▁IT': 656, '▁J': 657, '▁JTBC': 658, '▁K': 659, '▁KB': 660, '▁KBO': 661, '▁KBS': 662, '▁KDB': 663, '▁KIA': 664, '▁KT': 665, '▁L': 666, '▁LA': 667, '▁LG': 668, '▁LIG': 669, '▁LTE': 670, '▁M': 671, '▁MBC': 672, '▁MC': 673, '▁Mnet': 674, '▁N': 675, '▁NC': 676, '▁NH': 677, '▁NHN': 678, '▁NLL': 679, '▁O': 680, '▁P': 681, '▁PC': 682, '▁PD': 683, '▁Q': 684, '▁QPR': 685, '▁R': 686, '▁S': 687, '▁SBS': 688, '▁SK': 689, '▁SM': 690, '▁SNS': 691, '▁STX': 692, '▁T': 693, '▁TV': 694, '▁U': 695, '▁US': 696, '▁V': 697, '▁VIP': 698, '▁W': 699, '▁XML': 700, '▁YG': 701, '▁[': 702, '▁`': 703, '▁and': 704, '▁c': 705, '▁for': 706, '▁of': 707, '▁the': 708, '▁to': 709, '▁tvN': 710, '▁|': 711, '▁‘': 712, '▁‘2013': 713, '▁“': 714, '▁“‘': 715, '▁│': 716, '▁■': 717, '▁▦': 718, '▁▲': 719, '▁△': 720, '▁▷': 721, '▁◀': 722, '▁◆': 723, '▁◇': 724, '▁【': 725, '▁中': 726, '▁美': 727, '▁가격': 728, '▁가격은': 729, '▁가격이': 730, '▁가계': 731, '▁가계부채': 732, '▁가구': 733, '▁가까운': 734, '▁가까이': 735, '▁가는': 736, '▁가능': 737, '▁가능성': 738, '▁가능성도': 739, '▁가능성을': 740, '▁가능성이': 741, '▁가능하다': 742, '▁가능한': 743, '▁가동': 744, '▁가득': 745, '▁가량': 746, '▁가려': 747, '▁가로': 748, '▁가르': 749, '▁가리': 750, '▁가맹점': 751, '▁가방': 752, '▁가수': 753, '▁가스': 754, '▁가슴': 755, '▁가시': 756, '▁가운데': 757, '▁가입': 758, '▁가입자': 759, '▁가장': 760, '▁가전': 761, '▁가정': 762, '▁가져': 763, '▁가졌다': 764, '▁가족': 765, '▁가족들': 766, '▁가지': 767, '▁가지고': 768, '▁가진': 769, '▁가짜': 770, '▁가치': 771, '▁가치를': 772, '▁각': 773, '▁각각': 774, '▁각자': 775, '▁각종': 776, '▁간': 777, '▁간담회': 778, '▁간부': 779, '▁간사': 780, '▁갈': 781, '▁갈등': 782, '▁갈수록': 783, '▁감': 784, '▁감각': 785, '▁감독': 786, '▁감독님': 787, '▁감독은': 788, '▁감독의': 789, '▁감독이': 790, '▁감동': 791, '▁감면': 792, '▁감사': 793, '▁감사원': 794, '▁감성': 795, '▁감소': 796, '▁감소한': 797, '▁감소했다': 798, '▁감시': 799, '▁감안': 800, '▁감안하면': 801, '▁감염': 802, '▁감정': 803, '▁감추지': 804, '▁감축': 805, '▁갑자기': 806, '▁강': 807, '▁강남': 808, '▁강남구': 809, '▁강력': 810, '▁강력한': 811, '▁강렬한': 812, '▁강릉': 813, '▁강세': 814, '▁강원': 815, '▁강원도': 816, '▁강제': 817, '▁강조': 818, '▁강조했다': 819, '▁강하게': 820, '▁강한': 821, '▁강행': 822, '▁강호동': 823, '▁강화': 824, '▁갖': 825, '▁갖고': 826, '▁갖추': 827, '▁갖춘': 828, '▁갖춰': 829, '▁같': 830, '▁같다': 831, '▁같아': 832, '▁같은': 833, '▁같이': 834, '▁개': 835, '▁개그맨': 836, '▁개념': 837, '▁개막': 838, '▁개발': 839, '▁개방': 840, '▁개별': 841, '▁개봉': 842, '▁개선': 843, '▁개설': 844, '▁개성': 845, '▁개성공단': 846, '▁개인': 847, '▁개인정보': 848, '▁개입': 849, '▁개장': 850, '▁개정': 851, '▁개정안': 852, '▁개척': 853, '▁개최': 854, '▁개최한다': 855, '▁개통': 856, '▁개편': 857, '▁개편안': 858, '▁개혁': 859, '▁객관적': 860, '▁갤럭시': 861, '▁거': 862, '▁거두': 863, '▁거둔': 864, '▁거뒀다': 865, '▁거듭': 866, '▁거래': 867, '▁거래되고': 868, '▁거래량': 869, '▁거론': 870, '▁거리': 871, '▁거부': 872, '▁거의': 873, '▁거절': 874, '▁거주': 875, '▁거짓말': 876, '▁거쳐': 877, '▁거치': 878, '▁거친': 879, '▁걱정': 880, '▁건': 881, '▁건강': 882, '▁건립': 883, '▁건물': 884, '▁건설': 885, '▁건설사': 886, '▁건축': 887, '▁걷': 888, '▁걸': 889, '▁걸그룹': 890, '▁걸린': 891, '▁걸스데이': 892, '▁걸어': 893, '▁걸쳐': 894, '▁검': 895, '▁검거': 896, '▁검사': 897, '▁검색': 898, '▁검증': 899, '▁검찰': 900, '▁검찰에': 901, '▁검찰은': 902, '▁검토': 903, '▁겁니다': 904, '▁것': 905, '▁것과': 906, '▁것도': 907, '▁것에': 908, '▁것으로': 909, '▁것은': 910, '▁것을': 911, '▁것이': 912, '▁것이다': 913, '▁것이라고': 914, '▁것이라는': 915, '▁것이란': 916, '▁것인가': 917, '▁것인지': 918, '▁것입니다': 919, '▁것처럼': 920, '▁게': 921, '▁게다가': 922, '▁게스트': 923, '▁게시물': 924, '▁게시판에': 925, '▁게임': 926, '▁게재': 927, '▁게재했다': 928, '▁겨냥': 929, '▁겨울': 930, '▁격': 931, '▁격려': 932, '▁격차': 933, '▁겪': 934, '▁겪고': 935, '▁견': 936, '▁결': 937, '▁결과': 938, '▁결과를': 939, '▁결국': 940, '▁결론': 941, '▁결별': 942, '▁결승': 943, '▁결심': 944, '▁결정': 945, '▁결정했다': 946, '▁결제': 947, '▁결코': 948, '▁결합': 949, '▁결혼': 950, '▁결혼식': 951, '▁겸': 952, '▁경': 953, '▁경계': 954, '▁경고': 955, '▁경기': 956, '▁경기가': 957, '▁경기도': 958, '▁경기를': 959, '▁경기에서': 960, '▁경기침체': 961, '▁경남': 962, '▁경력': 963, '▁경매': 964, '▁경북': 965, '▁경비': 966, '▁경영': 967, '▁경우': 968, '▁경우가': 969, '▁경쟁': 970, '▁경쟁력': 971, '▁경제': 972, '▁경제성장률': 973, '▁경제적': 974, '▁경찰': 975, '▁경찰관': 976, '▁경찰에': 977, '▁경찰은': 978, '▁경험': 979, '▁계': 980, '▁계기가': 981, '▁계기로': 982, '▁계산': 983, '▁계속': 984, '▁계약': 985, '▁계약을': 986, '▁계열사': 987, '▁계절': 988, '▁계좌': 989, '▁계획': 990, '▁계획을': 991, '▁계획이다': 992, '▁고': 993, '▁고객': 994, '▁고객들': 995, '▁고객에게': 996, '▁고교': 997, '▁고급': 998, '▁고려': 999, '▁고려해': 1000, '▁고령': 1001, '▁고민': 1002, '▁고발': 1003, '▁고백': 1004, '▁고백했다': 1005, '▁고소': 1006, '▁고스란히': 1007, '▁고양': 1008, '▁고영욱': 1009, '▁고용': 1010, '▁고위': 1011, '▁고통': 1012, '▁곡': 1013, '▁곧': 1014, '▁곧바로': 1015, '▁골': 1016, '▁골든': 1017, '▁골키퍼': 1018, '▁골프': 1019, '▁골프장': 1020, '▁곳': 1021, '▁곳이': 1022, '▁공': 1023, '▁공간': 1024, '▁공감': 1025, '▁공개': 1026, '▁공개됐다': 1027, '▁공개된': 1028, '▁공개했다': 1029, '▁공격': 1030, '▁공격수': 1031, '▁공공': 1032, '▁공공기관': 1033, '▁공급': 1034, '▁공단': 1035, '▁공동': 1036, '▁공략': 1037, '▁공모': 1038, '▁공무원': 1039, '▁공방': 1040, '▁공백': 1041, '▁공부': 1042, '▁공사': 1043, '▁공시': 1044, '▁공시했다': 1045, '▁공식': 1046, '▁공약': 1047, '▁공연': 1048, '▁공연을': 1049, '▁공유': 1050, '▁공장': 1051, '▁공정': 1052, '▁공정위': 1053, '▁공포': 1054, '▁공항': 1055, '▁공화당': 1056, '▁과감': 1057, '▁과거': 1058, '▁과도한': 1059, '▁과세': 1060, '▁과시': 1061, '▁과시했다': 1062, '▁과연': 1063, '▁과장': 1064, '▁과정': 1065, '▁과정에서': 1066, '▁과정을': 1067, '▁과제': 1068, '▁과징금': 1069, '▁과태료': 1070, '▁과학': 1071, '▁곽': 1072, '▁관': 1073, '▁관객': 1074, '▁관객들': 1075, '▁관계': 1076, '▁관계자': 1077, '▁관계자는': 1078, '▁관계자들': 1079, '▁관광': 1080, '▁관광객': 1081, '▁관람': 1082, '▁관련': 1083, '▁관련된': 1084, '▁관련사진': 1085, '▁관련한': 1086, '▁관련해': 1087, '▁관리': 1088, '▁관심': 1089, '▁관심을': 1090, '▁관심이': 1091, '▁관중': 1092, '▁관측': 1093, '▁관한': 1094, '▁관행': 1095, '▁광': 1096, '▁광고': 1097, '▁광주': 1098, '▁광주시': 1099, '▁괜찮': 1100, '▁괴': 1101, '▁굉장히': 1102, '▁교': 1103, '▁교과부': 1104, '▁교류': 1105, '▁교사': 1106, '▁교수': 1107, '▁교수는': 1108, '▁교육': 1109, '▁교육부': 1110, '▁교체': 1111, '▁교통': 1112, '▁교통사고': 1113, '▁교환': 1114, '▁구': 1115, '▁구간': 1116, '▁구글': 1117, '▁구단': 1118, '▁구매': 1119, '▁구성': 1120, '▁구성된': 1121, '▁구속': 1122, '▁구속기소': 1123, '▁구입': 1124, '▁구자철': 1125, '▁구조': 1126, '▁구조조정': 1127, '▁구체적으로': 1128, '▁구체적인': 1129, '▁구축': 1130, '▁구현': 1131, '▁국': 1132, '▁국가': 1133, '▁국가기록원': 1134, '▁국가대표': 1135, '▁국가안보': 1136, '▁국가정보원': 1137, '▁국내': 1138, '▁국내에서': 1139, '▁국내외': 1140, '▁국립': 1141, '▁국무총리': 1142, '▁국무회의': 1143, '▁국민': 1144, '▁국민들': 1145, '▁국민연금': 1146, '▁국민은행': 1147, '▁국민의': 1148, '▁국방': 1149, '▁국방부': 1150, '▁국세청': 1151, '▁국정': 1152, '▁국정원': 1153, '▁국정조사': 1154, '▁국제': 1155, '▁국제사회': 1156, '▁국조': 1157, '▁국채': 1158, '▁국토교통부': 1159, '▁국토부': 1160, '▁국토해양부': 1161, '▁국회': 1162, '▁국회에서': 1163, '▁국회의원': 1164, '▁군': 1165, '▁군부': 1166, '▁군사': 1167, '▁굴': 1168, '▁궁금': 1169, '▁권': 1170, '▁권력': 1171, '▁권리': 1172, '▁권한': 1173, '▁귀': 1174, '▁귀국': 1175, '▁귀여운': 1176, '▁귀엽': 1177, '▁규명': 1178, '▁규모': 1179, '▁규모로': 1180, '▁규모의': 1181, '▁규정': 1182, '▁규제': 1183, '▁균형': 1184, '▁그': 1185, '▁그가': 1186, '▁그간': 1187, '▁그것이': 1188, '▁그냥': 1189, '▁그녀': 1190, '▁그는': 1191, '▁그대로': 1192, '▁그동안': 1193, '▁그때': 1194, '▁그래서': 1195, '▁그래픽': 1196, '▁그랬': 1197, '▁그러': 1198, '▁그러나': 1199, '▁그러면서': 1200, '▁그런': 1201, '▁그런데': 1202, '▁그럼에도': 1203, '▁그렇게': 1204, '▁그렇지': 1205, '▁그려': 1206, '▁그려졌다': 1207, '▁그룹': 1208, '▁그리': 1209, '▁그리고': 1210, '▁그린': 1211, '▁그림': 1212, '▁그만큼': 1213, '▁그의': 1214, '▁그쳤다': 1215, '▁극': 1216, '▁극대화': 1217, '▁극복': 1218, '▁극중': 1219, '▁극찬': 1220, '▁근': 1221, '▁근거': 1222, '▁근로': 1223, '▁근로자': 1224, '▁근무': 1225, '▁근본': 1226, '▁근육': 1227, '▁근절': 1228, '▁근처': 1229, '▁근황': 1230, '▁글': 1231, '▁글과': 1232, '▁글로벌': 1233, '▁글을': 1234, '▁금': 1235, '▁금감원': 1236, '▁금리': 1237, '▁금메달': 1238, '▁금액': 1239, '▁금융': 1240, '▁금융감독원': 1241, '▁금융권': 1242, '▁금융기관': 1243, '▁금융당국': 1244, '▁금융위기': 1245, '▁금융투자': 1246, '▁금융회사': 1247, '▁금지': 1248, '▁금품을': 1249, '▁급': 1250, '▁급격': 1251, '▁급등': 1252, '▁급락': 1253, '▁급여': 1254, '▁급증': 1255, '▁긍정적': 1256, '▁긍정적인': 1257, '▁기': 1258, '▁기간': 1259, '▁기계': 1260, '▁기관': 1261, '▁기기': 1262, '▁기념': 1263, '▁기능': 1264, '▁기능을': 1265, '▁기다리': 1266, '▁기대': 1267, '▁기대감': 1268, '▁기대된다': 1269, '▁기대를': 1270, '▁기대하고': 1271, '▁기대한다': 1272, '▁기록': 1273, '▁기록하고': 1274, '▁기록하며': 1275, '▁기록한': 1276, '▁기록했다': 1277, '▁기반': 1278, '▁기반으로': 1279, '▁기본': 1280, '▁기부': 1281, '▁기분': 1282, '▁기사': 1283, '▁기사본문': 1284, '▁기상': 1285, '▁기성용': 1286, '▁기소': 1287, '▁기소된': 1288, '▁기술': 1289, '▁기아차': 1290, '▁기억': 1291, '▁기업': 1292, '▁기업들': 1293, '▁기업의': 1294, '▁기여': 1295, '▁기온': 1296, '▁기울': 1297, '▁기재부': 1298, '▁기조': 1299, '▁기존': 1300, '▁기준': 1301, '▁기준금리': 1302, '▁기준으로': 1303, '▁기초': 1304, '▁기초자산': 1305, '▁기타': 1306, '▁기회': 1307, '▁기회를': 1308, '▁기획': 1309, '▁기획재정부': 1310, '▁긴': 1311, '▁긴급': 1312, '▁긴장': 1313, '▁긴장감': 1314, '▁길': 1315, '▁김': 1316, '▁김기': 1317, '▁김동': 1318, '▁김모': 1319, '▁김민': 1320, '▁김성': 1321, '▁김씨': 1322, '▁김씨는': 1323, '▁김연경': 1324, '▁김연아': 1325, '▁김영': 1326, '▁김용': 1327, '▁김용준': 1328, '▁김재': 1329, '▁김정': 1330, '▁김종': 1331, '▁김종학': 1332, '▁김진': 1333, '▁김태': 1334, '▁김태희': 1335, '▁김한길': 1336, '▁김현': 1337, '▁깊은': 1338, '▁깊이': 1339, '▁깔': 1340, '▁깜짝': 1341, '▁깨': 1342, '▁깨끗': 1343, '▁꺼': 1344, '▁꺾고': 1345, '▁꼬리': 1346, '▁꼭': 1347, '▁꼽았다': 1348, '▁꼽히': 1349, '▁꼽힌다': 1350, '▁꽃': 1351, '▁꽃미남': 1352, '▁꾸': 1353, '▁꾸준한': 1354, '▁꾸준히': 1355, '▁꿈': 1356, '▁꿈꾸': 1357, '▁끈다': 1358, '▁끊임없': 1359, '▁끌고': 1360, '▁끌어': 1361, '▁끌었다': 1362, '▁끝': 1363, '▁끝까지': 1364, '▁끝나': 1365, '▁끝난': 1366, '▁끝내': 1367, '▁끝에': 1368, '▁끼': 1369, '▁나': 1370, '▁나가': 1371, '▁나갈': 1372, '▁나누': 1373, '▁나눠': 1374, '▁나는': 1375, '▁나라': 1376, '▁나란히': 1377, '▁나로호': 1378, '▁나머지': 1379, '▁나쁜': 1380, '▁나서': 1381, '▁나선': 1382, '▁나선다': 1383, '▁나설': 1384, '▁나섰': 1385, '▁나섰다': 1386, '▁나아가': 1387, '▁나오': 1388, '▁나오고': 1389, '▁나오는': 1390, '▁나온': 1391, '▁나온다': 1392, '▁나올': 1393, '▁나와': 1394, '▁나왔': 1395, '▁나왔다': 1396, '▁나이': 1397, '▁나중에': 1398, '▁나타나': 1399, '▁나타난': 1400, '▁나타났다': 1401, '▁나타내': 1402, '▁나타냈다': 1403, '▁낙': 1404, '▁낙찰': 1405, '▁난': 1406, '▁날': 1407, '▁날씨': 1408, '▁남': 1409, '▁남겨': 1410, '▁남겼다': 1411, '▁남긴': 1412, '▁남녀': 1413, '▁남다른': 1414, '▁남부': 1415, '▁남북': 1416, '▁남북관계': 1417, '▁남북정상회담': 1418, '▁남성': 1419, '▁남아': 1420, '▁남아있': 1421, '▁남은': 1422, '▁남자': 1423, '▁남자친구': 1424, '▁남편': 1425, '▁납부': 1426, '▁납치': 1427, '▁납품': 1428, '▁낮': 1429, '▁낮아': 1430, '▁낮은': 1431, '▁낮추': 1432, '▁낳': 1433, '▁내': 1434, '▁내가': 1435, '▁내내': 1436, '▁내년': 1437, '▁내놓': 1438, '▁내놨다': 1439, '▁내다봤다': 1440, '▁내달': 1441, '▁내려': 1442, '▁내렸다': 1443, '▁내리': 1444, '▁내린': 1445, '▁내부': 1446, '▁내수': 1447, '▁내야': 1448, '▁내용': 1449, '▁내용은': 1450, '▁내용을': 1451, '▁내용의': 1452, '▁내용이': 1453, '▁낸': 1454, '▁냈다': 1455, '▁냉': 1456, '▁너': 1457, '▁너무': 1458, '▁넓': 1459, '▁넘': 1460, '▁넘게': 1461, '▁넘겨': 1462, '▁넘기': 1463, '▁넘는': 1464, '▁넘어': 1465, '▁넘치는': 1466, '▁넣': 1467, '▁넣어': 1468, '▁네': 1469, '▁네이버': 1470, '▁네트워크': 1471, '▁네티즌': 1472, '▁네티즌들': 1473, '▁네티즌들은': 1474, '▁넥센': 1475, '▁노': 1476, '▁노동': 1477, '▁노동자': 1478, '▁노래': 1479, '▁노량진': 1480, '▁노력': 1481, '▁노력을': 1482, '▁노력하겠다': 1483, '▁노무현': 1484, '▁노사': 1485, '▁노선': 1486, '▁노인': 1487, '▁노조': 1488, '▁노출': 1489, '▁노트북': 1490, '▁노하우': 1491, '▁노홍철': 1492, '▁노후': 1493, '▁녹': 1494, '▁녹색': 1495, '▁녹음': 1496, '▁녹화': 1497, '▁논': 1498, '▁논란': 1499, '▁논란이': 1500, '▁논리': 1501, '▁논의': 1502, '▁논쟁': 1503, '▁놀': 1504, '▁놀라': 1505, '▁놀라게': 1506, '▁농': 1507, '▁농업': 1508, '▁농촌': 1509, '▁농협': 1510, '▁높': 1511, '▁높게': 1512, '▁높다': 1513, '▁높아': 1514, '▁높아졌다': 1515, '▁높아지고': 1516, '▁높았다': 1517, '▁높여': 1518, '▁높였다': 1519, '▁높은': 1520, '▁높이': 1521, '▁놓': 1522, '▁놓고': 1523, '▁뇌': 1524, '▁뇌물': 1525, '▁누': 1526, '▁누가': 1527, '▁누구': 1528, '▁누구나': 1529, '▁누군가': 1530, '▁누리꾼들': 1531, '▁누리꾼들은': 1532, '▁누적': 1533, '▁누출': 1534, '▁눈': 1535, '▁눈길을': 1536, '▁눈물': 1537, '▁눈물을': 1538, '▁눈빛': 1539, '▁눈에': 1540, '▁뉴': 1541, '▁뉴스': 1542, '▁뉴욕': 1543, '▁느껴': 1544, '▁느꼈': 1545, '▁느끼': 1546, '▁느낀': 1547, '▁느낄': 1548, '▁느낌': 1549, '▁늘': 1550, '▁늘려': 1551, '▁늘리': 1552, '▁늘어': 1553, '▁늘어나': 1554, '▁늘어난': 1555, '▁늘어날': 1556, '▁늘어났다': 1557, '▁늘었다': 1558, '▁능력': 1559, '▁능력을': 1560, '▁늦어': 1561, '▁다': 1562, '▁다녀': 1563, '▁다니': 1564, '▁다룬': 1565, '▁다르다': 1566, '▁다른': 1567, '▁다리': 1568, '▁다만': 1569, '▁다문화': 1570, '▁다섯': 1571, '▁다소': 1572, '▁다수': 1573, '▁다시': 1574, '▁다양한': 1575, '▁다운로드': 1576, '▁다음': 1577, '▁다음날': 1578, '▁다음달': 1579, '▁다이어트': 1580, '▁다저스': 1581, '▁다저스는': 1582, '▁다짐': 1583, '▁다하겠다': 1584, '▁단': 1585, '▁단계': 1586, '▁단기': 1587, '▁단독': 1588, '▁단말기': 1589, '▁단속': 1590, '▁단순': 1591, '▁단장': 1592, '▁단지': 1593, '▁단체': 1594, '▁단축': 1595, '▁단행': 1596, '▁달': 1597, '▁달라': 1598, '▁달러': 1599, '▁달려': 1600, '▁달리': 1601, '▁달성': 1602, '▁달아': 1603, '▁달하는': 1604, '▁달한다': 1605, '▁닮은': 1606, '▁담': 1607, '▁담겨': 1608, '▁담긴': 1609, '▁담당': 1610, '▁담배': 1611, '▁담보': 1612, '▁담아': 1613, '▁담은': 1614, '▁답': 1615, '▁답변': 1616, '▁답했다': 1617, '▁당': 1618, '▁당국': 1619, '▁당기순이익': 1620, '▁당부했다': 1621, '▁당분간': 1622, '▁당사자': 1623, '▁당선': 1624, '▁당선인': 1625, '▁당시': 1626, '▁당연히': 1627, '▁당장': 1628, '▁당첨': 1629, '▁당초': 1630, '▁당했다': 1631, '▁당황': 1632, '▁대': 1633, '▁대거': 1634, '▁대결': 1635, '▁대구': 1636, '▁대규모': 1637, '▁대기': 1638, '▁대기업': 1639, '▁대다수': 1640, '▁대단': 1641, '▁대답': 1642, '▁대리점': 1643, '▁대만': 1644, '▁대법원': 1645, '▁대변인': 1646, '▁대부분': 1647, '▁대북': 1648, '▁대비': 1649, '▁대사': 1650, '▁대상': 1651, '▁대상으로': 1652, '▁대상자': 1653, '▁대선': 1654, '▁대신': 1655, '▁대안': 1656, '▁대외': 1657, '▁대우': 1658, '▁대응': 1659, '▁대전': 1660, '▁대중': 1661, '▁대중교통': 1662, '▁대책': 1663, '▁대처': 1664, '▁대체': 1665, '▁대출': 1666, '▁대통령': 1667, '▁대통령은': 1668, '▁대통령의': 1669, '▁대통령이': 1670, '▁대통령직': 1671, '▁대통령직인수위원회': 1672, '▁대폭': 1673, '▁대표': 1674, '▁대표는': 1675, '▁대표단': 1676, '▁대표이사': 1677, '▁대표적인': 1678, '▁대표팀': 1679, '▁대학': 1680, '▁대학생': 1681, '▁대한': 1682, '▁대한민국': 1683, '▁대한항공': 1684, '▁대해': 1685, '▁대해서': 1686, '▁대해서는': 1687, '▁대해서도': 1688, '▁대해선': 1689, '▁대형': 1690, '▁대형마트': 1691, '▁대화': 1692, '▁대화록': 1693, '▁대회': 1694, '▁대회에서': 1695, '▁댄스': 1696, '▁댓글': 1697, '▁더': 1698, '▁더불어': 1699, '▁더욱': 1700, '▁덕': 1701, '▁덕분에': 1702, '▁던지': 1703, '▁덜': 1704, '▁덧붙였다': 1705, '▁데': 1706, '▁데뷔': 1707, '▁데이터': 1708, '▁데이트': 1709, '▁도내': 1710, '▁도로': 1711, '▁도발': 1712, '▁도시': 1713, '▁도심': 1714, '▁도약': 1715, '▁도와': 1716, '▁도움': 1717, '▁도움을': 1718, '▁도움이': 1719, '▁도입': 1720, '▁도전': 1721, '▁도중': 1722, '▁도착': 1723, '▁도쿄': 1724, '▁독': 1725, '▁독립': 1726, '▁독일': 1727, '▁독자': 1728, '▁독특한': 1729, '▁돈': 1730, '▁돈을': 1731, '▁돌': 1732, '▁돌려': 1733, '▁돌아': 1734, '▁돌아가': 1735, '▁돌아오': 1736, '▁돌아온': 1737, '▁돌입': 1738, '▁돌파': 1739, '▁돕는': 1740, '▁동': 1741, '▁동결': 1742, '▁동기': 1743, '▁동남아': 1744, '▁동료': 1745, '▁동물': 1746, '▁동반': 1747, '▁동반성장': 1748, '▁동부': 1749, '▁동생': 1750, '▁동시에': 1751, '▁동아시안컵': 1752, '▁동아제약': 1753, '▁동안': 1754, '▁동양': 1755, '▁동영상': 1756, '▁동원': 1757, '▁동의': 1758, '▁동작': 1759, '▁동참': 1760, '▁돼': 1761, '▁됐다': 1762, '▁되': 1763, '▁되고': 1764, '▁되는': 1765, '▁되면': 1766, '▁되어': 1767, '▁되지': 1768, '▁되찾': 1769, '▁된': 1770, '▁된다': 1771, '▁될': 1772, '▁두': 1773, '▁두고': 1774, '▁두드러': 1775, '▁두산': 1776, '▁둔': 1777, '▁둔화': 1778, '▁둘': 1779, '▁둘러': 1780, '▁둘러싼': 1781, '▁둘째': 1782, '▁뒤': 1783, '▁뒤늦게': 1784, '▁뒤집': 1785, '▁뒷': 1786, '▁뒷받침': 1787, '▁드': 1788, '▁드라마': 1789, '▁드러나': 1790, '▁드러난': 1791, '▁드러났다': 1792, '▁드러내': 1793, '▁드러낸': 1794, '▁드러냈다': 1795, '▁드레스': 1796, '▁득점': 1797, '▁든다': 1798, '▁듣': 1799, '▁듣고': 1800, '▁들': 1801, '▁들고': 1802, '▁들려': 1803, '▁들어': 1804, '▁들어가': 1805, '▁들어간': 1806, '▁들어갈': 1807, '▁들어갔다': 1808, '▁들어서': 1809, '▁들어오': 1810, '▁들었다': 1811, '▁들여': 1812, '▁듯': 1813, '▁듯한': 1814, '▁등': 1815, '▁등과': 1816, '▁등도': 1817, '▁등록': 1818, '▁등록금': 1819, '▁등에': 1820, '▁등에서': 1821, '▁등으로': 1822, '▁등은': 1823, '▁등을': 1824, '▁등의': 1825, '▁등이': 1826, '▁등이다': 1827, '▁등장': 1828, '▁등판': 1829, '▁디': 1830, '▁디자인': 1831, '▁디지털': 1832, '▁따': 1833, '▁따뜻한': 1834, '▁따라': 1835, '▁따라서': 1836, '▁따로': 1837, '▁따르면': 1838, '▁따른': 1839, '▁딸': 1840, '▁땅': 1841, '▁땅볼': 1842, '▁땅에': 1843, '▁때': 1844, '▁때까지': 1845, '▁때는': 1846, '▁때마다': 1847, '▁때문': 1848, '▁때문에': 1849, '▁때문이다': 1850, '▁때부터': 1851, '▁떠': 1852, '▁떠나': 1853, '▁떠난': 1854, '▁떠오르': 1855, '▁떨': 1856, '▁떨어져': 1857, '▁떨어졌다': 1858, '▁떨어지': 1859, '▁떨어진': 1860, '▁또': 1861, '▁또는': 1862, '▁또다시': 1863, '▁또한': 1864, '▁똑같': 1865, '▁뚜렷': 1866, '▁뛰': 1867, '▁뛰어': 1868, '▁뛰어난': 1869, '▁뛰어넘': 1870, '▁뜨거운': 1871, '▁뜻': 1872, '▁뜻을': 1873, '▁띠고': 1874, '▁라': 1875, '▁라디오': 1876, '▁라이벌': 1877, '▁라이브': 1878, '▁라인업': 1879, '▁랭킹': 1880, '▁러시아': 1881, '▁런던': 1882, '▁런던올림픽': 1883, '▁레': 1884, '▁레드카펫': 1885, '▁레알': 1886, '▁레이': 1887, '▁레전드': 1888, '▁로그인': 1889, '▁로비': 1890, '▁로이킴': 1891, '▁로켓': 1892, '▁롯데': 1893, '▁롯데마트': 1894, '▁루': 1895, '▁루머': 1896, '▁류': 1897, '▁류현진': 1898, '▁류현진은': 1899, '▁리': 1900, '▁리그': 1901, '▁리더': 1902, '▁리더십': 1903, '▁리베이트': 1904, '▁리스크': 1905, '▁리포트': 1906, '▁마': 1907, '▁마감': 1908, '▁마감했다': 1909, '▁마드리드': 1910, '▁마련': 1911, '▁마련했다': 1912, '▁마리': 1913, '▁마무리': 1914, '▁마운드': 1915, '▁마을': 1916, '▁마음': 1917, '▁마음을': 1918, '▁마지막': 1919, '▁마지막으로': 1920, '▁마찬가지': 1921, '▁마쳤다': 1922, '▁마치': 1923, '▁마치고': 1924, '▁마친': 1925, '▁마케팅': 1926, '▁막': 1927, '▁막기': 1928, '▁막아': 1929, '▁막판': 1930, '▁만': 1931, '▁만기': 1932, '▁만나': 1933, '▁만난': 1934, '▁만날': 1935, '▁만남': 1936, '▁만났다': 1937, '▁만드는': 1938, '▁만든': 1939, '▁만들': 1940, '▁만들고': 1941, '▁만들기': 1942, '▁만들어': 1943, '▁만들었다': 1944, '▁만약': 1945, '▁만에': 1946, '▁만족': 1947, '▁만큼': 1948, '▁만한': 1949, '▁많고': 1950, '▁많다': 1951, '▁많아': 1952, '▁많았': 1953, '▁많았다': 1954, '▁많은': 1955, '▁많이': 1956, '▁많지': 1957, '▁말': 1958, '▁말까지': 1959, '▁말레이시아': 1960, '▁말씀': 1961, '▁말을': 1962, '▁말이': 1963, '▁말한다': 1964, '▁말해': 1965, '▁말했다': 1966, '▁맛': 1967, '▁망': 1968, '▁망명': 1969, '▁맞': 1970, '▁맞는': 1971, '▁맞대결': 1972, '▁맞서': 1973, '▁맞아': 1974, '▁맞았다': 1975, '▁맞추': 1976, '▁맞춘': 1977, '▁맞춤형': 1978, '▁맞춰': 1979, '▁맡': 1980, '▁맡고': 1981, '▁맡아': 1982, '▁맡았': 1983, '▁맡았다': 1984, '▁맡은': 1985, '▁매': 1986, '▁매각': 1987, '▁매년': 1988, '▁매달': 1989, '▁매도': 1990, '▁매력': 1991, '▁매력을': 1992, '▁매매': 1993, '▁매수': 1994, '▁매우': 1995, '▁매일': 1996, '▁매입': 1997, '▁매장': 1998, '▁매주': 1999, '▁매체': 2000, '▁매출': 2001, '▁매출액': 2002, '▁매출이': 2003, '▁맨': 2004, '▁맨유': 2005, '▁맺': 2006, '▁머': 2007, '▁머리': 2008, '▁머물': 2009, '▁먹': 2010, '▁먹고': 2011, '▁먼저': 2012, '▁멀티': 2013, '▁멀티미디어': 2014, '▁멋진': 2015, '▁메': 2016, '▁메뉴': 2017, '▁메시지': 2018, '▁메시지를': 2019, '▁메이저': 2020, '▁메이저리그': 2021, '▁메이크업': 2022, '▁메인': 2023, '▁멕시코': 2024, '▁멘토': 2025, '▁멜로': 2026, '▁멤버': 2027, '▁멤버들': 2028, '▁면': 2029, '▁면담': 2030, '▁면모를': 2031, '▁면접': 2032, '▁면제': 2033, '▁명': 2034, '▁명단': 2035, '▁명령': 2036, '▁명예': 2037, '▁명의': 2038, '▁명이': 2039, '▁명칭': 2040, '▁명품': 2041, '▁명확': 2042, '▁몇': 2043, '▁모': 2044, '▁모니터링': 2045, '▁모델': 2046, '▁모두': 2047, '▁모든': 2048, '▁모르': 2049, '▁모르겠다': 2050, '▁모른다': 2051, '▁모멘텀': 2052, '▁모바일': 2053, '▁모비스': 2054, '▁모색': 2055, '▁모습': 2056, '▁모습으로': 2057, '▁모습을': 2058, '▁모습이': 2059, '▁모습이다': 2060, '▁모아': 2061, '▁모았다': 2062, '▁모양': 2063, '▁모여': 2064, '▁모으고': 2065, '▁모자': 2066, '▁모집': 2067, '▁목': 2068, '▁목격': 2069, '▁목동': 2070, '▁목록': 2071, '▁목사': 2072, '▁목소리': 2073, '▁목소리가': 2074, '▁목숨': 2075, '▁목적': 2076, '▁목적으로': 2077, '▁목표': 2078, '▁목표로': 2079, '▁목표주가': 2080, '▁몰': 2081, '▁몰려': 2082, '▁몰아': 2083, '▁몸': 2084, '▁몸매': 2085, '▁못': 2086, '▁못하고': 2087, '▁못하는': 2088, '▁못한': 2089, '▁못한다': 2090, '▁못할': 2091, '▁못해': 2092, '▁못했다': 2093, '▁묘': 2094, '▁무': 2095, '▁무게': 2096, '▁무기': 2097, '▁무대': 2098, '▁무대를': 2099, '▁무대에': 2100, '▁무더위': 2101, '▁무려': 2102, '▁무료': 2103, '▁무료로': 2104, '▁무르시': 2105, '▁무릎': 2106, '▁무리': 2107, '▁무산': 2108, '▁무상': 2109, '▁무슨': 2110, '▁무승부': 2111, '▁무실점': 2112, '▁무엇보다': 2113, '▁무역': 2114, '▁무이자': 2115, '▁무제한': 2116, '▁무조건': 2117, '▁무죄': 2118, '▁묶': 2119, '▁문': 2120, '▁문서': 2121, '▁문의': 2122, '▁문자': 2123, '▁문재인': 2124, '▁문제': 2125, '▁문제가': 2126, '▁문제는': 2127, '▁문제로': 2128, '▁문제를': 2129, '▁문제에': 2130, '▁문제점': 2131, '▁문화': 2132, '▁문화체육관광부': 2133, '▁묻': 2134, '▁물': 2135, '▁물가': 2136, '▁물건': 2137, '▁물놀이': 2138, '▁물량': 2139, '▁물론': 2140, '▁물류': 2141, '▁물리': 2142, '▁물어': 2143, '▁물품': 2144, '▁뭐': 2145, '▁뭔가': 2146, '▁뮤지컬': 2147, '▁뮤직비디오': 2148, '▁미': 2149, '▁미국': 2150, '▁미국의': 2151, '▁미니': 2152, '▁미드필더': 2153, '▁미디어': 2154, '▁미래': 2155, '▁미래부': 2156, '▁미래창조과학부': 2157, '▁미뤄': 2158, '▁미리': 2159, '▁미만': 2160, '▁미모': 2161, '▁미사일': 2162, '▁미소': 2163, '▁미얀마': 2164, '▁미치는': 2165, '▁미치지': 2166, '▁미칠': 2167, '▁미투데이': 2168, '▁민': 2169, '▁민간': 2170, '▁민낯': 2171, '▁민생': 2172, '▁민원': 2173, '▁민족': 2174, '▁민주': 2175, '▁민주당': 2176, '▁민주당은': 2177, '▁민주주의': 2178, '▁민주통합당': 2179, '▁믿고': 2180, '▁밀': 2181, '▁밀려': 2182, '▁밀어': 2183, '▁및': 2184, '▁밑': 2185, '▁바': 2186, '▁바꾸': 2187, '▁바꿔': 2188, '▁바뀌': 2189, '▁바뀐': 2190, '▁바다': 2191, '▁바닥': 2192, '▁바라': 2193, '▁바란다': 2194, '▁바람': 2195, '▁바로': 2196, '▁바이러스': 2197, '▁바탕으로': 2198, '▁박': 2199, '▁박근혜': 2200, '▁박명수': 2201, '▁박수': 2202, '▁박인비': 2203, '▁박지성': 2204, '▁밖에': 2205, '▁밖으로': 2206, '▁반': 2207, '▁반대': 2208, '▁반도체': 2209, '▁반드시': 2210, '▁반등': 2211, '▁반면': 2212, '▁반박했다': 2213, '▁반발': 2214, '▁반복': 2215, '▁반영': 2216, '▁반응': 2217, '▁반응을': 2218, '▁반전': 2219, '▁받': 2220, '▁받게': 2221, '▁받고': 2222, '▁받기': 2223, '▁받는': 2224, '▁받는다': 2225, '▁받아': 2226, '▁받아들여': 2227, '▁받아들이': 2228, '▁받았': 2229, '▁받았다': 2230, '▁받으며': 2231, '▁받은': 2232, '▁받을': 2233, '▁받지': 2234, '▁발': 2235, '▁발견': 2236, '▁발견됐다': 2237, '▁발굴': 2238, '▁발급': 2239, '▁발매': 2240, '▁발목': 2241, '▁발사': 2242, '▁발생': 2243, '▁발생한': 2244, '▁발생할': 2245, '▁발생했다': 2246, '▁발언': 2247, '▁발언을': 2248, '▁발전': 2249, '▁발탁': 2250, '▁발표': 2251, '▁발표한': 2252, '▁발표했다': 2253, '▁발행': 2254, '▁발효': 2255, '▁발휘': 2256, '▁밝은': 2257, '▁밝혀': 2258, '▁밝혀졌다': 2259, '▁밝혔': 2260, '▁밝혔다': 2261, '▁밝혔습니다': 2262, '▁밝히': 2263, '▁밝힌': 2264, '▁밤': 2265, '▁밥': 2266, '▁방': 2267, '▁방문': 2268, '▁방문해': 2269, '▁방법': 2270, '▁방북': 2271, '▁방송': 2272, '▁방송되는': 2273, '▁방송된': 2274, '▁방송된다': 2275, '▁방송에서': 2276, '▁방식': 2277, '▁방식으로': 2278, '▁방안': 2279, '▁방안을': 2280, '▁방지': 2281, '▁방침': 2282, '▁방침이다': 2283, '▁방통위': 2284, '▁방향': 2285, '▁방향으로': 2286, '▁배': 2287, '▁배경': 2288, '▁배경으로': 2289, '▁배당': 2290, '▁배려': 2291, '▁배우': 2292, '▁배우들': 2293, '▁배제': 2294, '▁배출': 2295, '▁배치': 2296, '▁배터리': 2297, '▁백': 2298, '▁백악관': 2299, '▁백지영': 2300, '▁백화점': 2301, '▁버': 2302, '▁버냉키': 2303, '▁버디': 2304, '▁버스': 2305, '▁버전': 2306, '▁번': 2307, '▁번째': 2308, '▁벌': 2309, '▁벌금': 2310, '▁벌써': 2311, '▁벌어지': 2312, '▁벌어진': 2313, '▁벌였다': 2314, '▁벌이고': 2315, '▁벌이는': 2316, '▁벌인': 2317, '▁범': 2318, '▁범위': 2319, '▁범죄': 2320, '▁범행': 2321, '▁법': 2322, '▁법률': 2323, '▁법무부': 2324, '▁법안': 2325, '▁법원': 2326, '▁법인': 2327, '▁법적': 2328, '▁법정': 2329, '▁법칙': 2330, '▁벗': 2331, '▁벗어나': 2332, '▁베': 2333, '▁베이징': 2334, '▁베트남': 2335, '▁벤': 2336, '▁벤처': 2337, '▁벽': 2338, '▁변': 2339, '▁변경': 2340, '▁변동': 2341, '▁변동성': 2342, '▁변수': 2343, '▁변신': 2344, '▁변호사': 2345, '▁변화': 2346, '▁변화를': 2347, '▁별': 2348, '▁별도로': 2349, '▁별도의': 2350, '▁병': 2351, '▁병역': 2352, '▁병원': 2353, '▁병행': 2354, '▁보': 2355, '▁보건': 2356, '▁보건복지부': 2357, '▁보고': 2358, '▁보고서': 2359, '▁보관': 2360, '▁보급': 2361, '▁보기': 2362, '▁보내': 2363, '▁보낸': 2364, '▁보냈다': 2365, '▁보는': 2366, '▁보니': 2367, '▁보다': 2368, '▁보도': 2369, '▁보도자료': 2370, '▁보도했다': 2371, '▁보면': 2372, '▁보상': 2373, '▁보수': 2374, '▁보안': 2375, '▁보여': 2376, '▁보여주': 2377, '▁보여준': 2378, '▁보여줄': 2379, '▁보여줬다': 2380, '▁보였다': 2381, '▁보완': 2382, '▁보유': 2383, '▁보유하고': 2384, '▁보유한': 2385, '▁보육': 2386, '▁보이': 2387, '▁보이고': 2388, '▁보이는': 2389, '▁보이며': 2390, '▁보이지': 2391, '▁보인다': 2392, '▁보장': 2393, '▁보조금': 2394, '▁보증': 2395, '▁보통': 2396, '▁보험': 2397, '▁보험료': 2398, '▁보험사': 2399, '▁보호': 2400, '▁복': 2401, '▁복귀': 2402, '▁복무': 2403, '▁복수': 2404, '▁복잡': 2405, '▁복지': 2406, '▁복합': 2407, '▁본': 2408, '▁본격': 2409, '▁본격적으로': 2410, '▁본격적인': 2411, '▁본격화': 2412, '▁본다': 2413, '▁본사': 2414, '▁본인': 2415, '▁본회의': 2416, '▁볼': 2417, '▁봉': 2418, '▁봉사활동': 2419, '▁봐': 2420, '▁봤': 2421, '▁봤다': 2422, '▁부': 2423, '▁부각': 2424, '▁부과': 2425, '▁부담': 2426, '▁부담을': 2427, '▁부담이': 2428, '▁부당': 2429, '▁부대': 2430, '▁부동산': 2431, '▁부르': 2432, '▁부모': 2433, '▁부모님': 2434, '▁부문': 2435, '▁부부': 2436, '▁부분': 2437, '▁부분이': 2438, '▁부사장': 2439, '▁부산': 2440, '▁부산시': 2441, '▁부상': 2442, '▁부서': 2443, '▁부실': 2444, '▁부여': 2445, '▁부위원장': 2446, '▁부인': 2447, '▁부작용': 2448, '▁부정': 2449, '▁부정적': 2450, '▁부족': 2451, '▁부족한': 2452, '▁부지': 2453, '▁부진': 2454, '▁부채': 2455, '▁부처': 2456, '▁부총리': 2457, '▁부탁': 2458, '▁부품': 2459, '▁부활': 2460, '▁부회장': 2461, '▁북': 2462, '▁북미': 2463, '▁북측': 2464, '▁북한': 2465, '▁북한의': 2466, '▁북한이': 2467, '▁분': 2468, '▁분노': 2469, '▁분당': 2470, '▁분들': 2471, '▁분류': 2472, '▁분리': 2473, '▁분명': 2474, '▁분명히': 2475, '▁분석': 2476, '▁분석이다': 2477, '▁분석했다': 2478, '▁분야': 2479, '▁분야에서': 2480, '▁분양': 2481, '▁분위기': 2482, '▁분위기를': 2483, '▁분쟁': 2484, '▁불': 2485, '▁불가능': 2486, '▁불가피': 2487, '▁불공정': 2488, '▁불과': 2489, '▁불과하다': 2490, '▁불구속': 2491, '▁불구하고': 2492, '▁불러': 2493, '▁불리는': 2494, '▁불만을': 2495, '▁불법': 2496, '▁불안': 2497, '▁불투명': 2498, '▁불편': 2499, '▁불확실성': 2500, '▁불황': 2501, '▁붕괴': 2502, '▁붙': 2503, '▁붙잡': 2504, '▁브': 2505, '▁브라운': 2506, '▁브라질': 2507, '▁브랜드': 2508, '▁브리핑': 2509, '▁블랙': 2510, '▁블랙박스': 2511, '▁블로그': 2512, '▁블루': 2513, '▁비': 2514, '▁비가': 2515, '▁비공개': 2516, '▁비과세': 2517, '▁비교': 2518, '▁비교적': 2519, '▁비교해': 2520, '▁비난': 2521, '▁비대위': 2522, '▁비대위원장': 2523, '▁비롯': 2524, '▁비롯한': 2525, '▁비롯해': 2526, '▁비리': 2527, '▁비밀': 2528, '▁비상': 2529, '▁비서실': 2530, '▁비서실장': 2531, '▁비스트': 2532, '▁비슷': 2533, '▁비슷한': 2534, '▁비용': 2535, '▁비율': 2536, '▁비자금': 2537, '▁비정규직': 2538, '▁비중': 2539, '▁비중이': 2540, '▁비즈니스': 2541, '▁비판': 2542, '▁비판했다': 2543, '▁비해': 2544, '▁비행기': 2545, '▁빅': 2546, '▁빈': 2547, '▁빈소': 2548, '▁빌': 2549, '▁빌려': 2550, '▁빙': 2551, '▁빚': 2552, '▁빛': 2553, '▁빠': 2554, '▁빠르게': 2555, '▁빠르고': 2556, '▁빠른': 2557, '▁빠져': 2558, '▁빠졌다': 2559, '▁빠지': 2560, '▁빠진': 2561, '▁빨': 2562, '▁빨리': 2563, '▁빼': 2564, '▁빼앗': 2565, '▁뽐내': 2566, '▁뽐냈다': 2567, '▁뽑': 2568, '▁뽑아': 2569, '▁뿌리': 2570, '▁뿐': 2571, '▁뿐만': 2572, '▁사': 2573, '▁사건': 2574, '▁사건을': 2575, '▁사고': 2576, '▁사고가': 2577, '▁사고로': 2578, '▁사과': 2579, '▁사기': 2580, '▁사나이': 2581, '▁사는': 2582, '▁사라지': 2583, '▁사람': 2584, '▁사람들': 2585, '▁사람들이': 2586, '▁사람은': 2587, '▁사람의': 2588, '▁사람이': 2589, '▁사랑': 2590, '▁사랑을': 2591, '▁사례': 2592, '▁사로잡': 2593, '▁사로잡았다': 2594, '▁사망': 2595, '▁사망자': 2596, '▁사면': 2597, '▁사명': 2598, '▁사무': 2599, '▁사무실': 2600, '▁사무총장': 2601, '▁사법': 2602, '▁사상': 2603, '▁사실': 2604, '▁사실상': 2605, '▁사실을': 2606, '▁사실이': 2607, '▁사안': 2608, '▁사업': 2609, '▁사업을': 2610, '▁사업자': 2611, '▁사연': 2612, '▁사용': 2613, '▁사용자': 2614, '▁사용하는': 2615, '▁사용할': 2616, '▁사유': 2617, '▁사이': 2618, '▁사이버': 2619, '▁사이에': 2620, '▁사이에서': 2621, '▁사이트': 2622, '▁사장': 2623, '▁사장은': 2624, '▁사전': 2625, '▁사정': 2626, '▁사진': 2627, '▁사진을': 2628, '▁사진이': 2629, '▁사태': 2630, '▁사퇴': 2631, '▁사항': 2632, '▁사회': 2633, '▁사회공헌': 2634, '▁사회복지': 2635, '▁사회적': 2636, '▁사흘': 2637, '▁삭감': 2638, '▁삭제': 2639, '▁산': 2640, '▁산업': 2641, '▁산하': 2642, '▁살': 2643, '▁살아': 2644, '▁살인': 2645, '▁살펴': 2646, '▁살펴보면': 2647, '▁살해': 2648, '▁삶': 2649, '▁삼': 2650, '▁삼성': 2651, '▁삼성생명': 2652, '▁삼성전자': 2653, '▁삼성전자는': 2654, '▁삼성화재': 2655, '▁삼진': 2656, '▁삼청동': 2657, '▁상': 2658, '▁상담': 2659, '▁상당': 2660, '▁상당수': 2661, '▁상당의': 2662, '▁상당한': 2663, '▁상대': 2664, '▁상대로': 2665, '▁상대적으로': 2666, '▁상무': 2667, '▁상반기': 2668, '▁상생': 2669, '▁상승': 2670, '▁상승세': 2671, '▁상승세를': 2672, '▁상승한': 2673, '▁상승했다': 2674, '▁상위': 2675, '▁상임위': 2676, '▁상장': 2677, '▁상징': 2678, '▁상처': 2679, '▁상태': 2680, '▁상태다': 2681, '▁상태에서': 2682, '▁상품': 2683, '▁상하이': 2684, '▁상한가': 2685, '▁상향': 2686, '▁상호': 2687, '▁상환': 2688, '▁상황': 2689, '▁상황에': 2690, '▁상황에서': 2691, '▁상황을': 2692, '▁상황이': 2693, '▁상황이다': 2694, '▁새': 2695, '▁새누리당': 2696, '▁새누리당은': 2697, '▁새로': 2698, '▁새로운': 2699, '▁새롭게': 2700, '▁새벽': 2701, '▁새해': 2702, '▁샌프란시스코': 2703, '▁생': 2704, '▁생각': 2705, '▁생각을': 2706, '▁생각이': 2707, '▁생각한다': 2708, '▁생각해': 2709, '▁생기': 2710, '▁생긴': 2711, '▁생명': 2712, '▁생방송': 2713, '▁생산': 2714, '▁생존': 2715, '▁생태계': 2716, '▁생활': 2717, '▁서': 2718, '▁서귀포': 2719, '▁서로': 2720, '▁서류': 2721, '▁서민': 2722, '▁서부': 2723, '▁서비스': 2724, '▁서비스를': 2725, '▁서울': 2726, '▁서울대': 2727, '▁서울시': 2728, '▁서울중앙지검': 2729, '▁서울중앙지법': 2730, '▁서초구': 2731, '▁석': 2732, '▁석유': 2733, '▁선': 2734, '▁선거': 2735, '▁선고': 2736, '▁선고받': 2737, '▁선고했다': 2738, '▁선두': 2739, '▁선물': 2740, '▁선박': 2741, '▁선발': 2742, '▁선발투수': 2743, '▁선배': 2744, '▁선보여': 2745, '▁선보였다': 2746, '▁선보이며': 2747, '▁선보인': 2748, '▁선보인다': 2749, '▁선보일': 2750, '▁선사': 2751, '▁선생님': 2752, '▁선수': 2753, '▁선수가': 2754, '▁선수단': 2755, '▁선수들': 2756, '▁선수들이': 2757, '▁선언': 2758, '▁선예': 2759, '▁선임': 2760, '▁선정': 2761, '▁선정됐다': 2762, '▁선정된': 2763, '▁선제골': 2764, '▁선진국': 2765, '▁선출': 2766, '▁선택': 2767, '▁선호': 2768, '▁설': 2769, '▁설계': 2770, '▁설득': 2771, '▁설립': 2772, '▁설명': 2773, '▁설명이다': 2774, '▁설명했다': 2775, '▁설문조사': 2776, '▁설비': 2777, '▁설정': 2778, '▁설치': 2779, '▁섬': 2780, '▁성': 2781, '▁성격': 2782, '▁성공': 2783, '▁성공했다': 2784, '▁성과': 2785, '▁성과를': 2786, '▁성남': 2787, '▁성능': 2788, '▁성동일': 2789, '▁성매매': 2790, '▁성명을': 2791, '▁성장': 2792, '▁성장률': 2793, '▁성장세': 2794, '▁성적': 2795, '▁성추행': 2796, '▁성폭력': 2797, '▁성폭행': 2798, '▁성향': 2799, '▁성형': 2800, '▁세': 2801, '▁세계': 2802, '▁세계적인': 2803, '▁세금': 2804, '▁세대': 2805, '▁세력': 2806, '▁세무조사': 2807, '▁세미나': 2808, '▁세부': 2809, '▁세븐': 2810, '▁세상': 2811, '▁세상을': 2812, '▁세우': 2813, '▁세워': 2814, '▁세웠다': 2815, '▁세종': 2816, '▁세종시': 2817, '▁세트': 2818, '▁섹시': 2819, '▁셀카': 2820, '▁셈이다': 2821, '▁소': 2822, '▁소감을': 2823, '▁소개': 2824, '▁소개했다': 2825, '▁소녀': 2826, '▁소녀시대': 2827, '▁소득': 2828, '▁소리': 2829, '▁소방': 2830, '▁소방당국': 2831, '▁소비': 2832, '▁소비자': 2833, '▁소비자들': 2834, '▁소설': 2835, '▁소셜': 2836, '▁소속': 2837, '▁소속사': 2838, '▁소송': 2839, '▁소식에': 2840, '▁소식을': 2841, '▁소식통': 2842, '▁소요': 2843, '▁소유': 2844, '▁소장': 2845, '▁소재': 2846, '▁소중한': 2847, '▁소지섭': 2848, '▁소집': 2849, '▁소통': 2850, '▁소폭': 2851, '▁소프트웨어': 2852, '▁소형': 2853, '▁소화': 2854, '▁소환': 2855, '▁속': 2856, '▁속도': 2857, '▁속에': 2858, '▁속에서': 2859, '▁손': 2860, '▁손님': 2861, '▁손실': 2862, '▁손연재': 2863, '▁손을': 2864, '▁손해배상': 2865, '▁손흥민': 2866, '▁솔로': 2867, '▁솔루션': 2868, '▁송': 2869, '▁쇼': 2870, '▁쇼핑': 2871, '▁수': 2872, '▁수급': 2873, '▁수도': 2874, '▁수도권': 2875, '▁수립': 2876, '▁수많은': 2877, '▁수목드라마': 2878, '▁수밖에': 2879, '▁수비': 2880, '▁수비수': 2881, '▁수사': 2882, '▁수사를': 2883, '▁수상': 2884, '▁수석': 2885, '▁수석대표': 2886, '▁수수료': 2887, '▁수술': 2888, '▁수십': 2889, '▁수업': 2890, '▁수영': 2891, '▁수요': 2892, '▁수요가': 2893, '▁수용': 2894, '▁수원': 2895, '▁수익': 2896, '▁수익률': 2897, '▁수익성': 2898, '▁수익을': 2899, '▁수입': 2900, '▁수정': 2901, '▁수주': 2902, '▁수준': 2903, '▁수준으로': 2904, '▁수준이다': 2905, '▁수출': 2906, '▁수치': 2907, '▁수치다': 2908, '▁수행': 2909, '▁수혜': 2910, '▁숙': 2911, '▁순': 2912, '▁순간': 2913, '▁순매도': 2914, '▁순매수': 2915, '▁순위': 2916, '▁순이익': 2917, '▁술': 2918, '▁숨': 2919, '▁숨졌다': 2920, '▁숨진': 2921, '▁숫자': 2922, '▁쉬': 2923, '▁쉽게': 2924, '▁쉽지': 2925, '▁슈': 2926, '▁슈팅': 2927, '▁슈퍼': 2928, '▁스': 2929, '▁스노든': 2930, '▁스마트': 2931, '▁스마트폰': 2932, '▁스몰캡': 2933, '▁스스로': 2934, '▁스위스': 2935, '▁스케줄': 2936, '▁스크린': 2937, '▁스타': 2938, '▁스타일': 2939, '▁스태프': 2940, '▁스토리': 2941, '▁스트레스': 2942, '▁스페셜올림픽': 2943, '▁스페인': 2944, '▁스포츠': 2945, '▁스포츠조선닷컴': 2946, '▁스피드': 2947, '▁슬': 2948, '▁승': 2949, '▁승객': 2950, '▁승리': 2951, '▁승리를': 2952, '▁승무원': 2953, '▁승부': 2954, '▁승부주': 2955, '▁승용차': 2956, '▁승인': 2957, '▁승진': 2958, '▁시': 2959, '▁시가총액': 2960, '▁시각': 2961, '▁시간': 2962, '▁시간을': 2963, '▁시간이': 2964, '▁시기': 2965, '▁시나리오': 2966, '▁시너지': 2967, '▁시는': 2968, '▁시달리': 2969, '▁시대': 2970, '▁시도': 2971, '▁시리아': 2972, '▁시리즈': 2973, '▁시민': 2974, '▁시민단체': 2975, '▁시민들': 2976, '▁시범': 2977, '▁시사': 2978, '▁시상식': 2979, '▁시선을': 2980, '▁시설': 2981, '▁시스템': 2982, '▁시스템을': 2983, '▁시신': 2984, '▁시위': 2985, '▁시작': 2986, '▁시작된': 2987, '▁시작으로': 2988, '▁시작한': 2989, '▁시작했다': 2990, '▁시장': 2991, '▁시장에': 2992, '▁시장에서': 2993, '▁시장은': 2994, '▁시장의': 2995, '▁시절': 2996, '▁시점': 2997, '▁시즌': 2998, '▁시청률': 2999, '▁시청자': 3000, '▁시청자들': 3001, '▁시청자들의': 3002, '▁시카고': 3003, '▁시행': 3004, '▁시험': 3005, '▁식': 3006, '▁식당': 3007, '▁식사': 3008, '▁식품': 3009, '▁신': 3010, '▁신경': 3011, '▁신고': 3012, '▁신곡': 3013, '▁신규': 3014, '▁신동엽': 3015, '▁신뢰': 3016, '▁신문': 3017, '▁신분': 3018, '▁신설': 3019, '▁신세계': 3020, '▁신속': 3021, '▁신시내티': 3022, '▁신용': 3023, '▁신용등급': 3024, '▁신용카드': 3025, '▁신인': 3026, '▁신임': 3027, '▁신제품': 3028, '▁신중': 3029, '▁신청': 3030, '▁신한금융투자': 3031, '▁신한은행': 3032, '▁신호': 3033, '▁신화': 3034, '▁신흥국': 3035, '▁실': 3036, '▁실내': 3037, '▁실력': 3038, '▁실망': 3039, '▁실무': 3040, '▁실무회담': 3041, '▁실수': 3042, '▁실시': 3043, '▁실시간': 3044, '▁실시한': 3045, '▁실시한다': 3046, '▁실적': 3047, '▁실적이': 3048, '▁실제': 3049, '▁실제로': 3050, '▁실종': 3051, '▁실질적인': 3052, '▁실천': 3053, '▁실태': 3054, '▁실패': 3055, '▁실행': 3056, '▁실험': 3057, '▁실현': 3058, '▁싫어': 3059, '▁심': 3060, '▁심각': 3061, '▁심각한': 3062, '▁심경': 3063, '▁심리': 3064, '▁심사': 3065, '▁심사위원': 3066, '▁심지어': 3067, '▁심판': 3068, '▁심화': 3069, '▁싱가포르': 3070, '▁싱글': 3071, '▁싶다': 3072, '▁싶어': 3073, '▁싶었': 3074, '▁싶은': 3075, '▁싸': 3076, '▁싸이': 3077, '▁쌍용차': 3078, '▁쌓': 3079, '▁써': 3080, '▁써니': 3081, '▁썼다': 3082, '▁쏟아': 3083, '▁쓰': 3084, '▁쓰레기': 3085, '▁쓴': 3086, '▁쓸': 3087, '▁씨': 3088, '▁씨가': 3089, '▁씨는': 3090, '▁씨스타': 3091, '▁씨엔블루': 3092, '▁아': 3093, '▁아끼': 3094, '▁아나운서': 3095, '▁아내': 3096, '▁아니': 3097, '▁아니냐': 3098, '▁아니냐는': 3099, '▁아니다': 3100, '▁아니라': 3101, '▁아니면': 3102, '▁아니었다': 3103, '▁아니지만': 3104, '▁아닌': 3105, '▁아동': 3106, '▁아들': 3107, '▁아래': 3108, '▁아름': 3109, '▁아름다운': 3110, '▁아무': 3111, '▁아버지': 3112, '▁아베': 3113, '▁아빠': 3114, '▁아쉬움': 3115, '▁아시아': 3116, '▁아시아나': 3117, '▁아시아나항공': 3118, '▁아예': 3119, '▁아울러': 3120, '▁아이': 3121, '▁아이돌': 3122, '▁아이들': 3123, '▁아이디어': 3124, '▁아이유': 3125, '▁아이템': 3126, '▁아이폰': 3127, '▁아주': 3128, '▁아직': 3129, '▁아침': 3130, '▁아파트': 3131, '▁아픔': 3132, '▁악': 3133, '▁악화': 3134, '▁안': 3135, '▁안내': 3136, '▁안보': 3137, '▁안에': 3138, '▁안전': 3139, '▁안정': 3140, '▁안정적인': 3141, '▁안타': 3142, '▁안타를': 3143, '▁안팎': 3144, '▁앉아': 3145, '▁않': 3146, '▁않게': 3147, '▁않겠다': 3148, '▁않고': 3149, '▁않기': 3150, '▁않는': 3151, '▁않는다': 3152, '▁않다': 3153, '▁않도록': 3154, '▁않아': 3155, '▁않았': 3156, '▁않았다': 3157, '▁않았던': 3158, '▁않았지만': 3159, '▁않으': 3160, '▁않으면': 3161, '▁않은': 3162, '▁않을': 3163, '▁않을까': 3164, '▁않지만': 3165, '▁알': 3166, '▁알게': 3167, '▁알고': 3168, '▁알려': 3169, '▁알려져': 3170, '▁알려졌다': 3171, '▁알려진': 3172, '▁알렸다': 3173, '▁알리': 3174, '▁알아': 3175, '▁알제리': 3176, '▁암': 3177, '▁압': 3178, '▁압도적': 3179, '▁압력': 3180, '▁압류': 3181, '▁압박': 3182, '▁압수수색': 3183, '▁앞': 3184, '▁앞두고': 3185, '▁앞둔': 3186, '▁앞서': 3187, '▁앞선': 3188, '▁앞세워': 3189, '▁앞에': 3190, '▁앞에서': 3191, '▁앞으로': 3192, '▁앞장서': 3193, '▁애': 3194, '▁애널리스트': 3195, '▁애니메이션': 3196, '▁애리조나': 3197, '▁애정': 3198, '▁애초': 3199, '▁애플': 3200, '▁애플리케이션': 3201, '▁액션': 3202, '▁앤': 3203, '▁앨범': 3204, '▁앱': 3205, '▁야': 3206, '▁야간': 3207, '▁야구': 3208, '▁야당': 3209, '▁야외': 3210, '▁약': 3211, '▁약세': 3212, '▁약속': 3213, '▁양': 3214, '▁양국': 3215, '▁양성': 3216, '▁양적완화': 3217, '▁양측': 3218, '▁얘기': 3219, '▁어': 3220, '▁어깨': 3221, '▁어느': 3222, '▁어디': 3223, '▁어떤': 3224, '▁어떻게': 3225, '▁어려운': 3226, '▁어려울': 3227, '▁어려움': 3228, '▁어려움을': 3229, '▁어려워': 3230, '▁어렵': 3231, '▁어렵다': 3232, '▁어린': 3233, '▁어린이': 3234, '▁어린이집': 3235, '▁어머니': 3236, '▁어울리': 3237, '▁어제': 3238, '▁억울': 3239, '▁억제': 3240, '▁언': 3241, '▁언급': 3242, '▁언급했다': 3243, '▁언론': 3244, '▁언제': 3245, '▁얻고': 3246, '▁얻어': 3247, '▁얻었다': 3248, '▁얻은': 3249, '▁얻을': 3250, '▁얼굴': 3251, '▁얼마': 3252, '▁얼마나': 3253, '▁얼음': 3254, '▁엄': 3255, '▁엄격': 3256, '▁엄마': 3257, '▁엄청난': 3258, '▁엄태웅': 3259, '▁업': 3260, '▁업계': 3261, '▁업그레이드': 3262, '▁업데이트': 3263, '▁업무': 3264, '▁업무를': 3265, '▁업무보고': 3266, '▁업종': 3267, '▁업체': 3268, '▁업체들': 3269, '▁없': 3270, '▁없고': 3271, '▁없는': 3272, '▁없다': 3273, '▁없다고': 3274, '▁없다는': 3275, '▁없도록': 3276, '▁없애': 3277, '▁없어': 3278, '▁없었': 3279, '▁없었다': 3280, '▁없었던': 3281, '▁없을': 3282, '▁없이': 3283, '▁없지만': 3284, '▁에너지': 3285, '▁에릭': 3286, '▁에스': 3287, '▁에어컨': 3288, '▁에이': 3289, '▁에이스': 3290, '▁에피소드': 3291, '▁엔': 3292, '▁엔진': 3293, '▁엔화': 3294, '▁엘': 3295, '▁엠': 3296, '▁엠넷': 3297, '▁여': 3298, '▁여객기': 3299, '▁여건': 3300, '▁여기': 3301, '▁여기에': 3302, '▁여당': 3303, '▁여러': 3304, '▁여러분': 3305, '▁여론': 3306, '▁여름': 3307, '▁여름철': 3308, '▁여배우': 3309, '▁여부': 3310, '▁여부를': 3311, '▁여성': 3312, '▁여성들': 3313, '▁여신': 3314, '▁여야': 3315, '▁여유': 3316, '▁여의도': 3317, '▁여자': 3318, '▁여자친구': 3319, '▁여전히': 3320, '▁여행': 3321, '▁역': 3322, '▁역대': 3323, '▁역량': 3324, '▁역사': 3325, '▁역사적': 3326, '▁역시': 3327, '▁역을': 3328, '▁역전': 3329, '▁역할': 3330, '▁역할을': 3331, '▁연': 3332, '▁연간': 3333, '▁연결': 3334, '▁연계': 3335, '▁연구': 3336, '▁연구개발': 3337, '▁연구원은': 3338, '▁연극': 3339, '▁연금': 3340, '▁연기': 3341, '▁연기를': 3342, '▁연락': 3343, '▁연령': 3344, '▁연말': 3345, '▁연방': 3346, '▁연봉': 3347, '▁연세대': 3348, '▁연속': 3349, '▁연습': 3350, '▁연애': 3351, '▁연예': 3352, '▁연예병사': 3353, '▁연예인': 3354, '▁연인': 3355, '▁연장': 3356, '▁연출': 3357, '▁열': 3358, '▁열고': 3359, '▁열람': 3360, '▁열렸다': 3361, '▁열리': 3362, '▁열리는': 3363, '▁열린': 3364, '▁열린다': 3365, '▁열릴': 3366, '▁열심히': 3367, '▁열애': 3368, '▁열애설': 3369, '▁열어': 3370, '▁열었다': 3371, '▁열정': 3372, '▁열차': 3373, '▁염': 3374, '▁염두에': 3375, '▁영': 3376, '▁영광': 3377, '▁영국': 3378, '▁영등포': 3379, '▁영상': 3380, '▁영어': 3381, '▁영업': 3382, '▁영업이익': 3383, '▁영업이익은': 3384, '▁영업익': 3385, '▁영업정지': 3386, '▁영역': 3387, '▁영입': 3388, '▁영하': 3389, '▁영향': 3390, '▁영향력': 3391, '▁영향으로': 3392, '▁영향을': 3393, '▁영화': 3394, '▁옆': 3395, '▁예': 3396, '▁예고': 3397, '▁예금': 3398, '▁예능': 3399, '▁예능프로그램': 3400, '▁예방': 3401, '▁예비': 3402, '▁예쁘': 3403, '▁예산': 3404, '▁예상': 3405, '▁예상되는': 3406, '▁예상된다': 3407, '▁예상치': 3408, '▁예상했다': 3409, '▁예술': 3410, '▁예약': 3411, '▁예전': 3412, '▁예정': 3413, '▁예정이다': 3414, '▁예측': 3415, '▁옛': 3416, '▁오': 3417, '▁오는': 3418, '▁오늘': 3419, '▁오늘의': 3420, '▁오디션': 3421, '▁오래': 3422, '▁오랜': 3423, '▁오랫동안': 3424, '▁오르': 3425, '▁오른': 3426, '▁오름세': 3427, '▁오바마': 3428, '▁오빠': 3429, '▁오연서': 3430, '▁오전': 3431, '▁오픈': 3432, '▁오피스텔': 3433, '▁오후': 3434, '▁오히려': 3435, '▁옥': 3436, '▁온': 3437, '▁온라인': 3438, '▁올': 3439, '▁올라': 3440, '▁올랐': 3441, '▁올랐다': 3442, '▁올려': 3443, '▁올렸다': 3444, '▁올리': 3445, '▁올린': 3446, '▁올림픽': 3447, '▁올스타': 3448, '▁올스타전': 3449, '▁올해': 3450, '▁올해부터': 3451, '▁옮겨': 3452, '▁옮기': 3453, '▁옷': 3454, '▁완': 3455, '▁완공': 3456, '▁완료': 3457, '▁완벽': 3458, '▁완벽한': 3459, '▁완성': 3460, '▁완전': 3461, '▁완전히': 3462, '▁완화': 3463, '▁왔다': 3464, '▁왕': 3465, '▁왜': 3466, '▁왜곡': 3467, '▁외': 3468, '▁외교': 3469, '▁외교부': 3470, '▁외국': 3471, '▁외국인': 3472, '▁외모': 3473, '▁외부': 3474, '▁외에': 3475, '▁외에도': 3476, '▁외환': 3477, '▁외환은행': 3478, '▁왼쪽': 3479, '▁요': 3480, '▁요구': 3481, '▁요구하는': 3482, '▁요구했다': 3483, '▁요금': 3484, '▁요금제': 3485, '▁요리': 3486, '▁요소': 3487, '▁요인': 3488, '▁요즘': 3489, '▁요청': 3490, '▁요청했다': 3491, '▁욕': 3492, '▁욕심': 3493, '▁용': 3494, '▁용산': 3495, '▁용의자': 3496, '▁용인': 3497, '▁우': 3498, '▁우려': 3499, '▁우려가': 3500, '▁우리': 3501, '▁우리가': 3502, '▁우리나라': 3503, '▁우리는': 3504, '▁우리은행': 3505, '▁우리투자증권': 3506, '▁우선': 3507, '▁우수': 3508, '▁우승': 3509, '▁우승을': 3510, '▁우주': 3511, '▁우즈': 3512, '▁운': 3513, '▁운동': 3514, '▁운명': 3515, '▁운송': 3516, '▁운영': 3517, '▁운영하는': 3518, '▁운용': 3519, '▁운전': 3520, '▁운전자': 3521, '▁운항': 3522, '▁운행': 3523, '▁울': 3524, '▁울산': 3525, '▁움직이': 3526, '▁움직임': 3527, '▁웃': 3528, '▁웃음을': 3529, '▁워': 3530, '▁워낙': 3531, '▁워싱턴': 3532, '▁원': 3533, '▁원내대변인': 3534, '▁원내대표': 3535, '▁원내대표는': 3536, '▁원래': 3537, '▁원인': 3538, '▁원장': 3539, '▁원전': 3540, '▁원정': 3541, '▁원정경기': 3542, '▁원칙': 3543, '▁원하는': 3544, '▁원화': 3545, '▁원활': 3546, '▁월드': 3547, '▁월드컵': 3548, '▁월화드라마': 3549, '▁웨딩': 3550, '▁웹': 3551, '▁위': 3552, '▁위기': 3553, '▁위로': 3554, '▁위반': 3555, '▁위안부': 3556, '▁위원': 3557, '▁위원장': 3558, '▁위원장은': 3559, '▁위조': 3560, '▁위촉': 3561, '▁위축': 3562, '▁위치': 3563, '▁위치한': 3564, '▁위탁': 3565, '▁위한': 3566, '▁위해': 3567, '▁위해서': 3568, '▁위해서는': 3569, '▁위헌': 3570, '▁위험': 3571, '▁위협': 3572, '▁윌리엄': 3573, '▁유': 3574, '▁유가증권시장': 3575, '▁유감': 3576, '▁유나이티드': 3577, '▁유니폼': 3578, '▁유도': 3579, '▁유동성': 3580, '▁유럽': 3581, '▁유력': 3582, '▁유로존': 3583, '▁유리': 3584, '▁유명': 3585, '▁유사': 3586, '▁유엔': 3587, '▁유일': 3588, '▁유입': 3589, '▁유재석': 3590, '▁유족': 3591, '▁유지': 3592, '▁유출': 3593, '▁유치': 3594, '▁유통': 3595, '▁유튜브': 3596, '▁육': 3597, '▁육군': 3598, '▁육박': 3599, '▁육성': 3600, '▁윤': 3601, '▁윤씨': 3602, '▁윤창중': 3603, '▁은퇴': 3604, '▁은행': 3605, '▁음': 3606, '▁음반': 3607, '▁음성': 3608, '▁음식': 3609, '▁음악': 3610, '▁음원': 3611, '▁응': 3612, '▁응급': 3613, '▁응답': 3614, '▁응답자': 3615, '▁응시': 3616, '▁응원': 3617, '▁의견': 3618, '▁의견을': 3619, '▁의견이': 3620, '▁의결': 3621, '▁의도': 3622, '▁의뢰': 3623, '▁의료': 3624, '▁의류': 3625, '▁의무': 3626, '▁의문': 3627, '▁의미': 3628, '▁의사': 3629, '▁의상': 3630, '▁의식': 3631, '▁의심': 3632, '▁의약품': 3633, '▁의원': 3634, '▁의원들': 3635, '▁의원은': 3636, '▁의원이': 3637, '▁의장': 3638, '▁의존': 3639, '▁의지': 3640, '▁의지를': 3641, '▁의한': 3642, '▁의해': 3643, '▁의혹': 3644, '▁의혹을': 3645, '▁의회': 3646, '▁이': 3647, '▁이같은': 3648, '▁이같이': 3649, '▁이것': 3650, '▁이곳': 3651, '▁이끄는': 3652, '▁이끌': 3653, '▁이끌어': 3654, '▁이끌었다': 3655, '▁이날': 3656, '▁이내': 3657, '▁이는': 3658, '▁이달': 3659, '▁이대호': 3660, '▁이데일리': 3661, '▁이동': 3662, '▁이동통신': 3663, '▁이동흡': 3664, '▁이들': 3665, '▁이들은': 3666, '▁이들의': 3667, '▁이들이': 3668, '▁이라크': 3669, '▁이래': 3670, '▁이러한': 3671, '▁이런': 3672, '▁이렇게': 3673, '▁이례적': 3674, '▁이로써': 3675, '▁이루': 3676, '▁이뤄': 3677, '▁이뤄졌다': 3678, '▁이뤄지': 3679, '▁이뤄진': 3680, '▁이뤄질': 3681, '▁이르는': 3682, '▁이르면': 3683, '▁이른다': 3684, '▁이른바': 3685, '▁이를': 3686, '▁이름': 3687, '▁이름을': 3688, '▁이마트': 3689, '▁이메일': 3690, '▁이명박': 3691, '▁이미': 3692, '▁이미지': 3693, '▁이미지를': 3694, '▁이민': 3695, '▁이밖에': 3696, '▁이번': 3697, '▁이번에': 3698, '▁이벤트': 3699, '▁이벤트를': 3700, '▁이병헌': 3701, '▁이사장': 3702, '▁이사회': 3703, '▁이상': 3704, '▁이상의': 3705, '▁이서진': 3706, '▁이수': 3707, '▁이슈': 3708, '▁이스라엘': 3709, '▁이슬람': 3710, '▁이승': 3711, '▁이승엽': 3712, '▁이시영': 3713, '▁이야기': 3714, '▁이야기를': 3715, '▁이어': 3716, '▁이어가고': 3717, '▁이어갔다': 3718, '▁이어졌다': 3719, '▁이어지고': 3720, '▁이어지는': 3721, '▁이어진': 3722, '▁이어질': 3723, '▁이에': 3724, '▁이와': 3725, '▁이용': 3726, '▁이용자': 3727, '▁이용한': 3728, '▁이용할': 3729, '▁이용해': 3730, '▁이웃': 3731, '▁이유': 3732, '▁이유는': 3733, '▁이유로': 3734, '▁이유를': 3735, '▁이익': 3736, '▁이재': 3737, '▁이적': 3738, '▁이전': 3739, '▁이정': 3740, '▁이정현': 3741, '▁이제': 3742, '▁이종': 3743, '▁이종석': 3744, '▁이준': 3745, '▁이중': 3746, '▁이집트': 3747, '▁이처럼': 3748, '▁이탈리아': 3749, '▁이틀': 3750, '▁이하': 3751, '▁이해': 3752, '▁이행': 3753, '▁이혼': 3754, '▁이효리': 3755, '▁이후': 3756, '▁익': 3757, '▁인': 3758, '▁인간': 3759, '▁인구': 3760, '▁인권': 3761, '▁인근': 3762, '▁인기': 3763, '▁인기를': 3764, '▁인도': 3765, '▁인도네시아': 3766, '▁인력': 3767, '▁인물': 3768, '▁인사': 3769, '▁인사들': 3770, '▁인사를': 3771, '▁인사청문회': 3772, '▁인상': 3773, '▁인생': 3774, '▁인선': 3775, '▁인쇄': 3776, '▁인수': 3777, '▁인수위': 3778, '▁인수위원': 3779, '▁인수위원회': 3780, '▁인식': 3781, '▁인연': 3782, '▁인원': 3783, '▁인재': 3784, '▁인정': 3785, '▁인정받': 3786, '▁인증': 3787, '▁인증샷': 3788, '▁인질': 3789, '▁인천': 3790, '▁인천공항': 3791, '▁인천시': 3792, '▁인터': 3793, '▁인터넷': 3794, '▁인터뷰': 3795, '▁인터뷰에서': 3796, '▁인턴기자': 3797, '▁인프라': 3798, '▁인피니트': 3799, '▁인하': 3800, '▁인한': 3801, '▁인해': 3802, '▁일': 3803, '▁일각에서는': 3804, '▁일단': 3805, '▁일대': 3806, '▁일반': 3807, '▁일방적': 3808, '▁일본': 3809, '▁일본의': 3810, '▁일부': 3811, '▁일상': 3812, '▁일어나': 3813, '▁일어난': 3814, '▁일으키': 3815, '▁일으킨': 3816, '▁일을': 3817, '▁일이': 3818, '▁일자리': 3819, '▁일정': 3820, '▁일제히': 3821, '▁일주일': 3822, '▁일환으로': 3823, '▁읽': 3824, '▁잃은': 3825, '▁임': 3826, '▁임금': 3827, '▁임기': 3828, '▁임대': 3829, '▁임명': 3830, '▁임시': 3831, '▁임시국회': 3832, '▁임신': 3833, '▁임원': 3834, '▁임직원': 3835, '▁입': 3836, '▁입건': 3837, '▁입고': 3838, '▁입단': 3839, '▁입력': 3840, '▁입법': 3841, '▁입원': 3842, '▁입은': 3843, '▁입장': 3844, '▁입장을': 3845, '▁입장이다': 3846, '▁입주': 3847, '▁입증': 3848, '▁입지': 3849, '▁입찰': 3850, '▁입학': 3851, '▁잇': 3852, '▁잇따라': 3853, '▁있': 3854, '▁있게': 3855, '▁있겠': 3856, '▁있고': 3857, '▁있기': 3858, '▁있느냐': 3859, '▁있는': 3860, '▁있는데': 3861, '▁있다': 3862, '▁있다고': 3863, '▁있다는': 3864, '▁있던': 3865, '▁있도록': 3866, '▁있습니다': 3867, '▁있어': 3868, '▁있어서': 3869, '▁있어야': 3870, '▁있었': 3871, '▁있었는데': 3872, '▁있었다': 3873, '▁있었던': 3874, '▁있었지만': 3875, '▁있으나': 3876, '▁있으니': 3877, '▁있으며': 3878, '▁있으면': 3879, '▁있을': 3880, '▁있을까': 3881, '▁있을지': 3882, '▁있음': 3883, '▁있지': 3884, '▁있지만': 3885, '▁자': 3886, '▁자격': 3887, '▁자극': 3888, '▁자금': 3889, '▁자기': 3890, '▁자녀': 3891, '▁자동': 3892, '▁자동차': 3893, '▁자랑': 3894, '▁자료': 3895, '▁자료를': 3896, '▁자리': 3897, '▁자리를': 3898, '▁자리에': 3899, '▁자리에서': 3900, '▁자발적': 3901, '▁자본': 3902, '▁자본시장': 3903, '▁자사': 3904, '▁자산': 3905, '▁자살': 3906, '▁자세': 3907, '▁자세한': 3908, '▁자신': 3909, '▁자신감': 3910, '▁자신들': 3911, '▁자신을': 3912, '▁자신의': 3913, '▁자신이': 3914, '▁자아냈다': 3915, '▁자연': 3916, '▁자연스럽게': 3917, '▁자원': 3918, '▁자원봉사': 3919, '▁자유': 3920, '▁자율': 3921, '▁자전거': 3922, '▁자존심': 3923, '▁자주': 3924, '▁자체': 3925, '▁자체가': 3926, '▁자칫': 3927, '▁자택': 3928, '▁자회사': 3929, '▁작': 3930, '▁작가': 3931, '▁작곡': 3932, '▁작년': 3933, '▁작동': 3934, '▁작성': 3935, '▁작업': 3936, '▁작업을': 3937, '▁작용': 3938, '▁작은': 3939, '▁작품': 3940, '▁잔': 3941, '▁잘': 3942, '▁잘못': 3943, '▁잘못된': 3944, '▁잠': 3945, '▁잠시': 3946, '▁잠실': 3947, '▁잠재': 3948, '▁잠정': 3949, '▁잡': 3950, '▁잡고': 3951, '▁잡아': 3952, '▁잡았다': 3953, '▁장': 3954, '▁장관': 3955, '▁장관은': 3956, '▁장기': 3957, '▁장난': 3958, '▁장르': 3959, '▁장면': 3960, '▁장비': 3961, '▁장소': 3962, '▁장애': 3963, '▁장애인': 3964, '▁장윤정': 3965, '▁장점': 3966, '▁장착': 3967, '▁장학금': 3968, '▁재': 3969, '▁재가동': 3970, '▁재개': 3971, '▁재건축': 3972, '▁재계약': 3973, '▁재난': 3974, '▁재능': 3975, '▁재무': 3976, '▁재미': 3977, '▁재미있': 3978, '▁재발': 3979, '▁재발방지': 3980, '▁재벌': 3981, '▁재산': 3982, '▁재원': 3983, '▁재정': 3984, '▁재정절벽': 3985, '▁재판': 3986, '▁재판부는': 3987, '▁재활': 3988, '▁쟁점': 3989, '▁저': 3990, '▁저녁': 3991, '▁저렴': 3992, '▁저소득층': 3993, '▁저장': 3994, '▁저지른': 3995, '▁적': 3996, '▁적극': 3997, '▁적극적으로': 3998, '▁적극적인': 3999, '▁적립': 4000, '▁적발': 4001, '▁적시타': 4002, '▁적어': 4003, '▁적용': 4004, '▁적은': 4005, '▁적응': 4006, '▁적이': 4007, '▁적자': 4008, '▁적절': 4009, '▁적지': 4010, '▁적합': 4011, '▁전': 4012, '▁전개': 4013, '▁전국': 4014, '▁전기': 4015, '▁전날': 4016, '▁전날보다': 4017, '▁전남': 4018, '▁전년': 4019, '▁전년대비': 4020, '▁전년동기': 4021, '▁전달': 4022, '▁전담': 4023, '▁전두환': 4024, '▁전략': 4025, '▁전력': 4026, '▁전망': 4027, '▁전망된다': 4028, '▁전망이다': 4029, '▁전망치': 4030, '▁전망했다': 4031, '▁전면': 4032, '▁전문': 4033, '▁전문가': 4034, '▁전문가들': 4035, '▁전문가들은': 4036, '▁전반': 4037, '▁전반기': 4038, '▁전부터': 4039, '▁전북': 4040, '▁전세': 4041, '▁전세계': 4042, '▁전시': 4043, '▁전액': 4044, '▁전에': 4045, '▁전역': 4046, '▁전용': 4047, '▁전원': 4048, '▁전자': 4049, '▁전쟁': 4050, '▁전주': 4051, '▁전지현': 4052, '▁전지훈련': 4053, '▁전체': 4054, '▁전체회의': 4055, '▁전통': 4056, '▁전투': 4057, '▁전파': 4058, '▁전해': 4059, '▁전해졌다': 4060, '▁전했다': 4061, '▁전혀': 4062, '▁전형': 4063, '▁전화': 4064, '▁전환': 4065, '▁전후': 4066, '▁절': 4067, '▁절감': 4068, '▁절대': 4069, '▁절반': 4070, '▁절차': 4071, '▁절차를': 4072, '▁젊은': 4073, '▁점': 4074, '▁점검': 4075, '▁점도': 4076, '▁점수': 4077, '▁점에서': 4078, '▁점유율': 4079, '▁점을': 4080, '▁점이': 4081, '▁점이다': 4082, '▁점점': 4083, '▁점차': 4084, '▁점포': 4085, '▁접': 4086, '▁접근': 4087, '▁접속': 4088, '▁접수': 4089, '▁접촉': 4090, '▁접한': 4091, '▁정': 4092, '▁정권': 4093, '▁정규': 4094, '▁정규직': 4095, '▁정기': 4096, '▁정당': 4097, '▁정당공천': 4098, '▁정도': 4099, '▁정도로': 4100, '▁정리': 4101, '▁정말': 4102, '▁정보': 4103, '▁정보를': 4104, '▁정보통신': 4105, '▁정부': 4106, '▁정부가': 4107, '▁정부는': 4108, '▁정부와': 4109, '▁정부의': 4110, '▁정부조직': 4111, '▁정비': 4112, '▁정상': 4113, '▁정상화': 4114, '▁정상회담': 4115, '▁정식': 4116, '▁정신': 4117, '▁정착': 4118, '▁정책': 4119, '▁정책을': 4120, '▁정체': 4121, '▁정치': 4122, '▁정치권': 4123, '▁정치적': 4124, '▁정확한': 4125, '▁정확히': 4126, '▁정황': 4127, '▁제': 4128, '▁제거': 4129, '▁제공': 4130, '▁제공하고': 4131, '▁제공하는': 4132, '▁제공한다': 4133, '▁제기': 4134, '▁제기되고': 4135, '▁제대로': 4136, '▁제도': 4137, '▁제목으로': 4138, '▁제시': 4139, '▁제시한': 4140, '▁제시했다': 4141, '▁제안': 4142, '▁제약': 4143, '▁제외': 4144, '▁제외한': 4145, '▁제임스': 4146, '▁제작': 4147, '▁제작발표회': 4148, '▁제작진': 4149, '▁제재': 4150, '▁제조': 4151, '▁제조업': 4152, '▁제조업체': 4153, '▁제주': 4154, '▁제주도': 4155, '▁제출': 4156, '▁제치고': 4157, '▁제품': 4158, '▁제품을': 4159, '▁제한': 4160, '▁제휴': 4161, '▁조': 4162, '▁조건': 4163, '▁조금': 4164, '▁조금씩': 4165, '▁조기': 4166, '▁조달': 4167, '▁조례': 4168, '▁조만간': 4169, '▁조명': 4170, '▁조사': 4171, '▁조사결과': 4172, '▁조사됐다': 4173, '▁조사를': 4174, '▁조사하고': 4175, '▁조선': 4176, '▁조성': 4177, '▁조성민': 4178, '▁조심': 4179, '▁조언': 4180, '▁조율': 4181, '▁조작': 4182, '▁조절': 4183, '▁조정': 4184, '▁조종사': 4185, '▁조직': 4186, '▁조직개편': 4187, '▁조치': 4188, '▁조치를': 4189, '▁조합원': 4190, '▁조항': 4191, '▁존': 4192, '▁존재': 4193, '▁존중': 4194, '▁졸업': 4195, '▁좀': 4196, '▁종': 4197, '▁종교': 4198, '▁종로구': 4199, '▁종료': 4200, '▁종류': 4201, '▁종목': 4202, '▁종합': 4203, '▁좋': 4204, '▁좋겠다': 4205, '▁좋다': 4206, '▁좋아': 4207, '▁좋았': 4208, '▁좋은': 4209, '▁좋지': 4210, '▁좌': 4211, '▁죄': 4212, '▁주': 4213, '▁주가': 4214, '▁주가가': 4215, '▁주거': 4216, '▁주고': 4217, '▁주관': 4218, '▁주는': 4219, '▁주도': 4220, '▁주력': 4221, '▁주로': 4222, '▁주말': 4223, '▁주말드라마': 4224, '▁주목': 4225, '▁주목된다': 4226, '▁주문': 4227, '▁주민': 4228, '▁주변': 4229, '▁주식': 4230, '▁주식시장': 4231, '▁주연': 4232, '▁주요': 4233, '▁주요뉴스': 4234, '▁주인공': 4235, '▁주장': 4236, '▁주장했다': 4237, '▁주재': 4238, '▁주제': 4239, '▁주제로': 4240, '▁주주': 4241, '▁주최': 4242, '▁주택': 4243, '▁죽': 4244, '▁죽음': 4245, '▁준': 4246, '▁준공': 4247, '▁준다': 4248, '▁준비': 4249, '▁준수': 4250, '▁준우승': 4251, '▁줄': 4252, '▁줄어든': 4253, '▁줄어들': 4254, '▁줄었다': 4255, '▁줄여': 4256, '▁중': 4257, '▁중간': 4258, '▁중국': 4259, '▁중국의': 4260, '▁중단': 4261, '▁중반': 4262, '▁중소': 4263, '▁중소기업': 4264, '▁중소형주': 4265, '▁중순': 4266, '▁중심': 4267, '▁중심으로': 4268, '▁중앙': 4269, '▁중요': 4270, '▁중요성': 4271, '▁중요하다': 4272, '▁중요한': 4273, '▁중이다': 4274, '▁중인': 4275, '▁중장기': 4276, '▁중점': 4277, '▁중흥': 4278, '▁즉': 4279, '▁즉각': 4280, '▁즉시': 4281, '▁즐거운': 4282, '▁즐기': 4283, '▁즐길': 4284, '▁증': 4285, '▁증가': 4286, '▁증가율': 4287, '▁증가한': 4288, '▁증가했다': 4289, '▁증거': 4290, '▁증권': 4291, '▁증권사': 4292, '▁증명': 4293, '▁증시': 4294, '▁증액': 4295, '▁증인': 4296, '▁지': 4297, '▁지구': 4298, '▁지금': 4299, '▁지금까지': 4300, '▁지금은': 4301, '▁지급': 4302, '▁지나': 4303, '▁지난': 4304, '▁지난달': 4305, '▁지난해': 4306, '▁지내': 4307, '▁지낸': 4308, '▁지닌': 4309, '▁지도': 4310, '▁지도부': 4311, '▁지도자': 4312, '▁지동원': 4313, '▁지명': 4314, '▁지목': 4315, '▁지방': 4316, '▁지방자치단체': 4317, '▁지배': 4318, '▁지분': 4319, '▁지사': 4320, '▁지속': 4321, '▁지속될': 4322, '▁지속적으로': 4323, '▁지속적인': 4324, '▁지수': 4325, '▁지시': 4326, '▁지식': 4327, '▁지식경제부': 4328, '▁지역': 4329, '▁지연': 4330, '▁지원': 4331, '▁지원을': 4332, '▁지원하는': 4333, '▁지원한다': 4334, '▁지자체': 4335, '▁지적': 4336, '▁지적했다': 4337, '▁지점': 4338, '▁지정': 4339, '▁지지': 4340, '▁지출': 4341, '▁지켜': 4342, '▁지켜보': 4343, '▁지키': 4344, '▁지표': 4345, '▁지하': 4346, '▁지하철': 4347, '▁지휘': 4348, '▁직': 4349, '▁직구': 4350, '▁직무': 4351, '▁직업': 4352, '▁직원': 4353, '▁직원들': 4354, '▁직장': 4355, '▁직장인': 4356, '▁직전': 4357, '▁직접': 4358, '▁직후': 4359, '▁진': 4360, '▁진단': 4361, '▁진료': 4362, '▁진보': 4363, '▁진술': 4364, '▁진실': 4365, '▁진입': 4366, '▁진주의료원': 4367, '▁진짜': 4368, '▁진출': 4369, '▁진행': 4370, '▁진행됐다': 4371, '▁진행되는': 4372, '▁진행된': 4373, '▁진행된다': 4374, '▁진행될': 4375, '▁진행하고': 4376, '▁진행한다': 4377, '▁진화': 4378, '▁질': 4379, '▁질문': 4380, '▁질문에': 4381, '▁질병': 4382, '▁짐': 4383, '▁집': 4384, '▁집계': 4385, '▁집계됐다': 4386, '▁집권': 4387, '▁집단': 4388, '▁집중': 4389, '▁집행': 4390, '▁집행유예': 4391, '▁집회': 4392, '▁짓': 4393, '▁징계': 4394, '▁징역': 4395, '▁짜': 4396, '▁짧은': 4397, '▁쪽': 4398, '▁쪽으로': 4399, '▁찍': 4400, '▁찍은': 4401, '▁차': 4402, '▁차관': 4403, '▁차기': 4404, '▁차단': 4405, '▁차량': 4406, '▁차례': 4407, '▁차별': 4408, '▁차별화된': 4409, '▁차세대': 4410, '▁차원에서': 4411, '▁차원의': 4412, '▁차이': 4413, '▁차이가': 4414, '▁차지': 4415, '▁차지하는': 4416, '▁차지한': 4417, '▁차지했다': 4418, '▁차질': 4419, '▁착': 4420, '▁착공': 4421, '▁착륙': 4422, '▁착수': 4423, '▁착용': 4424, '▁찬': 4425, '▁찬성': 4426, '▁참': 4427, '▁참가': 4428, '▁참가자들': 4429, '▁참고': 4430, '▁참석': 4431, '▁참석한': 4432, '▁참석해': 4433, '▁참석했다': 4434, '▁참여': 4435, '▁참여한': 4436, '▁참의원': 4437, '▁창': 4438, '▁창단': 4439, '▁창업': 4440, '▁창원': 4441, '▁창조': 4442, '▁창출': 4443, '▁찾기': 4444, '▁찾는': 4445, '▁찾아': 4446, '▁찾았다': 4447, '▁찾은': 4448, '▁찾을': 4449, '▁찾지': 4450, '▁채': 4451, '▁채권': 4452, '▁채널': 4453, '▁채무': 4454, '▁채용': 4455, '▁채택': 4456, '▁책': 4457, '▁책임': 4458, '▁책임을': 4459, '▁챔피언십': 4460, '▁챙겨': 4461, '▁챙기': 4462, '▁챙긴': 4463, '▁처': 4464, '▁처리': 4465, '▁처벌': 4466, '▁처분': 4467, '▁처음': 4468, '▁처음으로': 4469, '▁처음이다': 4470, '▁천': 4471, '▁천안': 4472, '▁철': 4473, '▁철강': 4474, '▁철거': 4475, '▁철저한': 4476, '▁철저히': 4477, '▁철학': 4478, '▁철회': 4479, '▁첨단': 4480, '▁첫': 4481, '▁첫날': 4482, '▁청': 4483, '▁청구': 4484, '▁청년': 4485, '▁청문회': 4486, '▁청소년': 4487, '▁청약': 4488, '▁청와대': 4489, '▁청주': 4490, '▁체': 4491, '▁체결': 4492, '▁체결했다': 4493, '▁체계적': 4494, '▁체력': 4495, '▁체제': 4496, '▁체크': 4497, '▁체포': 4498, '▁체험': 4499, '▁첼시': 4500, '▁초': 4501, '▁초과': 4502, '▁초기': 4503, '▁초대': 4504, '▁초등학교': 4505, '▁초반': 4506, '▁초점을': 4507, '▁초청': 4508, '▁촉구': 4509, '▁촉구했다': 4510, '▁촉진': 4511, '▁총': 4512, '▁총괄': 4513, '▁총기': 4514, '▁총리': 4515, '▁총장': 4516, '▁총재': 4517, '▁촬영': 4518, '▁최': 4519, '▁최강': 4520, '▁최강희': 4521, '▁최고': 4522, '▁최고위원': 4523, '▁최고의': 4524, '▁최근': 4525, '▁최다': 4526, '▁최대': 4527, '▁최대주주': 4528, '▁최대한': 4529, '▁최선을': 4530, '▁최소': 4531, '▁최소화': 4532, '▁최신': 4533, '▁최악': 4534, '▁최우선': 4535, '▁최우수': 4536, '▁최저': 4537, '▁최종': 4538, '▁최초': 4539, '▁최초로': 4540, '▁추': 4541, '▁추가': 4542, '▁추가로': 4543, '▁추격': 4544, '▁추구': 4545, '▁추락': 4546, '▁추산': 4547, '▁추세': 4548, '▁추신수': 4549, '▁추신수는': 4550, '▁추억': 4551, '▁추적': 4552, '▁추정': 4553, '▁추정된다': 4554, '▁추진': 4555, '▁추진하고': 4556, '▁추진할': 4557, '▁추징금': 4558, '▁추천': 4559, '▁추첨': 4560, '▁축': 4561, '▁축구': 4562, '▁축구협회': 4563, '▁축소': 4564, '▁축제': 4565, '▁축하': 4566, '▁춘천': 4567, '▁출': 4568, '▁출구전략': 4569, '▁출국': 4570, '▁출근': 4571, '▁출동': 4572, '▁출발': 4573, '▁출범': 4574, '▁출산': 4575, '▁출석': 4576, '▁출시': 4577, '▁출신': 4578, '▁출연': 4579, '▁출연한': 4580, '▁출연해': 4581, '▁출연했다': 4582, '▁출입': 4583, '▁출장': 4584, '▁출전': 4585, '▁출처': 4586, '▁출판': 4587, '▁충': 4588, '▁충격': 4589, '▁충남': 4590, '▁충돌': 4591, '▁충북': 4592, '▁충분': 4593, '▁충분히': 4594, '▁충실': 4595, '▁충족': 4596, '▁충청': 4597, '▁취': 4598, '▁취급': 4599, '▁취득': 4600, '▁취득세': 4601, '▁취소': 4602, '▁취약': 4603, '▁취업': 4604, '▁취임': 4605, '▁취임식': 4606, '▁취재': 4607, '▁취재진': 4608, '▁취지': 4609, '▁취하고': 4610, '▁측': 4611, '▁측근': 4612, '▁측면': 4613, '▁측면에서': 4614, '▁측은': 4615, '▁측정': 4616, '▁치': 4617, '▁치러': 4618, '▁치료': 4619, '▁치료를': 4620, '▁치르': 4621, '▁치른': 4622, '▁치솟': 4623, '▁치열': 4624, '▁치열한': 4625, '▁친': 4626, '▁친구': 4627, '▁친구들': 4628, '▁친환경': 4629, '▁침': 4630, '▁침수': 4631, '▁침체': 4632, '▁침해': 4633, '▁칭찬': 4634, '▁카': 4635, '▁카드': 4636, '▁카드사': 4637, '▁카리스마': 4638, '▁카메라': 4639, '▁카카오': 4640, '▁카페': 4641, '▁칼': 4642, '▁캐': 4643, '▁캐나다': 4644, '▁캐릭터': 4645, '▁캐스팅': 4646, '▁캘리포니아': 4647, '▁캠페인': 4648, '▁캠프': 4649, '▁캠핑': 4650, '▁캡처': 4651, '▁커': 4652, '▁커뮤니티': 4653, '▁커지고': 4654, '▁커플': 4655, '▁커피': 4656, '▁컨트롤': 4657, '▁컬러': 4658, '▁컴백': 4659, '▁컴퓨터': 4660, '▁컸다': 4661, '▁케': 4662, '▁케이': 4663, '▁케이블': 4664, '▁코': 4665, '▁코너': 4666, '▁코넥스': 4667, '▁코리아': 4668, '▁코미디': 4669, '▁코스': 4670, '▁코스닥': 4671, '▁코스피': 4672, '▁코스피지수': 4673, '▁코치': 4674, '▁콘': 4675, '▁콘서트': 4676, '▁콘셉트': 4677, '▁콘텐츠': 4678, '▁콜': 4679, '▁쿠': 4680, '▁크': 4681, '▁크게': 4682, '▁크기': 4683, '▁크다': 4684, '▁크로스': 4685, '▁크루즈': 4686, '▁크리스': 4687, '▁큰': 4688, '▁클': 4689, '▁클라라': 4690, '▁클래식': 4691, '▁클럽': 4692, '▁키': 4693, '▁키스': 4694, '▁키우': 4695, '▁키워': 4696, '▁키워드': 4697, '▁타': 4698, '▁타격': 4699, '▁타고': 4700, '▁타선': 4701, '▁타율': 4702, '▁타이': 4703, '▁타이틀': 4704, '▁타이틀곡': 4705, '▁타자': 4706, '▁탄': 4707, '▁탄력': 4708, '▁탄생': 4709, '▁탄탄한': 4710, '▁탈': 4711, '▁탈락': 4712, '▁탈출': 4713, '▁탐': 4714, '▁탑승': 4715, '▁탑재': 4716, '▁탓': 4717, '▁탓에': 4718, '▁탔다': 4719, '▁태': 4720, '▁태국': 4721, '▁태도': 4722, '▁태블릿': 4723, '▁태양': 4724, '▁태양광': 4725, '▁태어났다': 4726, '▁택시': 4727, '▁터': 4728, '▁터치': 4729, '▁터키': 4730, '▁털어놓': 4731, '▁털어놨다': 4732, '▁테': 4733, '▁테러': 4734, '▁테마': 4735, '▁테스트': 4736, '▁토': 4737, '▁토대로': 4738, '▁토론회': 4739, '▁토지': 4740, '▁토크쇼': 4741, '▁톱스타': 4742, '▁통': 4743, '▁통계': 4744, '▁통과': 4745, '▁통보': 4746, '▁통산': 4747, '▁통상': 4748, '▁통신': 4749, '▁통일': 4750, '▁통일부': 4751, '▁통제': 4752, '▁통증': 4753, '▁통한': 4754, '▁통합': 4755, '▁통해': 4756, '▁통행': 4757, '▁통화': 4758, '▁통화정책': 4759, '▁퇴': 4760, '▁퇴직': 4761, '▁투': 4762, '▁투구': 4763, '▁투명': 4764, '▁투수': 4765, '▁투어': 4766, '▁투입': 4767, '▁투자': 4768, '▁투자의견': 4769, '▁투자자': 4770, '▁투자자들': 4771, '▁투표': 4772, '▁트': 4773, '▁트렌드': 4774, '▁트위터': 4775, '▁트위터에': 4776, '▁특': 4777, '▁특별': 4778, '▁특별사면': 4779, '▁특별한': 4780, '▁특사': 4781, '▁특성': 4782, '▁특수': 4783, '▁특위': 4784, '▁특유': 4785, '▁특정': 4786, '▁특집': 4787, '▁특징': 4788, '▁특징이다': 4789, '▁특허': 4790, '▁특혜': 4791, '▁특히': 4792, '▁틀': 4793, '▁티아라': 4794, '▁티저': 4795, '▁티켓': 4796, '▁팀': 4797, '▁팀의': 4798, '▁파': 4799, '▁파견': 4800, '▁파괴': 4801, '▁파악': 4802, '▁파워': 4803, '▁파트너': 4804, '▁판': 4805, '▁판결': 4806, '▁판단': 4807, '▁판단했다': 4808, '▁판매': 4809, '▁판매량': 4810, '▁판문점': 4811, '▁판사': 4812, '▁판정': 4813, '▁팔': 4814, '▁패': 4815, '▁패널': 4816, '▁패배': 4817, '▁패션': 4818, '▁패스': 4819, '▁패키지': 4820, '▁팬': 4821, '▁팬들': 4822, '▁팬들에게': 4823, '▁팬들의': 4824, '▁팽팽': 4825, '▁퍼': 4826, '▁퍼포먼스': 4827, '▁펀드': 4828, '▁페': 4829, '▁페널티': 4830, '▁페이스북': 4831, '▁편': 4832, '▁편리하게': 4833, '▁편성': 4834, '▁편의점': 4835, '▁편집': 4836, '▁펼쳐': 4837, '▁펼쳤다': 4838, '▁펼치고': 4839, '▁펼칠': 4840, '▁평': 4841, '▁평가': 4842, '▁평가를': 4843, '▁평가했다': 4844, '▁평균': 4845, '▁평균자책점': 4846, '▁평생': 4847, '▁평소': 4848, '▁평창': 4849, '▁평택': 4850, '▁평화': 4851, '▁폐': 4852, '▁폐기': 4853, '▁폐쇄': 4854, '▁폐지': 4855, '▁포': 4856, '▁포기': 4857, '▁포스코': 4858, '▁포인트': 4859, '▁포즈를': 4860, '▁포착': 4861, '▁포털': 4862, '▁포토': 4863, '▁포함': 4864, '▁포함돼': 4865, '▁포함됐다': 4866, '▁포함된': 4867, '▁포함한': 4868, '▁포함해': 4869, '▁포항': 4870, '▁폭': 4871, '▁폭력': 4872, '▁폭로': 4873, '▁폭발': 4874, '▁폭염': 4875, '▁폭우': 4876, '▁폭풍': 4877, '▁폭행': 4878, '▁폴': 4879, '▁표': 4880, '▁표명': 4881, '▁표시': 4882, '▁표정': 4883, '▁표준': 4884, '▁표현': 4885, '▁푸': 4886, '▁푸이그': 4887, '▁풀': 4888, '▁풀어': 4889, '▁풀이된다': 4890, '▁품': 4891, '▁품목': 4892, '▁품질': 4893, '▁풍': 4894, '▁풍부한': 4895, '▁프랑스': 4896, '▁프로': 4897, '▁프로그램': 4898, '▁프로그램을': 4899, '▁프로모션': 4900, '▁프로야구': 4901, '▁프로젝트': 4902, '▁프로축구': 4903, '▁프리': 4904, '▁프리미어리그': 4905, '▁프리미엄': 4906, '▁플랫폼': 4907, '▁플레이': 4908, '▁피': 4909, '▁피부': 4910, '▁피의자': 4911, '▁피해': 4912, '▁피해가': 4913, '▁피해를': 4914, '▁피해자': 4915, '▁필': 4916, '▁필리핀': 4917, '▁필수': 4918, '▁필요': 4919, '▁필요가': 4920, '▁필요성': 4921, '▁필요하다': 4922, '▁필요한': 4923, '▁하': 4924, '▁하겠다': 4925, '▁하고': 4926, '▁하기': 4927, '▁하나': 4928, '▁하나로': 4929, '▁하는': 4930, '▁하는데': 4931, '▁하다': 4932, '▁하락': 4933, '▁하락세': 4934, '▁하락한': 4935, '▁하락했다': 4936, '▁하루': 4937, '▁하며': 4938, '▁하면': 4939, '▁하면서': 4940, '▁하반기': 4941, '▁하이': 4942, '▁하자': 4943, '▁하정우': 4944, '▁하지': 4945, '▁하지만': 4946, '▁하향': 4947, '▁학': 4948, '▁학교': 4949, '▁학교폭력': 4950, '▁학부모': 4951, '▁학생': 4952, '▁학생들': 4953, '▁학습': 4954, '▁한': 4955, '▁한강': 4956, '▁한계': 4957, '▁한국': 4958, '▁한국거래소': 4959, '▁한국은': 4960, '▁한국은행': 4961, '▁한국의': 4962, '▁한국인': 4963, '▁한국전력': 4964, '▁한다': 4965, '▁한다고': 4966, '▁한다는': 4967, '▁한때': 4968, '▁한마디': 4969, '▁한반도': 4970, '▁한번': 4971, '▁한층': 4972, '▁한파': 4973, '▁한편': 4974, '▁한혜진': 4975, '▁한화': 4976, '▁할': 4977, '▁할리우드': 4978, '▁할머니': 4979, '▁할부': 4980, '▁할인': 4981, '▁함': 4982, '▁함께': 4983, '▁합': 4984, '▁합격': 4985, '▁합니다': 4986, '▁합동': 4987, '▁합류': 4988, '▁합리적': 4989, '▁합병': 4990, '▁합의': 4991, '▁항': 4992, '▁항공': 4993, '▁항공기': 4994, '▁항목': 4995, '▁항상': 4996, '▁항소심': 4997, '▁해': 4998, '▁해결': 4999, '▁해당': 5000, '▁해당하는': 5001, '▁해도': 5002, '▁해명': 5003, '▁해명했다': 5004, '▁해병대': 5005, '▁해상': 5006, '▁해서': 5007, '▁해석': 5008, '▁해소': 5009, '▁해야': 5010, '▁해양': 5011, '▁해외': 5012, '▁해운대': 5013, '▁핵': 5014, '▁핵실험': 5015, '▁핵심': 5016, '▁했': 5017, '▁했는데': 5018, '▁했다': 5019, '▁했던': 5020, '▁했지만': 5021, '▁행': 5022, '▁행동': 5023, '▁행보': 5024, '▁행복': 5025, '▁행사': 5026, '▁행사를': 5027, '▁행사에': 5028, '▁행위': 5029, '▁행정': 5030, '▁행진': 5031, '▁향': 5032, '▁향상': 5033, '▁향한': 5034, '▁향해': 5035, '▁향후': 5036, '▁허': 5037, '▁허가': 5038, '▁허리': 5039, '▁허용': 5040, '▁허위': 5041, '▁헌': 5042, '▁헌법': 5043, '▁헌법재판소장': 5044, '▁헌재': 5045, '▁헤': 5046, '▁헤어': 5047, '▁혁신': 5048, '▁현': 5049, '▁현금': 5050, '▁현대': 5051, '▁현대건설': 5052, '▁현대모비스': 5053, '▁현대자동차': 5054, '▁현대중공업': 5055, '▁현대차': 5056, '▁현대캐피탈': 5057, '▁현상': 5058, '▁현실': 5059, '▁현안': 5060, '▁현역': 5061, '▁현장': 5062, '▁현장에서': 5063, '▁현재': 5064, '▁현재까지': 5065, '▁현지': 5066, '▁현행': 5067, '▁현황': 5068, '▁혈': 5069, '▁혐의': 5070, '▁혐의로': 5071, '▁혐의를': 5072, '▁협력': 5073, '▁협력업체': 5074, '▁협박': 5075, '▁협상': 5076, '▁협약': 5077, '▁협업': 5078, '▁협의': 5079, '▁협조': 5080, '▁형': 5081, '▁형사': 5082, '▁형성': 5083, '▁형식': 5084, '▁형제': 5085, '▁형태': 5086, '▁형태로': 5087, '▁혜택': 5088, '▁혜택을': 5089, '▁호': 5090, '▁호소': 5091, '▁호응': 5092, '▁호조': 5093, '▁호주': 5094, '▁호텔': 5095, '▁호투': 5096, '▁호평': 5097, '▁호흡': 5098, '▁혹은': 5099, '▁혼': 5100, '▁혼란': 5101, '▁혼자': 5102, '▁홀': 5103, '▁홈': 5104, '▁홈경기': 5105, '▁홈런': 5106, '▁홈페이지': 5107, '▁홍': 5108, '▁홍명보': 5109, '▁홍보': 5110, '▁홍콩': 5111, '▁화': 5112, '▁화려한': 5113, '▁화면': 5114, '▁화보': 5115, '▁화성': 5116, '▁화이트': 5117, '▁화장실': 5118, '▁화장품': 5119, '▁화재': 5120, '▁화제': 5121, '▁화제다': 5122, '▁화학': 5123, '▁확': 5124, '▁확대': 5125, '▁확보': 5126, '▁확산': 5127, '▁확실한': 5128, '▁확실히': 5129, '▁확인': 5130, '▁확인됐다': 5131, '▁확인할': 5132, '▁확장': 5133, '▁확정': 5134, '▁확충': 5135, '▁환': 5136, '▁환경': 5137, '▁환영': 5138, '▁환율': 5139, '▁환자': 5140, '▁활동': 5141, '▁활동을': 5142, '▁활발': 5143, '▁활성화': 5144, '▁활약': 5145, '▁활용': 5146, '▁활용해': 5147, '▁활주로': 5148, '▁황': 5149, '▁황금': 5150, '▁황우여': 5151, '▁회': 5152, '▁회계': 5153, '▁회담': 5154, '▁회복': 5155, '▁회사': 5156, '▁회사채': 5157, '▁회원': 5158, '▁회원가입': 5159, '▁회의': 5160, '▁회의록': 5161, '▁회의를': 5162, '▁회장': 5163, '▁회장은': 5164, '▁회장의': 5165, '▁회장이': 5166, '▁획득': 5167, '▁횡령': 5168, '▁효': 5169, '▁효과': 5170, '▁효과가': 5171, '▁효과를': 5172, '▁효과적': 5173, '▁효율성': 5174, '▁효율적': 5175, '▁후': 5176, '▁후문이다': 5177, '▁후반': 5178, '▁후반기': 5179, '▁후보': 5180, '▁후보자': 5181, '▁후속': 5182, '▁후원': 5183, '▁훈련': 5184, '▁훌륭': 5185, '▁훔친': 5186, '▁훨씬': 5187, '▁훼손': 5188, '▁휘': 5189, '▁휩쓸': 5190, '▁휴': 5191, '▁휴가': 5192, '▁휴대전화': 5193, '▁휴대폰': 5194, '▁휴식': 5195, '▁흐': 5196, '▁흐름': 5197, '▁흑자': 5198, '▁흔들': 5199, '▁흔적': 5200, '▁흘러': 5201, '▁흘리': 5202, '▁흡수': 5203, '▁흡연': 5204, '▁흥국생명': 5205, '▁흥미': 5206, '▁흥행': 5207, '▁희망': 5208, '▁희생': 5209, '▁히트': 5210, '▁힘': 5211, '▁힘든': 5212, '▁힘들': 5213, '▁힘을': 5214, '▁힘입어': 5215, '▁힙합': 5216, '■': 5217, '□': 5218, '▦': 5219, '▲': 5220, '△': 5221, '▶': 5222, '▷': 5223, '▼': 5224, '▽': 5225, '◀': 5226, '◆': 5227, '◇': 5228, '◈': 5229, '○': 5230, '★': 5231, '☎': 5232, '〃': 5233, '〈': 5234, '〉': 5235, '「': 5236, '」': 5237, '『': 5238, '』': 5239, '【': 5240, '】': 5241, '一': 5242, '三': 5243, '上': 5244, '下': 5245, '不': 5246, '中': 5247, '亞': 5248, '京': 5249, '人': 5250, '企': 5251, '倍': 5252, '先': 5253, '公': 5254, '前': 5255, '北': 5256, '南': 5257, '反': 5258, '史': 5259, '國': 5260, '報': 5261, '外': 5262, '大': 5263, '天': 5264, '女': 5265, '子': 5266, '安': 5267, '家': 5268, '對': 5269, '小': 5270, '山': 5271, '島': 5272, '州': 5273, '市': 5274, '平': 5275, '年': 5276, '弗': 5277, '心': 5278, '性': 5279, '故': 5280, '文': 5281, '新': 5282, '日': 5283, '晋': 5284, '朴': 5285, '李': 5286, '東': 5287, '株': 5288, '檢': 5289, '母': 5290, '比': 5291, '民': 5292, '江': 5293, '海': 5294, '無': 5295, '獨': 5296, '王': 5297, '生': 5298, '田': 5299, '甲': 5300, '男': 5301, '百': 5302, '盧': 5303, '知': 5304, '硏': 5305, '社': 5306, '美': 5307, '習': 5308, '胎': 5309, '與': 5310, '英': 5311, '草': 5312, '行': 5313, '西': 5314, '證': 5315, '車': 5316, '軍': 5317, '近': 5318, '道': 5319, '重': 5320, '野': 5321, '金': 5322, '銀': 5323, '電': 5324, '靑': 5325, '非': 5326, '韓': 5327, '高': 5328, '鬼': 5329, '가': 5330, '가격': 5331, '가구': 5332, '가량': 5333, '가족': 5334, '가지': 5335, '각': 5336, '간': 5337, '간다': 5338, '갇': 5339, '갈': 5340, '감': 5341, '감독': 5342, '감사': 5343, '감을': 5344, '갑': 5345, '값': 5346, '갓': 5347, '갔': 5348, '갔다': 5349, '강': 5350, '강남스타일': 5351, '강심장': 5352, '갖': 5353, '같': 5354, '같은': 5355, '갚': 5356, '개': 5357, '개국': 5358, '개그콘서트': 5359, '개로': 5360, '개를': 5361, '개발': 5362, '개사': 5363, '개선': 5364, '개성공단': 5365, '개월': 5366, '개월간': 5367, '개의': 5368, '개혁': 5369, '객': 5370, '갤': 5371, '갭': 5372, '갯': 5373, '갱': 5374, '갸': 5375, '걀': 5376, '거': 5377, '거나': 5378, '거래': 5379, '거래소': 5380, '거래일': 5381, '거리': 5382, '걱': 5383, '건': 5384, '건강': 5385, '건설': 5386, '건으로': 5387, '건전성': 5388, '걷': 5389, '걸': 5390, '걸음': 5391, '검': 5392, '검사': 5393, '검색': 5394, '검증': 5395, '검찰': 5396, '겁': 5397, '것': 5398, '겉': 5399, '게': 5400, '게임': 5401, '겐': 5402, '겔': 5403, '겟': 5404, '겠': 5405, '겠다': 5406, '겠다고': 5407, '겠다는': 5408, '겠습니다': 5409, '겠지만': 5410, '겨': 5411, '격': 5412, '겪': 5413, '견': 5414, '결': 5415, '결과': 5416, '결정': 5417, '결제': 5418, '결혼': 5419, '겸': 5420, '겹': 5421, '겼': 5422, '겼다': 5423, '경': 5424, '경기': 5425, '경기에서': 5426, '경영': 5427, '경쟁': 5428, '경쟁력': 5429, '경제': 5430, '경찰': 5431, '경찰서': 5432, '경찰서는': 5433, '경찰청': 5434, '곁': 5435, '계': 5436, '계약': 5437, '계획': 5438, '고': 5439, '고객': 5440, '고등학교': 5441, '고속도로': 5442, '곡': 5443, '곤': 5444, '곧': 5445, '골': 5446, '골을': 5447, '골프': 5448, '곰': 5449, '곱': 5450, '곳': 5451, '공': 5452, '공간': 5453, '공개': 5454, '공급': 5455, '공단': 5456, '공동체': 5457, '공무원': 5458, '공사': 5459, '공약': 5460, '공업': 5461, '공연': 5462, '공원': 5463, '공장': 5464, '공항': 5465, '공화국': 5466, '곶': 5467, '과': 5468, '과정': 5469, '과정에서': 5470, '과학': 5471, '과학기술': 5472, '곽': 5473, '관': 5474, '관계': 5475, '관광': 5476, '관련': 5477, '관리': 5478, '괄': 5479, '괌': 5480, '광': 5481, '광고': 5482, '광역시': 5483, '괜': 5484, '괴': 5485, '괴물': 5486, '굉': 5487, '교': 5488, '교사': 5489, '교섭': 5490, '교육': 5491, '교육청': 5492, '교통': 5493, '교회': 5494, '구': 5495, '구나': 5496, '구단': 5497, '구를': 5498, '구역': 5499, '구장에서': 5500, '구조': 5501, '구청장': 5502, '국': 5503, '국가': 5504, '국내': 5505, '국내외': 5506, '국민': 5507, '국장': 5508, '국정원': 5509, '국제': 5510, '국회': 5511, '군': 5512, '군은': 5513, '군의': 5514, '굳': 5515, '굴': 5516, '굵': 5517, '굶': 5518, '굽': 5519, '굿': 5520, '궁': 5521, '궂': 5522, '궈': 5523, '권': 5524, '권을': 5525, '궐': 5526, '궜': 5527, '궤': 5528, '귀': 5529, '귀태': 5530, '귄': 5531, '규': 5532, '규모': 5533, '규제': 5534, '규칙': 5535, '균': 5536, '귤': 5537, '그': 5538, '그동안': 5539, '그래': 5540, '그런': 5541, '그룹': 5542, '극': 5543, '극본': 5544, '극장': 5545, '근': 5546, '글': 5547, '글로벌': 5548, '긁': 5549, '금': 5550, '금리': 5551, '금속': 5552, '금액': 5553, '금융': 5554, '금융그룹': 5555, '금융지주': 5556, '금을': 5557, '급': 5558, '긋': 5559, '긍': 5560, '기': 5561, '기가': 5562, '기간': 5563, '기관': 5564, '기구': 5565, '기금': 5566, '기념': 5567, '기는': 5568, '기능': 5569, '기도': 5570, '기로': 5571, '기록': 5572, '기를': 5573, '기사': 5574, '기술': 5575, '기아차': 5576, '기업': 5577, '기업은행': 5578, '기에': 5579, '기자': 5580, '기준': 5581, '기지': 5582, '기획': 5583, '긴': 5584, '길': 5585, '김': 5586, '깁': 5587, '깃': 5588, '깅': 5589, '깊': 5590, '까': 5591, '까지': 5592, '깍': 5593, '깎': 5594, '깐': 5595, '깔': 5596, '깜': 5597, '깜찍': 5598, '깝': 5599, '깡': 5600, '깥': 5601, '깨': 5602, '깬': 5603, '꺼': 5604, '꺾': 5605, '껄': 5606, '껌': 5607, '껍': 5608, '껏': 5609, '껑': 5610, '께': 5611, '께서': 5612, '껴': 5613, '꼈': 5614, '꼬': 5615, '꼭': 5616, '꼴': 5617, '꼼': 5618, '꼽': 5619, '꽁': 5620, '꽂': 5621, '꽃': 5622, '꽃보다': 5623, '꽉': 5624, '꽝': 5625, '꽤': 5626, '꾀': 5627, '꾸': 5628, '꾼': 5629, '꿀': 5630, '꿇': 5631, '꿈': 5632, '꿋': 5633, '꿔': 5634, '꿨': 5635, '꿰': 5636, '뀌': 5637, '뀐': 5638, '뀔': 5639, '끄': 5640, '끄러': 5641, '끈': 5642, '끊': 5643, '끌': 5644, '끓': 5645, '끔': 5646, '끗': 5647, '끝': 5648, '끼': 5649, '끼리': 5650, '끽': 5651, '낀': 5652, '낄': 5653, '낌': 5654, '나': 5655, '나갈': 5656, '나갔다': 5657, '나는': 5658, '나라': 5659, '나무': 5660, '낙': 5661, '낚': 5662, '난': 5663, '날': 5664, '낡': 5665, '남': 5666, '남북': 5667, '남자': 5668, '납': 5669, '낫': 5670, '났': 5671, '났다': 5672, '낭': 5673, '낮': 5674, '낯': 5675, '낱': 5676, '낳': 5677, '내': 5678, '내가': 5679, '내기': 5680, '낵': 5681, '낸': 5682, '낼': 5683, '냄': 5684, '냅': 5685, '냈': 5686, '냈다': 5687, '냉': 5688, '냐': 5689, '냥': 5690, '너': 5691, '너목들': 5692, '넉': 5693, '넋': 5694, '넌': 5695, '널': 5696, '넓': 5697, '넘': 5698, '넛': 5699, '넝': 5700, '넣': 5701, '네': 5702, '네요': 5703, '네트워크': 5704, '넥': 5705, '넨': 5706, '넬': 5707, '넷': 5708, '넸': 5709, '녀': 5710, '녁': 5711, '년': 5712, '년간': 5713, '년까지': 5714, '년대': 5715, '년만에': 5716, '년부터': 5717, '년생': 5718, '년에': 5719, '년에는': 5720, '년째': 5721, '념': 5722, '녔': 5723, '녕': 5724, '노': 5725, '노동': 5726, '노조': 5727, '노컷': 5728, '노트': 5729, '녹': 5730, '논': 5731, '놀': 5732, '놈': 5733, '농': 5734, '농협': 5735, '높': 5736, '놓': 5737, '놓고': 5738, '놔': 5739, '놨': 5740, '뇌': 5741, '뇨': 5742, '누': 5743, '눅': 5744, '눈': 5745, '눌': 5746, '눔': 5747, '눕': 5748, '눠': 5749, '눴': 5750, '뉘': 5751, '뉜': 5752, '뉴': 5753, '뉴스': 5754, '뉴시스': 5755, '늄': 5756, '느': 5757, '느냐': 5758, '늑': 5759, '는': 5760, '는데': 5761, '늘': 5762, '늙': 5763, '늠': 5764, '능': 5765, '능력': 5766, '늦': 5767, '늪': 5768, '늬': 5769, '니': 5770, '니까': 5771, '니다': 5772, '니스': 5773, '니아': 5774, '닉': 5775, '닌': 5776, '닐': 5777, '님': 5778, '닙': 5779, '닛': 5780, '닝': 5781, '다': 5782, '다른': 5783, '다면': 5784, '다운': 5785, '닥': 5786, '닦': 5787, '단': 5788, '단계': 5789, '단지': 5790, '단체': 5791, '닫': 5792, '달': 5793, '달라': 5794, '달러': 5795, '닭': 5796, '닮': 5797, '담': 5798, '담당': 5799, '담보대출': 5800, '답': 5801, '닷': 5802, '닷컴': 5803, '당': 5804, '당국': 5805, '당선인': 5806, '닿': 5807, '대': 5808, '대가': 5809, '대강': 5810, '대교': 5811, '대로': 5812, '대를': 5813, '대비': 5814, '대사관': 5815, '대상': 5816, '대우증권': 5817, '대의': 5818, '대책': 5819, '대출': 5820, '대통령': 5821, '대표': 5822, '대표팀': 5823, '대학': 5824, '대학교': 5825, '대학원': 5826, '대한': 5827, '대화록': 5828, '대회': 5829, '댁': 5830, '댄': 5831, '댈': 5832, '댐': 5833, '댓': 5834, '댔': 5835, '댜': 5836, '더': 5837, '더니': 5838, '더라': 5839, '더라도': 5840, '덕': 5841, '던': 5842, '덜': 5843, '덟': 5844, '덤': 5845, '덥': 5846, '덧': 5847, '덩': 5848, '덮': 5849, '데': 5850, '데다': 5851, '데일리': 5852, '덱': 5853, '덴': 5854, '델': 5855, '뎀': 5856, '뎁': 5857, '뎌': 5858, '도': 5859, '도로': 5860, '도록': 5861, '도서관': 5862, '도시': 5863, '도지사': 5864, '독': 5865, '돈': 5866, '돋': 5867, '돌': 5868, '돔': 5869, '돕': 5870, '돗': 5871, '동': 5872, '동맹': 5873, '동안': 5874, '동향': 5875, '돼': 5876, '돼야': 5877, '됐': 5878, '됐고': 5879, '됐다': 5880, '됐던': 5881, '됐습니다': 5882, '됐으나': 5883, '됐으며': 5884, '됐지만': 5885, '되': 5886, '되고': 5887, '되기': 5888, '되는': 5889, '되도록': 5890, '되며': 5891, '되면': 5892, '되면서': 5893, '되어': 5894, '되었': 5895, '되었다': 5896, '되자': 5897, '되지': 5898, '된': 5899, '된다': 5900, '된다면': 5901, '될': 5902, '됨': 5903, '됨에': 5904, '됩': 5905, '됩니다': 5906, '두': 5907, '둑': 5908, '둔': 5909, '둘': 5910, '둠': 5911, '둡': 5912, '둥': 5913, '둬': 5914, '뒀': 5915, '뒤': 5916, '뒷': 5917, '듀': 5918, '듈': 5919, '드': 5920, '드는': 5921, '드라마': 5922, '드래곤': 5923, '드리': 5924, '드립니다': 5925, '득': 5926, '득점': 5927, '든': 5928, '든지': 5929, '듣': 5930, '들': 5931, '들과': 5932, '들도': 5933, '들어': 5934, '들에': 5935, '들에게': 5936, '들은': 5937, '들을': 5938, '들의': 5939, '들이': 5940, '듬': 5941, '듭': 5942, '듯': 5943, '등': 5944, '등급': 5945, '등록': 5946, '디': 5947, '디스크': 5948, '디스플레이': 5949, '디지털': 5950, '딕': 5951, '딘': 5952, '딛': 5953, '딜': 5954, '딤': 5955, '딧': 5956, '딩': 5957, '딪': 5958, '따': 5959, '딱': 5960, '딴': 5961, '딸': 5962, '땀': 5963, '땅': 5964, '때': 5965, '땐': 5966, '땠': 5967, '땡': 5968, '떠': 5969, '떡': 5970, '떤': 5971, '떨': 5972, '떳': 5973, '떴': 5974, '떻': 5975, '떼': 5976, '뗀': 5977, '뗄': 5978, '뗐': 5979, '또': 5980, '똑': 5981, '똘': 5982, '똥': 5983, '뚜': 5984, '뚝': 5985, '뚫': 5986, '뚱': 5987, '뛰': 5988, '뛴': 5989, '뛸': 5990, '뜨': 5991, '뜩': 5992, '뜬': 5993, '뜯': 5994, '뜰': 5995, '뜸': 5996, '뜻': 5997, '띄': 5998, '띈': 5999, '띔': 6000, '띠': 6001, '띤': 6002, '라': 6003, '라고': 6004, '라는': 6005, '라도': 6006, '라디오스타': 6007, '라며': 6008, '라면': 6009, '라운드': 6010, '라이': 6011, '라이트': 6012, '라이프': 6013, '라인': 6014, '락': 6015, '란': 6016, '랄': 6017, '람': 6018, '랍': 6019, '랏': 6020, '랐': 6021, '랑': 6022, '래': 6023, '랙': 6024, '랜': 6025, '랜드': 6026, '랠': 6027, '램': 6028, '랩': 6029, '랫': 6030, '랬': 6031, '랭': 6032, '랴': 6033, '략': 6034, '량': 6035, '량이': 6036, '러': 6037, '러스': 6038, '럭': 6039, '런': 6040, '런닝맨': 6041, '럴': 6042, '럼': 6043, '럽': 6044, '럿': 6045, '렀': 6046, '렀다': 6047, '렁': 6048, '렇': 6049, '레': 6050, '레드': 6051, '레미제라블': 6052, '레스': 6053, '레이': 6054, '렉': 6055, '렌': 6056, '렐': 6057, '렘': 6058, '렛': 6059, '려': 6060, '려고': 6061, '려는': 6062, '려면': 6063, '력': 6064, '력을': 6065, '력이': 6066, '련': 6067, '렬': 6068, '렴': 6069, '렵': 6070, '렷': 6071, '렸': 6072, '렸고': 6073, '렸다': 6074, '렸던': 6075, '렸지만': 6076, '령': 6077, '례': 6078, '로': 6079, '로부터': 6080, '로서': 6081, '로운': 6082, '록': 6083, '론': 6084, '롤': 6085, '롬': 6086, '롭': 6087, '롭게': 6088, '롯': 6089, '롯데': 6090, '롱': 6091, '뢰': 6092, '료': 6093, '룡': 6094, '루': 6095, '루타': 6096, '룩': 6097, '룬': 6098, '룰': 6099, '룸': 6100, '룹': 6101, '룻': 6102, '룽': 6103, '뤄': 6104, '뤘': 6105, '뤼': 6106, '류': 6107, '륙': 6108, '륜': 6109, '률': 6110, '륨': 6111, '륭': 6112, '르': 6113, '르트': 6114, '른': 6115, '를': 6116, '름': 6117, '릅': 6118, '릇': 6119, '릉': 6120, '릎': 6121, '리': 6122, '리그': 6123, '리는': 6124, '리를': 6125, '리바운드': 6126, '리스': 6127, '리스트': 6128, '리아': 6129, '리조트': 6130, '리지': 6131, '릭': 6132, '린': 6133, '린다': 6134, '릴': 6135, '림': 6136, '립': 6137, '릿': 6138, '링': 6139, '링크': 6140, '마': 6141, '마다': 6142, '마을': 6143, '마음': 6144, '마저': 6145, '마케팅': 6146, '마켓': 6147, '마트': 6148, '막': 6149, '만': 6150, '만달러': 6151, '만명': 6152, '만원': 6153, '만원으로': 6154, '만원을': 6155, '만으로': 6156, '만큼': 6157, '많': 6158, '맏': 6159, '말': 6160, '맑': 6161, '맘': 6162, '맙': 6163, '맛': 6164, '망': 6165, '맞': 6166, '맡': 6167, '매': 6168, '매매': 6169, '매출': 6170, '매치': 6171, '맥': 6172, '맨': 6173, '맴': 6174, '맵': 6175, '맷': 6176, '맹': 6177, '맺': 6178, '머': 6179, '머니': 6180, '머니투데이': 6181, '머리': 6182, '먹': 6183, '먼': 6184, '먼트': 6185, '멀': 6186, '멈': 6187, '멋': 6188, '멍': 6189, '메': 6190, '메이': 6191, '멕': 6192, '멘': 6193, '멜': 6194, '멤': 6195, '멧': 6196, '며': 6197, '면': 6198, '면서': 6199, '면서도': 6200, '면세점': 6201, '멸': 6202, '몄': 6203, '명': 6204, '명과': 6205, '명에게': 6206, '명으로': 6207, '명은': 6208, '명을': 6209, '명의': 6210, '명이': 6211, '몇': 6212, '모': 6213, '모델': 6214, '모바일': 6215, '모씨': 6216, '목': 6217, '목표': 6218, '몫': 6219, '몬': 6220, '몰': 6221, '몸': 6222, '몹': 6223, '못': 6224, '몽': 6225, '뫼': 6226, '묘': 6227, '무': 6228, '무릎팍도사': 6229, '무역': 6230, '무한도전': 6231, '묵': 6232, '묶': 6233, '문': 6234, '문을': 6235, '문제': 6236, '문학': 6237, '문화': 6238, '문화예술': 6239, '묻': 6240, '물': 6241, '물을': 6242, '물질': 6243, '뭄': 6244, '뭇': 6245, '뭉': 6246, '뭐': 6247, '뭔': 6248, '뭘': 6249, '뮌': 6250, '뮤': 6251, '뮤직': 6252, '뮬': 6253, '므': 6254, '미': 6255, '미국': 6256, '미디어': 6257, '미래': 6258, '미술관': 6259, '미스터': 6260, '믹': 6261, '믹스': 6262, '민': 6263, '민주': 6264, '민주당': 6265, '믿': 6266, '밀': 6267, '밋': 6268, '밌': 6269, '밍': 6270, '및': 6271, '밑': 6272, '바': 6273, '바다': 6274, '바람': 6275, '바르셀로나': 6276, '바이': 6277, '바이오': 6278, '바퀴': 6279, '박': 6280, '박근혜': 6281, '박람회': 6282, '박물관': 6283, '밖': 6284, '밖에': 6285, '반': 6286, '반도체': 6287, '받': 6288, '받고': 6289, '받는': 6290, '받아': 6291, '받았다': 6292, '받은': 6293, '받을': 6294, '발': 6295, '발굴': 6296, '발언': 6297, '발전': 6298, '발전소': 6299, '발표': 6300, '밝': 6301, '밟': 6302, '밤': 6303, '밥': 6304, '방': 6305, '방송': 6306, '방식': 6307, '방안': 6308, '방위': 6309, '방향': 6310, '밭': 6311, '배': 6312, '배우': 6313, '백': 6314, '백화점': 6315, '밴': 6316, '밴드': 6317, '밸': 6318, '뱀': 6319, '뱃': 6320, '뱅': 6321, '뱉': 6322, '버': 6323, '버리': 6324, '버린': 6325, '버스': 6326, '벅': 6327, '번': 6328, '번째': 6329, '번호': 6330, '번홀': 6331, '벌': 6332, '범': 6333, '범죄': 6334, '법': 6335, '법원': 6336, '법을': 6337, '법인': 6338, '벗': 6339, '벙': 6340, '베': 6341, '베르': 6342, '베를린': 6343, '베이': 6344, '베이스': 6345, '벡': 6346, '벤': 6347, '벤처': 6348, '벨': 6349, '벨트': 6350, '벳': 6351, '벵': 6352, '벼': 6353, '벽': 6354, '변': 6355, '별': 6356, '별로': 6357, '별로는': 6358, '볍': 6359, '볐': 6360, '병': 6361, '병원': 6362, '볕': 6363, '보': 6364, '보건': 6365, '보고': 6366, '보고서': 6367, '보기': 6368, '보는': 6369, '보니': 6370, '보다': 6371, '보다는': 6372, '보드': 6373, '보이': 6374, '보장': 6375, '보증': 6376, '보험': 6377, '보호': 6378, '복': 6379, '복지': 6380, '복합': 6381, '볶': 6382, '본': 6383, '본부': 6384, '본부장': 6385, '볼': 6386, '볼넷': 6387, '볼륨': 6388, '봄': 6389, '봅': 6390, '봇': 6391, '봉': 6392, '봐': 6393, '봐야': 6394, '봤': 6395, '봤다': 6396, '뵙': 6397, '부': 6398, '부가': 6399, '부는': 6400, '부담': 6401, '부동산': 6402, '부르크': 6403, '부문': 6404, '부산': 6405, '부장': 6406, '부장검사': 6407, '부장판사': 6408, '부처': 6409, '부터': 6410, '부품': 6411, '북': 6412, '북방한계선': 6413, '북부': 6414, '북한': 6415, '분': 6416, '분과': 6417, '분기': 6418, '분기에': 6419, '분께': 6420, '분석': 6421, '분야': 6422, '분쯤': 6423, '불': 6424, '붉': 6425, '붐': 6426, '붓': 6427, '붕': 6428, '붙': 6429, '뷔': 6430, '뷰': 6431, '브': 6432, '브라': 6433, '브랜드': 6434, '브레이크': 6435, '브리': 6436, '븐': 6437, '블': 6438, '블랙': 6439, '블록': 6440, '비': 6441, '비가': 6442, '비를': 6443, '비서관': 6444, '비아': 6445, '비용': 6446, '비율': 6447, '비치': 6448, '빅': 6449, '빈': 6450, '빌': 6451, '빌딩': 6452, '빔': 6453, '빕': 6454, '빗': 6455, '빙': 6456, '빚': 6457, '빛': 6458, '빠': 6459, '빡': 6460, '빨': 6461, '빴': 6462, '빵': 6463, '빼': 6464, '빽': 6465, '뺀': 6466, '뺏': 6467, '뺐': 6468, '뺑': 6469, '뺨': 6470, '뻐': 6471, '뻑': 6472, '뻔': 6473, '뻗': 6474, '뻤': 6475, '뻥': 6476, '뼈': 6477, '뽀': 6478, '뽐': 6479, '뽑': 6480, '뽕': 6481, '뾰': 6482, '뿌': 6483, '뿐': 6484, '뿐만': 6485, '뿔': 6486, '뿜': 6487, '쁘': 6488, '쁜': 6489, '쁠': 6490, '쁨': 6491, '삐': 6492, '사': 6493, '사가': 6494, '사건': 6495, '사고': 6496, '사는': 6497, '사람': 6498, '사랑': 6499, '사를': 6500, '사무소': 6501, '사실': 6502, '사업': 6503, '사업본부': 6504, '사업부': 6505, '사업자': 6506, '사와': 6507, '사의': 6508, '사이드': 6509, '사이트': 6510, '사진': 6511, '사진제공': 6512, '사항': 6513, '사회': 6514, '삭': 6515, '산': 6516, '산업': 6517, '산업단지': 6518, '살': 6519, '삶': 6520, '삼': 6521, '삼성': 6522, '삼성전자': 6523, '삽': 6524, '삿': 6525, '샀': 6526, '상': 6527, '상공회의소': 6528, '상담': 6529, '상당히': 6530, '상을': 6531, '상태': 6532, '상품': 6533, '상황': 6534, '샅': 6535, '새': 6536, '새누리당': 6537, '색': 6538, '샌': 6539, '샐': 6540, '샘': 6541, '생': 6542, '생명': 6543, '생산': 6544, '생활': 6545, '샤': 6546, '샬': 6547, '샴': 6548, '샵': 6549, '샷': 6550, '샹': 6551, '섀': 6552, '서': 6553, '서는': 6554, '서비스': 6555, '서울': 6556, '석': 6557, '섞': 6558, '선': 6559, '선거': 6560, '선물': 6561, '선수': 6562, '선수권대회': 6563, '선을': 6564, '섣': 6565, '설': 6566, '설국열차': 6567, '설명회': 6568, '섬': 6569, '섭': 6570, '섯': 6571, '섰': 6572, '성': 6573, '성과': 6574, '성은': 6575, '성을': 6576, '성이': 6577, '성장': 6578, '세': 6579, '세가': 6580, '세계': 6581, '세계일보': 6582, '세대': 6583, '세력': 6584, '세를': 6585, '세요': 6586, '세이브': 6587, '세트': 6588, '세포': 6589, '섹': 6590, '센': 6591, '센스': 6592, '센터': 6593, '센터에서': 6594, '센트': 6595, '셀': 6596, '셈': 6597, '셉': 6598, '셋': 6599, '셍': 6600, '셔': 6601, '션': 6602, '셜': 6603, '셨': 6604, '셰': 6605, '셸': 6606, '소': 6607, '소득': 6608, '소리': 6609, '소방서': 6610, '소비자': 6611, '소송': 6612, '소연': 6613, '소프트': 6614, '속': 6615, '손': 6616, '손해보험': 6617, '솔': 6618, '솜': 6619, '솟': 6620, '송': 6621, '솥': 6622, '쇄': 6623, '쇠': 6624, '쇼': 6625, '숀': 6626, '숍': 6627, '숏': 6628, '수': 6629, '수가': 6630, '수는': 6631, '수록': 6632, '수를': 6633, '수사': 6634, '수석': 6635, '수수료': 6636, '수술': 6637, '수원': 6638, '수익': 6639, '수익률': 6640, '수지': 6641, '숙': 6642, '순': 6643, '순위': 6644, '술': 6645, '숨': 6646, '숨어있': 6647, '숫': 6648, '숭': 6649, '숱': 6650, '숲': 6651, '쉐': 6652, '쉬': 6653, '쉰': 6654, '쉴': 6655, '쉼': 6656, '쉽': 6657, '슈': 6658, '슈퍼': 6659, '슈퍼스타': 6660, '슐': 6661, '슘': 6662, '슛': 6663, '스': 6664, '스가': 6665, '스는': 6666, '스러운': 6667, '스러워': 6668, '스럽': 6669, '스럽게': 6670, '스럽다': 6671, '스를': 6672, '스마트': 6673, '스와': 6674, '스완지시티': 6675, '스의': 6676, '스카': 6677, '스캔들': 6678, '스케': 6679, '스코': 6680, '스쿨': 6681, '스크': 6682, '스키': 6683, '스타': 6684, '스탠': 6685, '스터': 6686, '스턴': 6687, '스테': 6688, '스토리': 6689, '스토어': 6690, '스트': 6691, '스티': 6692, '스페셜': 6693, '스포츠': 6694, '스포츠조선': 6695, '슨': 6696, '슬': 6697, '슴': 6698, '습': 6699, '습니까': 6700, '습니다': 6701, '슷': 6702, '승': 6703, '승을': 6704, '시': 6705, '시간': 6706, '시께': 6707, '시대': 6708, '시리즈': 6709, '시민': 6710, '시부터': 6711, '시설': 6712, '시스템': 6713, '시아': 6714, '시의회': 6715, '시장': 6716, '시장에서': 6717, '시즌': 6718, '시청': 6719, '시켜': 6720, '시켰다': 6721, '시키': 6722, '시키고': 6723, '시키기': 6724, '시키는': 6725, '시킨': 6726, '시킬': 6727, '시티': 6728, '시험': 6729, '식': 6730, '식을': 6731, '식품': 6732, '신': 6733, '신도시': 6734, '신문': 6735, '신청': 6736, '싣': 6737, '실': 6738, '실리콘': 6739, '실에서': 6740, '실장': 6741, '실적': 6742, '실점': 6743, '싫': 6744, '심': 6745, '심리': 6746, '심사': 6747, '심을': 6748, '십': 6749, '싱': 6750, '싶': 6751, '싸': 6752, '싸움': 6753, '싹': 6754, '싼': 6755, '쌀': 6756, '쌈': 6757, '쌌': 6758, '쌍': 6759, '쌓': 6760, '써': 6761, '썩': 6762, '썬': 6763, '썰': 6764, '썸': 6765, '썹': 6766, '썼': 6767, '썽': 6768, '쎄': 6769, '쏘': 6770, '쏙': 6771, '쏜': 6772, '쏟': 6773, '쏠': 6774, '쏴': 6775, '쐐': 6776, '쑤': 6777, '쑥': 6778, '쓰': 6779, '쓴': 6780, '쓸': 6781, '씀': 6782, '씁': 6783, '씌': 6784, '씨': 6785, '씨가': 6786, '씨는': 6787, '씨를': 6788, '씨에게': 6789, '씨와': 6790, '씨의': 6791, '씩': 6792, '씬': 6793, '씹': 6794, '씻': 6795, '씽': 6796, '아': 6797, '아버지': 6798, '아빠': 6799, '아시아': 6800, '아시아경제': 6801, '아시아투데이': 6802, '아웃': 6803, '아이': 6804, '아일랜드': 6805, '아직': 6806, '아카데미': 6807, '아트': 6808, '아파트': 6809, '아프리카': 6810, '악': 6811, '안': 6812, '안드로이드': 6813, '안보': 6814, '안을': 6815, '안전': 6816, '안정': 6817, '안타': 6818, '앉': 6819, '않': 6820, '알': 6821, '알리미': 6822, '앓': 6823, '암': 6824, '압': 6825, '앗': 6826, '았': 6827, '았다': 6828, '았던': 6829, '았지만': 6830, '앙': 6831, '앞': 6832, '앞으로': 6833, '애': 6834, '액': 6835, '액은': 6836, '앤': 6837, '앨': 6838, '앰': 6839, '앱': 6840, '앳': 6841, '앵': 6842, '앵커멘트': 6843, '야': 6844, '야구': 6845, '약': 6846, '약품': 6847, '얀': 6848, '얄': 6849, '얇': 6850, '얌': 6851, '얏': 6852, '양': 6853, '얘': 6854, '어': 6855, '어야': 6856, '어요': 6857, '억': 6858, '억달러': 6859, '억여원': 6860, '억원': 6861, '억원으로': 6862, '억원을': 6863, '억원의': 6864, '언': 6865, '언더파': 6866, '언론': 6867, '얹': 6868, '얻': 6869, '얼': 6870, '얽': 6871, '엄': 6872, '업': 6873, '업계': 6874, '업무': 6875, '업소': 6876, '업자': 6877, '업종': 6878, '업체': 6879, '업체들': 6880, '없': 6881, '없는': 6882, '없이': 6883, '엇': 6884, '었': 6885, '었고': 6886, '었는데': 6887, '었다': 6888, '었던': 6889, '었습니다': 6890, '었으나': 6891, '었으며': 6892, '었지만': 6893, '엉': 6894, '엎': 6895, '에': 6896, '에게': 6897, '에게는': 6898, '에너지': 6899, '에는': 6900, '에도': 6901, '에만': 6902, '에서': 6903, '에서는': 6904, '에서도': 6905, '에선': 6906, '엑': 6907, '엑스': 6908, '엔': 6909, '엔지니어링': 6910, '엔터테인먼트': 6911, '엘': 6912, '엠': 6913, '엣': 6914, '엥': 6915, '여': 6916, '여개': 6917, '여건': 6918, '여름': 6919, '여명': 6920, '여명의': 6921, '여명이': 6922, '여성': 6923, '여자': 6924, '여행': 6925, '역': 6926, '엮': 6927, '연': 6928, '연구': 6929, '연구소': 6930, '연구원': 6931, '연금': 6932, '연맹': 6933, '연수원': 6934, '연승': 6935, '연예': 6936, '연패': 6937, '연합': 6938, '연합회': 6939, '열': 6940, '염': 6941, '엽': 6942, '엿': 6943, '였': 6944, '였고': 6945, '였다': 6946, '였던': 6947, '였습니다': 6948, '였으며': 6949, '였지만': 6950, '영': 6951, '영상': 6952, '영업': 6953, '영화': 6954, '영화제': 6955, '옆': 6956, '예': 6957, '예방': 6958, '예산': 6959, '예술': 6960, '옌': 6961, '옐': 6962, '옛': 6963, '오': 6964, '오는': 6965, '오늘': 6966, '오른쪽': 6967, '오토': 6968, '오픈': 6969, '옥': 6970, '온': 6971, '온라인': 6972, '올': 6973, '올림픽': 6974, '올해': 6975, '옮': 6976, '옳': 6977, '옴': 6978, '옵': 6979, '옵션': 6980, '옷': 6981, '옹': 6982, '와': 6983, '와의': 6984, '완': 6985, '왈': 6986, '왑': 6987, '왓': 6988, '왔': 6989, '왔다': 6990, '왔던': 6991, '왕': 6992, '왜': 6993, '왠': 6994, '외': 6995, '외국인': 6996, '왼': 6997, '왼쪽': 6998, '요': 6999, '요금': 7000, '욕': 7001, '욘': 7002, '용': 7003, '용품': 7004, '우': 7005, '우는': 7006, '우리': 7007, '우스': 7008, '욱': 7009, '운': 7010, '운동': 7011, '운영': 7012, '울': 7013, '움': 7014, '웃': 7015, '웃음': 7016, '웅': 7017, '워': 7018, '웍': 7019, '원': 7020, '원에': 7021, '원에서': 7022, '원으로': 7023, '원은': 7024, '원을': 7025, '원의': 7026, '원이': 7027, '월': 7028, '월까지': 7029, '월드': 7030, '월드컵': 7031, '월말': 7032, '월부터': 7033, '월에': 7034, '월에는': 7035, '웠': 7036, '웠다': 7037, '웨': 7038, '웨어': 7039, '웨이': 7040, '웬': 7041, '웰': 7042, '웹': 7043, '위': 7044, '위권': 7045, '위기': 7046, '위는': 7047, '위로': 7048, '위를': 7049, '위안': 7050, '위에': 7051, '위원': 7052, '위원장': 7053, '위원회': 7054, '위원회는': 7055, '위험': 7056, '윅': 7057, '윈': 7058, '윌': 7059, '윔': 7060, '윗': 7061, '윙': 7062, '유': 7063, '유럽': 7064, '유통': 7065, '유플러스': 7066, '육': 7067, '윤': 7068, '율': 7069, '율은': 7070, '율을': 7071, '율이': 7072, '융': 7073, '으': 7074, '으나': 7075, '으니': 7076, '으려': 7077, '으로': 7078, '으로부터': 7079, '으로서': 7080, '으로써': 7081, '으며': 7082, '으면': 7083, '으면서': 7084, '윽': 7085, '은': 7086, '은행': 7087, '을': 7088, '음': 7089, '음악': 7090, '음에도': 7091, '음을': 7092, '읍': 7093, '응': 7094, '의': 7095, '이': 7096, '이기도': 7097, '이나': 7098, '이닝': 7099, '이다': 7100, '이라': 7101, '이라고': 7102, '이라는': 7103, '이라도': 7104, '이라며': 7105, '이라면서': 7106, '이란': 7107, '이며': 7108, '이번': 7109, '이상': 7110, '이어서': 7111, '이었다': 7112, '이었던': 7113, '이익': 7114, '이자': 7115, '이지만': 7116, '이하': 7117, '익': 7118, '인': 7119, '인데': 7120, '인사': 7121, '인수위': 7122, '인지': 7123, '인터내셔널': 7124, '인터넷': 7125, '일': 7126, '일간': 7127, '일까지': 7128, '일반': 7129, '일보': 7130, '일본': 7131, '일부터': 7132, '일에는': 7133, '읽': 7134, '잃': 7135, '임': 7136, '임을': 7137, '입': 7138, '입니다': 7139, '잇': 7140, '있': 7141, '있는': 7142, '있다': 7143, '잉': 7144, '잊': 7145, '잎': 7146, '자': 7147, '자가': 7148, '자금': 7149, '자는': 7150, '자동차': 7151, '자들': 7152, '자들은': 7153, '자들의': 7154, '자들이': 7155, '자로': 7156, '자료': 7157, '자를': 7158, '자리': 7159, '자마자': 7160, '자본': 7161, '자산': 7162, '자산운용': 7163, '자에게': 7164, '자와': 7165, '자원': 7166, '자의': 7167, '자인': 7168, '자치': 7169, '작': 7170, '작업': 7171, '잔': 7172, '잖': 7173, '잘': 7174, '잠': 7175, '잡': 7176, '잣': 7177, '장': 7178, '장과': 7179, '장관': 7180, '장비': 7181, '장애': 7182, '장애인': 7183, '장에서': 7184, '장으로': 7185, '장은': 7186, '장을': 7187, '장이': 7188, '장치': 7189, '잦': 7190, '재': 7191, '재단': 7192, '재료': 7193, '재산': 7194, '재정': 7195, '재판': 7196, '잭': 7197, '쟁': 7198, '저': 7199, '저축': 7200, '저축은행': 7201, '적': 7202, '적으로': 7203, '적이고': 7204, '적이다': 7205, '적인': 7206, '전': 7207, '전략': 7208, '전문': 7209, '전에': 7210, '전에서': 7211, '전우치': 7212, '전을': 7213, '전자': 7214, '전쟁': 7215, '전환': 7216, '절': 7217, '절차': 7218, '젊': 7219, '점': 7220, '점검': 7221, '점으로': 7222, '점을': 7223, '점이': 7224, '접': 7225, '젓': 7226, '정': 7227, '정글의': 7228, '정보': 7229, '정부': 7230, '정책': 7231, '정치': 7232, '젖': 7233, '제': 7234, '제도': 7235, '제를': 7236, '제약': 7237, '제주': 7238, '제품': 7239, '젝': 7240, '젠': 7241, '젤': 7242, '젬': 7243, '젯': 7244, '져': 7245, '져야': 7246, '젼': 7247, '졌': 7248, '졌고': 7249, '졌다': 7250, '졌습니다': 7251, '졌지만': 7252, '조': 7253, '조건': 7254, '조사': 7255, '조선': 7256, '조선해양': 7257, '조원': 7258, '조정': 7259, '조직': 7260, '조차': 7261, '조치': 7262, '족': 7263, '존': 7264, '졸': 7265, '좀': 7266, '좁': 7267, '종': 7268, '종목': 7269, '종합': 7270, '좋': 7271, '좋은': 7272, '좌': 7273, '죄': 7274, '죠': 7275, '주': 7276, '주가': 7277, '주기': 7278, '주년': 7279, '주는': 7280, '주민': 7281, '주세요': 7282, '주식': 7283, '주의': 7284, '주의보': 7285, '주택': 7286, '죽': 7287, '준': 7288, '준비': 7289, '준호': 7290, '준희': 7291, '줄': 7292, '줌': 7293, '줍': 7294, '중': 7295, '중견기업': 7296, '중공업': 7297, '중국': 7298, '중소기업': 7299, '중심': 7300, '중앙': 7301, '중인': 7302, '줘': 7303, '줘야': 7304, '줬': 7305, '줬다': 7306, '쥐': 7307, '쥔': 7308, '쥬': 7309, '즈': 7310, '즉': 7311, '즌': 7312, '즐': 7313, '즘': 7314, '즙': 7315, '증': 7316, '증권': 7317, '지': 7318, '지가': 7319, '지검': 7320, '지고': 7321, '지구': 7322, '지금': 7323, '지나치게': 7324, '지난': 7325, '지난해': 7326, '지는': 7327, '지도': 7328, '지를': 7329, '지만': 7330, '지면서': 7331, '지방': 7332, '지방경찰청': 7333, '지법': 7334, '지수': 7335, '지수는': 7336, '지역': 7337, '지원': 7338, '지원센터': 7339, '지지': 7340, '지훈': 7341, '직': 7342, '직원': 7343, '진': 7344, '진다': 7345, '진영': 7346, '진짜': 7347, '진흥': 7348, '진흥원': 7349, '질': 7350, '질서': 7351, '질환': 7352, '짐': 7353, '집': 7354, '집행': 7355, '짓': 7356, '징': 7357, '짖': 7358, '짙': 7359, '짚': 7360, '짜': 7361, '짜리': 7362, '짝': 7363, '짠': 7364, '짤': 7365, '짧': 7366, '짬': 7367, '짱': 7368, '째': 7369, '쨌': 7370, '쩌': 7371, '쩍': 7372, '쩔': 7373, '쩡': 7374, '쪼': 7375, '쪽': 7376, '쫄': 7377, '쫓': 7378, '쭈': 7379, '쭉': 7380, '쯔': 7381, '쯤': 7382, '찌': 7383, '찍': 7384, '찐': 7385, '찔': 7386, '찜': 7387, '찢': 7388, '차': 7389, '차량': 7390, '차례': 7391, '차익': 7392, '차전': 7393, '착': 7394, '찬': 7395, '찮': 7396, '찰': 7397, '참': 7398, '참여': 7399, '찹': 7400, '찼': 7401, '창': 7402, '창업': 7403, '찾': 7404, '채': 7405, '채권': 7406, '채널': 7407, '책': 7408, '책을': 7409, '책임': 7410, '챈': 7411, '챌': 7412, '챔': 7413, '챔피언': 7414, '챙': 7415, '처': 7416, '처럼': 7417, '처리': 7418, '처분': 7419, '처음': 7420, '척': 7421, '천': 7422, '천만': 7423, '천만원': 7424, '천명': 7425, '천억원': 7426, '철': 7427, '첨': 7428, '첩': 7429, '첫': 7430, '청': 7431, '청담동': 7432, '청사': 7433, '청소년': 7434, '청은': 7435, '체': 7436, '체계': 7437, '체육': 7438, '체제': 7439, '체험': 7440, '첸': 7441, '첼': 7442, '쳐': 7443, '쳤': 7444, '쳤다': 7445, '초': 7446, '초등학교': 7447, '촉': 7448, '촌': 7449, '촘': 7450, '촛': 7451, '총': 7452, '총리': 7453, '총회': 7454, '촨': 7455, '촬': 7456, '촬영': 7457, '최': 7458, '최고': 7459, '최근': 7460, '추': 7461, '추진': 7462, '축': 7463, '축구': 7464, '축구연맹': 7465, '축제': 7466, '춘': 7467, '출': 7468, '춤': 7469, '춥': 7470, '춧': 7471, '충': 7472, '춰': 7473, '췄': 7474, '췌': 7475, '취': 7476, '츄': 7477, '츠': 7478, '측': 7479, '측은': 7480, '츰': 7481, '층': 7482, '치': 7483, '치고': 7484, '치는': 7485, '치료': 7486, '치를': 7487, '칙': 7488, '친': 7489, '칠': 7490, '침': 7491, '칩': 7492, '칫': 7493, '칭': 7494, '카': 7495, '카드': 7496, '카페': 7497, '칵': 7498, '칸': 7499, '칼': 7500, '캄': 7501, '캉': 7502, '캐': 7503, '캐피탈': 7504, '캔': 7505, '캘': 7506, '캠': 7507, '캠퍼스': 7508, '캠프': 7509, '캡': 7510, '캣': 7511, '커': 7512, '컥': 7513, '컨': 7514, '컨설팅': 7515, '컫': 7516, '컬': 7517, '컴': 7518, '컴퍼니': 7519, '컵': 7520, '컷': 7521, '컸': 7522, '케': 7523, '케미칼': 7524, '케이': 7525, '켄': 7526, '켈': 7527, '켐': 7528, '켓': 7529, '켜': 7530, '켠': 7531, '켰': 7532, '코': 7533, '코드': 7534, '코리아': 7535, '코스닥': 7536, '코스피': 7537, '콕': 7538, '콘': 7539, '콘서트': 7540, '콘텐츠': 7541, '콜': 7542, '콤': 7543, '콥': 7544, '콧': 7545, '콩': 7546, '콰': 7547, '쾌': 7548, '쿄': 7549, '쿠': 7550, '쿠키': 7551, '쿡': 7552, '쿤': 7553, '쿨': 7554, '쿵': 7555, '쿼': 7556, '쿼터': 7557, '퀄': 7558, '퀘': 7559, '퀴': 7560, '퀵': 7561, '퀸': 7562, '큐': 7563, '큘': 7564, '크': 7565, '크레': 7566, '큰': 7567, '클': 7568, '클라우드': 7569, '클럽': 7570, '클리': 7571, '큼': 7572, '키': 7573, '키로': 7574, '킥': 7575, '킨': 7576, '킬': 7577, '킴': 7578, '킷': 7579, '킹': 7580, '타': 7581, '타를': 7582, '타수': 7583, '타운': 7584, '타워': 7585, '타임': 7586, '타자': 7587, '타점': 7588, '탁': 7589, '탄': 7590, '탈': 7591, '탈삼진': 7592, '탐': 7593, '탑': 7594, '탓': 7595, '탔': 7596, '탕': 7597, '태': 7598, '태영': 7599, '태평양': 7600, '택': 7601, '탠': 7602, '탤': 7603, '탬': 7604, '탭': 7605, '탰': 7606, '탱': 7607, '탱크': 7608, '터': 7609, '터미널': 7610, '턱': 7611, '턴': 7612, '털': 7613, '텀': 7614, '텁': 7615, '텃': 7616, '텅': 7617, '테': 7618, '테크': 7619, '텍': 7620, '텐': 7621, '텔': 7622, '텔레콤': 7623, '템': 7624, '텝': 7625, '텨': 7626, '톈': 7627, '토': 7628, '토록': 7629, '토크': 7630, '톡': 7631, '톤': 7632, '톨': 7633, '톰': 7634, '톱': 7635, '통': 7636, '통신': 7637, '통합': 7638, '통화': 7639, '퇴': 7640, '투': 7641, '투데이': 7642, '투수': 7643, '투어': 7644, '투자': 7645, '투자증권': 7646, '투표': 7647, '툭': 7648, '툰': 7649, '툴': 7650, '툼': 7651, '퉁': 7652, '퉈': 7653, '튀': 7654, '튕': 7655, '튜': 7656, '튠': 7657, '튬': 7658, '트': 7659, '트랙': 7660, '트로': 7661, '트리': 7662, '특': 7663, '특별': 7664, '특위': 7665, '특징주': 7666, '특히': 7667, '튼': 7668, '튿': 7669, '틀': 7670, '틈': 7671, '틋': 7672, '티': 7673, '티브': 7674, '틱': 7675, '틴': 7676, '틸': 7677, '팀': 7678, '팀장': 7679, '팁': 7680, '팅': 7681, '파': 7682, '파운드': 7683, '파이어': 7684, '파크': 7685, '팍': 7686, '팎': 7687, '판': 7688, '판매': 7689, '팔': 7690, '팜': 7691, '팝': 7692, '팟': 7693, '팠': 7694, '팡': 7695, '팥': 7696, '패': 7697, '패밀리': 7698, '패션': 7699, '팩': 7700, '팬': 7701, '팬들': 7702, '팰': 7703, '팸': 7704, '팽': 7705, '퍼': 7706, '펀': 7707, '펀드': 7708, '펄': 7709, '펌': 7710, '펑': 7711, '페': 7712, '페스티벌': 7713, '페이스': 7714, '펙': 7715, '펜': 7716, '펠': 7717, '펫': 7718, '펴': 7719, '편': 7720, '펼': 7721, '폄': 7722, '폈': 7723, '평': 7724, '평가': 7725, '평균': 7726, '폐': 7727, '포': 7728, '포럼': 7729, '포스트': 7730, '포인트': 7731, '포토': 7732, '포트': 7733, '폭': 7734, '폭력': 7735, '폰': 7736, '폴': 7737, '폴리': 7738, '폼': 7739, '퐁': 7740, '표': 7741, '표를': 7742, '푸': 7743, '푸드': 7744, '푹': 7745, '푼': 7746, '풀': 7747, '품': 7748, '풋': 7749, '풍': 7750, '퓨': 7751, '퓰': 7752, '프': 7753, '프라': 7754, '프로': 7755, '프로골프': 7756, '프로그램': 7757, '프로야구': 7758, '프리': 7759, '픈': 7760, '플': 7761, '플라이': 7762, '플랜트': 7763, '플러스': 7764, '플레이': 7765, '픔': 7766, '피': 7767, '피스': 7768, '피아': 7769, '피안타': 7770, '피해': 7771, '픽': 7772, '핀': 7773, '필': 7774, '필드': 7775, '필름': 7776, '핌': 7777, '핍': 7778, '핏': 7779, '핑': 7780, '핑크': 7781, '하': 7782, '하거나': 7783, '하게': 7784, '하겠다': 7785, '하겠다고': 7786, '하겠다는': 7787, '하고': 7788, '하기': 7789, '하기도': 7790, '하기로': 7791, '하나': 7792, '하느냐': 7793, '하는': 7794, '하는데': 7795, '하늘': 7796, '하니': 7797, '하다': 7798, '하다고': 7799, '하다는': 7800, '하더라도': 7801, '하던': 7802, '하도록': 7803, '하라': 7804, '하라고': 7805, '하려': 7806, '하려고': 7807, '하려는': 7808, '하려면': 7809, '하며': 7810, '하면': 7811, '하면서': 7812, '하면서도': 7813, '하세요': 7814, '하여': 7815, '하우스': 7816, '하이닉스': 7817, '하자': 7818, '하지': 7819, '하지만': 7820, '학': 7821, '학과': 7822, '학교': 7823, '학년': 7824, '학생': 7825, '학습': 7826, '학원': 7827, '한': 7828, '한국': 7829, '한국시간': 7830, '한다': 7831, '한다고': 7832, '한다는': 7833, '한다면': 7834, '한테': 7835, '할': 7836, '함': 7837, '함께': 7838, '함에': 7839, '함으로써': 7840, '함을': 7841, '합': 7842, '합니다': 7843, '핫': 7844, '핫이슈': 7845, '항': 7846, '항공': 7847, '해': 7848, '해달라': 7849, '해서': 7850, '해수욕장': 7851, '해야': 7852, '해온': 7853, '해왔다': 7854, '해졌다': 7855, '해주고': 7856, '해주는': 7857, '해진': 7858, '핵': 7859, '핸': 7860, '핸드': 7861, '햄': 7862, '햇': 7863, '했': 7864, '했고': 7865, '했기': 7866, '했는데': 7867, '했는지': 7868, '했다': 7869, '했다고': 7870, '했다는': 7871, '했던': 7872, '했습니다': 7873, '했어요': 7874, '했었다': 7875, '했으나': 7876, '했으며': 7877, '했으면': 7878, '했을': 7879, '했지만': 7880, '행': 7881, '행복': 7882, '행사': 7883, '행위': 7884, '행정': 7885, '향': 7886, '허': 7887, '허가': 7888, '헉': 7889, '헌': 7890, '헐': 7891, '험': 7892, '헛': 7893, '헝': 7894, '헤': 7895, '헨': 7896, '헬': 7897, '헷': 7898, '혀': 7899, '혁': 7900, '혁명': 7901, '혁신': 7902, '현': 7903, '현대': 7904, '현장': 7905, '현재': 7906, '현지시각': 7907, '현지시간': 7908, '혈': 7909, '혐': 7910, '협': 7911, '협동조합': 7912, '협력': 7913, '협상': 7914, '협의체': 7915, '협의회': 7916, '협정': 7917, '협회': 7918, '혔': 7919, '혔다': 7920, '형': 7921, '혜': 7922, '혜진': 7923, '혜택': 7924, '호': 7925, '호는': 7926, '호선': 7927, '호텔': 7928, '호텔에서': 7929, '혹': 7930, '혼': 7931, '홀': 7932, '홀딩스': 7933, '홀에서': 7934, '홈': 7935, '홈런': 7936, '홈쇼핑': 7937, '홉': 7938, '홍': 7939, '홍보': 7940, '화': 7941, '화를': 7942, '화학': 7943, '확': 7944, '확인': 7945, '환': 7946, '환경': 7947, '활': 7948, '활동': 7949, '활동을': 7950, '황': 7951, '황금어장': 7952, '회': 7953, '회담': 7954, '회를': 7955, '회말': 7956, '회사': 7957, '회의를': 7958, '회의에서': 7959, '회장': 7960, '회초': 7961, '획': 7962, '횟': 7963, '횡': 7964, '효': 7965, '효과': 7966, '효율': 7967, '후': 7968, '후보': 7969, '훈': 7970, '훈련': 7971, '훌': 7972, '훔': 7973, '훗': 7974, '훙': 7975, '훤': 7976, '훨': 7977, '훼': 7978, '휘': 7979, '휠': 7980, '휩': 7981, '휴': 7982, '흉': 7983, '흐': 7984, '흑': 7985, '흔': 7986, '흘': 7987, '흙': 7988, '흠': 7989, '흡': 7990, '흥': 7991, '흩': 7992, '희': 7993, '희망': 7994, '흰': 7995, '히': 7996, '힌': 7997, '힐': 7998, '힐링캠프': 7999, '힘': 8000, '힙': 8001, '[SOS]': 8002, '[EOS]': 8003}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SE-LC1TkCEb",
        "outputId": "cda44a64-22bd-4533-8cab-306d09397214"
      },
      "source": [
        "# Print the sentence split into tokens.\n",
        "toks = tokenizer.tokenize_wl(c_sentences[0])\n",
        "print('Tokenized: ', toks)\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(toks))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenized:  ['[SOS]', '▁그동안', '▁팔', '아', '치', '웠', '던', '▁삼성전자', '와', '▁SK', '하이닉스', '▁등', '▁반도체', '주', '를', '▁다시', '▁담', '기', '▁시작했다', '[EOS]']\n",
            "Token IDs:  [8002, 1193, 4814, 6797, 7483, 7036, 5842, 2653, 6983, 689, 7817, 1815, 2209, 7276, 6116, 1574, 1607, 5561, 2990, 8003]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eno9dC3RZpwL"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhZS9qUlQmc7"
      },
      "source": [
        "## dataset을 위한 Batch 구성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "CKdA9wbuCqD9",
        "outputId": "1b303d22-f57b-49dc-e30f-0c98b59065ff"
      },
      "source": [
        "class Batch:\n",
        "  src = []\n",
        "  trg = []\n",
        "max_length = 100\n",
        "batch_size = 128 #4 # 64, 128\n",
        "dataset_iterator = []\n",
        "batch_counter = 0\n",
        "\n",
        "sentences2 = c_sentences\n",
        "for i in range(0,len(sentences2),batch_size):\n",
        "#for i in range(0,1,batch_size):\n",
        "    nSRC = []\n",
        "    nTRG = []\n",
        "    src_max = 0\n",
        "    trg_max = 0\n",
        "    batch = Batch()\n",
        "    batch.src = []\n",
        "    batch.trg = []\n",
        "    \n",
        "    percent = (\"{0:.2f}\").format(100 * (i / float(len(sentences2))))\n",
        "    print(f'\\r {percent}% {i}/{str(len(sentences2))}', end=\"\", flush=True)\n",
        "    for j in range(i,i+batch_size):\n",
        "        try:\n",
        "            sentence = sentences2[j]+'.'\n",
        "            keys = extract_key(sentence)\n",
        "            srct = tokenizer.convert_tokens_to_ids(tokenizer.tokenize_wl((' '.join(keys)).strip()))\n",
        "            trgt = tokenizer.convert_tokens_to_ids(tokenizer.tokenize_wl(sentence))\n",
        "            if len(srct) < max_length and len(trgt) < max_length:\n",
        "                if len(trgt) > trg_max:\n",
        "                    trg_max = len(trgt)            \n",
        "                nTRG.append(trgt)\n",
        "                if len(srct) > src_max:\n",
        "                    src_max = len(srct)\n",
        "                nSRC.append(srct)\n",
        "\n",
        "        except Exception as e:\n",
        "            #print('Error:',e)\n",
        "            pass\n",
        "\n",
        "    for s in nSRC:\n",
        "        ss = s\n",
        "        if len(s) < src_max:\n",
        "            ss += [1 for i in range(src_max-len(s))]\n",
        "        batch.src.append(ss)\n",
        "\n",
        "    for t in nTRG:\n",
        "        tt = t\n",
        "        if len(t) < trg_max:        \n",
        "            tt += [1 for i in range(trg_max-len(t))]\n",
        "        batch.trg.append(tt)\n",
        "\n",
        "    #print(len(batch.src),len(batch.src[0]))\n",
        "    #print(len(batch.trg),len(batch.trg[0]))\n",
        "\n",
        "    batch.src = torch.tensor(batch.src).to(device)\n",
        "    batch.trg = torch.tensor(batch.trg).to(device)\n",
        "    dataset_iterator.append(batch)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 0.13% 1152/867766"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-f704bd4436ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0msrct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize_wl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mtrgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize_wl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-b3c1082d4d95>\u001b[0m in \u001b[0;36mextract_key\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#print(s)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtop_n\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_keywords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtop_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keybert/model.py\u001b[0m in \u001b[0;36mextract_keywords\u001b[0;34m(self, docs, keyphrase_ngram_range, stop_words, top_n, min_df, use_maxsum, use_mmr, diversity, nr_candidates, vectorizer)\u001b[0m\n\u001b[1;32m    115\u001b[0m                                                      \u001b[0mdiversity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                                                      \u001b[0mnr_candidates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                                                      vectorizer)\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             warnings.warn(\"Although extracting keywords for multiple documents is faster \"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keybert/model.py\u001b[0m in \u001b[0;36m_extract_keywords_single_doc\u001b[0;34m(self, doc, keyphrase_ngram_range, stop_words, top_n, use_maxsum, use_mmr, diversity, nr_candidates, vectorizer)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;31m# Extract Embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mdoc_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0mword_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m             \u001b[0;31m# doc_embedding = self.model.encode([doc])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;31m# word_embeddings = self.model.encode(words)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keybert/model.py\u001b[0m in \u001b[0;36m_extract_embeddings\u001b[0;34m(self, documents)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;31m# Infer embeddings with SentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;31m# Infer embeddings with Flair\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, is_pretokenized)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                 \u001b[0mout_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m                 \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput_value\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sentence_transformers/models/XLMRoBERTa.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;34m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m#RoBERTa does not use token_type_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0moutput_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlm_roberta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0moutput_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mcls_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# CLS token is first token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_extended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m         )\n\u001b[1;32m    764\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    437\u001b[0m                     \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m                 )\n\u001b[1;32m    441\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqGxOzeKxjJQ",
        "outputId": "bb738e68-9969-48e5-f4d2-f9d1c99b8943"
      },
      "source": [
        "len(dataset_iterator)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1707"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abp5XMPnxoK3",
        "outputId": "599767ec-ae38-4f8e-a8de-17f13ffc7caf"
      },
      "source": [
        "dataset_iterator[1].src"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[8002, 3016, 1076,  ...,    1,    1,    1],\n",
              "        [8002, 3490, 6896,  ...,    1,    1,    1],\n",
              "        [8002,  760,  517,  ...,    1,    1,    1],\n",
              "        ...,\n",
              "        [8002, 3341, 5550,  ...,    1,    1,    1],\n",
              "        [8002, 1997, 6615,  ...,    1,    1,    1],\n",
              "        [8002, 2136, 6983,  ...,    1,    1,    1]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOZJjTgBR8PH"
      },
      "source": [
        "# dataset_iterator의 저장!\n",
        "\n",
        "import pickle\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/summary/dataset_iterator.pkl', 'wb') as f:\n",
        "    pickle.dump(dataset_iterator, f)"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiX6UBJSSP2B"
      },
      "source": [
        "# dataset_iterator를 읽어들임.\n",
        "\n",
        "import pickle\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/summary/dataset_iterator.pkl', 'rb') as f:\n",
        "    dataset_iterator2 = pickle.load(f)"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sStA4MqONtVK"
      },
      "source": [
        "# encoder / decoder 구성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpqrfqHRNxKm"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim,\n",
        "                 dropout, \n",
        "                 device,\n",
        "                 max_length = 100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim,\n",
        "                                                  dropout, \n",
        "                                                  device) \n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        \n",
        "        batch_size = src.shape[0]\n",
        "        src_len = src.shape[1]\n",
        "        \n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        \n",
        "        #pos = [batch size, src len]\n",
        "        \n",
        "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "            \n",
        "        #src = [batch size, src len, hid dim]\n",
        "            \n",
        "        return src"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thHItmM6N6Uu"
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim,  \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
        "                                                                     pf_dim, \n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        #src_mask = [batch size, 1, 1, src len] \n",
        "                \n",
        "        #self attention\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        #positionwise feedforward\n",
        "        _src = self.positionwise_feedforward(src)\n",
        "        \n",
        "        #dropout, residual and layer norm\n",
        "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        return src"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsZN2uo2OCC1"
      },
      "source": [
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        assert hid_dim % n_heads == 0\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = hid_dim // n_heads\n",
        "        \n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "        \n",
        "    def forward(self, query, key, value, mask = None):\n",
        "        \n",
        "        batch_size = query.shape[0]\n",
        "        \n",
        "        #query = [batch size, query len, hid dim]\n",
        "        #key = [batch size, key len, hid dim]\n",
        "        #value = [batch size, value len, hid dim]\n",
        "                \n",
        "        Q = self.fc_q(query)\n",
        "        K = self.fc_k(key)\n",
        "        V = self.fc_v(value)\n",
        "        \n",
        "        #Q = [batch size, query len, hid dim]\n",
        "        #K = [batch size, key len, hid dim]\n",
        "        #V = [batch size, value len, hid dim]\n",
        "                \n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        \n",
        "        #Q = [batch size, n heads, query len, head dim]\n",
        "        #K = [batch size, n heads, key len, head dim]\n",
        "        #V = [batch size, n heads, value len, head dim]\n",
        "                \n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "        \n",
        "        #energy = [batch size, n heads, query len, key len]\n",
        "        \n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\n",
        "        \n",
        "        attention = torch.softmax(energy, dim = -1)\n",
        "                \n",
        "        #attention = [batch size, n heads, query len, key len]\n",
        "                \n",
        "        x = torch.matmul(self.dropout(attention), V)\n",
        "        \n",
        "        #x = [batch size, n heads, query len, head dim]\n",
        "        \n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        \n",
        "        #x = [batch size, query len, n heads, head dim]\n",
        "        \n",
        "        x = x.view(batch_size, -1, self.hid_dim)\n",
        "        \n",
        "        #x = [batch size, query len, hid dim]\n",
        "        \n",
        "        x = self.fc_o(x)\n",
        "        \n",
        "        #x = [batch size, query len, hid dim]\n",
        "        \n",
        "        return x, attention"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZTi8NJZOJUK"
      },
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
        "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "        \n",
        "        #x = [batch size, seq len, pf dim]\n",
        "        \n",
        "        x = self.fc_2(x)\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SV8CVUlOPMM"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 output_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 dropout, \n",
        "                 device,\n",
        "                 max_length = 100):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim, \n",
        "                                                  dropout, \n",
        "                                                  device)\n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch size, trg len]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "                \n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "                            \n",
        "        #pos = [batch size, trg len]\n",
        "            \n",
        "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
        "                \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        output = self.fc_out(trg)\n",
        "        \n",
        "        #output = [batch size, trg len, output dim]\n",
        "            \n",
        "        return output, attention"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhO0Bjx0OWdk"
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
        "                                                                     pf_dim, \n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        \n",
        "        #self attention\n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
        "            \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "            \n",
        "        #encoder attention\n",
        "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
        "                    \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        \n",
        "        #positionwise feedforward\n",
        "        _trg = self.positionwise_feedforward(trg)\n",
        "        \n",
        "        #dropout, residual and layer norm\n",
        "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return trg, attention"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmUg6r57ObEf"
      },
      "source": [
        "##seq2seq 구성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIqJaJq1Odvy"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, \n",
        "                 encoder, \n",
        "                 decoder, \n",
        "                 src_pad_idx, \n",
        "                 trg_pad_idx, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "        \n",
        "    def make_src_mask(self, src):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        \n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "\n",
        "        return src_mask\n",
        "    \n",
        "    def make_trg_mask(self, trg):\n",
        "        \n",
        "        #trg = [batch size, trg len]\n",
        "        \n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        \n",
        "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
        "        \n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
        "        \n",
        "        #trg_sub_mask = [trg len, trg len]\n",
        "            \n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "        \n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        \n",
        "        return trg_mask\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #trg = [batch size, trg len]\n",
        "                \n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        \n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        \n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        \n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "                \n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        #output = [batch size, trg len, output dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return output, attention"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaaEpra3KEnS"
      },
      "source": [
        "# Building the Model --> 실행"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQntx79IdEGp"
      },
      "source": [
        "# Train 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Z8Z8q_fcL1B"
      },
      "source": [
        "INPUT_DIM = tokenizer.vocab_size\n",
        "OUTPUT_DIM = tokenizer.vocab_size\n",
        "#INPUT_DIM = len(tk.vocab)\n",
        "#OUTPUT_DIM = len(tk.vocab)\n",
        "HID_DIM = 256\n",
        "ENC_LAYERS = 3\n",
        "DEC_LAYERS = 3\n",
        "ENC_HEADS = 8\n",
        "DEC_HEADS = 8\n",
        "ENC_PF_DIM = 512\n",
        "DEC_PF_DIM = 512\n",
        "ENC_DROPOUT = 0.1\n",
        "DEC_DROPOUT = 0.1\n",
        "\n",
        "enc = Encoder(INPUT_DIM, \n",
        "              HID_DIM, \n",
        "              ENC_LAYERS, \n",
        "              ENC_HEADS, \n",
        "              ENC_PF_DIM, \n",
        "              ENC_DROPOUT, \n",
        "              device)\n",
        "\n",
        "dec = Decoder(OUTPUT_DIM, \n",
        "              HID_DIM, \n",
        "              DEC_LAYERS, \n",
        "              DEC_HEADS, \n",
        "              DEC_PF_DIM, \n",
        "              DEC_DROPOUT, \n",
        "              device)"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD8VBiXFc2uo"
      },
      "source": [
        "SRC_PAD_IDX = tokenizer.pad_token_id\n",
        "TRG_PAD_IDX = tokenizer.pad_token_id\n",
        "#SRC_PAD_IDX = tk.stoi['<pad>']\n",
        "#TRG_PAD_IDX = tk.stoi['<pad>']\n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyFRjDzpdJ76",
        "outputId": "16656cc4-5b52-4a11-fd08-b1f39f2a36c5"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 10,159,940 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QucNKWCBdUC0"
      },
      "source": [
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)\n",
        "\n",
        "model.apply(initialize_weights);"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNBgedENdeRY"
      },
      "source": [
        "LEARNING_RATE = 0.0005\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G6OgbLfdoJN"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        ##print(src.shape)\n",
        "        ##print(trg.shape)\n",
        "        output, _ = model(src, trg[:,:-1])\n",
        "        ##print(output.shape)\n",
        "        #output = [batch size, trg len - 1, output dim]\n",
        "        #trg = [batch size, trg len]\n",
        "            \n",
        "        output_dim = output.shape[-1]\n",
        "            \n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "                \n",
        "        #output = [batch size * trg len - 1, output dim]\n",
        "        #trg = [batch size * trg len - 1]\n",
        "            \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        ##print('loss',loss)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        ##print('epoch_loss',epoch_loss)\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdfEmlNHdvkX"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output, _ = model(src, trg[:,:-1])\n",
        "            \n",
        "            #output = [batch size, trg len - 1, output dim]\n",
        "            #trg = [batch size, trg len]\n",
        "            \n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "            \n",
        "            #output = [batch size * trg len - 1, output dim]\n",
        "            #trg = [batch size * trg len - 1]\n",
        "            \n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsdX1KJHd9LI"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEZ2o1g6dIh-"
      },
      "source": [
        "# Train !!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8ZsADe2e3av",
        "outputId": "a5ed409c-c81a-46e3-eb5a-d383a99dd944"
      },
      "source": [
        "bcnt = len(dataset_iterator)\n",
        "print(bcnt)\n",
        "train_iterator = dataset_iterator[0:bcnt-int(bcnt/10)]\n",
        "valid_iterator = dataset_iterator[bcnt-int(bcnt/10):bcnt]\n",
        "#train_iterator = dataset_iterator[0:bcnt-1]\n",
        "#valid_iterator = dataset_iterator[bcnt-1:bcnt]\n",
        "print('train',len(train_iterator))\n",
        "print('valid',len(valid_iterator))"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1707\n",
            "train 1537\n",
            "valid 170\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "po1OXs3sgmvI",
        "outputId": "65c92d7b-8d5b-4018-8acf-e3e2f37d304d"
      },
      "source": [
        "train_iterator[0].src"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[8002, 4814, 6797,  ..., 8003,    1,    1],\n",
              "        [8002, 2915, 7872,  ...,    1,    1,    1],\n",
              "        [8002, 2546, 7996,  ...,    1,    1,    1],\n",
              "        ...,\n",
              "        [8002, 1986, 5377,  ...,    1,    1,    1],\n",
              "        [8002, 1562, 6113,  ...,    1,    1,    1],\n",
              "        [8002, 2756, 5468,  ...,    1,    1,    1]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6lxOV5jd_id",
        "outputId": "dc8a2bc3-fdd1-4121-8537-151cf1253f59"
      },
      "source": [
        "\n",
        "N_EPOCHS = 100\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/summary/tut6-model.pt')\n",
        "        print('Save model!')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Save model!\n",
            "Epoch: 01 | Time: 1m 56s\n",
            "\tTrain Loss: 3.579 | Train PPL:  35.820\n",
            "\t Val. Loss: 3.537 |  Val. PPL:  34.355\n",
            "Save model!\n",
            "Epoch: 02 | Time: 1m 57s\n",
            "\tTrain Loss: 2.875 | Train PPL:  17.725\n",
            "\t Val. Loss: 3.214 |  Val. PPL:  24.877\n",
            "Save model!\n",
            "Epoch: 03 | Time: 1m 57s\n",
            "\tTrain Loss: 2.493 | Train PPL:  12.099\n",
            "\t Val. Loss: 3.017 |  Val. PPL:  20.434\n",
            "Save model!\n",
            "Epoch: 04 | Time: 1m 57s\n",
            "\tTrain Loss: 2.259 | Train PPL:   9.577\n",
            "\t Val. Loss: 2.918 |  Val. PPL:  18.497\n",
            "Save model!\n",
            "Epoch: 05 | Time: 1m 57s\n",
            "\tTrain Loss: 2.103 | Train PPL:   8.189\n",
            "\t Val. Loss: 2.861 |  Val. PPL:  17.472\n",
            "Save model!\n",
            "Epoch: 06 | Time: 1m 57s\n",
            "\tTrain Loss: 1.988 | Train PPL:   7.299\n",
            "\t Val. Loss: 2.820 |  Val. PPL:  16.778\n",
            "Save model!\n",
            "Epoch: 07 | Time: 1m 57s\n",
            "\tTrain Loss: 1.896 | Train PPL:   6.661\n",
            "\t Val. Loss: 2.793 |  Val. PPL:  16.324\n",
            "Save model!\n",
            "Epoch: 08 | Time: 1m 57s\n",
            "\tTrain Loss: 1.822 | Train PPL:   6.187\n",
            "\t Val. Loss: 2.773 |  Val. PPL:  16.010\n",
            "Save model!\n",
            "Epoch: 09 | Time: 1m 57s\n",
            "\tTrain Loss: 1.762 | Train PPL:   5.825\n",
            "\t Val. Loss: 2.762 |  Val. PPL:  15.827\n",
            "Epoch: 10 | Time: 1m 57s\n",
            "\tTrain Loss: 1.709 | Train PPL:   5.524\n",
            "\t Val. Loss: 2.763 |  Val. PPL:  15.847\n",
            "Save model!\n",
            "Epoch: 11 | Time: 1m 57s\n",
            "\tTrain Loss: 1.663 | Train PPL:   5.276\n",
            "\t Val. Loss: 2.747 |  Val. PPL:  15.596\n",
            "Epoch: 12 | Time: 1m 57s\n",
            "\tTrain Loss: 1.623 | Train PPL:   5.067\n",
            "\t Val. Loss: 2.750 |  Val. PPL:  15.650\n",
            "Epoch: 13 | Time: 1m 56s\n",
            "\tTrain Loss: 1.587 | Train PPL:   4.891\n",
            "\t Val. Loss: 2.755 |  Val. PPL:  15.713\n",
            "Epoch: 14 | Time: 1m 56s\n",
            "\tTrain Loss: 1.554 | Train PPL:   4.732\n",
            "\t Val. Loss: 2.759 |  Val. PPL:  15.783\n",
            "Epoch: 15 | Time: 1m 56s\n",
            "\tTrain Loss: 1.526 | Train PPL:   4.601\n",
            "\t Val. Loss: 2.755 |  Val. PPL:  15.722\n",
            "Epoch: 16 | Time: 1m 56s\n",
            "\tTrain Loss: 1.499 | Train PPL:   4.477\n",
            "\t Val. Loss: 2.763 |  Val. PPL:  15.841\n",
            "Epoch: 17 | Time: 1m 56s\n",
            "\tTrain Loss: 1.476 | Train PPL:   4.376\n",
            "\t Val. Loss: 2.752 |  Val. PPL:  15.676\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdgVPC6zodcu"
      },
      "source": [
        "model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/summary/tut6-model.pt'))\n",
        "test_loss = evaluate(model, valid_iterator, criterion)\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXsqseyMlPzB"
      },
      "source": [
        "def complete_sentence(sentence, model, device, max_len = 50):\n",
        "    \n",
        "    model.eval()\n",
        "    '''\n",
        "    if isinstance(sentence, str):\n",
        "        nlp = spacy.load('de_core_news_sm')\n",
        "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "    '''\n",
        "    tokens = tokenizer.tokenize_wl(sentence)\n",
        "    tokens = [tokenizer.bos_token] + tokens + [tokenizer.eos_token]\n",
        "    \n",
        "\n",
        "    src_indexes = [tokenizer.convert_tokens_to_ids(token) for token in tokens]\n",
        "    #src_indexes = get_stoi(tk,sentence)\n",
        "    print(src_indexes)\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "    print(src_tensor)\n",
        "    src_mask = model.make_src_mask(src_tensor)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\n",
        "\n",
        "    trg_indexes = [tokenizer.bos_token_id]\n",
        "    \n",
        "\n",
        "    for i in range(max_len):\n",
        "\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "        trg_mask = model.make_trg_mask(trg_tensor)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        pred_token = output.argmax(2)[:,-1].item()\n",
        "        \n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == tokenizer.eos_token_id: #tk.stoi['<eos>']: #.eos_token_id: # trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "            break\n",
        "    \n",
        "    trg_tokens = tokenizer.convert_ids_to_tokens(trg_indexes) # :[tk.itos[i] for i in trg_indexes]\n",
        "    \n",
        "    return trg_tokens[1:], attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guSK6Fasm8rC"
      },
      "source": [
        "trg, _ = complete_sentence('북한 미국 미사일',model,device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7bUE-sNnL97"
      },
      "source": [
        "print(''.join(trg))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1g-rZK5nkEE"
      },
      "source": [
        "extract_key(sentences[101])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alNmLf1bnnDc"
      },
      "source": [
        "trg, _ = complete_sentence('차갑게 공주의 일어서려던 그라시그',model,device)\n",
        "print(' '.join(trg))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}